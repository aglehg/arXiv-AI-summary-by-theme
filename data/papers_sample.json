{"id":"2503.17894","title":"Generative AI for Validating Physics Laws","abstract":"  We present generative artificial intelligence (AI) to empirically validate\nfundamental laws of physics, focusing on the Stefan-Boltzmann law linking\nstellar temperature and luminosity. Our approach simulates counterfactual\nluminosities under hypothetical temperature regimes for each individual star\nand iteratively refines the temperature-luminosity relationship in a deep\nlearning architecture. We use Gaia DR3 data and find that, on average,\ntemperature's effect on luminosity increases with stellar radius and decreases\nwith absolute magnitude, consistent with theoretical predictions. By framing\nphysics laws as causal problems, our method offers a novel, data-driven\napproach to refine theoretical understanding and inform evidence-based policy\nand practice.\n","date":"2025-03-23"}
{"id":"2503.17896","title":"Multi-Disease-Aware Training Strategy for Cardiac MR Image Segmentation","abstract":"  Accurate segmentation of the ventricles from cardiac magnetic resonance\nimages (CMRIs) is crucial for enhancing the diagnosis and analysis of heart\nconditions. Deep learning-based segmentation methods have recently garnered\nsignificant attention due to their impressive performance. However, these\nsegmentation methods are typically good at partitioning regularly shaped\norgans, such as the left ventricle (LV) and the myocardium (MYO), whereas they\nperform poorly on irregularly shaped organs, such as the right ventricle (RV).\nIn this study, we argue that this limitation of segmentation models stems from\ntheir insufficient generalization ability to address the distribution shift of\nsegmentation targets across slices, cardiac phases, and disease conditions. To\novercome this issue, we present a Multi-Disease-Aware Training Strategy (MTS)\nand restructure the introduced CMRI datasets into multi-disease datasets.\nAdditionally, we propose a specialized data processing technique for\npreprocessing input images to support the MTS. To validate the effectiveness of\nour method, we performed control group experiments and cross-validation tests.\nThe experimental results show that (1) network models trained using our\nproposed strategy achieved superior segmentation performance, particularly in\nRV segmentation, and (2) these networks exhibited robust performance even when\napplied to data from unknown diseases.\n","date":"2025-03-23"}
{"id":"2503.17897","title":"Real-time Global Illumination for Dynamic 3D Gaussian Scenes","abstract":"  We present a real-time global illumination approach along with a pipeline for\ndynamic 3D Gaussian models and meshes. Building on a formulated surface light\ntransport model for 3D Gaussians, we address key performance challenges with a\nfast compound stochastic ray-tracing algorithm and an optimized 3D Gaussian\nrasterizer. Our pipeline integrates multiple real-time techniques to accelerate\nperformance and achieve high-quality lighting effects. Our approach enables\nreal-time rendering of dynamic scenes with interactively editable materials and\ndynamic lighting of diverse multi-lights settings, capturing mutual\nmulti-bounce light transport (indirect illumination) between 3D Gaussians and\nmesh. Additionally, we present a real-time renderer with an interactive user\ninterface, validating our approach and demonstrating its practicality and high\nefficiency with over 40 fps in scenes including both 3D Gaussians and mesh.\nFurthermore, our work highlights the potential of 3D Gaussians in real-time\napplications with dynamic lighting, offering insights into performance and\noptimization.\n","date":"2025-03-23"}
{"id":"2503.17899","title":"What Time Tells Us? An Explorative Study of Time Awareness Learned from\n  Static Images","abstract":"  Time becomes visible through illumination changes in what we see. Inspired by\nthis, in this paper we explore the potential to learn time awareness from\nstatic images, trying to answer: what time tells us? To this end, we first\nintroduce a Time-Oriented Collection (TOC) dataset, which contains 130,906\nimages with reliable timestamps. Leveraging this dataset, we propose a\nTime-Image Contrastive Learning (TICL) approach to jointly model timestamps and\nrelated visual representations through cross-modal contrastive learning. We\nfound that the proposed TICL, 1) not only achieves state-of-the-art performance\non the timestamp estimation task, over various benchmark metrics, 2) but also,\ninterestingly, though only seeing static images, the time-aware embeddings\nlearned from TICL show strong capability in several time-aware downstream tasks\nsuch as time-based image retrieval, video scene classification, and time-aware\nimage editing. Our findings suggest that time-related visual cues can be\nlearned from static images and are beneficial for various vision tasks, laying\na foundation for future research on understanding time-related visual context.\nProject page:https:\/\/rathgrith.github.io\/timetells\/.\n","date":"2025-03-23"}
{"id":"2503.17900","title":"MedPlan:A Two-Stage RAG-Based System for Personalized Medical Plan\n  Generation","abstract":"  Despite recent success in applying large language models (LLMs) to electronic\nhealth records (EHR), most systems focus primarily on assessment rather than\ntreatment planning. We identify three critical limitations in current\napproaches: they generate treatment plans in a single pass rather than\nfollowing the sequential reasoning process used by clinicians; they rarely\nincorporate patient-specific historical context; and they fail to effectively\ndistinguish between subjective and objective clinical information. Motivated by\nthe SOAP methodology (Subjective, Objective, Assessment, Plan), we introduce\nMedPlan, a novel framework that structures LLM reasoning to align with\nreal-life clinician workflows. Our approach employs a two-stage architecture\nthat first generates a clinical assessment based on patient symptoms and\nobjective data, then formulates a structured treatment plan informed by this\nassessment and enriched with patient-specific information through\nretrieval-augmented generation. Comprehensive evaluation demonstrates that our\nmethod significantly outperforms baseline approaches in both assessment\naccuracy and treatment plan quality.\n","date":"2025-03-23"}
{"id":"2503.17903","title":"GLADMamba: Unsupervised Graph-Level Anomaly Detection Powered by\n  Selective State Space Model","abstract":"  Unsupervised graph-level anomaly detection (UGLAD) is a critical and\nchallenging task across various domains, such as social network analysis,\nanti-cancer drug discovery, and toxic molecule identification. However,\nexisting methods often struggle to capture the long-range dependencies\nefficiently and neglect the spectral information. Recently, selective State\nSpace Models (SSMs), particularly Mamba, have demonstrated remarkable\nadvantages in capturing long-range dependencies with linear complexity and a\nselection mechanism. Motivated by their success across various domains, we\npropose GLADMamba, a novel framework that adapts the selective state space\nmodel into UGLAD field. We design View-Fused Mamba (VFM) with a\nMamba-Transformer-style architecture to efficiently fuse information from\ndifferent views with a selective state mechanism. We also design\nSpectrum-Guided Mamba (SGM) with a Mamba-Transformer-style architecture to\nleverage the Rayleigh quotient to guide the embedding refining process.\nGLADMamba can dynamically focus on anomaly-related information while discarding\nirrelevant information for anomaly detection. To the best of our knowledge,\nthis is the first work to introduce Mamba and explicit spectral information to\nUGLAD. Extensive experiments on 12 real-world datasets demonstrate that\nGLADMamba outperforms existing state-of-the-art methods, achieving superior\nperformance in UGLAD. The code is available at\nhttps:\/\/github.com\/Yali-F\/GLADMamba.\n","date":"2025-03-23"}
{"id":"2503.17905","title":"Finding Stable Subnetworks at Initialization with Dataset Distillation","abstract":"  Recent works have shown that Dataset Distillation, the process for\nsummarizing the training data, can be leveraged to accelerate the training of\ndeep learning models. However, its impact on training dynamics, particularly in\nneural network pruning, remains largely unexplored. In our work, we use\ndistilled data in the inner loop of iterative magnitude pruning to produce\nsparse, trainable subnetworks at initialization -- more commonly known as\nlottery tickets. While using 150x less training points, our algorithm matches\nthe performance of traditional lottery ticket rewinding on ResNet-18 &\nCIFAR-10. Previous work highlights that lottery tickets can be found when the\ndense initialization is stable to SGD noise (i.e. training across different\nordering of the data converges to the same minima). We extend this discovery,\ndemonstrating that stable subnetworks can exist even within an unstable dense\ninitialization. In our linear mode connectivity studies, we find that pruning\nwith distilled data discards parameters that contribute to the sharpness of the\nloss landscape. Lastly, we show that by first generating a stable sparsity mask\nat initialization, we can find lottery tickets at significantly higher\nsparsities than traditional iterative magnitude pruning.\n","date":"2025-03-23"}
{"id":"2503.17907","title":"Guided Diffusion for the Extension of Machine Vision to Human Visual\n  Perception","abstract":"  Image compression technology eliminates redundant information to enable\nefficient transmission and storage of images, serving both machine vision and\nhuman visual perception. For years, image coding focused on human perception\nhas been well-studied, leading to the development of various image compression\nstandards. On the other hand, with the rapid advancements in image recognition\nmodels, image compression for AI tasks, known as Image Coding for Machines\n(ICM), has gained significant importance. Therefore, scalable image coding\ntechniques that address the needs of both machines and humans have become a key\narea of interest. Additionally, there is increasing demand for research\napplying the diffusion model, which can generate human-viewable images from a\nsmall amount of data to image compression methods for human vision. Image\ncompression methods that use diffusion models can partially reconstruct the\ntarget image by guiding the generation process with a small amount of\nconditioning information. Inspired by the diffusion model's potential, we\npropose a method for extending machine vision to human visual perception using\nguided diffusion. Utilizing the diffusion model guided by the output of the ICM\nmethod, we generate images for human perception from random noise. Guided\ndiffusion acts as a bridge between machine vision and human vision, enabling\ntransitions between them without any additional bitrate overhead. The generated\nimages then evaluated based on bitrate and image quality, and we compare their\ncompression performance with other scalable image coding methods for humans and\nmachines.\n","date":"2025-03-23"}
{"id":"2503.17908","title":"Does GCL Need a Large Number of Negative Samples? Enhancing Graph\n  Contrastive Learning with Effective and Efficient Negative Sampling","abstract":"  Graph Contrastive Learning (GCL) aims to self-supervised learn\nlow-dimensional graph representations, primarily through instance\ndiscrimination, which involves manually mining positive and negative pairs from\ngraphs, increasing the similarity of positive pairs while decreasing negative\npairs. Drawing from the success of Contrastive Learning (CL) in other domains,\na consensus has been reached that the effectiveness of GCLs depends on a large\nnumber of negative pairs. As a result, despite the significant computational\noverhead, GCLs typically leverage as many negative node pairs as possible to\nimprove model performance. However, given that nodes within a graph are\ninterconnected, we argue that nodes cannot be treated as independent instances.\nTherefore, we challenge this consensus: Does employing more negative nodes lead\nto a more effective GCL model? To answer this, we explore the role of negative\nnodes in the commonly used InfoNCE loss for GCL and observe that: (1)\nCounterintuitively, a large number of negative nodes can actually hinder the\nmodel's ability to distinguish nodes with different semantics. (2) A smaller\nnumber of high-quality and non-topologically coupled negative nodes are\nsufficient to enhance the discriminability of representations. Based on these\nfindings, we propose a new method called GCL with Effective and Efficient\nNegative samples, E2Neg, which learns discriminative representations using only\na very small set of representative negative samples. E2Neg significantly\nreduces computational overhead and speeds up model training. We demonstrate the\neffectiveness and efficiency of E2Neg across multiple datasets compared to\nother GCL methods.\n","date":"2025-03-23"}
{"id":"2503.17909","title":"Financial Wind Tunnel: A Retrieval-Augmented Market Simulator","abstract":"  Market simulator tries to create high-quality synthetic financial data that\nmimics real-world market dynamics, which is crucial for model development and\nrobust assessment. Despite continuous advancements in simulation methodologies,\nmarket fluctuations vary in terms of scale and sources, but existing frameworks\noften excel in only specific tasks. To address this challenge, we propose\nFinancial Wind Tunnel (FWT), a retrieval-augmented market simulator designed to\ngenerate controllable, reasonable, and adaptable market dynamics for model\ntesting. FWT offers a more comprehensive and systematic generative capability\nacross different data frequencies. By leveraging a retrieval method to discover\ncross-sectional information as the augmented condition, our diffusion-based\nsimulator seamlessly integrates both macro- and micro-level market patterns.\nFurthermore, our framework allows the simulation to be controlled with wide\napplicability, including causal generation through \"what-if\" prompts or\nunprecedented cross-market trend synthesis. Additionally, we develop an\nautomated optimizer for downstream quantitative models, using stress testing of\nsimulated scenarios via FWT to enhance returns while controlling risks.\nExperimental results demonstrate that our approach enables the generalizable\nand reliable market simulation, significantly improve the performance and\nadaptability of downstream models, particularly in highly complex and volatile\nmarket conditions. Our code and data sample is available at\nhttps:\/\/anonymous.4open.science\/r\/fwt_-E852\n","date":"2025-03-23"}
{"id":"2503.17914","title":"Semi-supervised Semantic Segmentation with Multi-Constraint Consistency\n  Learning","abstract":"  Consistency regularization has prevailed in semi-supervised semantic\nsegmentation and achieved promising performance. However, existing methods\ntypically concentrate on enhancing the Image-augmentation based Prediction\nconsistency and optimizing the segmentation network as a whole, resulting in\ninsufficient utilization of potential supervisory information. In this paper,\nwe propose a Multi-Constraint Consistency Learning (MCCL) approach to\nfacilitate the staged enhancement of the encoder and decoder. Specifically, we\nfirst design a feature knowledge alignment (FKA) strategy to promote the\nfeature consistency learning of the encoder from image-augmentation. Our FKA\nencourages the encoder to derive consistent features for strongly and weakly\naugmented views from the perspectives of point-to-point alignment and\nprototype-based intra-class compactness. Moreover, we propose a self-adaptive\nintervention (SAI) module to increase the discrepancy of aligned intermediate\nfeature representations, promoting Feature-perturbation based Prediction\nconsistency learning. Self-adaptive feature masking and noise injection are\ndesigned in an instance-specific manner to perturb the features for robust\nlearning of the decoder. Experimental results on Pascal VOC2012 and Cityscapes\ndatasets demonstrate that our proposed MCCL achieves new state-of-the-art\nperformance. The source code and models are made available at\nhttps:\/\/github.com\/NUST-Machine-Intelligence-Laboratory\/MCCL.\n","date":"2025-03-23"}
{"id":"2503.17915","title":"Cat-AIR: Content and Task-Aware All-in-One Image Restoration","abstract":"  All-in-one image restoration seeks to recover high-quality images from\nvarious types of degradation using a single model, without prior knowledge of\nthe corruption source. However, existing methods often struggle to effectively\nand efficiently handle multiple degradation types. We present Cat-AIR, a novel\n\\textbf{C}ontent \\textbf{A}nd \\textbf{T}ask-aware framework for\n\\textbf{A}ll-in-one \\textbf{I}mage \\textbf{R}estoration. Cat-AIR incorporates\nan alternating spatial-channel attention mechanism that adaptively balances the\nlocal and global information for different tasks. Specifically, we introduce\ncross-layer channel attentions and cross-feature spatial attentions that\nallocate computations based on content and task complexity. Furthermore, we\npropose a smooth learning strategy that allows for seamless adaptation to new\nrestoration tasks while maintaining performance on existing ones. Extensive\nexperiments demonstrate that Cat-AIR achieves state-of-the-art results across a\nwide range of restoration tasks, requiring fewer FLOPs than previous methods,\nestablishing new benchmarks for efficient all-in-one image restoration.\n","date":"2025-03-23"}
{"id":"2503.17919","title":"Predicting performance-related properties of refrigerant based on\n  tailored small-molecule functional group contribution","abstract":"  As current group contribution (GC) methods are mostly proposed for a wide\nsize-range of molecules, applying them to property prediction of small\nrefrigerant molecules could lead to unacceptable errors. In this sense, for the\ndesign of novel refrigerants and refrigeration systems, tailoring GC-based\nmodels specifically fitted to refrigerant molecules is of great interest. In\nthis work, databases of potential refrigerant molecules are first collected,\nfocusing on five key properties related to the operational efficiency of\nrefrigeration systems, namely normal boiling point, critical temperature,\ncritical pressure, enthalpy of vaporization, and acentric factor. Based on\ntailored small-molecule groups, the GC method is combined with machine learning\n(ML) to model these performance-related properties. Following the development\nof GC-ML models, their performance is analyzed to highlight the potential\ngroup-to-property contributions. Additionally, the refrigerant property\ndatabases are extended internally and externally, based on which examples are\npresented to highlight the significance of the developed models.\n","date":"2025-03-23"}
{"id":"2503.17922","title":"WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for\n  Efficient LLM Inference","abstract":"  With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness.\n","date":"2025-03-23"}
{"id":"2503.17924","title":"WLB-LLM: Workload-Balanced 4D Parallelism for Large Language Model\n  Training","abstract":"  In this work, we present WLB-LLM, a workLoad-balanced 4D parallelism for\nlarge language model training. We first thoroughly analyze the workload\nimbalance issue in LLM training and identify two primary sources of imbalance\nat the pipeline parallelism and context parallelism levels. Then, to address\nthe imbalance issue, at the pipeline parallelism level, WLB-LLM incorporates a\nworkload-aware variable-length document packing method to balance the\ncomputation and communication workload across micro-batches. Additionally, at\nthe context parallelism level, WLB-LLM introduces a novel fine-grained\nper-document sharding strategy, ensuring each worker within a context\nparallelism group has an identical workload. Comprehensive experiments under\ndifferent model scales demonstrate that WLB-LLM significantly mitigates the\nworkload imbalance during 4D parallelism LLM training and achieves an average\nspeedup of 1.23x when applying WLB-LLM in our internal LLM training framework.\n","date":"2025-03-23"}
{"id":"2503.17928","title":"Debiasing Multimodal Large Language Models via Noise-Aware Preference\n  Optimization","abstract":"  Multimodal Large Language Models excel in various tasks, yet often struggle\nwith modality bias, where the model tends to rely heavily on a single modality\nand overlook critical information in other modalities, which leads to incorrect\nfocus and generating irrelevant responses. In this paper, we propose using the\nparadigm of preference optimization to solve the modality bias problem,\nincluding RLAIFVBias, a debiased preference optimization dataset, and a Noise\nAware Preference Optimization algorithm. Specifically, we first construct the\ndataset by introducing perturbations to reduce the informational content of\ncertain modalities, compelling the model to rely on a specific modality when\ngenerating negative responses. To address the inevitable noise in automatically\nconstructed data, we combine the noise robust Mean Absolute Error with the\nBinary Cross Entropy in Direct Preference Optimization by a negative Box Cox\ntransformation, and dynamically adjust the algorithm noise robustness based on\nthe evaluated noise levels in the data. Extensive experiments validate our\napproach, demonstrating not only its effectiveness in mitigating modality bias\nbut also its significant role in minimizing hallucinations.\n","date":"2025-03-23"}
{"id":"2503.17932","title":"STShield: Single-Token Sentinel for Real-Time Jailbreak Detection in\n  Large Language Models","abstract":"  Large Language Models (LLMs) have become increasingly vulnerable to jailbreak\nattacks that circumvent their safety mechanisms. While existing defense methods\neither suffer from adaptive attacks or require computationally expensive\nauxiliary models, we present STShield, a lightweight framework for real-time\njailbroken judgement. STShield introduces a novel single-token sentinel\nmechanism that appends a binary safety indicator to the model's response\nsequence, leveraging the LLM's own alignment capabilities for detection. Our\nframework combines supervised fine-tuning on normal prompts with adversarial\ntraining using embedding-space perturbations, achieving robust detection while\npreserving model utility. Extensive experiments demonstrate that STShield\nsuccessfully defends against various jailbreak attacks, while maintaining the\nmodel's performance on legitimate queries. Compared to existing approaches,\nSTShield achieves superior defense performance with minimal computational\noverhead, making it a practical solution for real-world LLM deployment.\n","date":"2025-03-23"}
{"id":"2503.17933","title":"Experience Retrieval-Augmentation with Electronic Health Records Enables\n  Accurate Discharge QA","abstract":"  To improve the reliability of Large Language Models (LLMs) in clinical\napplications, retrieval-augmented generation (RAG) is extensively applied to\nprovide factual medical knowledge. However, beyond general medical knowledge\nfrom open-ended datasets, clinical case-based knowledge is also critical for\neffective medical reasoning, as it provides context grounded in real-world\npatient experiences. Motivated by this, we propose Experience Retrieval\nAugmentation - ExpRAG framework based on Electronic Health Record (EHR), aiming\nto offer the relevant context from other patients' discharge reports. ExpRAG\nperforms retrieval through a coarse-to-fine process, utilizing an EHR-based\nreport ranker to efficiently identify similar patients, followed by an\nexperience retriever to extract task-relevant content for enhanced medical\nreasoning. To evaluate ExpRAG, we introduce DischargeQA, a clinical QA dataset\nwith 1,280 discharge-related questions across diagnosis, medication, and\ninstruction tasks. Each problem is generated using EHR data to ensure realistic\nand challenging scenarios. Experimental results demonstrate that ExpRAG\nconsistently outperforms a text-based ranker, achieving an average relative\nimprovement of 5.2%, highlighting the importance of case-based knowledge for\nmedical reasoning.\n","date":"2025-03-23"}
{"id":"2503.17934","title":"TransAnimate: Taming Layer Diffusion to Generate RGBA Video","abstract":"  Text-to-video generative models have made remarkable advancements in recent\nyears. However, generating RGBA videos with alpha channels for transparency and\nvisual effects remains a significant challenge due to the scarcity of suitable\ndatasets and the complexity of adapting existing models for this purpose. To\naddress these limitations, we present TransAnimate, an innovative framework\nthat integrates RGBA image generation techniques with video generation modules,\nenabling the creation of dynamic and transparent videos. TransAnimate\nefficiently leverages pre-trained text-to-transparent image model weights and\ncombines them with temporal models and controllability plugins trained on RGB\nvideos, adapting them for controllable RGBA video generation tasks.\nAdditionally, we introduce an interactive motion-guided control mechanism,\nwhere directional arrows define movement and colors adjust scaling, offering\nprecise and intuitive control for designing game effects. To further alleviate\ndata scarcity, we have developed a pipeline for creating an RGBA video dataset,\nincorporating high-quality game effect videos, extracted foreground objects,\nand synthetic transparent videos. Comprehensive experiments demonstrate that\nTransAnimate generates high-quality RGBA videos, establishing it as a practical\nand effective tool for applications in gaming and visual effects.\n","date":"2025-03-23"}
{"id":"2503.17935","title":"Dataset Distillation for Quantum Neural Networks","abstract":"  Training Quantum Neural Networks (QNNs) on large amount of classical data can\nbe both time consuming as well as expensive. Higher amount of training data\nwould require higher number of gradient descent steps to reach convergence.\nThis, in turn would imply that the QNN will require higher number of quantum\nexecutions, thereby driving up its overall execution cost. In this work, we\npropose performing the dataset distillation process for QNNs, where we use a\nnovel quantum variant of classical LeNet model containing residual connection\nand trainable Hermitian observable in the Parametric Quantum Circuit (PQC) of\nthe QNN. This approach yields highly informative yet small number of training\ndata at similar performance as the original data. We perform distillation for\nMNIST and Cifar-10 datasets, and on comparison with classical models observe\nthat both the datasets yield reasonably similar post-inferencing accuracy on\nquantum LeNet (91.9% MNIST, 50.3% Cifar-10) compared to classical LeNet (94%\nMNIST, 54% Cifar-10). We also introduce a non-trainable Hermitian for ensuring\nstability in the distillation process and note marginal reduction of up to 1.8%\n(1.3%) for MNIST (Cifar-10) dataset.\n","date":"2025-03-23"}
