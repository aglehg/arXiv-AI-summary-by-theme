{"id":"2503.17894","title":"Generative AI for Validating Physics Laws","abstract":"  We present generative artificial intelligence (AI) to empirically validate\nfundamental laws of physics, focusing on the Stefan-Boltzmann law linking\nstellar temperature and luminosity. Our approach simulates counterfactual\nluminosities under hypothetical temperature regimes for each individual star\nand iteratively refines the temperature-luminosity relationship in a deep\nlearning architecture. We use Gaia DR3 data and find that, on average,\ntemperature's effect on luminosity increases with stellar radius and decreases\nwith absolute magnitude, consistent with theoretical predictions. By framing\nphysics laws as causal problems, our method offers a novel, data-driven\napproach to refine theoretical understanding and inform evidence-based policy\nand practice.\n","date":"2025-03-23"}
{"id":"2503.17896","title":"Multi-Disease-Aware Training Strategy for Cardiac MR Image Segmentation","abstract":"  Accurate segmentation of the ventricles from cardiac magnetic resonance\nimages (CMRIs) is crucial for enhancing the diagnosis and analysis of heart\nconditions. Deep learning-based segmentation methods have recently garnered\nsignificant attention due to their impressive performance. However, these\nsegmentation methods are typically good at partitioning regularly shaped\norgans, such as the left ventricle (LV) and the myocardium (MYO), whereas they\nperform poorly on irregularly shaped organs, such as the right ventricle (RV).\nIn this study, we argue that this limitation of segmentation models stems from\ntheir insufficient generalization ability to address the distribution shift of\nsegmentation targets across slices, cardiac phases, and disease conditions. To\novercome this issue, we present a Multi-Disease-Aware Training Strategy (MTS)\nand restructure the introduced CMRI datasets into multi-disease datasets.\nAdditionally, we propose a specialized data processing technique for\npreprocessing input images to support the MTS. To validate the effectiveness of\nour method, we performed control group experiments and cross-validation tests.\nThe experimental results show that (1) network models trained using our\nproposed strategy achieved superior segmentation performance, particularly in\nRV segmentation, and (2) these networks exhibited robust performance even when\napplied to data from unknown diseases.\n","date":"2025-03-23"}
{"id":"2503.17897","title":"Real-time Global Illumination for Dynamic 3D Gaussian Scenes","abstract":"  We present a real-time global illumination approach along with a pipeline for\ndynamic 3D Gaussian models and meshes. Building on a formulated surface light\ntransport model for 3D Gaussians, we address key performance challenges with a\nfast compound stochastic ray-tracing algorithm and an optimized 3D Gaussian\nrasterizer. Our pipeline integrates multiple real-time techniques to accelerate\nperformance and achieve high-quality lighting effects. Our approach enables\nreal-time rendering of dynamic scenes with interactively editable materials and\ndynamic lighting of diverse multi-lights settings, capturing mutual\nmulti-bounce light transport (indirect illumination) between 3D Gaussians and\nmesh. Additionally, we present a real-time renderer with an interactive user\ninterface, validating our approach and demonstrating its practicality and high\nefficiency with over 40 fps in scenes including both 3D Gaussians and mesh.\nFurthermore, our work highlights the potential of 3D Gaussians in real-time\napplications with dynamic lighting, offering insights into performance and\noptimization.\n","date":"2025-03-23"}
{"id":"2503.17899","title":"What Time Tells Us? An Explorative Study of Time Awareness Learned from\n  Static Images","abstract":"  Time becomes visible through illumination changes in what we see. Inspired by\nthis, in this paper we explore the potential to learn time awareness from\nstatic images, trying to answer: what time tells us? To this end, we first\nintroduce a Time-Oriented Collection (TOC) dataset, which contains 130,906\nimages with reliable timestamps. Leveraging this dataset, we propose a\nTime-Image Contrastive Learning (TICL) approach to jointly model timestamps and\nrelated visual representations through cross-modal contrastive learning. We\nfound that the proposed TICL, 1) not only achieves state-of-the-art performance\non the timestamp estimation task, over various benchmark metrics, 2) but also,\ninterestingly, though only seeing static images, the time-aware embeddings\nlearned from TICL show strong capability in several time-aware downstream tasks\nsuch as time-based image retrieval, video scene classification, and time-aware\nimage editing. Our findings suggest that time-related visual cues can be\nlearned from static images and are beneficial for various vision tasks, laying\na foundation for future research on understanding time-related visual context.\nProject page:https:\/\/rathgrith.github.io\/timetells\/.\n","date":"2025-03-23"}
{"id":"2503.17900","title":"MedPlan:A Two-Stage RAG-Based System for Personalized Medical Plan\n  Generation","abstract":"  Despite recent success in applying large language models (LLMs) to electronic\nhealth records (EHR), most systems focus primarily on assessment rather than\ntreatment planning. We identify three critical limitations in current\napproaches: they generate treatment plans in a single pass rather than\nfollowing the sequential reasoning process used by clinicians; they rarely\nincorporate patient-specific historical context; and they fail to effectively\ndistinguish between subjective and objective clinical information. Motivated by\nthe SOAP methodology (Subjective, Objective, Assessment, Plan), we introduce\nMedPlan, a novel framework that structures LLM reasoning to align with\nreal-life clinician workflows. Our approach employs a two-stage architecture\nthat first generates a clinical assessment based on patient symptoms and\nobjective data, then formulates a structured treatment plan informed by this\nassessment and enriched with patient-specific information through\nretrieval-augmented generation. Comprehensive evaluation demonstrates that our\nmethod significantly outperforms baseline approaches in both assessment\naccuracy and treatment plan quality.\n","date":"2025-03-23"}
{"id":"2503.17903","title":"GLADMamba: Unsupervised Graph-Level Anomaly Detection Powered by\n  Selective State Space Model","abstract":"  Unsupervised graph-level anomaly detection (UGLAD) is a critical and\nchallenging task across various domains, such as social network analysis,\nanti-cancer drug discovery, and toxic molecule identification. However,\nexisting methods often struggle to capture the long-range dependencies\nefficiently and neglect the spectral information. Recently, selective State\nSpace Models (SSMs), particularly Mamba, have demonstrated remarkable\nadvantages in capturing long-range dependencies with linear complexity and a\nselection mechanism. Motivated by their success across various domains, we\npropose GLADMamba, a novel framework that adapts the selective state space\nmodel into UGLAD field. We design View-Fused Mamba (VFM) with a\nMamba-Transformer-style architecture to efficiently fuse information from\ndifferent views with a selective state mechanism. We also design\nSpectrum-Guided Mamba (SGM) with a Mamba-Transformer-style architecture to\nleverage the Rayleigh quotient to guide the embedding refining process.\nGLADMamba can dynamically focus on anomaly-related information while discarding\nirrelevant information for anomaly detection. To the best of our knowledge,\nthis is the first work to introduce Mamba and explicit spectral information to\nUGLAD. Extensive experiments on 12 real-world datasets demonstrate that\nGLADMamba outperforms existing state-of-the-art methods, achieving superior\nperformance in UGLAD. The code is available at\nhttps:\/\/github.com\/Yali-F\/GLADMamba.\n","date":"2025-03-23"}
{"id":"2503.17905","title":"Finding Stable Subnetworks at Initialization with Dataset Distillation","abstract":"  Recent works have shown that Dataset Distillation, the process for\nsummarizing the training data, can be leveraged to accelerate the training of\ndeep learning models. However, its impact on training dynamics, particularly in\nneural network pruning, remains largely unexplored. In our work, we use\ndistilled data in the inner loop of iterative magnitude pruning to produce\nsparse, trainable subnetworks at initialization -- more commonly known as\nlottery tickets. While using 150x less training points, our algorithm matches\nthe performance of traditional lottery ticket rewinding on ResNet-18 &\nCIFAR-10. Previous work highlights that lottery tickets can be found when the\ndense initialization is stable to SGD noise (i.e. training across different\nordering of the data converges to the same minima). We extend this discovery,\ndemonstrating that stable subnetworks can exist even within an unstable dense\ninitialization. In our linear mode connectivity studies, we find that pruning\nwith distilled data discards parameters that contribute to the sharpness of the\nloss landscape. Lastly, we show that by first generating a stable sparsity mask\nat initialization, we can find lottery tickets at significantly higher\nsparsities than traditional iterative magnitude pruning.\n","date":"2025-03-23"}
{"id":"2503.17907","title":"Guided Diffusion for the Extension of Machine Vision to Human Visual\n  Perception","abstract":"  Image compression technology eliminates redundant information to enable\nefficient transmission and storage of images, serving both machine vision and\nhuman visual perception. For years, image coding focused on human perception\nhas been well-studied, leading to the development of various image compression\nstandards. On the other hand, with the rapid advancements in image recognition\nmodels, image compression for AI tasks, known as Image Coding for Machines\n(ICM), has gained significant importance. Therefore, scalable image coding\ntechniques that address the needs of both machines and humans have become a key\narea of interest. Additionally, there is increasing demand for research\napplying the diffusion model, which can generate human-viewable images from a\nsmall amount of data to image compression methods for human vision. Image\ncompression methods that use diffusion models can partially reconstruct the\ntarget image by guiding the generation process with a small amount of\nconditioning information. Inspired by the diffusion model's potential, we\npropose a method for extending machine vision to human visual perception using\nguided diffusion. Utilizing the diffusion model guided by the output of the ICM\nmethod, we generate images for human perception from random noise. Guided\ndiffusion acts as a bridge between machine vision and human vision, enabling\ntransitions between them without any additional bitrate overhead. The generated\nimages then evaluated based on bitrate and image quality, and we compare their\ncompression performance with other scalable image coding methods for humans and\nmachines.\n","date":"2025-03-23"}
{"id":"2503.17908","title":"Does GCL Need a Large Number of Negative Samples? Enhancing Graph\n  Contrastive Learning with Effective and Efficient Negative Sampling","abstract":"  Graph Contrastive Learning (GCL) aims to self-supervised learn\nlow-dimensional graph representations, primarily through instance\ndiscrimination, which involves manually mining positive and negative pairs from\ngraphs, increasing the similarity of positive pairs while decreasing negative\npairs. Drawing from the success of Contrastive Learning (CL) in other domains,\na consensus has been reached that the effectiveness of GCLs depends on a large\nnumber of negative pairs. As a result, despite the significant computational\noverhead, GCLs typically leverage as many negative node pairs as possible to\nimprove model performance. However, given that nodes within a graph are\ninterconnected, we argue that nodes cannot be treated as independent instances.\nTherefore, we challenge this consensus: Does employing more negative nodes lead\nto a more effective GCL model? To answer this, we explore the role of negative\nnodes in the commonly used InfoNCE loss for GCL and observe that: (1)\nCounterintuitively, a large number of negative nodes can actually hinder the\nmodel's ability to distinguish nodes with different semantics. (2) A smaller\nnumber of high-quality and non-topologically coupled negative nodes are\nsufficient to enhance the discriminability of representations. Based on these\nfindings, we propose a new method called GCL with Effective and Efficient\nNegative samples, E2Neg, which learns discriminative representations using only\na very small set of representative negative samples. E2Neg significantly\nreduces computational overhead and speeds up model training. We demonstrate the\neffectiveness and efficiency of E2Neg across multiple datasets compared to\nother GCL methods.\n","date":"2025-03-23"}
{"id":"2503.17909","title":"Financial Wind Tunnel: A Retrieval-Augmented Market Simulator","abstract":"  Market simulator tries to create high-quality synthetic financial data that\nmimics real-world market dynamics, which is crucial for model development and\nrobust assessment. Despite continuous advancements in simulation methodologies,\nmarket fluctuations vary in terms of scale and sources, but existing frameworks\noften excel in only specific tasks. To address this challenge, we propose\nFinancial Wind Tunnel (FWT), a retrieval-augmented market simulator designed to\ngenerate controllable, reasonable, and adaptable market dynamics for model\ntesting. FWT offers a more comprehensive and systematic generative capability\nacross different data frequencies. By leveraging a retrieval method to discover\ncross-sectional information as the augmented condition, our diffusion-based\nsimulator seamlessly integrates both macro- and micro-level market patterns.\nFurthermore, our framework allows the simulation to be controlled with wide\napplicability, including causal generation through \"what-if\" prompts or\nunprecedented cross-market trend synthesis. Additionally, we develop an\nautomated optimizer for downstream quantitative models, using stress testing of\nsimulated scenarios via FWT to enhance returns while controlling risks.\nExperimental results demonstrate that our approach enables the generalizable\nand reliable market simulation, significantly improve the performance and\nadaptability of downstream models, particularly in highly complex and volatile\nmarket conditions. Our code and data sample is available at\nhttps:\/\/anonymous.4open.science\/r\/fwt_-E852\n","date":"2025-03-23"}
{"id":"2503.17914","title":"Semi-supervised Semantic Segmentation with Multi-Constraint Consistency\n  Learning","abstract":"  Consistency regularization has prevailed in semi-supervised semantic\nsegmentation and achieved promising performance. However, existing methods\ntypically concentrate on enhancing the Image-augmentation based Prediction\nconsistency and optimizing the segmentation network as a whole, resulting in\ninsufficient utilization of potential supervisory information. In this paper,\nwe propose a Multi-Constraint Consistency Learning (MCCL) approach to\nfacilitate the staged enhancement of the encoder and decoder. Specifically, we\nfirst design a feature knowledge alignment (FKA) strategy to promote the\nfeature consistency learning of the encoder from image-augmentation. Our FKA\nencourages the encoder to derive consistent features for strongly and weakly\naugmented views from the perspectives of point-to-point alignment and\nprototype-based intra-class compactness. Moreover, we propose a self-adaptive\nintervention (SAI) module to increase the discrepancy of aligned intermediate\nfeature representations, promoting Feature-perturbation based Prediction\nconsistency learning. Self-adaptive feature masking and noise injection are\ndesigned in an instance-specific manner to perturb the features for robust\nlearning of the decoder. Experimental results on Pascal VOC2012 and Cityscapes\ndatasets demonstrate that our proposed MCCL achieves new state-of-the-art\nperformance. The source code and models are made available at\nhttps:\/\/github.com\/NUST-Machine-Intelligence-Laboratory\/MCCL.\n","date":"2025-03-23"}
{"id":"2503.17915","title":"Cat-AIR: Content and Task-Aware All-in-One Image Restoration","abstract":"  All-in-one image restoration seeks to recover high-quality images from\nvarious types of degradation using a single model, without prior knowledge of\nthe corruption source. However, existing methods often struggle to effectively\nand efficiently handle multiple degradation types. We present Cat-AIR, a novel\n\\textbf{C}ontent \\textbf{A}nd \\textbf{T}ask-aware framework for\n\\textbf{A}ll-in-one \\textbf{I}mage \\textbf{R}estoration. Cat-AIR incorporates\nan alternating spatial-channel attention mechanism that adaptively balances the\nlocal and global information for different tasks. Specifically, we introduce\ncross-layer channel attentions and cross-feature spatial attentions that\nallocate computations based on content and task complexity. Furthermore, we\npropose a smooth learning strategy that allows for seamless adaptation to new\nrestoration tasks while maintaining performance on existing ones. Extensive\nexperiments demonstrate that Cat-AIR achieves state-of-the-art results across a\nwide range of restoration tasks, requiring fewer FLOPs than previous methods,\nestablishing new benchmarks for efficient all-in-one image restoration.\n","date":"2025-03-23"}
{"id":"2503.17919","title":"Predicting performance-related properties of refrigerant based on\n  tailored small-molecule functional group contribution","abstract":"  As current group contribution (GC) methods are mostly proposed for a wide\nsize-range of molecules, applying them to property prediction of small\nrefrigerant molecules could lead to unacceptable errors. In this sense, for the\ndesign of novel refrigerants and refrigeration systems, tailoring GC-based\nmodels specifically fitted to refrigerant molecules is of great interest. In\nthis work, databases of potential refrigerant molecules are first collected,\nfocusing on five key properties related to the operational efficiency of\nrefrigeration systems, namely normal boiling point, critical temperature,\ncritical pressure, enthalpy of vaporization, and acentric factor. Based on\ntailored small-molecule groups, the GC method is combined with machine learning\n(ML) to model these performance-related properties. Following the development\nof GC-ML models, their performance is analyzed to highlight the potential\ngroup-to-property contributions. Additionally, the refrigerant property\ndatabases are extended internally and externally, based on which examples are\npresented to highlight the significance of the developed models.\n","date":"2025-03-23"}
{"id":"2503.17922","title":"WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for\n  Efficient LLM Inference","abstract":"  With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness.\n","date":"2025-03-23"}
{"id":"2503.17924","title":"WLB-LLM: Workload-Balanced 4D Parallelism for Large Language Model\n  Training","abstract":"  In this work, we present WLB-LLM, a workLoad-balanced 4D parallelism for\nlarge language model training. We first thoroughly analyze the workload\nimbalance issue in LLM training and identify two primary sources of imbalance\nat the pipeline parallelism and context parallelism levels. Then, to address\nthe imbalance issue, at the pipeline parallelism level, WLB-LLM incorporates a\nworkload-aware variable-length document packing method to balance the\ncomputation and communication workload across micro-batches. Additionally, at\nthe context parallelism level, WLB-LLM introduces a novel fine-grained\nper-document sharding strategy, ensuring each worker within a context\nparallelism group has an identical workload. Comprehensive experiments under\ndifferent model scales demonstrate that WLB-LLM significantly mitigates the\nworkload imbalance during 4D parallelism LLM training and achieves an average\nspeedup of 1.23x when applying WLB-LLM in our internal LLM training framework.\n","date":"2025-03-23"}
{"id":"2503.17928","title":"Debiasing Multimodal Large Language Models via Noise-Aware Preference\n  Optimization","abstract":"  Multimodal Large Language Models excel in various tasks, yet often struggle\nwith modality bias, where the model tends to rely heavily on a single modality\nand overlook critical information in other modalities, which leads to incorrect\nfocus and generating irrelevant responses. In this paper, we propose using the\nparadigm of preference optimization to solve the modality bias problem,\nincluding RLAIFVBias, a debiased preference optimization dataset, and a Noise\nAware Preference Optimization algorithm. Specifically, we first construct the\ndataset by introducing perturbations to reduce the informational content of\ncertain modalities, compelling the model to rely on a specific modality when\ngenerating negative responses. To address the inevitable noise in automatically\nconstructed data, we combine the noise robust Mean Absolute Error with the\nBinary Cross Entropy in Direct Preference Optimization by a negative Box Cox\ntransformation, and dynamically adjust the algorithm noise robustness based on\nthe evaluated noise levels in the data. Extensive experiments validate our\napproach, demonstrating not only its effectiveness in mitigating modality bias\nbut also its significant role in minimizing hallucinations.\n","date":"2025-03-23"}
{"id":"2503.17932","title":"STShield: Single-Token Sentinel for Real-Time Jailbreak Detection in\n  Large Language Models","abstract":"  Large Language Models (LLMs) have become increasingly vulnerable to jailbreak\nattacks that circumvent their safety mechanisms. While existing defense methods\neither suffer from adaptive attacks or require computationally expensive\nauxiliary models, we present STShield, a lightweight framework for real-time\njailbroken judgement. STShield introduces a novel single-token sentinel\nmechanism that appends a binary safety indicator to the model's response\nsequence, leveraging the LLM's own alignment capabilities for detection. Our\nframework combines supervised fine-tuning on normal prompts with adversarial\ntraining using embedding-space perturbations, achieving robust detection while\npreserving model utility. Extensive experiments demonstrate that STShield\nsuccessfully defends against various jailbreak attacks, while maintaining the\nmodel's performance on legitimate queries. Compared to existing approaches,\nSTShield achieves superior defense performance with minimal computational\noverhead, making it a practical solution for real-world LLM deployment.\n","date":"2025-03-23"}
{"id":"2503.17933","title":"Experience Retrieval-Augmentation with Electronic Health Records Enables\n  Accurate Discharge QA","abstract":"  To improve the reliability of Large Language Models (LLMs) in clinical\napplications, retrieval-augmented generation (RAG) is extensively applied to\nprovide factual medical knowledge. However, beyond general medical knowledge\nfrom open-ended datasets, clinical case-based knowledge is also critical for\neffective medical reasoning, as it provides context grounded in real-world\npatient experiences. Motivated by this, we propose Experience Retrieval\nAugmentation - ExpRAG framework based on Electronic Health Record (EHR), aiming\nto offer the relevant context from other patients' discharge reports. ExpRAG\nperforms retrieval through a coarse-to-fine process, utilizing an EHR-based\nreport ranker to efficiently identify similar patients, followed by an\nexperience retriever to extract task-relevant content for enhanced medical\nreasoning. To evaluate ExpRAG, we introduce DischargeQA, a clinical QA dataset\nwith 1,280 discharge-related questions across diagnosis, medication, and\ninstruction tasks. Each problem is generated using EHR data to ensure realistic\nand challenging scenarios. Experimental results demonstrate that ExpRAG\nconsistently outperforms a text-based ranker, achieving an average relative\nimprovement of 5.2%, highlighting the importance of case-based knowledge for\nmedical reasoning.\n","date":"2025-03-23"}
{"id":"2503.17934","title":"TransAnimate: Taming Layer Diffusion to Generate RGBA Video","abstract":"  Text-to-video generative models have made remarkable advancements in recent\nyears. However, generating RGBA videos with alpha channels for transparency and\nvisual effects remains a significant challenge due to the scarcity of suitable\ndatasets and the complexity of adapting existing models for this purpose. To\naddress these limitations, we present TransAnimate, an innovative framework\nthat integrates RGBA image generation techniques with video generation modules,\nenabling the creation of dynamic and transparent videos. TransAnimate\nefficiently leverages pre-trained text-to-transparent image model weights and\ncombines them with temporal models and controllability plugins trained on RGB\nvideos, adapting them for controllable RGBA video generation tasks.\nAdditionally, we introduce an interactive motion-guided control mechanism,\nwhere directional arrows define movement and colors adjust scaling, offering\nprecise and intuitive control for designing game effects. To further alleviate\ndata scarcity, we have developed a pipeline for creating an RGBA video dataset,\nincorporating high-quality game effect videos, extracted foreground objects,\nand synthetic transparent videos. Comprehensive experiments demonstrate that\nTransAnimate generates high-quality RGBA videos, establishing it as a practical\nand effective tool for applications in gaming and visual effects.\n","date":"2025-03-23"}
{"id":"2503.17935","title":"Dataset Distillation for Quantum Neural Networks","abstract":"  Training Quantum Neural Networks (QNNs) on large amount of classical data can\nbe both time consuming as well as expensive. Higher amount of training data\nwould require higher number of gradient descent steps to reach convergence.\nThis, in turn would imply that the QNN will require higher number of quantum\nexecutions, thereby driving up its overall execution cost. In this work, we\npropose performing the dataset distillation process for QNNs, where we use a\nnovel quantum variant of classical LeNet model containing residual connection\nand trainable Hermitian observable in the Parametric Quantum Circuit (PQC) of\nthe QNN. This approach yields highly informative yet small number of training\ndata at similar performance as the original data. We perform distillation for\nMNIST and Cifar-10 datasets, and on comparison with classical models observe\nthat both the datasets yield reasonably similar post-inferencing accuracy on\nquantum LeNet (91.9% MNIST, 50.3% Cifar-10) compared to classical LeNet (94%\nMNIST, 54% Cifar-10). We also introduce a non-trainable Hermitian for ensuring\nstability in the distillation process and note marginal reduction of up to 1.8%\n(1.3%) for MNIST (Cifar-10) dataset.\n","date":"2025-03-23"}
{"id":"2503.17936","title":"An Empirical Study of the Role of Incompleteness and Ambiguity in\n  Interactions with Large Language Models","abstract":"  Natural language as a medium for human-computer interaction has long been\nanticipated, has been undergoing a sea-change with the advent of Large Language\nModels (LLMs) with startling capacities for processing and generating language.\nMany of us now treat LLMs as modern-day oracles, asking it almost any kind of\nquestion. Unlike its Delphic predecessor, consulting an LLM does not have to be\na single-turn activity (ask a question, receive an answer, leave); and -- also\nunlike the Pythia -- it is widely acknowledged that answers from LLMs can be\nimproved with additional context. In this paper, we aim to study when we need\nmulti-turn interactions with LLMs to successfully get a question answered; or\nconclude that a question is unanswerable. We present a neural symbolic\nframework that models the interactions between human and LLM agents. Through\nthe proposed framework, we define incompleteness and ambiguity in the questions\nas properties deducible from the messages exchanged in the interaction, and\nprovide results from benchmark problems, in which the answer-correctness is\nshown to depend on whether or not questions demonstrate the presence of\nincompleteness or ambiguity (according to the properties we identify). Our\nresults show multi-turn interactions are usually required for datasets which\nhave a high proportion of incompleteness or ambiguous questions; and that that\nincreasing interaction length has the effect of reducing incompleteness or\nambiguity. The results also suggest that our measures of incompleteness and\nambiguity can be useful tools for characterising interactions with an LLM on\nquestion-answeringproblems\n","date":"2025-03-23"}
{"id":"2503.17937","title":"Cross-Domain Underwater Image Enhancement Guided by No-Reference Image\n  Quality Assessment: A Transfer Learning Approach","abstract":"  Single underwater image enhancement (UIE) is a challenging ill-posed problem,\nbut its development is hindered by two major issues: (1) The labels in\nunderwater reference datasets are pseudo labels, relying on these pseudo ground\ntruths in supervised learning leads to domain discrepancy. (2) Underwater\nreference datasets are scarce, making training on such small datasets prone to\noverfitting and distribution shift. To address these challenges, we propose\nTrans-UIE, a transfer learning-based UIE model that captures the fundamental\nparadigms of UIE through pretraining and utilizes a dataset composed of both\nreference and non-reference datasets for fine-tuning. However, fine-tuning the\nmodel using only reconstruction loss may introduce confirmation bias. To\nmitigate this, our method leverages no-reference image quality assessment\n(NR-IQA) metrics from above-water scenes to guide the transfer learning process\nacross domains while generating enhanced images with the style of the\nabove-water image domain. Additionally, to reduce the risk of overfitting\nduring the pretraining stage, we introduce Pearson correlation loss.\nExperimental results on both full-reference and no-reference underwater\nbenchmark datasets demonstrate that Trans-UIE significantly outperforms\nstate-of-the-art methods.\n","date":"2025-03-23"}
{"id":"2503.17938","title":"Selecting and Pruning: A Differentiable Causal Sequentialized\n  State-Space Model for Two-View Correspondence Learning","abstract":"  Two-view correspondence learning aims to discern true and false\ncorrespondences between image pairs by recognizing their underlying different\ninformation. Previous methods either treat the information equally or require\nthe explicit storage of the entire context, tending to be laborious in\nreal-world scenarios. Inspired by Mamba's inherent selectivity, we propose\n\\textbf{CorrMamba}, a \\textbf{Corr}espondence filter leveraging\n\\textbf{Mamba}'s ability to selectively mine information from true\ncorrespondences while mitigating interference from false ones, thus achieving\nadaptive focus at a lower cost. To prevent Mamba from being potentially\nimpacted by unordered keypoints that obscured its ability to mine spatial\ninformation, we customize a causal sequential learning approach based on the\nGumbel-Softmax technique to establish causal dependencies between features in a\nfully autonomous and differentiable manner. Additionally, a local-context\nenhancement module is designed to capture critical contextual cues essential\nfor correspondence pruning, complementing the core framework. Extensive\nexperiments on relative pose estimation, visual localization, and analysis\ndemonstrate that CorrMamba achieves state-of-the-art performance. Notably, in\noutdoor relative pose estimation, our method surpasses the previous SOTA by\n$2.58$ absolute percentage points in AUC@20\\textdegree, highlighting its\npractical superiority. Our code will be publicly available.\n","date":"2025-03-23"}
{"id":"2503.17940","title":"FisherTune: Fisher-Guided Robust Tuning of Vision Foundation Models for\n  Domain Generalized Segmentation","abstract":"  Vision Foundation Models (VFMs) excel in generalization due to large-scale\npretraining, but fine-tuning them for Domain Generalized Semantic Segmentation\n(DGSS) while maintaining this ability remains challenging. Existing approaches\neither selectively fine-tune parameters or freeze the VFMs and update only the\nadapters, both of which may underutilize the VFMs' full potential in DGSS\ntasks. We observe that domain-sensitive parameters in VFMs, arising from task\nand distribution differences, can hinder generalization. To address this, we\npropose \\textbf{FisherTune}, a robust fine-tuning method guided by the\nDomain-Related Fisher Information Matrix (DR-FIM). DR-FIM measures parameter\nsensitivity across tasks and domains, enabling selective updates that preserve\ngeneralization and enhance DGSS adaptability. FisherTune incorporates\nvariational inference to stabilize DR-FIM estimation, treating parameters as\nGaussian-distributed variables and leveraging pre-trained priors. Extensive\nexperiments show that FisherTune achieves superior cross-domain segmentation\nwhile maintaining generalization, outperforming selective-parameter and\nadapter-based methods.\n","date":"2025-03-23"}
{"id":"2503.17941","title":"Physics-Guided Multi-Fidelity DeepONet for Data-Efficient Flow Field\n  Prediction","abstract":"  This study presents an enhanced multi-fidelity deep operator network\n(DeepONet) framework for efficient spatio-temporal flow field prediction, with\nparticular emphasis on practical scenarios where high-fidelity data is scarce.\nWe introduce several key innovations to improve the framework's efficiency and\naccuracy. First, we enhance the DeepONet architecture by incorporating a merge\nnetwork that enables more complex feature interactions between operator and\ncoordinate spaces, achieving a 50.4% reduction in prediction error compared to\ntraditional dot-product operations. We further optimize the architecture\nthrough temporal positional encoding and point-based sampling strategies,\nachieving a 7.57% improvement in prediction accuracy while reducing training\ntime by 96% through efficient sampling and automatic mixed precision training.\nBuilding upon this foundation, we develop a transfer learning-based\nmulti-fidelity framework that leverages knowledge from pre-trained low-fidelity\nmodels to guide high-fidelity predictions. Our approach freezes the pre-trained\nbranch and trunk networks while making only the merge network trainable during\nhigh-fidelity training, preserving valuable low-fidelity representations while\nefficiently adapting to high-fidelity features. Through systematic\ninvestigation, we demonstrate that this fine-tuning strategy not only\nsignificantly outperforms linear probing and full-tuning alternatives but also\nsurpasses conventional multi-fidelity frameworks by up to 76%, while achieving\nup to 43.7% improvement in prediction accuracy compared to single-fidelity\ntraining. The core contribution lies in our novel time-derivative guided\nsampling approach: it maintains prediction accuracy equivalent to models\ntrained with the full dataset while requiring only 60% of the original\nhigh-fidelity samples.\n","date":"2025-03-23"}
{"id":"2503.17943","title":"Supervised Manifold Learning for Functional Data","abstract":"  Classification is a core topic in functional data analysis. A large number of\nfunctional classifiers have been proposed in the literature, most of which are\nbased on functional principal component analysis or functional regression. In\ncontrast, we investigate this topic from the perspective of manifold learning.\nIt is assumed that functional data lie on an unknown low-dimensional manifold,\nand we expect that better classifiers can be built upon the manifold structure.\nTo this end, we propose a novel proximity measure that takes the label\ninformation into account to learn the low-dimensional representations, also\nknown as the supervised manifold learning outcomes. When the outcomes are\ncoupled with multivariate classifiers, the procedure induces a family of new\nfunctional classifiers. In theory, we show that our functional classifier\ninduced by the $k$-NN classifier is asymptotically optimal. In practice, we\nshow that our method, coupled with several classical multivariate classifiers,\nachieves outstanding classification performance compared to existing functional\nclassifiers in both synthetic and real data examples.\n","date":"2025-03-23"}
{"id":"2503.17949","title":"Equivariant Machine Learning Interatomic Potentials with Global Charge\n  Redistribution","abstract":"  Machine learning interatomic potentials (MLIPs) provide a computationally\nefficient alternative to quantum mechanical simulations for predicting material\nproperties. Message-passing graph neural networks, commonly used in these\nMLIPs, rely on local descriptor-based symmetry functions to model atomic\ninteractions. However, such local descriptor-based approaches struggle with\nsystems exhibiting long-range interactions, charge transfer, and compositional\nheterogeneity. In this work, we develop a new equivariant MLIP incorporating\nlong-range Coulomb interactions through explicit treatment of electronic\ndegrees of freedom, specifically global charge distribution within the system.\nThis is achieved using a charge equilibration scheme based on predicted atomic\nelectronegativities. We systematically evaluate our model across a range of\nbenchmark periodic and non-periodic datasets, demonstrating that it outperforms\nboth short-range equivariant and long-range invariant MLIPs in energy and force\npredictions. Our approach enables more accurate and efficient simulations of\nsystems with long-range interactions and charge heterogeneity, expanding the\napplicability of MLIPs in computational materials science.\n","date":"2025-03-23"}
{"id":"2503.17952","title":"SLIDE: Sliding Localized Information for Document Extraction","abstract":"  Constructing accurate knowledge graphs from long texts and low-resource\nlanguages is challenging, as large language models (LLMs) experience degraded\nperformance with longer input chunks. This problem is amplified in low-resource\nsettings where data scarcity hinders accurate entity and relationship\nextraction. Contextual retrieval methods, while improving retrieval accuracy,\nstruggle with long documents. They truncate critical information in texts\nexceeding maximum context lengths of LLMs, significantly limiting knowledge\ngraph construction. We introduce SLIDE (Sliding Localized Information for\nDocument Extraction), a chunking method that processes long documents by\ngenerating local context through overlapping windows. SLIDE ensures that\nessential contextual information is retained, enhancing knowledge graph\nextraction from documents exceeding LLM context limits. It significantly\nimproves GraphRAG performance, achieving a 24% increase in entity extraction\nand a 39% improvement in relationship extraction for English. For Afrikaans, a\nlow-resource language, SLIDE achieves a 49% increase in entity extraction and\nan 82% improvement in relationship extraction. Furthermore, it improves upon\nstate-of-the-art in question-answering metrics such as comprehensiveness,\ndiversity and empowerment, demonstrating its effectiveness in multilingual and\nresource-constrained settings.\n","date":"2025-03-23"}
{"id":"2503.17955","title":"Human-AI Interaction and User Satisfaction: Empirical Evidence from\n  Online Reviews of AI Products","abstract":"  Human-AI Interaction (HAI) guidelines and design principles have become\nincreasingly important in both industry and academia to guide the development\nof AI systems that align with user needs and expectations. However, large-scale\nempirical evidence on how HAI principles shape user satisfaction in practice\nremains limited. This study addresses that gap by analyzing over 100,000 user\nreviews of AI-related products from G2, a leading review platform for business\nsoftware and services. Based on widely adopted industry guidelines, we identify\nseven core HAI dimensions and examine their coverage and sentiment within the\nreviews. We find that the sentiment on four HAI dimensions-adaptability,\ncustomization, error recovery, and security-is positively associated with\noverall user satisfaction. Moreover, we show that engagement with HAI\ndimensions varies by professional background: Users with technical job roles\nare more likely to discuss system-focused aspects, such as reliability, while\nnon-technical users emphasize interaction-focused features like customization\nand feedback. Interestingly, the relationship between HAI sentiment and overall\nsatisfaction is not moderated by job role, suggesting that once an HAI\ndimension has been identified by users, its effect on satisfaction is\nconsistent across job roles.\n","date":"2025-03-23"}
{"id":"2503.17956","title":"On the Origins of Sampling Bias: Implications on Fairness Measurement\n  and Mitigation","abstract":"  Accurately measuring discrimination is crucial to faithfully assessing\nfairness of trained machine learning (ML) models. Any bias in measuring\ndiscrimination leads to either amplification or underestimation of the existing\ndisparity. Several sources of bias exist and it is assumed that bias resulting\nfrom machine learning is born equally by different groups (e.g. females vs\nmales, whites vs blacks, etc.). If, however, bias is born differently by\ndifferent groups, it may exacerbate discrimination against specific\nsub-populations. Sampling bias, in particular, is inconsistently used in the\nliterature to describe bias due to the sampling procedure. In this paper, we\nattempt to disambiguate this term by introducing clearly defined variants of\nsampling bias, namely, sample size bias (SSB) and underrepresentation bias\n(URB). Through an extensive set of experiments on benchmark datasets and using\nmainstream learning algorithms, we expose relevant observations in several\nmodel training scenarios. The observations are finally framed as actionable\nrecommendations for practitioners.\n","date":"2025-03-23"}
{"id":"2503.17959","title":"Dynamic Gradient Sparse Update for Edge Training","abstract":"  Training on edge devices enables personalized model fine-tuning to enhance\nreal-world performance and maintain data privacy. However, the gradient\ncomputation for backpropagation in the training requires significant memory\nbuffers to store intermediate features and compute losses. This is unacceptable\nfor memory-constrained edge devices such as microcontrollers. To tackle this\nissue, we propose a training acceleration method using dynamic gradient sparse\nupdates. This method updates the important channels and layers only and skips\ngradient computation for the less important channels and layers to reduce\nmemory usage for each update iteration. In addition, the channel selection is\ndynamic for different iterations to traverse most of the parameters in the\nupdate layers along the time dimension for better performance. The experimental\nresult shows that the proposed method enables an ImageNet pre-trained\nMobileNetV2 trained on CIFAR-10 to achieve an accuracy of 85.77\\% while\nupdating only 2\\% of convolution weights within 256KB on-chip memory. This\nresults in a remarkable 98\\% reduction in feature memory usage compared to\ndense model training.\n","date":"2025-03-23"}
{"id":"2503.17963","title":"Won: Establishing Best Practices for Korean Financial NLP","abstract":"  In this work, we present the first open leaderboard for evaluating Korean\nlarge language models focused on finance. Operated for about eight weeks, the\nleaderboard evaluated 1,119 submissions on a closed benchmark covering five\nMCQA categories: finance and accounting, stock price prediction, domestic\ncompany analysis, financial markets, and financial agent tasks and one\nopen-ended qa task. Building on insights from these evaluations, we release an\nopen instruction dataset of 80k instances and summarize widely used training\nstrategies observed among top-performing models. Finally, we introduce Won, a\nfully open and transparent LLM built using these best practices. We hope our\ncontributions help advance the development of better and safer financial LLMs\nfor Korean and other languages.\n","date":"2025-03-23"}
{"id":"2503.17965","title":"Understanding the Effects of RLHF on the Quality and Detectability of\n  LLM-Generated Texts","abstract":"  Large Language Models (LLMs) have demonstrated exceptional performance on a\nrange of downstream NLP tasks by generating text that closely resembles human\nwriting. However, the ease of achieving this similarity raises concerns from\npotential malicious uses at scale by bad actors, as LLM-generated text becomes\nincreasingly difficult to discern from human text. Although detection methods\nhave been developed to address this issue, bad actors can further manipulate\nLLM-generated texts to make them less detectable. In this work, we study how\nfurther editing texts with Reinforcement Learning from Human Feedback (RLHF),\nwhich aligns model outputs with human preferences, affects (a) the quality of\ngenerated texts for two tasks, and (b) the performance of LLM-generated text\ndetectors, looking at both training-based and zero-shot detection methods.\nAlthough RLHF improves the quality of LLM-generated texts, we find that it also\ntends to produce more detectable, lengthy, and repetitive outputs.\nAdditionally, we observe that training-based detectors are vulnerable to short\ntexts and to texts that incorporate code, whereas zero-shot detectors exhibit\ngreater robustness.\n","date":"2025-03-23"}
{"id":"2503.17966","title":"Real-World Remote Sensing Image Dehazing: Benchmark and Baseline","abstract":"  Remote Sensing Image Dehazing (RSID) poses significant challenges in\nreal-world scenarios due to the complex atmospheric conditions and severe color\ndistortions that degrade image quality. The scarcity of real-world remote\nsensing hazy image pairs has compelled existing methods to rely primarily on\nsynthetic datasets. However, these methods struggle with real-world\napplications due to the inherent domain gap between synthetic and real data. To\naddress this, we introduce Real-World Remote Sensing Hazy Image Dataset\n(RRSHID), the first large-scale dataset featuring real-world hazy and dehazed\nimage pairs across diverse atmospheric conditions. Based on this, we propose\nMCAF-Net, a novel framework tailored for real-world RSID. Its effectiveness\narises from three innovative components: Multi-branch Feature Integration Block\nAggregator (MFIBA), which enables robust feature extraction through cascaded\nintegration blocks and parallel multi-branch processing; Color-Calibrated\nSelf-Supervised Attention Module (CSAM), which mitigates complex color\ndistortions via self-supervised learning and attention-guided refinement; and\nMulti-Scale Feature Adaptive Fusion Module (MFAFM), which integrates features\neffectively while preserving local details and global context. Extensive\nexperiments validate that MCAF-Net demonstrates state-of-the-art performance in\nreal-world RSID, while maintaining competitive performance on synthetic\ndatasets. The introduction of RRSHID and MCAF-Net sets new benchmarks for\nreal-world RSID research, advancing practical solutions for this complex task.\nThe code and dataset are publicly available at\n\\url{https:\/\/github.com\/lwCVer\/RRSHID}.\n","date":"2025-03-23"}
{"id":"2503.17970","title":"PathoHR: Breast Cancer Survival Prediction on High-Resolution\n  Pathological Images","abstract":"  Breast cancer survival prediction in computational pathology presents a\nremarkable challenge due to tumor heterogeneity. For instance, different\nregions of the same tumor in the pathology image can show distinct\nmorphological and molecular characteristics. This makes it difficult to extract\nrepresentative features from whole slide images (WSIs) that truly reflect the\ntumor's aggressive potential and likely survival outcomes. In this paper, we\npresent PathoHR, a novel pipeline for accurate breast cancer survival\nprediction that enhances any size of pathological images to enable more\neffective feature learning. Our approach entails (1) the incorporation of a\nplug-and-play high-resolution Vision Transformer (ViT) to enhance patch-wise\nWSI representation, enabling more detailed and comprehensive feature\nextraction, (2) the systematic evaluation of multiple advanced similarity\nmetrics for comparing WSI-extracted features, optimizing the representation\nlearning process to better capture tumor characteristics, (3) the demonstration\nthat smaller image patches enhanced follow the proposed pipeline can achieve\nequivalent or superior prediction accuracy compared to raw larger patches,\nwhile significantly reducing computational overhead. Experimental findings\nvalid that PathoHR provides the potential way of integrating enhanced image\nresolution with optimized feature learning to advance computational pathology,\noffering a promising direction for more accurate and efficient breast cancer\nsurvival prediction. Code will be available at\nhttps:\/\/github.com\/AIGeeksGroup\/PathoHR.\n","date":"2025-03-23"}
{"id":"2503.17973","title":"PhysTwin: Physics-Informed Reconstruction and Simulation of Deformable\n  Objects from Videos","abstract":"  Creating a physical digital twin of a real-world object has immense potential\nin robotics, content creation, and XR. In this paper, we present PhysTwin, a\nnovel framework that uses sparse videos of dynamic objects under interaction to\nproduce a photo- and physically realistic, real-time interactive virtual\nreplica. Our approach centers on two key components: (1) a physics-informed\nrepresentation that combines spring-mass models for realistic physical\nsimulation, generative shape models for geometry, and Gaussian splats for\nrendering; and (2) a novel multi-stage, optimization-based inverse modeling\nframework that reconstructs complete geometry, infers dense physical\nproperties, and replicates realistic appearance from videos. Our method\nintegrates an inverse physics framework with visual perception cues, enabling\nhigh-fidelity reconstruction even from partial, occluded, and limited\nviewpoints. PhysTwin supports modeling various deformable objects, including\nropes, stuffed animals, cloth, and delivery packages. Experiments show that\nPhysTwin outperforms competing methods in reconstruction, rendering, future\nprediction, and simulation under novel interactions. We further demonstrate its\napplications in interactive real-time simulation and model-based robotic motion\nplanning.\n","date":"2025-03-23"}
{"id":"2503.17975","title":"Shot Sequence Ordering for Video Editing: Benchmarks, Metrics, and\n  Cinematology-Inspired Computing Methods","abstract":"  With the rising popularity of short video platforms, the demand for video\nproduction has increased substantially. However, high-quality video creation\ncontinues to rely heavily on professional editing skills and a nuanced\nunderstanding of visual language. To address this challenge, the Shot Sequence\nOrdering (SSO) task in AI-assisted video editing has emerged as a pivotal\napproach for enhancing video storytelling and the overall viewing experience.\nNevertheless, the progress in this field has been impeded by a lack of publicly\navailable benchmark datasets. In response, this paper introduces two novel\nbenchmark datasets, AVE-Order and ActivityNet-Order. Additionally, we employ\nthe Kendall Tau distance as an evaluation metric for the SSO task and propose\nthe Kendall Tau Distance-Cross Entropy Loss. We further introduce the concept\nof Cinematology Embedding, which incorporates movie metadata and shot labels as\nprior knowledge into the SSO model, and constructs the AVE-Meta dataset to\nvalidate the method's effectiveness. Experimental results indicate that the\nproposed loss function and method substantially enhance SSO task accuracy. All\ndatasets are publicly accessible at https:\/\/github.com\/litchiar\/ShotSeqBench.\n","date":"2025-03-23"}
{"id":"2503.17978","title":"PIM: Physics-Informed Multi-task Pre-training for Improving Inertial\n  Sensor-Based Human Activity Recognition","abstract":"  Human activity recognition (HAR) with deep learning models relies on large\namounts of labeled data, often challenging to obtain due to associated cost,\ntime, and labor. Self-supervised learning (SSL) has emerged as an effective\napproach to leverage unlabeled data through pretext tasks, such as masked\nreconstruction and multitask learning with signal processing-based data\naugmentations, to pre-train encoder models. However, such methods are often\nderived from computer vision approaches that disregard physical mechanisms and\nconstraints that govern wearable sensor data and the phenomena they reflect. In\nthis paper, we propose a physics-informed multi-task pre-training (PIM)\nframework for IMU-based HAR. PIM generates pre-text tasks based on the\nunderstanding of basic physical aspects of human motion: including movement\nspeed, angles of movement, and symmetry between sensor placements. Given a\nsensor signal, we calculate corresponding features using physics-based\nequations and use them as pretext tasks for SSL. This enables the model to\ncapture fundamental physical characteristics of human activities, which is\nespecially relevant for multi-sensor systems. Experimental evaluations on four\nHAR benchmark datasets demonstrate that the proposed method outperforms\nexisting state-of-the-art methods, including data augmentation and masked\nreconstruction, in terms of accuracy and F1 score. We have observed gains of\nalmost 10\\% in macro f1 score and accuracy with only 2 to 8 labeled examples\nper class and up to 3% when there is no reduction in the amount of training\ndata.\n","date":"2025-03-23"}
{"id":"2503.17979","title":"Trade-offs in Large Reasoning Models: An Empirical Analysis of\n  Deliberative and Adaptive Reasoning over Foundational Capabilities","abstract":"  Recent advancements in Large Reasoning Models (LRMs), such as OpenAI's o1\/o3\nand DeepSeek-R1, have demonstrated remarkable performance in specialized\nreasoning tasks through human-like deliberative thinking and long\nchain-of-thought reasoning. However, our systematic evaluation across various\nmodel families (DeepSeek, Qwen, and LLaMA) and scales (7B to 671B) reveals that\nacquiring these deliberative reasoning capabilities significantly reduces the\nfoundational capabilities of LRMs, including notable declines in helpfulness\nand harmlessness, alongside substantially increased inference costs.\nImportantly, we demonstrate that adaptive reasoning -- employing modes like\nZero-Thinking, Less-Thinking, and Summary-Thinking -- can effectively alleviate\nthese drawbacks. Our empirical insights underline the critical need for\ndeveloping more versatile LRMs capable of dynamically allocating inference-time\ncompute according to specific task characteristics.\n","date":"2025-03-23"}
{"id":"2503.17982","title":"Co-SemDepth: Fast Joint Semantic Segmentation and Depth Estimation on\n  Aerial Images","abstract":"  Understanding the geometric and semantic properties of the scene is crucial\nin autonomous navigation and particularly challenging in the case of Unmanned\nAerial Vehicle (UAV) navigation. Such information may be by obtained by\nestimating depth and semantic segmentation maps of the surrounding environment\nand for their practical use in autonomous navigation, the procedure must be\nperformed as close to real-time as possible. In this paper, we leverage\nmonocular cameras on aerial robots to predict depth and semantic maps in\nlow-altitude unstructured environments. We propose a joint deep-learning\narchitecture that can perform the two tasks accurately and rapidly, and\nvalidate its effectiveness on MidAir and Aeroscapes benchmark datasets. Our\njoint-architecture proves to be competitive or superior to the other single and\njoint architecture methods while performing its task fast predicting 20.2 FPS\non a single NVIDIA quadro p5000 GPU and it has a low memory footprint. All\ncodes for training and prediction can be found on this link:\nhttps:\/\/github.com\/Malga-Vision\/Co-SemDepth\n","date":"2025-03-23"}
{"id":"2503.17983","title":"Histomorphology-driven multi-instance learning for breast cancer WSI\n  classification","abstract":"  Histomorphology is crucial in breast cancer diagnosis. However, existing\nwhole slide image (WSI) classification methods struggle to effectively\nincorporate histomorphology information, limiting their ability to capture key\nand fine-grained pathological features. To address this limitation, we propose\na novel framework that explicitly incorporates histomorphology (tumor\ncellularity, cellular morphology, and tissue architecture) into WSI\nclassification. Specifically, our approach consists of three key components:\n(1) estimating the importance of tumor-related histomorphology information at\nthe patch level based on medical prior knowledge; (2) generating representative\ncluster-level features through histomorphology-driven cluster pooling; and (3)\nenabling WSI-level classification through histomorphology-driven multi-instance\naggregation. With the incorporation of histomorphological information, our\nframework strengthens the model's ability to capture key and fine-grained\npathological patterns, thereby enhancing WSI classification performance.\nExperimental results demonstrate its effectiveness, achieving high diagnostic\naccuracy for molecular subtyping and cancer subtyping. The code will be made\navailable at https:\/\/github.com\/Badgewho\/HMDMIL.\n","date":"2025-03-23"}
{"id":"2503.17984","title":"Taste More, Taste Better: Diverse Data and Strong Model Boost\n  Semi-Supervised Crowd Counting","abstract":"  Semi-supervised crowd counting is crucial for addressing the high annotation\ncosts of densely populated scenes. Although several methods based on\npseudo-labeling have been proposed, it remains challenging to effectively and\naccurately utilize unlabeled data. In this paper, we propose a novel framework\ncalled Taste More Taste Better (TMTB), which emphasizes both data and model\naspects. Firstly, we explore a data augmentation technique well-suited for the\ncrowd counting task. By inpainting the background regions, this technique can\neffectively enhance data diversity while preserving the fidelity of the entire\nscenes. Secondly, we introduce the Visual State Space Model as backbone to\ncapture the global context information from crowd scenes, which is crucial for\nextremely crowded, low-light, and adverse weather scenarios. In addition to the\ntraditional regression head for exact prediction, we employ an Anti-Noise\nclassification head to provide less exact but more accurate supervision, since\nthe regression head is sensitive to noise in manual annotations. We conduct\nextensive experiments on four benchmark datasets and show that our method\noutperforms state-of-the-art methods by a large margin. Code is publicly\navailable on https:\/\/github.com\/syhien\/taste_more_taste_better.\n","date":"2025-03-23"}
{"id":"2503.17985","title":"Optimizing Navigation And Chemical Application in Precision Agriculture\n  With Deep Reinforcement Learning And Conditional Action Tree","abstract":"  This paper presents a novel reinforcement learning (RL)-based planning scheme\nfor optimized robotic management of biotic stresses in precision agriculture.\nThe framework employs a hierarchical decision-making structure with conditional\naction masking, where high-level actions direct the robot's exploration, while\nlow-level actions optimize its navigation and efficient chemical spraying in\naffected areas. The key objectives of optimization include improving the\ncoverage of infected areas with limited battery power and reducing chemical\nusage, thus preventing unnecessary spraying of healthy areas of the field. Our\nnumerical experimental results demonstrate that the proposed method,\nHierarchical Action Masking Proximal Policy Optimization (HAM-PPO),\nsignificantly outperforms baseline practices, such as LawnMower navigation +\nindiscriminate spraying (Carpet Spray), in terms of yield recovery and resource\nefficiency. HAM-PPO consistently achieves higher yield recovery percentages and\nlower chemical costs across a range of infection scenarios. The framework also\nexhibits robustness to observation noise and generalizability under diverse\nenvironmental conditions, adapting to varying infection ranges and spatial\ndistribution patterns.\n","date":"2025-03-23"}
{"id":"2503.17987","title":"Metaphor-based Jailbreaking Attacks on Text-to-Image Models","abstract":"  To mitigate misuse, text-to-image~(T2I) models commonly incorporate safety\nfilters to prevent the generation of sensitive images. Unfortunately, recent\njailbreaking attack methods use LLMs to generate adversarial prompts that\neffectively bypass safety filters while generating sensitive images, revealing\nthe safety vulnerabilities within the T2I model. However, existing LLM-based\nattack methods lack explicit guidance, relying on substantial queries to\nachieve a successful attack, which limits their practicality in real-world\nscenarios. In this work, we introduce \\textbf{MJA}, a \\textbf{m}etaphor-based\n\\textbf{j}ailbreaking \\textbf{a}ttack method inspired by the Taboo game, aiming\nto balance the attack effectiveness and query efficiency by generating\nmetaphor-based adversarial prompts. Specifically, MJA consists of two modules:\nan LLM-based multi-agent generation module~(MLAG) and an adversarial prompt\noptimization module~(APO). MLAG decomposes the generation of metaphor-based\nadversarial prompts into three subtasks: metaphor retrieval, context matching,\nand adversarial prompt generation. Subsequently, MLAG coordinates three\nLLM-based agents to generate diverse adversarial prompts by exploring various\nmetaphors and contexts. To enhance the attack efficiency, APO first trains a\nsurrogate model to predict the attack results of adversarial prompts and then\ndesigns an acquisition strategy to adaptively identify optimal adversarial\nprompts. Experiments demonstrate that MJA achieves better attack effectiveness\nwhile requiring fewer queries compared to baseline methods. Moreover, our\nadversarial prompts exhibit strong transferability across various open-source\nand commercial T2I models. \\textcolor{red}{This paper includes model-generated\ncontent that may contain offensive or distressing material.}\n","date":"2025-03-23"}
{"id":"2503.17992","title":"Geometric Constrained Non-Line-of-Sight Imaging","abstract":"  Normal reconstruction is crucial in non-line-of-sight (NLOS) imaging, as it\nprovides key geometric and lighting information about hidden objects, which\nsignificantly improves reconstruction accuracy and scene understanding.\nHowever, jointly estimating normals and albedo expands the problem from\nmatrix-valued functions to tensor-valued functions that substantially\nincreasing complexity and computational difficulty. In this paper, we propose a\nnovel joint albedo-surface reconstruction method, which utilizes the Frobenius\nnorm of the shape operator to control the variation rate of the normal field.\nIt is the first attempt to apply regularization methods to the reconstruction\nof surface normals for hidden objects. By improving the accuracy of the normal\nfield, it enhances detail representation and achieves high-precision\nreconstruction of hidden object geometry. The proposed method demonstrates\nrobustness and effectiveness on both synthetic and experimental datasets. On\ntransient data captured within 15 seconds, our surface normal-regularized\nreconstruction model produces more accurate surfaces than recently proposed\nmethods and is 30 times faster than the existing surface reconstruction\napproach.\n","date":"2025-03-23"}
{"id":"2503.17993","title":"Predicting Multitasking in Manual and Automated Driving with Optimal\n  Supervisory Control","abstract":"  Modern driving involves interactive technologies that can divert attention,\nincreasing the risk of accidents. This paper presents a computational cognitive\nmodel that simulates human multitasking while driving. Based on optimal\nsupervisory control theory, the model predicts how multitasking adapts to\nvariations in driving demands, interactive tasks, and automation levels. Unlike\nprevious models, it accounts for context-dependent multitasking across\ndifferent degrees of driving automation. The model predicts longer in-car\nglances on straight roads and shorter glances during curves. It also\nanticipates increased glance durations with driver aids such as lane-centering\nassistance and their interaction with environmental demands. Validated against\ntwo empirical datasets, the model offers insights into driver multitasking amid\nevolving in-car technologies and automation.\n","date":"2025-03-23"}
{"id":"2503.17994","title":"Instructing the Architecture Search for Spatial-temporal Sequence\n  Forecasting with LLM","abstract":"  Spatial-temporal sequence forecasting (STSF) is a long-standing research\nproblem with widespread real-world applications. Neural architecture search\n(NAS), which automates the neural network design, has been shown effective in\ntackling the STSF problem. However, the existing NAS methods for STSF focus on\ngenerating architectures in a time-consuming data-driven fashion, which heavily\nlimits their ability to use background knowledge and explore the complicated\nsearch trajectory. Large language models (LLMs) have shown remarkable ability\nin decision-making with comprehensive internal world knowledge, but how it\ncould benefit NAS for STSF remains unexplored. In this paper, we propose a\nnovel NAS method for STSF based on LLM. Instead of directly generate\narchitectures with LLM, We inspire the LLM's capability with a multi-level\nenhancement mechanism. Specifically, on the step-level, we decompose the\ngeneration task into decision steps with powerful prompt engineering and\ninspire LLM to serve as instructor for architecture search based on its\ninternal knowledge. On the instance-level, we utilize a one-step tuning\nframework to quickly evaluate the architecture instance and a memory bank to\ncumulate knowledge to improve LLM's search ability. On the task-level, we\npropose a two-stage architecture search, balancing the exploration stage and\noptimization stage, to reduce the possibility of being trapped in local optima.\nExtensive experimental results demonstrate that our method can achieve\ncompetitive effectiveness with superior efficiency against existing NAS methods\nfor STSF.\n","date":"2025-03-23"}
{"id":"2503.18007","title":"SymmCompletion: High-Fidelity and High-Consistency Point Cloud\n  Completion with Symmetry Guidance","abstract":"  Point cloud completion aims to recover a complete point shape from a partial\npoint cloud. Although existing methods can form satisfactory point clouds in\nglobal completeness, they often lose the original geometry details and face the\nproblem of geometric inconsistency between existing point clouds and\nreconstructed missing parts. To tackle this problem, we introduce\nSymmCompletion, a highly effective completion method based on symmetry\nguidance. Our method comprises two primary components: a Local Symmetry\nTransformation Network (LSTNet) and a Symmetry-Guidance Transformer (SGFormer).\nFirst, LSTNet efficiently estimates point-wise local symmetry transformation to\ntransform key geometries of partial inputs into missing regions, thereby\ngenerating geometry-align partial-missing pairs and initial point clouds.\nSecond, SGFormer leverages the geometric features of partial-missing pairs as\nthe explicit symmetric guidance that can constrain the refinement process for\ninitial point clouds. As a result, SGFormer can exploit provided priors to form\nhigh-fidelity and geometry-consistency final point clouds. Qualitative and\nquantitative evaluations on several benchmark datasets demonstrate that our\nmethod outperforms state-of-the-art completion networks.\n","date":"2025-03-23"}
{"id":"2503.18008","title":"Personalized Language Models via Privacy-Preserving Evolutionary Model\n  Merging","abstract":"  Personalization in large language models (LLMs) seeks to tailor models to\nindividual user or user group preferences. Prompt-based methods augment queries\nwith user preference information, whereas training-based methods directly\nencode preferences into model parameters for more effective personalization.\nDespite achieving some success in personalizing LLMs, prior methods often fail\nto directly optimize task-specific metrics and lack explicit\nprivacy-preservation mechanisms. To address these limitations, we propose\nPrivacy-Preserving Model Merging via Evolutionary Algorithms (PriME), a novel\napproach to personalization that employs gradient-free methods to directly\noptimize task-specific metrics while preserving user privacy. By incorporating\nprivacy preservation into optimization, PriME produces a personalized module\nthat effectively captures the target user's preferences while minimizing the\nprivacy risks for the users sharing their private information. Experiments on\nthe LaMP benchmark show that PriME outperforms both prompt-based and\ntraining-based methods, achieving up to a 45% performance improvement over the\nprior art. Further analysis shows that PriME achieves a significantly better\nprivacy-utility trade-off, highlighting the potential of evolutionary\napproaches for privacy-preserving LLM personalization.\n","date":"2025-03-23"}
{"id":"2503.18010","title":"Finsler Multi-Dimensional Scaling: Manifold Learning for Asymmetric\n  Dimensionality Reduction and Embedding","abstract":"  Dimensionality reduction is a fundamental task that aims to simplify complex\ndata by reducing its feature dimensionality while preserving essential\npatterns, with core applications in data analysis and visualisation. To\npreserve the underlying data structure, multi-dimensional scaling (MDS) methods\nfocus on preserving pairwise dissimilarities, such as distances. They optimise\nthe embedding to have pairwise distances as close as possible to the data\ndissimilarities. However, the current standard is limited to embedding data in\nRiemannian manifolds. Motivated by the lack of asymmetry in the Riemannian\nmetric of the embedding space, this paper extends the MDS problem to a natural\nasymmetric generalisation of Riemannian manifolds called Finsler manifolds.\nInspired by Euclidean space, we define a canonical Finsler space for embedding\nasymmetric data. Due to its simplicity with respect to geodesics, data\nrepresentation in this space is both intuitive and simple to analyse. We\ndemonstrate that our generalisation benefits from the same theoretical\nconvergence guarantees. We reveal the effectiveness of our Finsler embedding\nacross various types of non-symmetric data, highlighting its value in\napplications such as data visualisation, dimensionality reduction, directed\ngraph embedding, and link prediction.\n","date":"2025-03-23"}
{"id":"2503.18012","title":"Scalable physics-informed deep generative model for solving forward and\n  inverse stochastic differential equations","abstract":"  Physics-informed deep learning approaches have been developed to solve\nforward and inverse stochastic differential equation (SDE) problems with\nhigh-dimensional stochastic space. However, the existing deep learning models\nhave difficulties solving SDEs with high-dimensional spatial space. In the\npresent study, we propose a scalable physics-informed deep generative model\n(sPI-GeM), which is capable of solving SDE problems with both high-dimensional\nstochastic and spatial space. The sPI-GeM consists of two deep learning models,\ni.e., (1) physics-informed basis networks (PI-BasisNet), which are used to\nlearn the basis functions as well as the coefficients given data on a certain\nstochastic process or random field, and (2) physics-informed deep generative\nmodel (PI-GeM), which learns the distribution over the coefficients obtained\nfrom the PI-BasisNet. The new samples for the learned stochastic process can\nthen be obtained using the inner product between the output of the generator\nand the basis functions from the trained PI-BasisNet. The sPI-GeM addresses the\nscalability in the spatial space in a similar way as in the widely used\ndimensionality reduction technique, i.e., principal component analysis (PCA). A\nseries of numerical experiments, including approximation of Gaussian and\nnon-Gaussian stochastic processes, forward and inverse SDE problems, are\nperformed to demonstrate the accuracy of the proposed model. Furthermore, we\nalso show the scalability of the sPI-GeM in both the stochastic and spatial\nspace using an example of a forward SDE problem with 38- and 20-dimension\nstochastic and spatial space, respectively.\n","date":"2025-03-23"}
{"id":"2503.18013","title":"Vision-R1: Evolving Human-Free Alignment in Large Vision-Language Models\n  via Vision-Guided Reinforcement Learning","abstract":"  Large Vision-Language Models (LVLMs) typically follow a two-stage training\nparadigm-pretraining and supervised fine-tuning. Recently, preference\noptimization, derived from the language domain, has emerged as an effective\npost-training reinforcement strategy to enhance capabilities of LVLMs. However,\nconstructing high-quality human-annotated preference data and developing robust\nreward models to mimic these preferences are both costly and challenging.\nMotivated by this observation, we propose Vision-R1, a novel vision-guided\nR1-like reinforcement learning algorithm for LVLMs that rewards models with\ndefinitive vision feedback. It only leverages curated instruction data,\neliminating the need for specialized reward models and handcrafted preference\ndatasets. We incorporate a criterion-driven reward function that further\nintegrates multi-dimensional feedback to evaluate model completions\ncomprehensively based on the vision task logic. Furthermore, we introduce a\nprogressive rule refinement strategy that dynamically adjusts the reward\ncriteria during training, enabling continuous model improvement and mitigating\nreward hacking. Extensive experiments on both in-distribution and\nout-of-distribution benchmarks demonstrate that fine-tuning the 7B LVLMs with\nVision-R1 achieves consistent performance gains, with even up to 50%\nimprovement and surpassing the state-of-the-art 10x size model.\n","date":"2025-03-23"}
{"id":"2503.18016","title":"Retrieval Augmented Generation and Understanding in Vision: A Survey and\n  New Outlook","abstract":"  Retrieval-augmented generation (RAG) has emerged as a pivotal technique in\nartificial intelligence (AI), particularly in enhancing the capabilities of\nlarge language models (LLMs) by enabling access to external, reliable, and\nup-to-date knowledge sources. In the context of AI-Generated Content (AIGC),\nRAG has proven invaluable by augmenting model outputs with supplementary,\nrelevant information, thus improving their quality. Recently, the potential of\nRAG has extended beyond natural language processing, with emerging methods\nintegrating retrieval-augmented strategies into the computer vision (CV)\ndomain. These approaches aim to address the limitations of relying solely on\ninternal model knowledge by incorporating authoritative external knowledge\nbases, thereby improving both the understanding and generation capabilities of\nvision models. This survey provides a comprehensive review of the current state\nof retrieval-augmented techniques in CV, focusing on two main areas: (I) visual\nunderstanding and (II) visual generation. In the realm of visual understanding,\nwe systematically review tasks ranging from basic image recognition to complex\napplications such as medical report generation and multimodal question\nanswering. For visual content generation, we examine the application of RAG in\ntasks related to image, video, and 3D generation. Furthermore, we explore\nrecent advancements in RAG for embodied AI, with a particular focus on\napplications in planning, task execution, multimodal perception, interaction,\nand specialized domains. Given that the integration of retrieval-augmented\ntechniques in CV is still in its early stages, we also highlight the key\nlimitations of current approaches and propose future research directions to\ndrive the development of this promising area.\n","date":"2025-03-23"}
{"id":"2503.18018","title":"Lost in Cultural Translation: Do LLMs Struggle with Math Across Cultural\n  Contexts?","abstract":"  Large Language Models (LLMs) have significantly advanced various fields,\nparticularly coding, mathematical reasoning, and logical problem solving.\nHowever, a critical question remains: Do these mathematical reasoning abilities\npersist when LLMs are presented with culturally adapted math problems?\nSpecifically, how do LLMs perform when faced with math problems embedded in\ncultural contexts that have no significant representation in main stream\nweb-scale AI training data? To explore this, we generated six synthetic\ncultural datasets from GSM8K, a widely used benchmark for assessing LLMs'\nmathematical reasoning skills. While preserving the mathematical logic and\nnumerical values of the original GSM8K test set, we modify cultural elements\nsuch as personal names, food items, place names, etc. These culturally adapted\ndatasets provide a more reliable framework for evaluating LLMs' mathematical\nreasoning under shifting cultural contexts. Our findings reveal that LLMs\nstruggle with math problems when cultural references change, even though the\nunderlying mathematical structure remains constant. Smaller models exhibit\ngreater performance drops compared to larger models. Interestingly, our results\nalso suggest that cultural familiarity can enhance mathematical reasoning. Even\nmodels with no explicit mathematical training but exposure to relevant cultural\ncontexts sometimes outperform larger, mathematically proficient models on\nculturally embedded math problems. This study highlights the impact of cultural\ncontext on the mathematical reasoning abilities of LLMs, underscoring the need\nfor more diverse and representative training data to improve robustness in\nreal-world applications. The benchmark data sets and script for reproducing the\nresults are available at\nhttps:\/\/github.com\/akarim23131\/Lost_in_Cultural_Translation\n","date":"2025-03-23"}
{"id":"2503.18023","title":"Regularization of ML models for Earth systems by using longer model\n  timesteps","abstract":"  Regularization is a technique to improve generalization of machine learning\n(ML) models. A common form of regularization in the ML literature is to train\non data where similar inputs map to different outputs. This improves\ngeneralization by preventing ML models from becoming overconfident in their\npredictions. This paper shows how using longer timesteps when modelling chaotic\nEarth systems naturally leads to more of this regularization. We show this in\ntwo domains. We explain how using longer model timesteps can improve results\nand demonstrate that increased regularization is one of the causes. We explain\nwhy longer model timesteps lead to improved regularization in these systems and\npresent a procedure to pick the model timestep. We also carry out a\nbenchmarking exercise on ORAS5 ocean reanalysis data to show that a longer\nmodel timestep (28 days) than is typically used gives realistic simulations. We\nsuggest that there will be many opportunities to use this type of\nregularization in Earth system problems because the Earth system is chaotic and\nthe regularization is so easy to implement.\n","date":"2025-03-23"}
{"id":"2503.18025","title":"Decision from Suboptimal Classifiers: Excess Risk Pre- and\n  Post-Calibration","abstract":"  Probabilistic classifiers are central for making informed decisions under\nuncertainty. Based on the maximum expected utility principle, optimal decision\nrules can be derived using the posterior class probabilities and\nmisclassification costs. Yet, in practice only learned approximations of the\noracle posterior probabilities are available. In this work, we quantify the\nexcess risk (a.k.a. regret) incurred using approximate posterior probabilities\nin batch binary decision-making. We provide analytical expressions for\nmiscalibration-induced regret ($R^{\\mathrm{CL}}$), as well as tight and\ninformative upper and lower bounds on the regret of calibrated classifiers\n($R^{\\mathrm{GL}}$). These expressions allow us to identify regimes where\nrecalibration alone addresses most of the regret, and regimes where the regret\nis dominated by the grouping loss, which calls for post-training beyond\nrecalibration. Crucially, both $R^{\\mathrm{CL}}$ and $R^{\\mathrm{GL}}$ can be\nestimated in practice using a calibration curve and a recent grouping loss\nestimator. On NLP experiments, we show that these quantities identify when the\nexpected gain of more advanced post-training is worth the operational cost.\nFinally, we highlight the potential of multicalibration approaches as efficient\nalternatives to costlier fine-tuning approaches.\n","date":"2025-03-23"}
{"id":"2503.18032","title":"Anomaly Detection and Localization for Speech Deepfakes via Feature\n  Pyramid Matching","abstract":"  The rise of AI-driven generative models has enabled the creation of highly\nrealistic speech deepfakes - synthetic audio signals that can imitate target\nspeakers' voices - raising critical security concerns. Existing methods for\ndetecting speech deepfakes primarily rely on supervised learning, which suffers\nfrom two critical limitations: limited generalization to unseen synthesis\ntechniques and a lack of explainability. In this paper, we address these issues\nby introducing a novel interpretable one-class detection framework, which\nreframes speech deepfake detection as an anomaly detection task. Our model is\ntrained exclusively on real speech to characterize its distribution, enabling\nthe classification of out-of-distribution samples as synthetically generated.\nAdditionally, our framework produces interpretable anomaly maps during\ninference, highlighting anomalous regions across both time and frequency\ndomains. This is done through a Student-Teacher Feature Pyramid Matching\nsystem, enhanced with Discrepancy Scaling to improve generalization\ncapabilities across unseen data distributions. Extensive evaluations\ndemonstrate the superior performance of our approach compared to the considered\nbaselines, validating the effectiveness of framing speech deepfake detection as\nan anomaly detection problem.\n","date":"2025-03-23"}
{"id":"2503.18033","title":"OmnimatteZero: Training-free Real-time Omnimatte with Pre-trained Video\n  Diffusion Models","abstract":"  Omnimatte aims to decompose a given video into semantically meaningful\nlayers, including the background and individual objects along with their\nassociated effects, such as shadows and reflections. Existing methods often\nrequire extensive training or costly self-supervised optimization. In this\npaper, we present OmnimatteZero, a training-free approach that leverages\noff-the-shelf pre-trained video diffusion models for omnimatte. It can remove\nobjects from videos, extract individual object layers along with their effects,\nand composite those objects onto new videos. We accomplish this by adapting\nzero-shot image inpainting techniques for video object removal, a task they\nfail to handle effectively out-of-the-box. We then show that self-attention\nmaps capture information about the object and its footprints and use them to\ninpaint the object's effects, leaving a clean background. Additionally, through\nsimple latent arithmetic, object layers can be isolated and recombined\nseamlessly with new video layers to produce new videos. Evaluations show that\nOmnimatteZero not only achieves superior performance in terms of background\nreconstruction but also sets a new record for the fastest Omnimatte approach,\nachieving real-time performance with minimal frame runtime.\n","date":"2025-03-23"}
{"id":"2503.18034","title":"Expanding the Boundaries of Vision Prior Knowledge in Multi-modal Large\n  Language Models","abstract":"  Does the prior knowledge of the vision encoder constrain the capability\nboundary of Multi-modal Large Language Models (MLLMs)? While most existing\nresearch treats MLLMs as unified systems optimized through end-to-end training,\nthe impact of vision encoder's prior knowledge is seldom investigated. In this\nwork, we introduce a novel metric, $Rank_e$, to quantify the effect of the\nvision encoder's prior knowledge on MLLM performance. Our analysis reveals a\npositive correlation between prior knowledge and MLLM performance. Moreover, we\nfind that domain-specific fine-tuning using solely end-to-end visual question\nanswering (VQA) data is insufficient--particularly for entities with low\ninherent visual prior knowledge. To address this issue, we propose VisPRE\n(Vision Prior Remediation), a two-stage training framework that explicitly\nincorporates prior knowledge at the vision encoder level. Experimental results\ndemonstrate that augmenting vision encoder's prior knowledge substantially\nboosts the visual understanding capabilities of MLLMs, offering a novel and\neffective strategy for improving performance, especially in scenarios involving\nuncommon visual entities.\n","date":"2025-03-23"}
{"id":"2503.18035","title":"Text-Driven Cross-Modal Place Recognition Method for Remote Sensing\n  Localization","abstract":"  Environment description-based localization in large-scale point cloud maps\nconstructed through remote sensing is critically significant for the\nadvancement of large-scale autonomous systems, such as delivery robots\noperating in the last mile. However, current approaches encounter challenges\ndue to the inability of point cloud encoders to effectively capture local\ndetails and long-range spatial relationships, as well as a significant modality\ngap between text and point cloud representations. To address these challenges,\nwe present Des4Pos, a novel two-stage text-driven remote sensing localization\nframework. In the coarse stage, the point-cloud encoder utilizes the\nMulti-scale Fusion Attention Mechanism (MFAM) to enhance local geometric\nfeatures, followed by a bidirectional Long Short-Term Memory (LSTM) module to\nstrengthen global spatial relationships. Concurrently, the Stepped Text Encoder\n(STE) integrates cross-modal prior knowledge from CLIP [1] and aligns text and\npoint-cloud features using this prior knowledge, effectively bridging modality\ndiscrepancies. In the fine stage, we introduce a Cascaded Residual Attention\n(CRA) module to fuse cross-modal features and predict relative localization\noffsets, thereby achieving greater localization precision. Experiments on the\nKITTI360Pose test set demonstrate that Des4Pos achieves state-of-the-art\nperformance in text-to-point-cloud place recognition. Specifically, it attains\na top-1 accuracy of 40% and a top-10 accuracy of 77% under a 5-meter radius\nthreshold, surpassing the best existing methods by 7% and 7%, respectively.\n","date":"2025-03-23"}
{"id":"2503.18038","title":"Multiple-Particle Autofocusing Algorithm Using Axial Resolution and\n  Morphological Analyses Based on Digital Holography","abstract":"  We propose an autofocusing algorithm to obtain, relatively accurately, the 3D\nposition of each particle, particularly its axial location, and particle number\nof a dense transparent particle solution via its hologram. First, morphological\nanalyses and constrained intensity are used on raw reconstructed images to\nobtain information on candidate focused particles. Second, axial resolution is\nused to obtain the real focused particles. Based on the mean intensity and\nequivalent diameter of each candidate focused particle, all focused particles\nare eventually secured. Our proposed method can rapidly provide relatively\naccurate ground-truth axial positions to solve the autofocusing problem that\noccurs with dense particles.\n","date":"2025-03-23"}
{"id":"2503.18042","title":"DualCP: Rehearsal-Free Domain-Incremental Learning via Dual-Level\n  Concept Prototype","abstract":"  Domain-Incremental Learning (DIL) enables vision models to adapt to changing\nconditions in real-world environments while maintaining the knowledge acquired\nfrom previous domains. Given privacy concerns and training time, Rehearsal-Free\nDIL (RFDIL) is more practical. Inspired by the incremental cognitive process of\nthe human brain, we design Dual-level Concept Prototypes (DualCP) for each\nclass to address the conflict between learning new knowledge and retaining old\nknowledge in RFDIL. To construct DualCP, we propose a Concept Prototype\nGenerator (CPG) that generates both coarse-grained and fine-grained prototypes\nfor each class. Additionally, we introduce a Coarse-to-Fine calibrator (C2F) to\nalign image features with DualCP. Finally, we propose a Dual Dot-Regression\n(DDR) loss function to optimize our C2F module. Extensive experiments on the\nDomainNet, CDDB, and CORe50 datasets demonstrate the effectiveness of our\nmethod.\n","date":"2025-03-23"}
{"id":"2503.18048","title":"Interpretable Feature Interaction via Statistical Self-supervised\n  Learning on Tabular Data","abstract":"  In high-dimensional and high-stakes contexts, ensuring both rigorous\nstatistical guarantees and interpretability in feature extraction from complex\ntabular data remains a formidable challenge. Traditional methods such as\nPrincipal Component Analysis (PCA) reduce dimensionality and identify key\nfeatures that explain the most variance, but are constrained by their reliance\non linear assumptions. In contrast, neural networks offer assumption-free\nfeature extraction through self-supervised learning techniques such as\nautoencoders, though their interpretability remains a challenge in fields\nrequiring transparency. To address this gap, this paper introduces Spofe, a\nnovel self-supervised machine learning pipeline that marries the power of\nkernel principal components for capturing nonlinear dependencies with a sparse\nand principled polynomial representation to achieve clear interpretability with\nstatistical rigor. Underpinning our approach is a robust theoretical framework\nthat delivers precise error bounds and rigorous false discovery rate (FDR)\ncontrol via a multi-objective knockoff selection procedure; it effectively\nbridges the gap between data-driven complexity and statistical reliability via\nthree stages: (1) generating self-supervised signals using kernel principal\ncomponents to model complex patterns, (2) distilling these signals into sparse\npolynomial functions for improved interpretability, and (3) applying a\nmulti-objective knockoff selection procedure with significance testing to\nrigorously identify important features. Extensive experiments on diverse\nreal-world datasets demonstrate the effectiveness of Spofe, consistently\nsurpassing KPCA, SKPCA, and other methods in feature selection for regression\nand classification tasks. Visualization and case studies highlight its ability\nto uncover key insights, enhancing interpretability and practical utility.\n","date":"2025-03-23"}
{"id":"2503.18050","title":"(G)I-DLE: Generative Inference via Distribution-preserving Logit\n  Exclusion with KL Divergence Minimization for Constrained Decoding","abstract":"  We propose (G)I-DLE, a new approach to constrained decoding that leverages KL\ndivergence minimization to preserve the intrinsic conditional probability\ndistribution of autoregressive language models while excluding undesirable\ntokens. Unlike conventional methods that naively set banned tokens' logits to\n$-\\infty$, which can distort the conversion from raw logits to posterior\nprobabilities and increase output variance, (G)I-DLE re-normalizes the allowed\ntoken probabilities to minimize such distortion. We validate our method on the\nK2-Eval dataset, specifically designed to assess Korean language fluency,\nlogical reasoning, and cultural appropriateness. Experimental results on\nQwen2.5 models (ranging from 1.5B to 14B) demonstrate that G-IDLE not only\nboosts mean evaluation scores but also substantially reduces the variance of\noutput quality.\n","date":"2025-03-23"}
{"id":"2503.18052","title":"SceneSplat: Gaussian Splatting-based Scene Understanding with\n  Vision-Language Pretraining","abstract":"  Recognizing arbitrary or previously unseen categories is essential for\ncomprehensive real-world 3D scene understanding. Currently, all existing\nmethods rely on 2D or textual modalities during training, or together at\ninference. This highlights a clear absence of a model capable of processing 3D\ndata alone for learning semantics end-to-end, along with the necessary data to\ntrain such a model. Meanwhile, 3D Gaussian Splatting (3DGS) has emerged as the\nde facto standard for 3D scene representation across various vision tasks.\nHowever, effectively integrating semantic reasoning into 3DGS in a\ngeneralizable fashion remains an open challenge. To address these limitations\nwe introduce SceneSplat, to our knowledge the first large-scale 3D indoor scene\nunderstanding approach that operates natively on 3DGS. Furthermore, we propose\na self-supervised learning scheme that unlocks rich 3D feature learning from\nunlabeled scenes. In order to power the proposed methods, we introduce\nSceneSplat-7K, the first large-scale 3DGS dataset for indoor scenes, comprising\nof 6868 scenes derived from 7 established datasets like ScanNet, Matterport3D,\netc. Generating SceneSplat-7K required computational resources equivalent to\n119 GPU-days on an L4 GPU, enabling standardized benchmarking for 3DGS-based\nreasoning for indoor scenes. Our exhaustive experiments on SceneSplat-7K\ndemonstrate the significant benefit of the proposed methods over the\nestablished baselines.\n","date":"2025-03-23"}
{"id":"2503.18055","title":"PolarFree: Polarization-based Reflection-free Imaging","abstract":"  Reflection removal is challenging due to complex light interactions, where\nreflections obscure important details and hinder scene understanding.\nPolarization naturally provides a powerful cue to distinguish between reflected\nand transmitted light, enabling more accurate reflection removal. However,\nexisting methods often rely on small-scale or synthetic datasets, which fail to\ncapture the diversity and complexity of real-world scenarios. To this end, we\nconstruct a large-scale dataset, PolaRGB, for Polarization-based reflection\nremoval of RGB images, which enables us to train models that generalize\neffectively across a wide range of real-world scenarios. The PolaRGB dataset\ncontains 6,500 well-aligned mixed-transmission image pairs, 8x larger than\nexisting polarization datasets, and is the first to include both RGB and\npolarization images captured across diverse indoor and outdoor environments\nwith varying lighting conditions. Besides, to fully exploit the potential of\npolarization cues for reflection removal, we introduce PolarFree, which\nleverages diffusion process to generate reflection-free cues for accurate\nreflection removal. Extensive experiments show that PolarFree significantly\nenhances image clarity in challenging reflective scenarios, setting a new\nbenchmark for polarized imaging and reflection removal. Code and dataset are\navailable at https:\/\/github.com\/mdyao\/PolarFree.\n","date":"2025-03-23"}
{"id":"2503.18060","title":"Surrogate Learning in Meta-Black-Box Optimization: A Preliminary Study","abstract":"  Recent Meta-Black-Box Optimization (MetaBBO) approaches have shown\npossibility of enhancing the optimization performance through learning\nmeta-level policies to dynamically configure low-level optimizers. However,\nexisting MetaBBO approaches potentially consume massive function evaluations to\ntrain their meta-level policies. Inspired by the recent trend of using\nsurrogate models for cost-friendly evaluation of expensive optimization\nproblems, in this paper, we propose a novel MetaBBO framework which combines\nsurrogate learning process and reinforcement learning-aided Differential\nEvolution algorithm, namely Surr-RLDE, to address the intensive function\nevaluation in MetaBBO. Surr-RLDE comprises two learning stages: surrogate\nlearning and policy learning. In surrogate learning, we train a\nKolmogorov-Arnold Networks (KAN) with a novel relative-order-aware loss to\naccurately approximate the objective functions of the problem instances used\nfor subsequent policy learning. In policy learning, we employ reinforcement\nlearning (RL) to dynamically configure the mutation operator in DE. The learned\nsurrogate model is integrated into the training of the RL-based policy to\nsubstitute for the original objective function, which effectively reduces\nconsumed evaluations during policy learning. Extensive benchmark results\ndemonstrate that Surr-RLDE not only shows competitive performance to recent\nbaselines, but also shows compelling generalization for higher-dimensional\nproblems. Further ablation studies underscore the effectiveness of each\ntechnical components in Surr-RLDE. We open-source Surr-RLDE at\nhttps:\/\/github.com\/GMC-DRL\/Surr-RLDE.\n","date":"2025-03-23"}
{"id":"2503.18061","title":"Reinforcement Learning-based Self-adaptive Differential Evolution\n  through Automated Landscape Feature Learning","abstract":"  Recently, Meta-Black-Box-Optimization (MetaBBO) methods significantly enhance\nthe performance of traditional black-box optimizers through meta-learning\nflexible and generalizable meta-level policies that excel in dynamic algorithm\nconfiguration (DAC) tasks within the low-level optimization, reducing the\nexpertise required to adapt optimizers for novel optimization tasks. Though\npromising, existing MetaBBO methods heavily rely on human-crafted feature\nextraction approach to secure learning effectiveness. To address this issue,\nthis paper introduces a novel MetaBBO method that supports automated feature\nlearning during the meta-learning process, termed as RLDE-AFL, which integrates\na learnable feature extraction module into a reinforcement learning-based DE\nmethod to learn both the feature encoding and meta-level policy. Specifically,\nwe design an attention-based neural network with mantissa-exponent based\nembedding to transform the solution populations and corresponding objective\nvalues during the low-level optimization into expressive landscape features. We\nfurther incorporate a comprehensive algorithm configuration space including\ndiverse DE operators into a reinforcement learning-aided DAC paradigm to\nunleash the behavior diversity and performance of the proposed RLDE-AFL.\nExtensive benchmark results show that co-training the proposed feature learning\nmodule and DAC policy contributes to the superior optimization performance of\nRLDE-AFL to several advanced DE methods and recent MetaBBO baselines over both\nsynthetic and realistic BBO scenarios. The source codes of RLDE-AFL are\navailable at https:\/\/github.com\/GMC-DRL\/RLDE-AFL.\n","date":"2025-03-23"}
{"id":"2503.18062","title":"Investigating Recent Large Language Models for Vietnamese Machine\n  Reading Comprehension","abstract":"  Large Language Models (LLMs) have shown remarkable proficiency in Machine\nReading Comprehension (MRC) tasks; however, their effectiveness for\nlow-resource languages like Vietnamese remains largely unexplored. In this\npaper, we fine-tune and evaluate two state-of-the-art LLMs: Llama 3 (8B\nparameters) and Gemma (7B parameters), on ViMMRC, a Vietnamese MRC dataset. By\nutilizing Quantized Low-Rank Adaptation (QLoRA), we efficiently fine-tune these\nmodels and compare their performance against powerful LLM-based baselines.\nAlthough our fine-tuned models are smaller than GPT-3 and GPT-3.5, they\noutperform both traditional BERT-based approaches and these larger models. This\ndemonstrates the effectiveness of our fine-tuning process, showcasing how\nmodern LLMs can surpass the capabilities of older models like BERT while still\nbeing suitable for deployment in resource-constrained environments. Through\nintensive analyses, we explore various aspects of model performance, providing\nvaluable insights into adapting LLMs for low-resource languages like\nVietnamese. Our study contributes to the advancement of natural language\nprocessing in low-resource languages, and we make our fine-tuned models\npublicly available at: https:\/\/huggingface.co\/iaiuet.\n","date":"2025-03-23"}
{"id":"2503.18063","title":"Dynamic Task Vector Grouping for Efficient Multi-Task Prompt Tuning","abstract":"  Multi-task prompt tuning utilizes multiple high-resource source tasks to\nimprove performance on low-source target tasks. Existing approaches transfer\nthe soft prompt trained by combining all source tasks or a single\n``high-similar'' source task one-time-only. However, we find that the optimal\ntransfer performance often comes from a combination of source tasks, which is\nneither one nor all. Further, we find that the similarity between source and\ntarget tasks also changes dynamically during fine-tuning after transfering,\nmaking similarity calculation in the initiation stage inadequate. To address\nthese issues, we propose a method called Dynamic Task Vector Grouping (DTVG),\nwhose core ideas contain (1) measuring the task similarity with task vectors\ninstead of soft prompt, (2) grouping the optimal source task combination based\non two metrics: {\\it target similarity} and {\\it knowledge consistency}; (3)\ndynamically updating the combination in each iteration step. Extensive\nexperiments on the 26 NLP datasets under different settings demonstrate that\nDTVG effectively groups similar source tasks while reducing negative transfer,\nachieving the start-of-art performance.\n","date":"2025-03-23"}
{"id":"2503.18064","title":"Dynamic Allocation Hypernetwork with Adaptive Model Recalibration for\n  FCL","abstract":"  Federated continual learning (FCL) offers an emerging pattern to facilitate\nthe applicability of federated learning (FL) in real-world scenarios, where\ntasks evolve dynamically and asynchronously across clients, especially in\nmedical scenario. Existing server-side FCL methods in nature domain construct a\ncontinually learnable server model by client aggregation on all-involved tasks.\nHowever, they are challenged by: (1) Catastrophic forgetting for previously\nlearned tasks, leading to error accumulation in server model, making it\ndifficult to sustain comprehensive knowledge across all tasks. (2) Biased\noptimization due to asynchronous tasks handled across different clients,\nleading to the collision of optimization targets of different clients at the\nsame time steps. In this work, we take the first step to propose a novel\nserver-side FCL pattern in medical domain, Dynamic Allocation Hypernetwork with\nadaptive model recalibration (\\textbf{FedDAH}). It is to facilitate\ncollaborative learning under the distinct and dynamic task streams across\nclients. To alleviate the catastrophic forgetting, we propose a dynamic\nallocation hypernetwork (DAHyper) where a continually updated hypernetwork is\ndesigned to manage the mapping between task identities and their associated\nmodel parameters, enabling the dynamic allocation of the model across clients.\nFor the biased optimization, we introduce a novel adaptive model recalibration\n(AMR) to incorporate the candidate changes of historical models into current\nserver updates, and assign weights to identical tasks across different time\nsteps based on the similarity for continual optimization. Extensive experiments\non the AMOS dataset demonstrate the superiority of our FedDAH to other FCL\nmethods on sites with different task streams. The code is\navailable:https:\/\/github.com\/jinlab-imvr\/FedDAH.\n","date":"2025-03-23"}
{"id":"2503.18065","title":"Unseen from Seen: Rewriting Observation-Instruction Using Foundation\n  Models for Augmenting Vision-Language Navigation","abstract":"  Data scarcity is a long-standing challenge in the Vision-Language Navigation\n(VLN) field, which extremely hinders the generalization of agents to unseen\nenvironments. Previous works primarily rely on additional simulator data or\nweb-collected images\/videos to improve the generalization. However, the\nsimulator environments still face limited diversity, and the web-collected data\noften requires extensive labor to remove the noise. In this paper, we propose a\nRewriting-driven AugMentation (RAM) paradigm for VLN, which directly creates\nthe unseen observation-instruction pairs via rewriting human-annotated training\ndata. Benefiting from our rewriting mechanism, new observation-instruction can\nbe obtained in both simulator-free and labor-saving manners to promote\ngeneralization. Specifically, we first introduce Object-Enriched Observation\nRewriting, where we combine Vision-Language Models (VLMs) and Large Language\nModels (LLMs) to derive rewritten object-enriched scene descriptions, enabling\nobservation synthesis with diverse objects and spatial layouts via\nText-to-Image Generation Models (T2IMs). Then, we propose Observation-Contrast\nInstruction Rewriting, which generates observation-aligned rewritten\ninstructions by requiring LLMs to reason the difference between original and\nnew observations. We further develop a mixing-then-focusing training strategy\nwith a random observation cropping scheme, effectively enhancing data\ndistribution diversity while suppressing augmentation data noise during\ntraining. Experiments on both the discrete environments (R2R, REVERIE, and R4R\ndatasets) and continuous environments (R2R-CE dataset) show the superior\nperformance and impressive generalization ability of our method. Code is\navailable at https:\/\/github.com\/SaDil13\/VLN-RAM.\n","date":"2025-03-23"}
{"id":"2503.18066","title":"Accurate Peak Detection in Multimodal Optimization via Approximated\n  Landscape Learning","abstract":"  Detecting potential optimal peak areas and locating the accurate peaks in\nthese areas are two major challenges in Multimodal Optimization problems\n(MMOPs). To address them, much efforts have been spent on developing novel\nsearching operators, niching strategies and multi-objective problem\ntransformation pipelines. Though promising, existing approaches more or less\noverlook the potential usage of landscape knowledge. In this paper, we propose\na novel optimization framework tailored for MMOPs, termed as APDMMO, which\nfacilitates peak detection via fully leveraging the landscape knowledge and\nhence capable of providing strong optimization performance on MMOPs.\nSpecifically, we first design a novel surrogate landscape model which ensembles\na group of non-linear activation units to improve the regression accuracy on\ndiverse MMOPs. Then we propose a free-of-trial peak detection method which\nefficiently locates potential peak areas through back-propagation on the\nlearned surrogate landscape model. Based on the detected peak areas, we employ\nSEP-CMAES for local search within these areas in parallel to further improve\nthe accuracy of the found optima. Extensive benchmarking results demonstrate\nthat APDMMO outperforms several up-to-date baselines. Further ablation studies\nverify the effectiveness of the proposed novel designs. The source-code is\navailable at ~\\href{}{https:\/\/github.com\/GMC-DRL\/APDMMO}.\n","date":"2025-03-23"}
{"id":"2503.18067","title":"Self-Explaining Neural Networks for Business Process Monitoring","abstract":"  Tasks in Predictive Business Process Monitoring (PBPM), such as Next Activity\nPrediction, focus on generating useful business predictions from historical\ncase logs. Recently, Deep Learning methods, particularly sequence-to-sequence\nmodels like Long Short-Term Memory (LSTM), have become a dominant approach for\ntackling these tasks. However, to enhance model transparency, build trust in\nthe predictions, and gain a deeper understanding of business processes, it is\ncrucial to explain the decisions made by these models. Existing explainability\nmethods for PBPM decisions are typically *post-hoc*, meaning they provide\nexplanations only after the model has been trained. Unfortunately, these\npost-hoc approaches have shown to face various challenges, including lack of\nfaithfulness, high computational costs and a significant sensitivity to\nout-of-distribution samples. In this work, we introduce, to the best of our\nknowledge, the first *self-explaining neural network* architecture for\npredictive process monitoring. Our framework trains an LSTM model that not only\nprovides predictions but also outputs a concise explanation for each\nprediction, while adapting the optimization objective to improve the\nreliability of the explanation. We first demonstrate that incorporating\nexplainability into the training process does not hurt model performance, and\nin some cases, actually improves it. Additionally, we show that our method\noutperforms post-hoc approaches in terms of both the faithfulness of the\ngenerated explanations and substantial improvements in efficiency.\n","date":"2025-03-23"}
{"id":"2503.18069","title":"Long Is More Important Than Difficult for Training Reasoning Models","abstract":"  Difficult problems, which often result in long reasoning traces, are widely\nrecognized as key factors for enhancing the performance of reasoning models.\nHowever, such high-challenge problems are scarce, limiting the size of\navailable datasets. In this paper, we propose a simple method to decouple the\nreliance on problem difficulty. First, we empirically demonstrate that\nreasoning length, rather than problem difficulty, primarily influences the\nperformance of trained models. Second, we identify a scaling law on reasoning\nlength, showing that model performance increases in a log-linear fashion as the\nreasoning data length grows. Finally, we introduce a straightforward technique\nto generate reasoning data of arbitrary length, and show that synthesized data\nis effective for training reasoning models. After fine-tuning the\nQwen2.5-32B-Instruct language model on our Long1K dataset, we present our\nmodel, Long1K-32B, which achieves remarkable performance with only 1,000\ntraining samples, achieving 95.6\\% accuracy on MATH, and 71.1\\% on GPQA\noutperforming DeepSeek-R1-Distill-Qwen-32B. The model, code, and dataset are\nall open-sourced, available at https:\/\/huggingface.co\/ZTss\/LONG1.\n","date":"2025-03-23"}
{"id":"2503.18071","title":"Mind with Eyes: from Language Reasoning to Multimodal Reasoning","abstract":"  Language models have recently advanced into the realm of reasoning, yet it is\nthrough multimodal reasoning that we can fully unlock the potential to achieve\nmore comprehensive, human-like cognitive capabilities. This survey provides a\nsystematic overview of the recent multimodal reasoning approaches, categorizing\nthem into two levels: language-centric multimodal reasoning and collaborative\nmultimodal reasoning. The former encompasses one-pass visual perception and\nactive visual perception, where vision primarily serves a supporting role in\nlanguage reasoning. The latter involves action generation and state update\nwithin reasoning process, enabling a more dynamic interaction between\nmodalities. Furthermore, we analyze the technical evolution of these methods,\ndiscuss their inherent challenges, and introduce key benchmark tasks and\nevaluation metrics for assessing multimodal reasoning performance. Finally, we\nprovide insights into future research directions from the following two\nperspectives: (i) from visual-language reasoning to omnimodal reasoning and\n(ii) from multimodal reasoning to multimodal agents. This survey aims to\nprovide a structured overview that will inspire further advancements in\nmultimodal reasoning research.\n","date":"2025-03-23"}
{"id":"2503.18072","title":"On the effectiveness of LLMs for automatic grading of open-ended\n  questions in Spanish","abstract":"  Grading is a time-consuming and laborious task that educators must face. It\nis an important task since it provides feedback signals to learners, and it has\nbeen demonstrated that timely feedback improves the learning process. In recent\nyears, the irruption of LLMs has shed light on the effectiveness of automatic\ngrading. In this paper, we explore the performance of different LLMs and\nprompting techniques in automatically grading short-text answers to open-ended\nquestions. Unlike most of the literature, our study focuses on a use case where\nthe questions, answers, and prompts are all in Spanish. Experimental results\ncomparing automatic scores to those of human-expert evaluators show good\noutcomes in terms of accuracy, precision and consistency for advanced LLMs,\nboth open and proprietary. Results are notably sensitive to prompt styles,\nsuggesting biases toward certain words or content in the prompt. However, the\nbest combinations of models and prompt strategies, consistently surpasses an\naccuracy of 95% in a three-level grading task, which even rises up to more than\n98% when the it is simplified to a binary right or wrong rating problem, which\ndemonstrates the potential that LLMs have to implement this type of automation\nin education applications.\n","date":"2025-03-23"}
{"id":"2503.18073","title":"PanopticSplatting: End-to-End Panoptic Gaussian Splatting","abstract":"  Open-vocabulary panoptic reconstruction is a challenging task for\nsimultaneous scene reconstruction and understanding. Recently, methods have\nbeen proposed for 3D scene understanding based on Gaussian splatting. However,\nthese methods are multi-staged, suffering from the accumulated errors and the\ndependence of hand-designed components. To streamline the pipeline and achieve\nglobal optimization, we propose PanopticSplatting, an end-to-end system for\nopen-vocabulary panoptic reconstruction. Our method introduces query-guided\nGaussian segmentation with local cross attention, lifting 2D instance masks\nwithout cross-frame association in an end-to-end way. The local cross attention\nwithin view frustum effectively reduces the training memory, making our model\nmore accessible to large scenes with more Gaussians and objects. In addition,\nto address the challenge of noisy labels in 2D pseudo masks, we propose label\nblending to promote consistent 3D segmentation with less noisy floaters, as\nwell as label warping on 2D predictions which enhances multi-view coherence and\nsegmentation accuracy. Our method demonstrates strong performances in 3D scene\npanoptic reconstruction on the ScanNet-V2 and ScanNet++ datasets, compared with\nboth NeRF-based and Gaussian-based panoptic reconstruction methods. Moreover,\nPanopticSplatting can be easily generalized to numerous variants of Gaussian\nsplatting, and we demonstrate its robustness on different Gaussian base models.\n","date":"2025-03-23"}
{"id":"2503.18074","title":"WISE: A Framework for Gigapixel Whole-Slide-Image Lossless Compression","abstract":"  Whole-Slide Images (WSIs) have revolutionized medical analysis by presenting\nhigh-resolution images of the whole tissue slide. Despite avoiding the physical\nstorage of the slides, WSIs require considerable data volume, which makes the\nstorage and maintenance of WSI records costly and unsustainable. To this end,\nthis work presents the first investigation of lossless compression of WSI\nimages. Interestingly, we find that most existing compression methods fail to\ncompress the WSI images effectively. Furthermore, our analysis reveals that the\nfailure of existing compressors is mainly due to information irregularity in\nWSI images. To resolve this issue, we developed a simple yet effective lossless\ncompressor called WISE, specifically designed for WSI images. WISE employs a\nhierarchical encoding strategy to extract effective bits, reducing the entropy\nof the image and then adopting a dictionary-based method to handle the\nirregular frequency patterns. Through extensive experiments, we show that WISE\ncan effectively compress the gigapixel WSI images to 36 times on average and up\nto 136 times.\n","date":"2025-03-23"}
{"id":"2503.18076","title":"A Multi-Model Adaptation of Speculative Decoding for Classification","abstract":"  The current study introduces a novel adaptation of speculative decoding,\nrepurposed from generation to classification tasks. We propose a multi-model\nframework employing up to three lightweight worker models and a single, more\nrobust judge model analogous to draft models and target model, respectively, in\nspeculative decoding. The worker models, tasked with the bulk of the\ncomputation, independently predict discrete class labels for a given input.\nWhen majority worker models agree on a label, it is accepted as the final\nlabel, optimizing efficiency by bypassing the computationally expensive judge\nmodel. In cases of disagreement, the judge model intervenes to resolve the\nlabel. This approach minimizes redundant computation, leverages the redundancy\nof multiple workers for confidence, and confines the judge model's role to\nchallenging cases, offering a practical balance of efficiency and accuracy. Our\nanalysis suggests that smaller out of the box instruction\/chat finetuned worker\nmodels with 3 billion parameters (hereafter, 3B) demonstrate a level of\nalignment with judge models comparable to that of larger finetuned worker\nmodels with 7 billion parameters (hereafter, 7B) across both simple and higher\norder reasoning tasks. The top performing 3B worker model pair achieve an\nagreement rate of approximately 80-83% for sentiment and around 50-80% for\nsimilar ticket when compared to judge models. Additionally, 3B worker models\nprovide a speedup ranging from 2.8x to 9x relative to the judge models, while\n7B worker model combinations achieve a speedup ranging from 1.28x to 0.28x\n","date":"2025-03-23"}
{"id":"2503.18081","title":"Model-Guardian: Protecting against Data-Free Model Stealing Using\n  Gradient Representations and Deceptive Predictions","abstract":"  Model stealing attack is increasingly threatening the confidentiality of\nmachine learning models deployed in the cloud. Recent studies reveal that\nadversaries can exploit data synthesis techniques to steal machine learning\nmodels even in scenarios devoid of real data, leading to data-free model\nstealing attacks. Existing defenses against such attacks suffer from\nlimitations, including poor effectiveness, insufficient generalization ability,\nand low comprehensiveness. In response, this paper introduces a novel defense\nframework named Model-Guardian. Comprising two components, Data-Free Model\nStealing Detector (DFMS-Detector) and Deceptive Predictions (DPreds),\nModel-Guardian is designed to address the shortcomings of current defenses with\nthe help of the artifact properties of synthetic samples and gradient\nrepresentations of samples. Extensive experiments on seven prevalent data-free\nmodel stealing attacks showcase the effectiveness and superior generalization\nability of Model-Guardian, outperforming eleven defense methods and\nestablishing a new state-of-the-art performance. Notably, this work pioneers\nthe utilization of various GANs and diffusion models for generating highly\nrealistic query samples in attacks, with Model-Guardian demonstrating accurate\ndetection capabilities.\n","date":"2025-03-23"}
{"id":"2503.18082","title":"Vehicular Road Crack Detection with Deep Learning: A New Online\n  Benchmark for Comprehensive Evaluation of Existing Algorithms","abstract":"  In the emerging field of urban digital twins (UDTs), advancing intelligent\nroad inspection (IRI) vehicles with automatic road crack detection systems is\nessential for maintaining civil infrastructure. Over the past decade, deep\nlearning-based road crack detection methods have been developed to detect\ncracks more efficiently, accurately, and objectively, with the goal of\nreplacing manual visual inspection. Nonetheless, there is a lack of systematic\nreviews on state-of-the-art (SoTA) deep learning techniques, especially\ndata-fusion and label-efficient algorithms for this task. This paper thoroughly\nreviews the SoTA deep learning-based algorithms, including (1) supervised, (2)\nunsupervised, (3) semi-supervised, and (4) weakly-supervised methods developed\nfor road crack detection. Also, we create a dataset called UDTIRI-Crack,\ncomprising $2,500$ high-quality images from seven public annotated sources, as\nthe first extensive online benchmark in this field. Comprehensive experiments\nare conducted to compare the detection performance, computational efficiency,\nand generalizability of public SoTA deep learning-based algorithms for road\ncrack detection. In addition, the feasibility of foundation models and large\nlanguage models (LLMs) for road crack detection is explored. Afterwards, the\nexisting challenges and future development trends of deep learning-based road\ncrack detection algorithms are discussed. We believe this review can serve as\npractical guidance for developing intelligent road detection vehicles with the\nnext-generation road condition assessment systems. The released benchmark\nUDTIRI-Crack is available at https:\/\/udtiri.com\/submission\/.\n","date":"2025-03-23"}
{"id":"2503.18083","title":"Unified Geometry and Color Compression Framework for Point Clouds via\n  Generative Diffusion Priors","abstract":"  With the growth of 3D applications and the rapid increase in sensor-collected\n3D point cloud data, there is a rising demand for efficient compression\nalgorithms. Most existing learning-based compression methods handle geometry\nand color attributes separately, treating them as distinct tasks, making these\nmethods challenging to apply directly to point clouds with colors. Besides, the\nlimited capacities of training datasets also limit their generalizability\nacross points with different distributions. In this work, we introduce a\ntest-time unified geometry and color compression framework of 3D point clouds.\nInstead of training a compression model based on specific datasets, we adapt a\npre-trained generative diffusion model to compress original colored point\nclouds into sparse sets, termed 'seeds', using prompt tuning. Decompression is\nthen achieved through multiple denoising steps with separate sampling\nprocesses. Experiments on objects and indoor scenes demonstrate that our method\nhas superior performances compared to existing baselines for the compression of\ngeometry and color.\n","date":"2025-03-23"}
{"id":"2503.18085","title":"Temporal Relation Extraction in Clinical Texts: A Span-based Graph\n  Transformer Approach","abstract":"  Temporal information extraction from unstructured text is essential for\ncontextualizing events and deriving actionable insights, particularly in the\nmedical domain. We address the task of extracting clinical events and their\ntemporal relations using the well-studied I2B2 2012 Temporal Relations\nChallenge corpus. This task is inherently challenging due to complex clinical\nlanguage, long documents, and sparse annotations. We introduce GRAPHTREX, a\nnovel method integrating span-based entity-relation extraction, clinical large\npre-trained language models (LPLMs), and Heterogeneous Graph Transformers (HGT)\nto capture local and global dependencies. Our HGT component facilitates\ninformation propagation across the document through innovative global landmarks\nthat bridge distant entities. Our method improves the state-of-the-art with\n5.5% improvement in the tempeval $F_1$ score over the previous best and up to\n8.9% improvement on long-range relations, which presents a formidable\nchallenge. This work not only advances temporal information extraction but also\nlays the groundwork for improved diagnostic and prognostic models through\nenhanced temporal reasoning.\n","date":"2025-03-23"}
{"id":"2503.18087","title":"HyperNOs: Automated and Parallel Library for Neural Operators Research","abstract":"  This paper introduces HyperNOs, a PyTorch library designed to streamline and\nautomate the process of exploring neural operators, with a special focus on\nhyperparameter optimization for comprehensive and exhaustive exploration.\nIndeed, HyperNOs takes advantage of state-of-the-art optimization algorithms\nand parallel computing implemented in the Ray-tune library to efficiently\nexplore the hyperparameter space of neural operators. We also implement many\nuseful functionalities for studying neural operators with a user-friendly\ninterface, such as the possibility to train the model with a fixed number of\nparameters or to train the model with multiple datasets and different\nresolutions. We integrate Fourier neural operators and convolutional neural\noperators in our library, achieving state of the art results on many\nrepresentative benchmarks, demonstrating the capabilities of HyperNOs to handle\nreal datasets and modern architectures. The library is designed to be easy to\nuse with the provided model and datasets, but also to be easily extended to use\nnew datasets and custom neural operator architectures.\n","date":"2025-03-23"}
{"id":"2503.18089","title":"$D^2LoRA$: Data-Driven LoRA Initialization for Low Resource Tasks","abstract":"  Tuning large language models is essential for optimizing their performance\nacross diverse applications, particularly in scenarios with limited data\navailability. Tuning large language models in scarce data scenarios is crucial,\nparticularly given that the convergence speed of the LoRA method is lower than\nthat of full fine-tuning. In this paper, we present an analysis of\npost-training methods including Supervised Fine-Tuning (SFT), Direct Preference\nOptimization (DPO), and Odds Ratio Preference Optimization (ORPO) within the\ncontext of task-specific learning using the LoRA method. Next we introduce\n$D^2LoRA$, a data-driven approach for initializing LoRA metrics that enhances\ntraining efficiency, especially in limited-data settings. Our experiments\ncompare $D^2LoRA$ with vanilla LoRA in terms of performance and catastrophic\nforgetting under extremely data-constrained conditions. The results demonstrate\nthat $D^2LoRA$ achieves a 1% improvement GSM8K benchmark and a 2-point\nimprovement in ROUGE score in title generation tasks. $D^2LoRA$ facilitates the\nadaptation of LLMs to multiple tasks even when task-specific data is scarce,\nthereby reducing training expenses and offering data cost.\n","date":"2025-03-23"}
{"id":"2503.18094","title":"Anomize: Better Open Vocabulary Video Anomaly Detection","abstract":"  Open Vocabulary Video Anomaly Detection (OVVAD) seeks to detect and classify\nboth base and novel anomalies. However, existing methods face two specific\nchallenges related to novel anomalies. The first challenge is detection\nambiguity, where the model struggles to assign accurate anomaly scores to\nunfamiliar anomalies. The second challenge is categorization confusion, where\nnovel anomalies are often misclassified as visually similar base instances. To\naddress these challenges, we explore supplementary information from multiple\nsources to mitigate detection ambiguity by leveraging multiple levels of visual\ndata alongside matching textual information. Furthermore, we propose\nincorporating label relations to guide the encoding of new labels, thereby\nimproving alignment between novel videos and their corresponding labels, which\nhelps reduce categorization confusion. The resulting Anomize framework\neffectively tackles these issues, achieving superior performance on UCF-Crime\nand XD-Violence datasets, demonstrating its effectiveness in OVVAD.\n","date":"2025-03-23"}
{"id":"2503.18095","title":"Clarifying Misconceptions in COVID-19 Vaccine Sentiment and Stance\n  Analysis and Their Implications for Vaccine Hesitancy Mitigation: A\n  Systematic Review","abstract":"  Background Advances in machine learning (ML) models have increased the\ncapability of researchers to detect vaccine hesitancy in social media using\nNatural Language Processing (NLP). A considerable volume of research has\nidentified the persistence of COVID-19 vaccine hesitancy in discourse shared on\nvarious social media platforms. Methods Our objective in this study was to\nconduct a systematic review of research employing sentiment analysis or stance\ndetection to study discourse towards COVID-19 vaccines and vaccination spread\non Twitter (officially known as X since 2023). Following registration in the\nPROSPERO international registry of systematic reviews, we searched papers\npublished from 1 January 2020 to 31 December 2023 that used supervised machine\nlearning to assess COVID-19 vaccine hesitancy through stance detection or\nsentiment analysis on Twitter. We categorized the studies according to a\ntaxonomy of five dimensions: tweet sample selection approach, self-reported\nstudy type, classification typology, annotation codebook definitions, and\ninterpretation of results. We analyzed if studies using stance detection report\ndifferent hesitancy trends than those using sentiment analysis by examining how\nCOVID-19 vaccine hesitancy is measured, and whether efforts were made to avoid\nmeasurement bias. Results Our review found that measurement bias is widely\nprevalent in studies employing supervised machine learning to analyze sentiment\nand stance toward COVID-19 vaccines and vaccination. The reporting errors are\nsufficiently serious that they hinder the generalisability and interpretation\nof these studies to understanding whether individual opinions communicate\nreluctance to vaccinate against SARS-CoV-2. Conclusion Improving the reporting\nof NLP methods is crucial to addressing knowledge gaps in vaccine hesitancy\ndiscourse.\n","date":"2025-03-23"}
{"id":"2503.18096","title":"Informer in Algorithmic Investment Strategies on High Frequency Bitcoin\n  Data","abstract":"  The article investigates the usage of Informer architecture for building\nautomated trading strategies for high frequency Bitcoin data. Three strategies\nusing Informer model with different loss functions: Root Mean Squared Error\n(RMSE), Generalized Mean Absolute Directional Loss (GMADL) and Quantile loss,\nare proposed and evaluated against the Buy and Hold benchmark and two benchmark\nstrategies based on technical indicators. The evaluation is conducted using\ndata of various frequencies: 5 minute, 15 minute, and 30 minute intervals, over\nthe 6 different periods. Although the Informer-based model with Quantile loss\ndid not outperform the benchmark, two other models achieved better results. The\nperformance of the model using RMSE loss worsens when used with higher\nfrequency data while the model that uses novel GMADL loss function is\nbenefiting from higher frequency data and when trained on 5 minute interval it\nbeat all the other strategies on most of the testing periods. The primary\ncontribution of this study is the application and assessment of the RMSE,\nGMADL, and Quantile loss functions with the Informer model to forecast future\nreturns, subsequently using these forecasts to develop automated trading\nstrategies. The research provides evidence that employing an Informer model\ntrained with the GMADL loss function can result in superior trading outcomes\ncompared to the buy-and-hold approach.\n","date":"2025-03-23"}
{"id":"2503.18100","title":"M3Net: Multimodal Multi-task Learning for 3D Detection, Segmentation,\n  and Occupancy Prediction in Autonomous Driving","abstract":"  The perception system for autonomous driving generally requires to handle\nmultiple diverse sub-tasks. However, current algorithms typically tackle\nindividual sub-tasks separately, which leads to low efficiency when aiming at\nobtaining full-perception results. Some multi-task learning methods try to\nunify multiple tasks with one model, but do not solve the conflicts in\nmulti-task learning. In this paper, we introduce M3Net, a novel multimodal and\nmulti-task network that simultaneously tackles detection, segmentation, and 3D\noccupancy prediction for autonomous driving and achieves superior performance\nthan single task model. M3Net takes multimodal data as input and multiple tasks\nvia query-token interactions. To enhance the integration of multi-modal\nfeatures for multi-task learning, we first propose the Modality-Adaptive\nFeature Integration (MAFI) module, which enables single-modality features to\npredict channel-wise attention weights for their high-performing tasks,\nrespectively. Based on integrated features, we then develop task-specific query\ninitialization strategies to accommodate the needs of detection\/segmentation\nand 3D occupancy prediction. Leveraging the properly initialized queries, a\nshared decoder transforms queries and BEV features layer-wise, facilitating\nmulti-task learning. Furthermore, we propose a Task-oriented Channel Scaling\n(TCS) module in the decoder to mitigate conflicts between optimizing for\ndifferent tasks. Additionally, our proposed multi-task querying and TCS module\nsupport both Transformer-based decoder and Mamba-based decoder, demonstrating\nits flexibility to different architectures. M3Net achieves state-of-the-art\nmulti-task learning performance on the nuScenes benchmarks.\n","date":"2025-03-23"}
{"id":"2503.18102","title":"AgentRxiv: Towards Collaborative Autonomous Research","abstract":"  Progress in scientific discovery is rarely the result of a single \"Eureka\"\nmoment, but is rather the product of hundreds of scientists incrementally\nworking together toward a common goal. While existing agent workflows are\ncapable of producing research autonomously, they do so in isolation, without\nthe ability to continuously improve upon prior research results. To address\nthese challenges, we introduce AgentRxiv-a framework that lets LLM agent\nlaboratories upload and retrieve reports from a shared preprint server in order\nto collaborate, share insights, and iteratively build on each other's research.\nWe task agent laboratories to develop new reasoning and prompting techniques\nand find that agents with access to their prior research achieve higher\nperformance improvements compared to agents operating in isolation (11.4%\nrelative improvement over baseline on MATH-500). We find that the best\nperforming strategy generalizes to benchmarks in other domains (improving on\naverage by 3.3%). Multiple agent laboratories sharing research through\nAgentRxiv are able to work together towards a common goal, progressing more\nrapidly than isolated laboratories, achieving higher overall accuracy (13.7%\nrelative improvement over baseline on MATH-500). These findings suggest that\nautonomous agents may play a role in designing future AI systems alongside\nhumans. We hope that AgentRxiv allows agents to collaborate toward research\ngoals and enables researchers to accelerate discovery.\n","date":"2025-03-23"}
{"id":"2503.18107","title":"PanoGS: Gaussian-based Panoptic Segmentation for 3D Open Vocabulary\n  Scene Understanding","abstract":"  Recently, 3D Gaussian Splatting (3DGS) has shown encouraging performance for\nopen vocabulary scene understanding tasks. However, previous methods cannot\ndistinguish 3D instance-level information, which usually predicts a heatmap\nbetween the scene feature and text query. In this paper, we propose PanoGS, a\nnovel and effective 3D panoptic open vocabulary scene understanding approach.\nTechnically, to learn accurate 3D language features that can scale to large\nindoor scenarios, we adopt the pyramid tri-plane to model the latent continuous\nparametric feature space and use a 3D feature decoder to regress the multi-view\nfused 2D feature cloud. Besides, we propose language-guided graph cuts that\nsynergistically leverage reconstructed geometry and learned language cues to\ngroup 3D Gaussian primitives into a set of super-primitives. To obtain 3D\nconsistent instance, we perform graph clustering based segmentation with\nSAM-guided edge affinity computation between different super-primitives.\nExtensive experiments on widely used datasets show better or more competitive\nperformance on 3D panoptic open vocabulary scene understanding. Project page:\n\\href{https:\/\/zju3dv.github.io\/panogs}{https:\/\/zju3dv.github.io\/panogs}.\n","date":"2025-03-23"}
{"id":"2503.18108","title":"Unraveling the Effects of Synthetic Data on End-to-End Autonomous\n  Driving","abstract":"  End-to-end (E2E) autonomous driving (AD) models require diverse, high-quality\ndata to perform well across various driving scenarios. However, collecting\nlarge-scale real-world data is expensive and time-consuming, making\nhigh-fidelity synthetic data essential for enhancing data diversity and model\nrobustness. Existing driving simulators for synthetic data generation have\nsignificant limitations: game-engine-based simulators struggle to produce\nrealistic sensor data, while NeRF-based and diffusion-based methods face\nefficiency challenges. Additionally, recent simulators designed for closed-loop\nevaluation provide limited interaction with other vehicles, failing to simulate\ncomplex real-world traffic dynamics. To address these issues, we introduce\nSceneCrafter, a realistic, interactive, and efficient AD simulator based on 3D\nGaussian Splatting (3DGS). SceneCrafter not only efficiently generates\nrealistic driving logs across diverse traffic scenarios but also enables robust\nclosed-loop evaluation of end-to-end models. Experimental results demonstrate\nthat SceneCrafter serves as both a reliable evaluation platform and a efficient\ndata generator that significantly improves end-to-end model generalization.\n","date":"2025-03-23"}
{"id":"2503.18114","title":"Feature Learning beyond the Lazy-Rich Dichotomy: Insights from\n  Representational Geometry","abstract":"  The ability to integrate task-relevant information into neural\nrepresentations is a fundamental aspect of both biological and artificial\nintelligence. To enable theoretical analysis, recent work has examined whether\na network learns task-relevant features (rich learning) or resembles a random\nfeature model (or a kernel machine, i.e., lazy learning). However, this simple\nlazy-versus-rich dichotomy overlooks the possibility of various subtypes of\nfeature learning that emerge from different architectures, learning rules, and\ndata properties. Furthermore, most existing approaches emphasize weight\nmatrices or neural tangent kernels, limiting their applicability to\nneuroscience because they do not explicitly characterize representations.\n  In this work, we introduce an analysis framework based on representational\ngeometry to study feature learning. Instead of analyzing what are the learned\nfeatures, we focus on characterizing how task-relevant representational\nmanifolds evolve during the learning process. In both theory and experiment, we\nfind that when a network learns features useful for solving a task, the\ntask-relevant manifolds become increasingly untangled. Moreover, by tracking\nchanges in the underlying manifold geometry, we uncover distinct learning\nstages throughout training, as well as different learning strategies associated\nwith training hyperparameters, uncovering subtypes of feature learning beyond\nthe lazy-versus-rich dichotomy. Applying our method to neuroscience and machine\nlearning, we gain geometric insights into the structural inductive biases of\nneural circuits solving cognitive tasks and the mechanisms underlying\nout-of-distribution generalization in image classification. Our framework\nprovides a novel geometric perspective for understanding and quantifying\nfeature learning in both artificial and biological neural networks.\n","date":"2025-03-23"}
{"id":"2503.18117","title":"Detection of Somali-written Fake News and Toxic Messages on the Social\n  Media Using Transformer-based Language Models","abstract":"  The fact that everyone with a social media account can create and share\ncontent, and the increasing public reliance on social media platforms as a news\nand information source bring about significant challenges such as\nmisinformation, fake news, harmful content, etc. Although human content\nmoderation may be useful to an extent and used by these platforms to flag\nposted materials, the use of AI models provides a more sustainable, scalable,\nand effective way to mitigate these harmful contents. However, low-resourced\nlanguages such as the Somali language face limitations in AI automation,\nincluding scarce annotated training datasets and lack of language models\ntailored to their unique linguistic characteristics. This paper presents part\nof our ongoing research work to bridge some of these gaps for the Somali\nlanguage. In particular, we created two human-annotated social-media-sourced\nSomali datasets for two downstream applications, fake news \\& toxicity\nclassification, and developed a transformer-based monolingual Somali language\nmodel (named SomBERTa) -- the first of its kind to the best of our knowledge.\nSomBERTa is then fine-tuned and evaluated on toxic content, fake news and news\ntopic classification datasets. Comparative evaluation analysis of the proposed\nmodel against related multilingual models (e.g., AfriBERTa, AfroXLMR, etc)\ndemonstrated that SomBERTa consistently outperformed these comparators in both\nfake news and toxic content classification tasks while achieving the best\naverage accuracy (87.99%) across all tasks. This research contributes to Somali\nNLP by offering a foundational language model and a replicable framework for\nother low-resource languages, promoting digital and AI inclusivity and\nlinguistic diversity.\n","date":"2025-03-23"}
{"id":"2503.18123","title":"End-to-End Implicit Neural Representations for Classification","abstract":"  Implicit neural representations (INRs) such as NeRF and SIREN encode a signal\nin neural network parameters and show excellent results for signal\nreconstruction. Using INRs for downstream tasks, such as classification, is\nhowever not straightforward. Inherent symmetries in the parameters pose\nchallenges and current works primarily focus on designing architectures that\nare equivariant to these symmetries. However, INR-based classification still\nsignificantly under-performs compared to pixel-based methods like CNNs. This\nwork presents an end-to-end strategy for initializing SIRENs together with a\nlearned learning-rate scheme, to yield representations that improve\nclassification accuracy. We show that a simple, straightforward, Transformer\nmodel applied to a meta-learned SIREN, without incorporating explicit symmetry\nequivariances, outperforms the current state-of-the-art. On the CIFAR-10 SIREN\nclassification task, we improve the state-of-the-art without augmentations from\n38.8% to 59.6%, and from 63.4% to 64.7% with augmentations. We demonstrate\nscalability on the high-resolution Imagenette dataset achieving reasonable\nreconstruction quality with a classification accuracy of 60.8% and are the\nfirst to do INR classification on the full ImageNet-1K dataset where we achieve\na SIREN classification performance of 23.6%. To the best of our knowledge, no\nother SIREN classification approach has managed to set a classification\nbaseline for any high-resolution image dataset. Our code is available at\nhttps:\/\/github.com\/SanderGielisse\/MWT\n","date":"2025-03-23"}
{"id":"2503.18129","title":"GeoBenchX: Benchmarking LLMs for Multistep Geospatial Tasks","abstract":"  In this paper, we establish a benchmark for evaluating large language models\n(LLMs) on multi-step geospatial tasks relevant to commercial GIS practitioners.\nWe assess seven leading commercial LLMs (Sonnet 3.5 and 3.7, Haiku 3.5, Gemini\n2.0, GPT-4o, GPT-4o mini, and o3-mini) using a simple tool-calling agent\nequipped with 23 geospatial functions. Our benchmark comprises tasks across\nfour categories of increasing complexity, with both solvable and intentionally\nunsolvable tasks to test hallucination rejection. We develop an LLM-as-Judge\nevaluation framework to compare agent solutions against reference\nimplementations. Results show Sonnet 3.5 and GPT-4o achieve the best overall\nperformance, with Claude models excelling on solvable tasks while OpenAI models\nbetter identify unsolvable scenarios. We observe significant differences in\ntoken usage, with Anthropic models consuming substantially more tokens than\ncompetitors. Common errors include misunderstanding geometrical relationships,\nrelying on outdated knowledge, and inefficient data manipulation. The resulting\nbenchmark set, evaluation framework, and data generation pipeline are released\nas open-source resources, providing one more standardized method for ongoing\nevaluation of LLMs for GeoAI.\n","date":"2025-03-23"}
{"id":"2503.18130","title":"Mitigating Reward Over-Optimization in RLHF via Behavior-Supported\n  Regularization","abstract":"  Reinforcement learning from human feedback (RLHF) is an effective method for\naligning large language models (LLMs) with human values. However, reward\nover-optimization remains an open challenge leading to discrepancies between\nthe performance of LLMs under the reward model and the true human objectives. A\nprimary contributor to reward over-optimization is the extrapolation error that\narises when the reward model evaluates out-of-distribution (OOD) responses.\nHowever, current methods still fail to prevent the increasing frequency of OOD\nresponse generation during the reinforcement learning (RL) process and are not\neffective at handling extrapolation errors from OOD responses. In this work, we\npropose the Behavior-Supported Policy Optimization (BSPO) method to mitigate\nthe reward over-optimization issue. Specifically, we define behavior policy as\nthe next token distribution of the reward training dataset to model the\nin-distribution (ID) region of the reward model. Building on this, we introduce\nthe behavior-supported Bellman operator to regularize the value function,\npenalizing all OOD values without impacting the ID ones. Consequently, BSPO\nreduces the generation of OOD responses during the RL process, thereby avoiding\noverestimation caused by the reward model's extrapolation errors.\nTheoretically, we prove that BSPO guarantees a monotonic improvement of the\nsupported policy until convergence to the optimal behavior-supported policy.\nEmpirical results from extensive experiments show that BSPO outperforms\nbaselines in preventing reward over-optimization due to OOD evaluation and\nfinding the optimal ID policy.\n","date":"2025-03-23"}
{"id":"2503.18132","title":"MathAgent: Leveraging a Mixture-of-Math-Agent Framework for Real-World\n  Multimodal Mathematical Error Detection","abstract":"  Mathematical error detection in educational settings presents a significant\nchallenge for Multimodal Large Language Models (MLLMs), requiring a\nsophisticated understanding of both visual and textual mathematical content\nalong with complex reasoning capabilities. Though effective in mathematical\nproblem-solving, MLLMs often struggle with the nuanced task of identifying and\ncategorizing student errors in multimodal mathematical contexts. Therefore, we\nintroduce MathAgent, a novel Mixture-of-Math-Agent framework designed\nspecifically to address these challenges. Our approach decomposes error\ndetection into three phases, each handled by a specialized agent: an image-text\nconsistency validator, a visual semantic interpreter, and an integrative error\nanalyzer. This architecture enables more accurate processing of mathematical\ncontent by explicitly modeling relationships between multimodal problems and\nstudent solution steps. We evaluate MathAgent on real-world educational data,\ndemonstrating approximately 5% higher accuracy in error step identification and\n3% improvement in error categorization compared to baseline models. Besides,\nMathAgent has been successfully deployed in an educational platform that has\nserved over one million K-12 students, achieving nearly 90% student\nsatisfaction while generating significant cost savings by reducing manual error\ndetection.\n","date":"2025-03-23"}
{"id":"2503.18134","title":"An Image-like Diffusion Method for Human-Object Interaction Detection","abstract":"  Human-object interaction (HOI) detection often faces high levels of ambiguity\nand indeterminacy, as the same interaction can appear vastly different across\ndifferent human-object pairs. Additionally, the indeterminacy can be further\nexacerbated by issues such as occlusions and cluttered backgrounds. To handle\nsuch a challenging task, in this work, we begin with a key observation: the\noutput of HOI detection for each human-object pair can be recast as an image.\nThus, inspired by the strong image generation capabilities of image diffusion\nmodels, we propose a new framework, HOI-IDiff. In HOI-IDiff, we tackle HOI\ndetection from a novel perspective, using an Image-like Diffusion process to\ngenerate HOI detection outputs as images. Furthermore, recognizing that our\nrecast images differ in certain properties from natural images, we enhance our\nframework with a customized HOI diffusion process and a slice patchification\nmodel architecture, which are specifically tailored to generate our recast\n``HOI images''. Extensive experiments demonstrate the efficacy of our\nframework.\n","date":"2025-03-23"}
{"id":"2503.18135","title":"MLLM-For3D: Adapting Multimodal Large Language Model for 3D Reasoning\n  Segmentation","abstract":"  Reasoning segmentation aims to segment target objects in complex scenes based\non human intent and spatial reasoning. While recent multimodal large language\nmodels (MLLMs) have demonstrated impressive 2D image reasoning segmentation,\nadapting these capabilities to 3D scenes remains underexplored. In this paper,\nwe introduce MLLM-For3D, a simple yet effective framework that transfers\nknowledge from 2D MLLMs to 3D scene understanding. Specifically, we utilize\nMLLMs to generate multi-view pseudo segmentation masks and corresponding text\nembeddings, then unproject 2D masks into 3D space and align them with the text\nembeddings. The primary challenge lies in the absence of 3D context and spatial\nconsistency across multiple views, causing the model to hallucinate objects\nthat do not exist and fail to target objects consistently. Training the 3D\nmodel with such irrelevant objects leads to performance degradation. To address\nthis, we introduce a spatial consistency strategy to enforce that segmentation\nmasks remain coherent in the 3D space, effectively capturing the geometry of\nthe scene. Moreover, we develop a Token-for-Query approach for multimodal\nsemantic alignment, enabling consistent identification of the same object\nacross different views. Extensive evaluations on various challenging indoor\nscene benchmarks demonstrate that, even without any labeled 3D training data,\nMLLM-For3D outperforms existing 3D reasoning segmentation methods, effectively\ninterpreting user intent, understanding 3D scenes, and reasoning about spatial\nrelationships.\n","date":"2025-03-23"}
{"id":"2503.18137","title":"TCFG: Tangential Damping Classifier-free Guidance","abstract":"  Diffusion models have achieved remarkable success in text-to-image synthesis,\nlargely attributed to the use of classifier-free guidance (CFG), which enables\nhigh-quality, condition-aligned image generation. CFG combines the conditional\nscore (e.g., text-conditioned) with the unconditional score to control the\noutput. However, the unconditional score is in charge of estimating the\ntransition between manifolds of adjacent timesteps from $x_t$ to $x_{t-1}$,\nwhich may inadvertently interfere with the trajectory toward the specific\ncondition. In this work, we introduce a novel approach that leverages a\ngeometric perspective on the unconditional score to enhance CFG performance\nwhen conditional scores are available. Specifically, we propose a method that\nfilters the singular vectors of both conditional and unconditional scores using\nsingular value decomposition. This filtering process aligns the unconditional\nscore with the conditional score, thereby refining the sampling trajectory to\nstay closer to the manifold. Our approach improves image quality with\nnegligible additional computation. We provide deeper insights into the score\nfunction behavior in diffusion models and present a practical technique for\nachieving more accurate and contextually coherent image synthesis.\n","date":"2025-03-23"}
{"id":"2503.18138","title":"Machine learning based animal emotion classification using audio signals","abstract":"  This paper presents the machine learning approach to the automated\nclassification of a dog's emotional state based on the processing and\nrecognition of audio signals. It offers helpful information for improving\nhuman-machine interfaces and developing more precise tools for classifying\nemotions from acoustic data. The presented model demonstrates an overall\naccuracy value above 70% for audio signals recorded for one dog.\n","date":"2025-03-23"}
{"id":"2503.18141","title":"AGIR: Assessing 3D Gait Impairment with Reasoning based on LLMs","abstract":"  Assessing gait impairment plays an important role in early diagnosis, disease\nmonitoring, and treatment evaluation for neurodegenerative diseases. Despite\nits widespread use in clinical practice, it is limited by subjectivity and a\nlack of precision. While recent deep learning-based approaches have\nconsistently improved classification accuracies, they often lack\ninterpretability, hindering their utility in clinical decision-making. To\novercome these challenges, we introduce AGIR, a novel pipeline consisting of a\npre-trained VQ-VAE motion tokenizer and a subsequent Large Language Model (LLM)\nfine-tuned over pairs of motion tokens and Chain-of-Thought (CoT) reasonings.\nTo fine-tune an LLM for pathological gait analysis, we first introduce a\nmultimodal dataset by adding rationales dedicated to MDS-UPDRS gait score\nassessment to an existing PD gait dataset. We then introduce a two-stage\nsupervised fine-tuning (SFT) strategy to enhance the LLM's motion comprehension\nwith pathology-specific knowledge. This strategy includes: 1) a generative\nstage that aligns gait motions with analytic descriptions through bidirectional\nmotion-description generation, 2) a reasoning stage that integrates logical\nChain-of-Thought (CoT) reasoning for impairment assessment with UPDRS gait\nscore. Validation on an existing dataset and comparisons with state-of-the-art\nmethods confirm the robustness and accuracy of our pipeline, demonstrating its\nability to assign gait impairment scores from motion input with clinically\nmeaningful rationales.\n","date":"2025-03-23"}
{"id":"2503.18142","title":"LocDiffusion: Identifying Locations on Earth by Diffusing in the Hilbert\n  Space","abstract":"  Image geolocalization is a fundamental yet challenging task, aiming at\ninferring the geolocation on Earth where an image is taken. Existing methods\napproach it either via grid-based classification or via image retrieval. Their\nperformance significantly suffers when the spatial distribution of test images\ndoes not align with such choices. To address these limitations, we propose to\nleverage diffusion as a mechanism for image geolocalization. To avoid the\nproblematic manifold reprojection step in diffusion, we developed a novel\nspherical positional encoding-decoding framework, which encodes points on a\nspherical surface (e.g., geolocations on Earth) into a Hilbert space of\nSpherical Harmonics coefficients and decodes points (geolocations) by\nmode-seeking. We call this type of position encoding Spherical Harmonics Dirac\nDelta (SHDD) Representation. We also propose a novel SirenNet-based\narchitecture called CS-UNet to learn the conditional backward process in the\nlatent SHDD space by minimizing a latent KL-divergence loss. We train a\nconditional latent diffusion model called LocDiffusion that generates\ngeolocations under the guidance of images -- to the best of our knowledge, the\nfirst generative model for image geolocalization by diffusing geolocation\ninformation in a hidden location embedding space. We evaluate our method\nagainst SOTA image geolocalization baselines. LocDiffusion achieves competitive\ngeolocalization performance and demonstrates significantly stronger\ngeneralizability to unseen geolocations.\n","date":"2025-03-23"}
{"id":"2503.18147","title":"PHT-CAD: Efficient CAD Parametric Primitive Analysis with Progressive\n  Hierarchical Tuning","abstract":"  Computer-Aided Design (CAD) plays a pivotal role in industrial manufacturing,\nyet 2D Parametric Primitive Analysis (PPA) remains underexplored due to two key\nchallenges: structural constraint reasoning and advanced semantic\nunderstanding. To tackle these challenges, we first propose an Efficient Hybrid\nParametrization (EHP) for better representing 2D engineering drawings. EHP\ncontains four types of atomic component i.e., point, line, circle, and arc).\nAdditionally, we propose PHT-CAD, a novel 2D PPA framework that harnesses the\nmodality alignment and reasoning capabilities of Vision-Language Models (VLMs)\nfor precise engineering drawing analysis. In PHT-CAD, we introduce four\ndedicated regression heads to predict corresponding atomic components. To train\nPHT-CAD, a three-stage training paradigm Progressive Hierarchical Tuning (PHT)\nis proposed to progressively enhance PHT-CAD's capability to perceive\nindividual primitives, infer structural constraints, and align annotation\nlayers with their corresponding geometric representations. Considering that\nexisting datasets lack complete annotation layers and real-world engineering\ndrawings, we introduce ParaCAD, the first large-scale benchmark that explicitly\nintegrates both the geometric and annotation layers. ParaCAD comprises over 10\nmillion annotated drawings for training and 3,000 real-world industrial\ndrawings with complex topological structures and physical constraints for test.\nExtensive experiments demonstrate the effectiveness of PHT-CAD and highlight\nthe practical significance of ParaCAD in advancing 2D PPA research.\n","date":"2025-03-23"}
{"id":"2503.18150","title":"LongDiff: Training-Free Long Video Generation in One Go","abstract":"  Video diffusion models have recently achieved remarkable results in video\ngeneration. Despite their encouraging performance, most of these models are\nmainly designed and trained for short video generation, leading to challenges\nin maintaining temporal consistency and visual details in long video\ngeneration. In this paper, we propose LongDiff, a novel training-free method\nconsisting of carefully designed components \\ -- Position Mapping (PM) and\nInformative Frame Selection (IFS) \\ -- to tackle two key challenges that hinder\nshort-to-long video generation generalization: temporal position ambiguity and\ninformation dilution. Our LongDiff unlocks the potential of off-the-shelf video\ndiffusion models to achieve high-quality long video generation in one go.\nExtensive experiments demonstrate the efficacy of our method.\n","date":"2025-03-23"}
{"id":"2503.18151","title":"Efficient Deep Learning Approaches for Processing Ultra-Widefield\n  Retinal Imaging","abstract":"  Deep learning has emerged as the predominant solution for classifying medical\nimages. We intend to apply these developments to the ultra-widefield (UWF)\nretinal imaging dataset. Since UWF images can accurately diagnose various\nretina diseases, it is very important to clas sify them accurately and prevent\nthem with early treatment. However, processing images manually is\ntime-consuming and labor-intensive, and there are two challenges to automating\nthis process. First, high perfor mance usually requires high computational\nresources. Artificial intelli gence medical technology is better suited for\nplaces with limited medical resources, but using high-performance processing\nunits in such environ ments is challenging. Second, the problem of the accuracy\nof colour fun dus photography (CFP) methods. In general, the UWF method\nprovides more information for retinal diagnosis than the CFP method, but most\nof the research has been conducted based on the CFP method. Thus, we\ndemonstrate that these problems can be efficiently addressed in low performance\nunits using methods such as strategic data augmentation and model ensembles,\nwhich balance performance and computational re sources while utilizing UWF\nimages.\n","date":"2025-03-23"}
{"id":"2503.18155","title":"Decorum: A Language-Based Approach For Style-Conditioned Synthesis of\n  Indoor 3D Scenes","abstract":"  3D indoor scene generation is an important problem for the design of digital\nand real-world environments. To automate this process, a scene generation model\nshould be able to not only generate plausible scene layouts, but also take into\nconsideration visual features and style preferences. Existing methods for this\ntask exhibit very limited control over these attributes, only allowing text\ninputs in the form of simple object-level descriptions or pairwise spatial\nrelationships. Our proposed method Decorum enables users to control the scene\ngeneration process with natural language by adopting language-based\nrepresentations at each stage. This enables us to harness recent advancements\nin Large Language Models (LLMs) to model language-to-language mappings. In\naddition, we show that using a text-based representation allows us to select\nfurniture for our scenes using a novel object retrieval method based on\nmultimodal LLMs. Evaluations on the benchmark 3D-FRONT dataset show that our\nmethods achieve improvements over existing work in text-conditioned scene\nsynthesis and object retrieval.\n","date":"2025-03-23"}
{"id":"2503.18156","title":"Adoption of Watermarking for Generative AI Systems in Practice and\n  Implications under the new EU AI Act","abstract":"  AI-generated images have become so good in recent years that individuals\ncannot distinguish them any more from \"real\" images. This development creates a\nseries of societal risks, and challenges our perception of what is true and\nwhat is not, particularly with the emergence of \"deep fakes\" that impersonate\nreal individuals. Watermarking, a technique that involves embedding identifying\ninformation within images to indicate their AI-generated nature, has emerged as\na primary mechanism to address the risks posed by AI-generated images. The\nimplementation of watermarking techniques is now becoming a legal requirement\nin many jurisdictions, including under the new 2024 EU AI Act. Despite the\nwidespread use of AI image generation systems, the current status of\nwatermarking implementation remains largely unexamined. Moreover, the practical\nimplications of the AI Act's watermarking requirements have not previously been\nstudied. The present paper therefore both provides an empirical analysis of 50\nof the most widely used AI systems for image generation, and embeds this\nempirical analysis into a legal analysis of the AI Act. We identify four\ncategories of generative AI image systems relevant under the AI Act, outline\nthe legal obligations for each category, and find that only a minority number\nof providers currently implement adequate watermarking practices.\n","date":"2025-03-23"}
{"id":"2503.18159","title":"DiffusionTalker: Efficient and Compact Speech-Driven 3D Talking Head via\n  Personalizer-Guided Distillation","abstract":"  Real-time speech-driven 3D facial animation has been attractive in academia\nand industry. Traditional methods mainly focus on learning a deterministic\nmapping from speech to animation. Recent approaches start to consider the\nnondeterministic fact of speech-driven 3D face animation and employ the\ndiffusion model for the task. Existing diffusion-based methods can improve the\ndiversity of facial animation. However, personalized speaking styles conveying\naccurate lip language is still lacking, besides, efficiency and compactness\nstill need to be improved. In this work, we propose DiffusionTalker to address\nthe above limitations via personalizer-guided distillation. In terms of\npersonalization, we introduce a contrastive personalizer that learns identity\nand emotion embeddings to capture speaking styles from audio. We further\npropose a personalizer enhancer during distillation to enhance the influence of\nembeddings on facial animation. For efficiency, we use iterative distillation\nto reduce the steps required for animation generation and achieve more than 8x\nspeedup in inference. To achieve compactness, we distill the large teacher\nmodel into a smaller student model, reducing our model's storage by 86.4\\%\nwhile minimizing performance loss. After distillation, users can derive their\nidentity and emotion embeddings from audio to quickly create personalized\nanimations that reflect specific speaking styles. Extensive experiments are\nconducted to demonstrate that our method outperforms state-of-the-art methods.\nThe code will be released at: https:\/\/github.com\/ChenVoid\/DiffusionTalker.\n","date":"2025-03-23"}
{"id":"2503.18160","title":"MAO: Efficient Model-Agnostic Optimization of Prompt Tuning for\n  Vision-Language Models","abstract":"  Though CLIP-based prompt tuning significantly enhances pre-trained\nVision-Language Models, existing research focuses on reconstructing the model\narchitecture, e.g., additional loss calculation and meta-networks. These\napproaches generally lead to increased complexity and extended training cost.\nTo maintain the efficiency of the tuning process, we propose plug-and-play\nModel-Agnostic Optimization (MAO) for prompt tuning. Without altering any\ncomponents of the prompt tuning backbone, we introduce a Data-Driven\nEnhancement framework to optimize the distribution of the initial data, and\nincorporate an Alterable Regularization module to boost the task-specific\nfeature processing pipeline, thereby improving overall performance while\nmaintaining low computational cost. Extensive experiments on MAO demonstrate\nits outstanding performance and efficiency. The code of MAO is available at:\nhttps:\/\/github.com\/JREion\/M.A.O .\n","date":"2025-03-23"}
{"id":"2503.18161","title":"Active Inference for Energy Control and Planning in Smart Buildings and\n  Communities","abstract":"  Active Inference (AIF) is emerging as a powerful framework for\ndecision-making under uncertainty, yet its potential in engineering\napplications remains largely unexplored. In this work, we propose a novel\ndual-layer AIF architecture that addresses both building-level and\ncommunity-level energy management. By leveraging the free energy principle,\neach layer adapts to evolving conditions and handles partial observability\nwithout extensive sensor information and respecting data privacy. We validate\nthe continuous AIF model against both a perfect optimization baseline and a\nreinforcement learning-based approach. We also test the community AIF framework\nunder extreme pricing scenarios. The results highlight the model's robustness\nin handling abrupt changes. This study is the first to show how a distributed\nAIF works in engineering. It also highlights new opportunities for\nprivacy-preserving and uncertainty-aware control strategies in engineering\napplications.\n","date":"2025-03-23"}
{"id":"2503.18162","title":"SNRAware: Improved Deep Learning MRI Denoising with SNR Unit Training\n  and G-factor Map Augmentation","abstract":"  To develop and evaluate a new deep learning MR denoising method that\nleverages quantitative noise distribution information from the reconstruction\nprocess to improve denoising performance and generalization.\n  This retrospective study trained 14 different transformer and convolutional\nmodels with two backbone architectures on a large dataset of 2,885,236 images\nfrom 96,605 cardiac retro-gated cine complex series acquired at 3T. The\nproposed training scheme, termed SNRAware, leverages knowledge of the MRI\nreconstruction process to improve denoising performance by simulating large,\nhigh quality, and diverse synthetic datasets, and providing quantitative\ninformation about the noise distribution to the model. In-distribution testing\nwas performed on a hold-out dataset of 3000 samples with performance measured\nusing PSNR and SSIM, with ablation comparison without the noise augmentation.\nOut-of-distribution tests were conducted on cardiac real-time cine, first-pass\ncardiac perfusion, and neuro and spine MRI, all acquired at 1.5T, to test model\ngeneralization across imaging sequences, dynamically changing contrast,\ndifferent anatomies, and field strengths. The best model found in the\nin-distribution test generalized well to out-of-distribution samples,\ndelivering 6.5x and 2.9x CNR improvement for real-time cine and perfusion\nimaging, respectively. Further, a model trained with 100% cardiac cine data\ngeneralized well to a T1 MPRAGE neuro 3D scan and T2 TSE spine MRI.\n","date":"2025-03-23"}
{"id":"2503.18167","title":"Evaluating Negative Sampling Approaches for Neural Topic Models","abstract":"  Negative sampling has emerged as an effective technique that enables deep\nlearning models to learn better representations by introducing the paradigm of\nlearn-to-compare. The goal of this approach is to add robustness to deep\nlearning models to learn better representation by comparing the positive\nsamples against the negative ones. Despite its numerous demonstrations in\nvarious areas of computer vision and natural language processing, a\ncomprehensive study of the effect of negative sampling in an unsupervised\ndomain like topic modeling has not been well explored. In this paper, we\npresent a comprehensive analysis of the impact of different negative sampling\nstrategies on neural topic models. We compare the performance of several\npopular neural topic models by incorporating a negative sampling technique in\nthe decoder of variational autoencoder-based neural topic models. Experiments\non four publicly available datasets demonstrate that integrating negative\nsampling into topic models results in significant enhancements across multiple\naspects, including improved topic coherence, richer topic diversity, and more\naccurate document classification. Manual evaluations also indicate that the\ninclusion of negative sampling into neural topic models enhances the quality of\nthe generated topics. These findings highlight the potential of negative\nsampling as a valuable tool for advancing the effectiveness of neural topic\nmodels.\n","date":"2025-03-23"}
{"id":"2503.18168","title":"Strategic Prompt Pricing for AIGC Services: A User-Centric Approach","abstract":"  The rapid growth of AI-generated content (AIGC) services has created an\nurgent need for effective prompt pricing strategies, yet current approaches\noverlook users' strategic two-step decision-making process in selecting and\nutilizing generative AI models. This oversight creates two key technical\nchallenges: quantifying the relationship between user prompt capabilities and\ngeneration outcomes, and optimizing platform payoff while accounting for\nheterogeneous user behaviors. We address these challenges by introducing prompt\nambiguity, a theoretical framework that captures users' varying abilities in\nprompt engineering, and developing an Optimal Prompt Pricing (OPP) algorithm.\nOur analysis reveals a counterintuitive insight: users with higher prompt\nambiguity (i.e., lower capability) exhibit non-monotonic prompt usage patterns,\nfirst increasing then decreasing with ambiguity levels, reflecting complex\nchanges in marginal utility. Experimental evaluation using a character-level\nGPT-like model demonstrates that our OPP algorithm achieves up to 31.72%\nimprovement in platform payoff compared to existing pricing mechanisms,\nvalidating the importance of user-centric prompt pricing in AIGC services.\n","date":"2025-03-23"}
{"id":"2503.18170","title":"Self-Attention Diffusion Models for Zero-Shot Biomedical Image\n  Segmentation: Unlocking New Frontiers in Medical Imaging","abstract":"  Producing high-quality segmentation masks for medical images is a fundamental\nchallenge in biomedical image analysis. Recent research has explored\nlarge-scale supervised training to enable segmentation across various medical\nimaging modalities and unsupervised training to facilitate segmentation without\ndense annotations. However, constructing a model capable of segmenting diverse\nmedical images in a zero-shot manner without any annotations remains a\nsignificant hurdle. This paper introduces the Attention Diffusion Zero-shot\nUnsupervised System (ADZUS), a novel approach that leverages self-attention\ndiffusion models for zero-shot biomedical image segmentation. ADZUS harnesses\nthe intrinsic capabilities of pre-trained diffusion models, utilizing their\ngenerative and discriminative potentials to segment medical images without\nrequiring annotated training data or prior domain-specific knowledge. The ADZUS\narchitecture is detailed, with its integration of self-attention mechanisms\nthat facilitate context-aware and detail-sensitive segmentations being\nhighlighted. Experimental results across various medical imaging datasets,\nincluding skin lesion segmentation, chest X-ray infection segmentation, and\nwhite blood cell segmentation, reveal that ADZUS achieves state-of-the-art\nperformance. Notably, ADZUS reached Dice scores ranging from 88.7\\% to 92.9\\%\nand IoU scores from 66.3\\% to 93.3\\% across different segmentation tasks,\ndemonstrating significant improvements in handling novel, unseen medical\nimagery. It is noteworthy that while ADZUS demonstrates high effectiveness, it\ndemands substantial computational resources and extended processing times. The\nmodel's efficacy in zero-shot settings underscores its potential to reduce\nreliance on costly annotations and seamlessly adapt to new medical imaging\ntasks, thereby expanding the diagnostic capabilities of AI-driven medical\nimaging technologies.\n","date":"2025-03-23"}
{"id":"2503.18172","title":"Unmasking Deceptive Visuals: Benchmarking Multimodal Large Language\n  Models on Misleading Chart Question Answering","abstract":"  Misleading chart visualizations, which intentionally manipulate data\nrepresentations to support specific claims, can distort perceptions and lead to\nincorrect conclusions. Despite decades of research, misleading visualizations\nremain a widespread and pressing issue. Recent advances in multimodal large\nlanguage models (MLLMs) have demonstrated strong chart comprehension\ncapabilities, yet no existing work has systematically evaluated their ability\nto detect and interpret misleading charts. This paper introduces the Misleading\nChart Question Answering (Misleading ChartQA) Benchmark, a large-scale\nmultimodal dataset designed to assess MLLMs in identifying and reasoning about\nmisleading charts. It contains over 3,000 curated examples, covering 21 types\nof misleaders and 10 chart types. Each example includes standardized chart\ncode, CSV data, and multiple-choice questions with labeled explanations,\nvalidated through multi-round MLLM checks and exhausted expert human review. We\nbenchmark 16 state-of-the-art MLLMs on our dataset, revealing their limitations\nin identifying visually deceptive practices. We also propose a novel pipeline\nthat detects and localizes misleaders, enhancing MLLMs' accuracy in misleading\nchart interpretation. Our work establishes a foundation for advancing\nMLLM-driven misleading chart comprehension. We publicly release the sample\ndataset to support further research in this critical area.\n","date":"2025-03-23"}
{"id":"2503.18174","title":"GINGER: Grounded Information Nugget-Based Generation of Responses","abstract":"  Retrieval-augmented generation (RAG) faces challenges related to factual\ncorrectness, source attribution, and response completeness. To address them, we\npropose a modular pipeline for grounded response generation that operates on\ninformation nuggets-minimal, atomic units of relevant information extracted\nfrom retrieved documents. The multistage pipeline encompasses nugget detection,\nclustering, ranking, top cluster summarization, and fluency enhancement. It\nguarantees grounding in specific facts, facilitates source attribution, and\nensures maximum information inclusion within length constraints. Extensive\nexperiments on the TREC RAG'24 dataset evaluated with the AutoNuggetizer\nframework demonstrate that GINGER achieves state-of-the-art performance on this\nbenchmark.\n","date":"2025-03-23"}
{"id":"2503.18175","title":"Enhancing Software Vulnerability Detection Using Code Property Graphs\n  and Convolutional Neural Networks","abstract":"  The increasing complexity of modern software systems has led to a rise in\nvulnerabilities that malicious actors can exploit. Traditional methods of\nvulnerability detection, such as static and dynamic analysis, have limitations\nin scalability and automation. This paper proposes a novel approach to\ndetecting software vulnerabilities using a combination of code property graphs\nand machine learning techniques. By leveraging code property graphs, which\nintegrate abstract syntax trees, control flow graphs, and program dependency\ngraphs, we achieve a detailed representation of software code that enhances the\naccuracy and granularity of vulnerability detection. We introduce various\nneural network models, including convolutional neural networks adapted for\ngraph data, to process these representations. Our approach provides a scalable\nand automated solution for vulnerability detection, addressing the shortcomings\nof existing methods. We also present a newly generated dataset labeled with\nfunction-level vulnerability types sourced from open-source repositories. Our\ncontributions include a methodology for transforming software code into code\nproperty graphs, the implementation of a convolutional neural network model for\ngraph data, and the creation of a comprehensive dataset for training and\nevaluation. This work lays the foundation for more effective and efficient\nvulnerability detection in complex software systems.\n","date":"2025-03-23"}
{"id":"2503.18177","title":"Training A Neural Network For Partially Occluded Road Sign\n  Identification In The Context Of Autonomous Vehicles","abstract":"  The increasing number of autonomous vehicles and the rapid development of\ncomputer vision technologies underscore the particular importance of conducting\nresearch on the accuracy of traffic sign recognition. Numerous studies in this\nfield have already achieved significant results, demonstrating high\neffectiveness in addressing traffic sign recognition tasks. However, the task\nbecomes considerably more complex when a sign is partially obscured by\nsurrounding objects, such as tree branches, billboards, or other elements of\nthe urban environment. In our study, we investigated how partial occlusion of\ntraffic signs affects their recognition. For this purpose, we collected a\ndataset comprising 5,746 images, including both fully visible and partially\noccluded signs, and made it publicly available. Using this dataset, we compared\nthe performance of our custom convolutional neural network (CNN), which\nachieved 96% accuracy, with models trained using transfer learning. The best\nresult was obtained by VGG16 with full layer unfreezing, reaching 99% accuracy.\nAdditional experiments revealed that models trained solely on fully visible\nsigns lose effectiveness when recognizing occluded signs. This highlights the\ncritical importance of incorporating real-world data with partial occlusion\ninto training sets to ensure robust model performance in complex practical\nscenarios and to enhance the safety of autonomous driving.\n","date":"2025-03-23"}
{"id":"2503.18179","title":"Causality-Aware Next Location Prediction Framework based on Human\n  Mobility Stratification","abstract":"  Human mobility data are fused with multiple travel patterns and hidden\nspatiotemporal patterns are extracted by integrating user, location, and time\ninformation to improve next location prediction accuracy. In existing next\nlocation prediction methods, different causal relationships that result from\npatterns in human mobility data are ignored, which leads to confounding\ninformation that can have a negative effect on predictions. Therefore, this\nstudy introduces a causality-aware framework for next location prediction,\nfocusing on human mobility stratification for travel patterns. In our research,\na novel causal graph is developed that describes the relationships between\nvarious input variables. We use counterfactuals to enhance the indirect effects\nin our causal graph for specific travel patterns: non-anchor targeted travels.\nThe proposed framework is designed as a plug-and-play module that integrates\nmultiple next location prediction paradigms. We tested our proposed framework\nusing several state-of-the-art models and human mobility datasets, and the\nresults reveal that the proposed module improves the prediction performance. In\naddition, we provide results from the ablation study and quantitative study to\ndemonstrate the soundness of our causal graph and its ability to further\nenhance the interpretability of the current next location prediction models.\n","date":"2025-03-23"}
{"id":"2503.18181","title":"Adaptive Physics-informed Neural Networks: A Survey","abstract":"  Physics-informed neural networks (PINNs) have emerged as a promising approach\nto solving partial differential equations (PDEs) using neural networks,\nparticularly in data-scarce scenarios, due to their unsupervised training\ncapability. However, limitations related to convergence and the need for\nre-optimization with each change in PDE parameters hinder their widespread\nadoption across scientific and engineering applications. This survey reviews\nexisting research that addresses these limitations through transfer learning\nand meta-learning. The covered methods improve the training efficiency,\nallowing faster adaptation to new PDEs with fewer data and computational\nresources. While traditional numerical methods solve systems of differential\nequations directly, neural networks learn solutions implicitly by adjusting\ntheir parameters. One notable advantage of neural networks is their ability to\nabstract away from specific problem domains, allowing them to retain, discard,\nor adapt learned representations to efficiently address similar problems. By\nexploring the application of these techniques to PINNs, this survey identifies\npromising directions for future research to facilitate the broader adoption of\nPINNs in a wide range of scientific and engineering applications.\n","date":"2025-03-23"}
{"id":"2503.18182","title":"Exploring Topic Trends in COVID-19 Research Literature using\n  Non-Negative Matrix Factorization","abstract":"  In this work, we apply topic modeling using Non-Negative Matrix Factorization\n(NMF) on the COVID-19 Open Research Dataset (CORD-19) to uncover the underlying\nthematic structure and its evolution within the extensive body of COVID-19\nresearch literature. NMF factorizes the document-term matrix into two\nnon-negative matrices, effectively representing the topics and their\ndistribution across the documents. This helps us see how strongly documents\nrelate to topics and how topics relate to words. We describe the complete\nmethodology which involves a series of rigorous pre-processing steps to\nstandardize the available text data while preserving the context of phrases,\nand subsequently feature extraction using the term frequency-inverse document\nfrequency (tf-idf), which assigns weights to words based on their frequency and\nrarity in the dataset. To ensure the robustness of our topic model, we conduct\na stability analysis. This process assesses the stability scores of the NMF\ntopic model for different numbers of topics, enabling us to select the optimal\nnumber of topics for our analysis. Through our analysis, we track the evolution\nof topics over time within the CORD-19 dataset. Our findings contribute to the\nunderstanding of the knowledge structure of the COVID-19 research landscape,\nproviding a valuable resource for future research in this field.\n","date":"2025-03-23"}
{"id":"2503.18185","title":"Exploring Energy Landscapes for Minimal Counterfactual Explanations:\n  Applications in Cybersecurity and Beyond","abstract":"  Counterfactual explanations have emerged as a prominent method in Explainable\nArtificial Intelligence (XAI), providing intuitive and actionable insights into\nMachine Learning model decisions. In contrast to other traditional feature\nattribution methods that assess the importance of input variables,\ncounterfactual explanations focus on identifying the minimal changes required\nto alter a model's prediction, offering a ``what-if'' analysis that is close to\nhuman reasoning. In the context of XAI, counterfactuals enhance transparency,\ntrustworthiness and fairness, offering explanations that are not just\ninterpretable but directly applicable in the decision-making processes.\n  In this paper, we present a novel framework that integrates perturbation\ntheory and statistical mechanics to generate minimal counterfactual\nexplanations in explainable AI. We employ a local Taylor expansion of a Machine\nLearning model's predictive function and reformulate the counterfactual search\nas an energy minimization problem over a complex landscape. In sequence, we\nmodel the probability of candidate perturbations leveraging the Boltzmann\ndistribution and use simulated annealing for iterative refinement. Our approach\nsystematically identifies the smallest modifications required to change a\nmodel's prediction while maintaining plausibility. Experimental results on\nbenchmark datasets for cybersecurity in Internet of Things environments,\ndemonstrate that our method provides actionable, interpretable counterfactuals\nand offers deeper insights into model sensitivity and decision boundaries in\nhigh-dimensional spaces.\n","date":"2025-03-23"}
{"id":"2503.18190","title":"Quantile-Based Randomized Kaczmarz for Corrupted Tensor Linear Systems","abstract":"  The reconstruction of tensor-valued signals from corrupted measurements,\nknown as tensor regression, has become essential in many multi-modal\napplications such as hyperspectral image reconstruction and medical imaging. In\nthis work, we address the tensor linear system problem $\\mathcal{A}\n\\mathcal{X}=\\mathcal{B}$, where $\\mathcal{A}$ is a measurement operator,\n$\\mathcal{X}$ is the unknown tensor-valued signal, and $\\mathcal{B}$ contains\nthe measurements, possibly corrupted by arbitrary errors. Such corruption is\ncommon in large-scale tensor data, where transmission, sensory, or storage\nerrors are rare per instance but likely over the entire dataset and may be\narbitrarily large in magnitude. We extend the Kaczmarz method, a popular\niterative algorithm for solving large linear systems, to develop a Quantile\nTensor Randomized Kaczmarz (QTRK) method robust to large, sparse corruptions in\nthe observations $\\mathcal{B}$. This approach combines the tensor Kaczmarz\nframework with quantile-based statistics, allowing it to mitigate adversarial\ncorruptions and improve convergence reliability. We also propose and discuss\nthe Masked Quantile Randomized Kaczmarz (mQTRK) variant, which selectively\napplies partial updates to handle corruptions further. We present convergence\nguarantees, discuss the advantages and disadvantages of our approaches, and\ndemonstrate the effectiveness of our methods through experiments, including an\napplication for video deblurring.\n","date":"2025-03-23"}
{"id":"2503.18195","title":"Shapley-Guided Utility Learning for Effective Graph Inference Data\n  Valuation","abstract":"  Graph Neural Networks (GNNs) have demonstrated remarkable performance in\nvarious graph-based machine learning tasks, yet evaluating the importance of\nneighbors of testing nodes remains largely unexplored due to the challenge of\nassessing data importance without test labels. To address this gap, we propose\nShapley-Guided Utility Learning (SGUL), a novel framework for graph inference\ndata valuation. SGUL innovatively combines transferable data-specific and\nmodelspecific features to approximate test accuracy without relying on ground\ntruth labels. By incorporating Shapley values as a preprocessing step and using\nfeature Shapley values as input, our method enables direct optimization of\nShapley value prediction while reducing computational demands. SGUL overcomes\nkey limitations of existing methods, including poor generalization to unseen\ntest-time structures and indirect optimization. Experiments on diverse graph\ndatasets demonstrate that SGUL consistently outperforms existing baselines in\nboth inductive and transductive settings. SGUL offers an effective, efficient,\nand interpretable approach for quantifying the value of test-time neighbors.\n","date":"2025-03-23"}
{"id":"2503.18197","title":"FROG: Fair Removal on Graphs","abstract":"  As compliance with privacy regulations becomes increasingly critical, the\ngrowing demand for data privacy has highlighted the significance of machine\nunlearning in many real world applications, such as social network and\nrecommender systems, many of which can be represented as graph-structured data.\nHowever, existing graph unlearning algorithms indiscriminately modify edges or\nnodes from well-trained models without considering the potential impact of such\nstructural modifications on fairness. For example, forgetting links between\nnodes with different genders in a social network may exacerbate group\ndisparities, leading to significant fairness concerns. To address these\nchallenges, we propose a novel approach that jointly optimizes the graph\nstructure and the corresponding model for fair unlearning tasks.\nSpecifically,our approach rewires the graph to enhance unlearning efficiency by\nremoving redundant edges that hinder forgetting while preserving fairness\nthrough targeted edge augmentation. Additionally, we introduce a worst-case\nevaluation mechanism to assess the reliability of fair unlearning performance.\nExtensive experiments on real-world datasets demonstrate the effectiveness of\nthe proposed approach in achieving superior unlearning outcomes.\n","date":"2025-03-23"}
{"id":"2503.18201","title":"Iterative Multi-Agent Reinforcement Learning: A Novel Approach Toward\n  Real-World Multi-Echelon Inventory Optimization","abstract":"  Multi-echelon inventory optimization (MEIO) is critical for effective supply\nchain management, but its inherent complexity can pose significant challenges.\nHeuristics are commonly used to address this complexity, yet they often face\nlimitations in scope and scalability. Recent research has found deep\nreinforcement learning (DRL) to be a promising alternative to traditional\nheuristics, offering greater versatility by utilizing dynamic decision-making\ncapabilities. However, since DRL is known to struggle with the curse of\ndimensionality, its relevance to complex real-life supply chain scenarios is\nstill to be determined. This thesis investigates DRL's applicability to MEIO\nproblems of increasing complexity. A state-of-the-art DRL model was replicated,\nenhanced, and tested across 13 supply chain scenarios, combining diverse\nnetwork structures and parameters. To address DRL's challenges with\ndimensionality, additional models leveraging graph neural networks (GNNs) and\nmulti-agent reinforcement learning (MARL) were developed, culminating in the\nnovel iterative multi-agent reinforcement learning (IMARL) approach. IMARL\ndemonstrated superior scalability, effectiveness, and reliability in optimizing\ninventory policies, consistently outperforming benchmarks. These findings\nconfirm the potential of DRL, particularly IMARL, to address real-world supply\nchain challenges and call for additional research to further expand its\napplicability.\n","date":"2025-03-23"}
{"id":"2503.18210","title":"ViVa: Video-Trained Value Functions for Guiding Online RL from Diverse\n  Data","abstract":"  Online reinforcement learning (RL) with sparse rewards poses a challenge\npartly because of the lack of feedback on states leading to the goal.\nFurthermore, expert offline data with reward signal is rarely available to\nprovide this feedback and bootstrap online learning. How can we guide online\nagents to the right solution without this on-task data? Reward shaping offers a\nsolution by providing fine-grained signal to nudge the policy towards the\noptimal solution. However, reward shaping often requires domain knowledge to\nhand-engineer heuristics for a specific goal. To enable more general and\ninexpensive guidance, we propose and analyze a data-driven methodology that\nautomatically guides RL by learning from widely available video data such as\nInternet recordings, off-task demonstrations, task failures, and undirected\nenvironment interaction. By learning a model of optimal goal-conditioned value\nfrom diverse passive data, we open the floor to scaling up and using various\ndata sources to model general goal-reaching behaviors relevant to guiding\nonline RL. Specifically, we use intent-conditioned value functions to learn\nfrom diverse videos and incorporate these goal-conditioned values into the\nreward. Our experiments show that video-trained value functions work well with\na variety of data sources, exhibit positive transfer from human video\npre-training, can generalize to unseen goals, and scale with dataset size.\n","date":"2025-03-23"}
{"id":"2503.18211","title":"SimMotionEdit: Text-Based Human Motion Editing with Motion Similarity\n  Prediction","abstract":"  Text-based 3D human motion editing is a critical yet challenging task in\ncomputer vision and graphics. While training-free approaches have been\nexplored, the recent release of the MotionFix dataset, which includes\nsource-text-motion triplets, has opened new avenues for training, yielding\npromising results. However, existing methods struggle with precise control,\noften leading to misalignment between motion semantics and language\ninstructions. In this paper, we introduce a related task, motion similarity\nprediction, and propose a multi-task training paradigm, where we train the\nmodel jointly on motion editing and motion similarity prediction to foster the\nlearning of semantically meaningful representations. To complement this task,\nwe design an advanced Diffusion-Transformer-based architecture that separately\nhandles motion similarity prediction and motion editing. Extensive experiments\ndemonstrate the state-of-the-art performance of our approach in both editing\nalignment and fidelity.\n","date":"2025-03-23"}
{"id":"2503.18212","title":"LakotaBERT: A Transformer-based Model for Low Resource Lakota Language","abstract":"  Lakota, a critically endangered language of the Sioux people in North\nAmerica, faces significant challenges due to declining fluency among younger\ngenerations. This paper introduces LakotaBERT, the first large language model\n(LLM) tailored for Lakota, aiming to support language revitalization efforts.\nOur research has two primary objectives: (1) to create a comprehensive Lakota\nlanguage corpus and (2) to develop a customized LLM for Lakota. We compiled a\ndiverse corpus of 105K sentences in Lakota, English, and parallel texts from\nvarious sources, such as books and websites, emphasizing the cultural\nsignificance and historical context of the Lakota language. Utilizing the\nRoBERTa architecture, we pre-trained our model and conducted comparative\nevaluations against established models such as RoBERTa, BERT, and multilingual\nBERT. Initial results demonstrate a masked language modeling accuracy of 51%\nwith a single ground truth assumption, showcasing performance comparable to\nthat of English-based models. We also evaluated the model using additional\nmetrics, such as precision and F1 score, to provide a comprehensive assessment\nof its capabilities. By integrating AI and linguistic methodologies, we aspire\nto enhance linguistic diversity and cultural resilience, setting a valuable\nprecedent for leveraging technology in the revitalization of other endangered\nindigenous languages.\n","date":"2025-03-23"}
{"id":"2503.18213","title":"A Study on Neuro-Symbolic Artificial Intelligence: Healthcare\n  Perspectives","abstract":"  Over the last few decades, Artificial Intelligence (AI) scientists have been\nconducting investigations to attain human-level performance by a machine in\naccomplishing a cognitive task. Within machine learning, the ultimate\naspiration is to attain Artificial General Intelligence (AGI) through a\nmachine. This pursuit has led to the exploration of two distinct AI paradigms.\nSymbolic AI, also known as classical or GOFAI (Good Old-Fashioned AI) and\nConnectionist (Sub-symbolic) AI, represented by Neural Systems, are two\nmutually exclusive paradigms. Symbolic AI excels in reasoning, explainability,\nand knowledge representation but faces challenges in processing complex\nreal-world data with noise. Conversely, deep learning (Black-Box systems)\nresearch breakthroughs in neural networks are notable, yet they lack reasoning\nand interpretability. Neuro-symbolic AI (NeSy), an emerging area of AI\nresearch, attempts to bridge this gap by integrating logical reasoning into\nneural networks, enabling them to learn and reason with symbolic\nrepresentations. While a long path, this strategy has made significant progress\ntowards achieving common sense reasoning by systems. This article conducts an\nextensive review of over 977 studies from prominent scientific databases (DBLP,\nACL, IEEExplore, Scopus, PubMed, ICML, ICLR), thoroughly examining the\nmultifaceted capabilities of Neuro-Symbolic AI, with a particular focus on its\nhealthcare applications, particularly in drug discovery, and Protein\nengineering research. The survey addresses vital themes, including reasoning,\nexplainability, integration strategies, 41 healthcare-related use cases,\nbenchmarking, datasets, current approach limitations from both healthcare and\nbroader perspectives, and proposed novel approaches for future experiments.\n","date":"2025-03-23"}
{"id":"2503.18216","title":"Adaptive Rank Allocation: Speeding Up Modern Transformers with RaNA\n  Adapters","abstract":"  Large Language Models (LLMs) are computationally intensive, particularly\nduring inference. Neuron-adaptive techniques, which selectively activate\nneurons in Multi-Layer Perceptron (MLP) layers, offer some speedups but suffer\nfrom limitations in modern Transformers. These include reliance on sparse\nactivations, incompatibility with attention layers, and the use of costly\nneuron masking techniques. To address these issues, we propose the Adaptive\nRank Allocation framework and introduce the Rank and Neuron Allocator (RaNA)\nadapter. RaNA adapters leverage rank adapters, which operate on linear layers\nby applying both low-rank matrix decompositions and adaptive masking to\nefficiently allocate compute without depending on activation sparsity. This\nenables RaNA to be generally applied to MLPs and linear components of attention\nmodules, while eliminating the need for expensive maskers found in\nneuron-adaptive methods. Notably, when compared to neuron adapters, RaNA\nimproves perplexity by up to 7 points and increases accuracy by up to 8\npercentage-points when reducing FLOPs by $\\sim$44% in state-of-the-art\nTransformer architectures. These results position RaNA as a robust solution for\nimproving inference efficiency in modern Transformer architectures.\n","date":"2025-03-23"}
{"id":"2503.18219","title":"Theory-to-Practice Gap for Neural Networks and Neural Operators","abstract":"  This work studies the sampling complexity of learning with ReLU neural\nnetworks and neural operators. For mappings belonging to relevant approximation\nspaces, we derive upper bounds on the best-possible convergence rate of any\nlearning algorithm, with respect to the number of samples. In the\nfinite-dimensional case, these bounds imply a gap between the parametric and\nsampling complexities of learning, known as the \\emph{theory-to-practice gap}.\nIn this work, a unified treatment of the theory-to-practice gap is achieved in\na general $L^p$-setting, while at the same time improving available bounds in\nthe literature. Furthermore, based on these results the theory-to-practice gap\nis extended to the infinite-dimensional setting of operator learning. Our\nresults apply to Deep Operator Networks and integral kernel-based neural\noperators, including the Fourier neural operator. We show that the\nbest-possible convergence rate in a Bochner $L^p$-norm is bounded by\nMonte-Carlo rates of order $1\/p$.\n","date":"2025-03-23"}
{"id":"2503.18223","title":"MammAlps: A multi-view video behavior monitoring dataset of wild mammals\n  in the Swiss Alps","abstract":"  Monitoring wildlife is essential for ecology and ethology, especially in\nlight of the increasing human impact on ecosystems. Camera traps have emerged\nas habitat-centric sensors enabling the study of wildlife populations at scale\nwith minimal disturbance. However, the lack of annotated video datasets limits\nthe development of powerful video understanding models needed to process the\nvast amount of fieldwork data collected. To advance research in wild animal\nbehavior monitoring we present MammAlps, a multimodal and multi-view dataset of\nwildlife behavior monitoring from 9 camera-traps in the Swiss National Park.\nMammAlps contains over 14 hours of video with audio, 2D segmentation maps and\n8.5 hours of individual tracks densely labeled for species and behavior. Based\non 6135 single animal clips, we propose the first hierarchical and multimodal\nanimal behavior recognition benchmark using audio, video and reference scene\nsegmentation maps as inputs. Furthermore, we also propose a second\necology-oriented benchmark aiming at identifying activities, species, number of\nindividuals and meteorological conditions from 397 multi-view and long-term\necological events, including false positive triggers. We advocate that both\ntasks are complementary and contribute to bridging the gap between machine\nlearning and ecology. Code and data are available at:\nhttps:\/\/github.com\/eceo-epfl\/MammAlps\n","date":"2025-03-23"}
{"id":"2503.18224","title":"A Framework for Finding Local Saddle Points in Two-Player Zero-Sum\n  Black-Box Games","abstract":"  Saddle point optimization is a critical problem employed in numerous\nreal-world applications, including portfolio optimization, generative\nadversarial networks, and robotics. It has been extensively studied in cases\nwhere the objective function is known and differentiable. Existing work in\nblack-box settings with unknown objectives that can only be sampled either\nassumes convexity-concavity in the objective to simplify the problem or\noperates with noisy gradient estimators. In contrast, we introduce a framework\ninspired by Bayesian optimization which utilizes Gaussian processes to model\nthe unknown (potentially nonconvex-nonconcave) objective and requires only\nzeroth-order samples. Our approach frames the saddle point optimization problem\nas a two-level process which can flexibly integrate existing and novel\napproaches to this problem. The upper level of our framework produces a model\nof the objective function by sampling in promising locations, and the lower\nlevel of our framework uses the existing model to frame and solve a general-sum\ngame to identify locations to sample. This lower level procedure can be\ndesigned in complementary ways, and we demonstrate the flexibility of our\napproach by introducing variants which appropriately trade off between factors\nlike runtime, the cost of function evaluations, and the number of available\ninitial samples. We experimentally demonstrate these algorithms on synthetic\nand realistic datasets in black-box nonconvex-nonconcave settings, showcasing\ntheir ability to efficiently locate local saddle points in these contexts.\n","date":"2025-03-23"}
{"id":"2503.18225","title":"Decoupling Angles and Strength in Low-rank Adaptation","abstract":"  Parameter-Efficient FineTuning (PEFT) methods have recently gained\nsignificant popularity thanks to the widespread availability of large-scale\npretrained models. These methods allow for quick adaptation to downstream tasks\nwith minimal computational cost. However, popular finetuning methods such as\nLoRA exhibit limited robustness when it comes to hyperparameter choices or\nextended training regimes, preventing optimal out-of-the-box performance. In\ncontrast, bounded approaches, such as ETHER, provide greater robustness but are\nlimited to extremely low-rank adaptations and fixed-strength transformations,\nreducing their adaptation expressive power. In this work, we propose Decoupled\nLow-rank Adaptation (DeLoRA), a novel finetuning method that normalizes and\nscales learnable low-rank matrices. By bounding the distance of the\ntransformation, DeLoRA effectively decouples the angular learning from the\nadaptation strength, enhancing robustness without compromising performance.\nThrough evaluations on subject-driven image generation, natural language\nunderstanding, and instruction tuning, we show that DeLoRA matches or surpasses\nperformance of competing PEFT methods, while exhibiting stronger robustness.\nCode is available at https:\/\/github.com\/ExplainableML\/DeLoRA.\n","date":"2025-03-23"}
{"id":"2503.18226","title":"Mapping Hymns and Organizing Concepts in the Rigveda: Quantitatively\n  Connecting the Vedic Suktas","abstract":"  Accessing and gaining insight into the Rigveda poses a non-trivial challenge\ndue to its extremely ancient Sanskrit language, poetic structure, and large\nvolume of text. By using NLP techniques, this study identified topics and\nsemantic connections of hymns within the Rigveda that were corroborated by\nseven well-known groupings of hymns. The 1,028 suktas (hymns) from the modern\nEnglish translation of the Rigveda by Jamison and Brereton were preprocessed\nand sukta-level embeddings were obtained using, i) a novel adaptation of LSA,\npresented herein, ii) SBERT, and iii) Doc2Vec embeddings. Following an UMAP\ndimension reduction of the vectors, the network of suktas was formed using\nk-nearest neighbours. Then, community detection of topics in the sukta networks\nwas performed with the Louvain, Leiden, and label propagation methods, whose\nstatistical significance of the formed topics were determined using an\nappropriate null distribution. Only the novel adaptation of LSA using the\nLeiden method, had detected sukta topic networks that were significant (z =\n2.726, p < .01) with a modularity score of 0.944. Of the seven famous sukta\ngroupings analyzed (e.g., creation, funeral, water, etc.) the LSA derived\nnetwork was successful in all seven cases, while Doc2Vec was not significant\nand failed to detect the relevant suktas. SBERT detected four of the famous\nsuktas as separate groups, but mistakenly combined three of them into a single\nmixed group. Also, the SBERT network was not statistically significant.\n","date":"2025-03-23"}
{"id":"2503.18227","title":"PG-SAM: Prior-Guided SAM with Medical for Multi-organ Segmentation","abstract":"  Segment Anything Model (SAM) demonstrates powerful zero-shot capabilities;\nhowever, its accuracy and robustness significantly decrease when applied to\nmedical image segmentation. Existing methods address this issue through\nmodality fusion, integrating textual and image information to provide more\ndetailed priors. In this study, we argue that the granularity of text and the\ndomain gap affect the accuracy of the priors. Furthermore, the discrepancy\nbetween high-level abstract semantics and pixel-level boundary details in\nimages can introduce noise into the fusion process. To address this, we propose\nPrior-Guided SAM (PG-SAM), which employs a fine-grained modality prior aligner\nto leverage specialized medical knowledge for better modality alignment. The\ncore of our method lies in efficiently addressing the domain gap with\nfine-grained text from a medical LLM. Meanwhile, it also enhances the priors'\nquality after modality alignment, ensuring more accurate segmentation. In\naddition, our decoder enhances the model's expressive capabilities through\nmulti-level feature fusion and iterative mask optimizer operations, supporting\nunprompted learning. We also propose a unified pipeline that effectively\nsupplies high-quality semantic information to SAM. Extensive experiments on the\nSynapse dataset demonstrate that the proposed PG-SAM achieves state-of-the-art\nperformance. Our code is released at https:\/\/github.com\/logan-0623\/PG-SAM.\n","date":"2025-03-23"}
{"id":"2503.18229","title":"Adaptive Multi-Fidelity Reinforcement Learning for Variance Reduction in\n  Engineering Design Optimization","abstract":"  Multi-fidelity Reinforcement Learning (RL) frameworks efficiently utilize\ncomputational resources by integrating analysis models of varying accuracy and\ncosts. The prevailing methodologies, characterized by transfer learning,\nhuman-inspired strategies, control variate techniques, and adaptive sampling,\npredominantly depend on a structured hierarchy of models. However, this\nreliance on a model hierarchy can exacerbate variance in policy learning when\nthe underlying models exhibit heterogeneous error distributions across the\ndesign space. To address this challenge, this work proposes a novel adaptive\nmulti-fidelity RL framework, in which multiple heterogeneous, non-hierarchical\nlow-fidelity models are dynamically leveraged alongside a high-fidelity model\nto efficiently learn a high-fidelity policy. Specifically, low-fidelity\npolicies and their experience data are adaptively used for efficient targeted\nlearning, guided by their alignment with the high-fidelity policy. The\neffectiveness of the approach is demonstrated in an octocopter design\noptimization problem, utilizing two low-fidelity models alongside a\nhigh-fidelity simulator. The results demonstrate that the proposed approach\nsubstantially reduces variance in policy learning, leading to improved\nconvergence and consistent high-quality solutions relative to traditional\nhierarchical multi-fidelity RL methods. Moreover, the framework eliminates the\nneed for manually tuning model usage schedules, which can otherwise introduce\nsignificant computational overhead. This positions the framework as an\neffective variance-reduction strategy for multi-fidelity RL, while also\nmitigating the computational and operational burden of manual fidelity\nscheduling.\n","date":"2025-03-23"}
{"id":"2503.18234","title":"KEA: Keeping Exploration Alive by Proactively Coordinating Exploration\n  Strategies","abstract":"  Soft Actor-Critic (SAC) has achieved notable success in continuous control\ntasks but struggles in sparse reward settings, where infrequent rewards make\nefficient exploration challenging. While novelty-based exploration methods\naddress this issue by encouraging the agent to explore novel states, they are\nnot trivial to apply to SAC. In particular, managing the interaction between\nnovelty-based exploration and SAC's stochastic policy can lead to inefficient\nexploration and redundant sample collection. In this paper, we propose KEA\n(Keeping Exploration Alive) which tackles the inefficiencies in balancing\nexploration strategies when combining SAC with novelty-based exploration. KEA\nintroduces an additional co-behavior agent that works alongside SAC and a\nswitching mechanism to facilitate proactive coordination between exploration\nstrategies from novelty-based exploration and stochastic policy. This\ncoordination allows the agent to maintain stochasticity in high-novelty\nregions, enhancing exploration efficiency and reducing repeated sample\ncollection. We first analyze this potential issue in a 2D navigation task and\nthen evaluate KEA on sparse reward control tasks from the DeepMind Control\nSuite. Compared to state-of-the-art novelty-based exploration baselines, our\nexperiments show that KEA significantly improves learning efficiency and\nrobustness in sparse reward setups.\n","date":"2025-03-23"}
{"id":"2503.18235","title":"Enhance GNNs with Reliable Confidence Estimation via Adversarial\n  Calibration Learning","abstract":"  Despite their impressive predictive performance, GNNs often exhibit poor\nconfidence calibration, i.e., their predicted confidence scores do not\naccurately reflect true correctness likelihood. This issue raises concerns\nabout their reliability in high-stakes domains such as fraud detection, and\nrisk assessment, where well-calibrated predictions are essential for\ndecision-making. To ensure trustworthy predictions, several GNN calibration\nmethods are proposed. Though they can improve global calibration, our\nexperiments reveal that they often fail to generalize across different node\ngroups, leading to inaccurate confidence in node groups with different degree\nlevels, classes, and local structures. In certain cases, they even degrade\ncalibration compared to the original uncalibrated GNN. To address this\nchallenge, we propose a novel AdvCali framework that adaptively enhances\ncalibration across different node groups. Our method leverages adversarial\ntraining to automatically identify mis-calibrated node groups and applies a\ndifferentiable Group Expected Calibration Error (ECE) loss term to refine\nconfidence estimation within these groups. This allows the model to dynamically\nadjust its calibration strategy without relying on dataset-specific prior\nknowledge about miscalibrated subgroups. Extensive experiments on real-world\ndatasets demonstrate that our approach not only improves global calibration but\nalso significantly enhances calibration within groups defined by feature\nsimilarity, topology, and connectivity, outperforming previous methods and\ndemonstrating its effectiveness in practical scenarios.\n","date":"2025-03-23"}
{"id":"2503.18238","title":"Collaborating with AI Agents: Field Experiments on Teamwork,\n  Productivity, and Performance","abstract":"  To uncover how AI agents change productivity, performance, and work\nprocesses, we introduce MindMeld: an experimentation platform enabling humans\nand AI agents to collaborate in integrative workspaces. In a large-scale\nmarketing experiment on the platform, 2310 participants were randomly assigned\nto human-human and human-AI teams, with randomized AI personality traits. The\nteams exchanged 183,691 messages, and created 63,656 image edits, 1,960,095 ad\ncopy edits, and 10,375 AI-generated images while producing 11,138 ads for a\nlarge think tank. Analysis of fine-grained communication, collaboration, and\nworkflow logs revealed that collaborating with AI agents increased\ncommunication by 137% and allowed humans to focus 23% more on text and image\ncontent generation messaging and 20% less on direct text editing. Humans on\nHuman-AI teams sent 23% fewer social messages, creating 60% greater\nproductivity per worker and higher-quality ad copy. In contrast, human-human\nteams produced higher-quality images, suggesting that AI agents require\nfine-tuning for multimodal workflows. AI personality prompt randomization\nrevealed that AI traits can complement human personalities to enhance\ncollaboration. For example, conscientious humans paired with open AI agents\nimproved image quality, while extroverted humans paired with conscientious AI\nagents reduced the quality of text, images, and clicks. In field tests of ad\ncampaigns with ~5M impressions, ads with higher image quality produced by human\ncollaborations and higher text quality produced by AI collaborations performed\nsignificantly better on click-through rate and cost per click metrics. Overall,\nads created by human-AI teams performed similarly to those created by\nhuman-human teams. Together, these results suggest AI agents can improve\nteamwork and productivity, especially when tuned to complement human traits.\n","date":"2025-03-23"}
{"id":"2503.18242","title":"ShED-HD: A Shannon Entropy Distribution Framework for Lightweight\n  Hallucination Detection on Edge Devices","abstract":"  Large Language Models (LLMs) have demonstrated impressive capabilities on a\nbroad array of NLP tasks, but their tendency to produce\nhallucinations$\\unicode{x2013}$plausible-sounding but factually incorrect\ncontent$\\unicode{x2013}$poses severe challenges in high-stakes domains.\nExisting hallucination detection methods either bear the computational cost of\nmultiple inference passes or sacrifice accuracy for efficiency with single-pass\napproaches, neither of which is ideal in resource-constrained environments such\nas edge devices. We propose the Shannon Entropy Distribution Hallucination\nDetector (ShED-HD), a novel hallucination detection framework that bridges this\ngap by classifying sequence-level entropy patterns using a lightweight BiLSTM\narchitecture with single-headed attention. In contrast to prior approaches,\nShED-HD efficiently detects distinctive uncertainty patterns across entire\noutput sequences, preserving contextual awareness. Through in-depth evaluation\non three datasets (BioASQ, TriviaQA, and Jeopardy Questions), we show that\nShED-HD significantly outperforms other computationally efficient approaches in\nthe out-of-distribution setting, while achieving comparable performance in the\nin-distribution setting. ShED-HD facilitates hallucination detection that is\nlow-cost, accurate, and generalizable, improving the credibility of content\ngenerated by LLMs in resource-constrained environments where trustworthy AI\nfunctionality is crucial.\n","date":"2025-03-23"}
{"id":"2503.18244","title":"CustomKD: Customizing Large Vision Foundation for Edge Model Improvement\n  via Knowledge Distillation","abstract":"  We propose a novel knowledge distillation approach, CustomKD, that\neffectively leverages large vision foundation models (LVFMs) to enhance the\nperformance of edge models (e.g., MobileNetV3). Despite recent advancements in\nLVFMs, such as DINOv2 and CLIP, their potential in knowledge distillation for\nenhancing edge models remains underexplored. While knowledge distillation is a\npromising approach for improving the performance of edge models, the\ndiscrepancy in model capacities and heterogeneous architectures between LVFMs\nand edge models poses a significant challenge. Our observation indicates that\nalthough utilizing larger backbones (e.g., ViT-S to ViT-L) in teacher models\nimproves their downstream task performances, the knowledge distillation from\nthe large teacher models fails to bring as much performance gain for student\nmodels as for teacher models due to the large model discrepancy. Our simple yet\neffective CustomKD customizes the well-generalized features inherent in LVFMs\nto a given student model in order to reduce model discrepancies. Specifically,\nbeyond providing well-generalized original knowledge from teachers, CustomKD\naligns the features of teachers to those of students, making it easy for\nstudents to understand and overcome the large model discrepancy overall.\nCustomKD significantly improves the performances of edge models in scenarios\nwith unlabeled data such as unsupervised domain adaptation (e.g., OfficeHome\nand DomainNet) and semi-supervised learning (e.g., CIFAR-100 with 400 labeled\nsamples and ImageNet with 1% labeled samples), achieving the new\nstate-of-the-art performances.\n","date":"2025-03-23"}
{"id":"2503.18245","title":"DiffGED: Computing Graph Edit Distance via Diffusion-based Graph\n  Matching","abstract":"  The Graph Edit Distance (GED) problem, which aims to compute the minimum\nnumber of edit operations required to transform one graph into another, is a\nfundamental challenge in graph analysis with wide-ranging applications.\nHowever, due to its NP-hard nature, traditional A* approaches often suffer from\nscalability issue, making them computationally intractable for large graphs.\nMany recent deep learning frameworks address GED by formulating it as a\nregression task, which, while efficient, fails to recover the edit path -- a\ncentral interest in GED. Furthermore, recent hybrid approaches that combine\ndeep learning with traditional methods to recover the edit path often yield\npoor solution quality. These methods also struggle to generate candidate\nsolutions in parallel, resulting in increased running times.In this paper, we\npresent a novel approach, DiffGED, that leverages generative diffusion model to\nsolve GED and recover the corresponding edit path. Specifically, we first\ngenerate multiple diverse node matching matrices in parallel through a\ndiffusion-based graph matching model. Next, node mappings are extracted from\neach generated matching matrices in parallel, and each extracted node mapping\ncan be simply transformed into an edit path. Benefiting from the generative\ndiversity provided by the diffusion model, DiffGED is less likely to fall into\nlocal sub-optimal solutions, thereby achieving superior overall solution\nquality close to the exact solution. Experimental results on real-world\ndatasets demonstrate that DiffGED can generate multiple diverse edit paths with\nexceptionally high accuracy comparable to exact solutions while maintaining a\nrunning time shorter than most of hybrid approaches.\n","date":"2025-03-24"}
{"id":"2503.18246","title":"ZECO: ZeroFusion Guided 3D MRI Conditional Generation","abstract":"  Medical image segmentation is crucial for enhancing diagnostic accuracy and\ntreatment planning in Magnetic Resonance Imaging (MRI). However, acquiring\nprecise lesion masks for segmentation model training demands specialized\nexpertise and significant time investment, leading to a small dataset scale in\nclinical practice. In this paper, we present ZECO, a ZeroFusion guided 3D MRI\nconditional generation framework that extracts, compresses, and generates\nhigh-fidelity MRI images with corresponding 3D segmentation masks to mitigate\ndata scarcity. To effectively capture inter-slice relationships within volumes,\nwe introduce a Spatial Transformation Module that encodes MRI images into a\ncompact latent space for the diffusion process. Moving beyond unconditional\ngeneration, our novel ZeroFusion method progressively maps 3D masks to MRI\nimages in latent space, enabling robust training on limited datasets while\navoiding overfitting. ZECO outperforms state-of-the-art models in both\nquantitative and qualitative evaluations on Brain MRI datasets across various\nmodalities, showcasing its exceptional capability in synthesizing high-quality\nMRI images conditioned on segmentation masks.\n","date":"2025-03-24"}
{"id":"2503.18247","title":"AfroXLMR-Social: Adapting Pre-trained Language Models for African\n  Languages Social Media Text","abstract":"  Pretrained Language Models (PLMs) built from various sources are the\nfoundation of today's NLP progress. Language representations learned by such\nmodels achieve strong performance across many tasks with datasets of varying\nsizes drawn from various sources. We explore a thorough analysis of domain and\ntask adaptive continual pretraining approaches for low-resource African\nlanguages and a promising result is shown for the evaluated tasks. We create\nAfriSocial, a corpus designed for domain adaptive finetuning that passes\nthrough quality pre-processing steps. Continual pretraining PLMs using\nAfriSocial as domain adaptive pretraining (DAPT) data, consistently improves\nperformance on fine-grained emotion classification task of 16 targeted\nlanguages from 1% to 28.27% macro F1 score. Likewise, using the task adaptive\npertaining (TAPT) approach, further finetuning with small unlabeled but similar\ntask data shows promising results. For example, unlabeled sentiment data\n(source) for fine-grained emotion classification task (target) improves the\nbase model results by an F1 score ranging from 0.55% to 15.11%. Combining the\ntwo methods, DAPT + TAPT, achieves also better results than base models. All\nthe resources will be available to improve low-resource NLP tasks, generally,\nas well as other similar domain tasks such as hate speech and sentiment tasks.\n","date":"2025-03-24"}
{"id":"2503.18250","title":"PAD: Towards Efficient Data Generation for Transfer Learning Using\n  Phrase Alignment","abstract":"  Transfer learning leverages the abundance of English data to address the\nscarcity of resources in modeling non-English languages, such as Korean. In\nthis study, we explore the potential of Phrase Aligned Data (PAD) from\nstandardized Statistical Machine Translation (SMT) to enhance the efficiency of\ntransfer learning. Through extensive experiments, we demonstrate that PAD\nsynergizes effectively with the syntactic characteristics of the Korean\nlanguage, mitigating the weaknesses of SMT and significantly improving model\nperformance. Moreover, we reveal that PAD complements traditional data\nconstruction methods and enhances their effectiveness when combined. This\ninnovative approach not only boosts model performance but also suggests a\ncost-efficient solution for resource-scarce languages.\n","date":"2025-03-24"}
{"id":"2503.18253","title":"Enhancing Multi-Label Emotion Analysis and Corresponding Intensities for\n  Ethiopian Languages","abstract":"  In this digital world, people freely express their emotions using different\nsocial media platforms. As a result, modeling and integrating\nemotion-understanding models are vital for various human-computer interaction\ntasks such as decision-making, product and customer feedback analysis,\npolitical promotions, marketing research, and social media monitoring. As users\nexpress different emotions simultaneously in a single instance, annotating\nemotions in a multilabel setting such as the EthioEmo (Belay et al., 2025)\ndataset effectively captures this dynamic. Additionally, incorporating\nintensity, or the degree of emotion, is crucial, as emotions can significantly\ndiffer in their expressive strength and impact. This intensity is significant\nfor assessing whether further action is necessary in decision-making processes,\nespecially concerning negative emotions in applications such as healthcare and\nmental health studies. To enhance the EthioEmo dataset, we include annotations\nfor the intensity of each labeled emotion. Furthermore, we evaluate various\nstate-of-the-art encoder-only Pretrained Language Models (PLMs) and\ndecoder-only Large Language Models (LLMs) to provide comprehensive\nbenchmarking.\n","date":"2025-03-24"}
{"id":"2503.18254","title":"Surface-Aware Distilled 3D Semantic Features","abstract":"  Many 3D tasks such as pose alignment, animation, motion transfer, and 3D\nreconstruction rely on establishing correspondences between 3D shapes. This\nchallenge has recently been approached by matching of semantic features from\npre-trained vision models. However, despite their power, these features\nstruggle to differentiate instances of the same semantic class such as \"left\nhand\" versus \"right hand\" which leads to substantial mapping errors. To solve\nthis, we learn a surface-aware embedding space that is robust to these\nambiguities. Importantly, our approach is self-supervised and requires only a\nsmall number of unpaired training meshes to infer features for new 3D shapes at\ntest time. We achieve this by introducing a contrastive loss that preserves the\nsemantic content of the features distilled from foundational models while\ndisambiguating features located far apart on the shape's surface. We observe\nsuperior performance in correspondence matching benchmarks and enable\ndownstream applications including in-part segmentation, pose alignment, and\nmotion transfer. The project site is available at\nhttps:\/\/lukas.uzolas.com\/SurfaceAware3DFeaturesSite.\n","date":"2025-03-24"}
{"id":"2503.18255","title":"The Human-Machine Identity Blur: A Unified Framework for Cybersecurity\n  Risk Management in 2025","abstract":"  The modern enterprise is facing an unprecedented surge in digital identities,\nwith machine identities now significantly outnumbering human identities. This\npaper examines the cybersecurity risks emerging from what we define as the\n\"human-machine identity blur\" - the point at which human and machine identities\nintersect, delegate authority, and create new attack surfaces. Drawing from\nindustry data, expert insights, and real-world incident analysis, we identify\nkey governance gaps in current identity management models that treat human and\nmachine entities as separate domains. To address these challenges, we propose a\nUnified Identity Governance Framework based on four core principles: treating\nidentity as a continuum rather than a binary distinction, applying consistent\nrisk evaluation across all identity types, implementing continuous verification\nguided by zero trust principles, and maintaining governance throughout the\nentire identity lifecycle. Our research shows that organizations adopting this\nunified approach experience a 47 percent reduction in identity-related security\nincidents and a 62 percent improvement in incident response time. We conclude\nby offering a practical implementation roadmap and outlining future research\ndirections as AI-driven systems become increasingly autonomous.\n","date":"2025-03-24"}
{"id":"2503.18258","title":"Severing Spurious Correlations with Data Pruning","abstract":"  Deep neural networks have been shown to learn and rely on spurious\ncorrelations present in the data that they are trained on. Reliance on such\ncorrelations can cause these networks to malfunction when deployed in the real\nworld, where these correlations may no longer hold. To overcome the learning of\nand reliance on such correlations, recent studies propose approaches that yield\npromising results. These works, however, study settings where the strength of\nthe spurious signal is significantly greater than that of the core, invariant\nsignal, making it easier to detect the presence of spurious features in\nindividual training samples and allow for further processing. In this paper, we\nidentify new settings where the strength of the spurious signal is relatively\nweaker, making it difficult to detect any spurious information while continuing\nto have catastrophic consequences. We also discover that spurious correlations\nare learned primarily due to only a handful of all the samples containing the\nspurious feature and develop a novel data pruning technique that identifies and\nprunes small subsets of the training data that contain these samples. Our\nproposed technique does not require inferred domain knowledge, information\nregarding the sample-wise presence or nature of spurious information, or human\nintervention. Finally, we show that such data pruning attains state-of-the-art\nperformance on previously studied settings where spurious information is\nidentifiable.\n","date":"2025-03-24"}
{"id":"2503.18260","title":"Bridging Emotions and Architecture: Sentiment Analysis in Modern\n  Distributed Systems","abstract":"  Sentiment analysis is a field within NLP that has gained importance because\nit is applied in various areas such as; social media surveillance, customer\nfeedback evaluation and market research. At the same time, distributed systems\nallow for effective processing of large amounts of data. Therefore, this paper\nexamines how sentiment analysis converges with distributed systems by\nconcentrating on different approaches, challenges and future investigations.\nFurthermore, we do an extensive experiment where we train sentiment analysis\nmodels using both single node configuration and distributed architecture to\nbring out the benefits and shortcomings of each method in terms of performance\nand accuracy.\n","date":"2025-03-24"}
{"id":"2503.18263","title":"PNN: A Novel Progressive Neural Network for Fault Classification in\n  Rotating Machinery under Small Dataset Constraint","abstract":"  Fault detection in rotating machinery is a complex task, particularly in\nsmall and heterogeneous dataset scenarios. Variability in sensor placement,\nmachinery configurations, and structural differences further increase the\ncomplexity of the problem. Conventional deep learning approaches often demand\nlarge, homogeneous datasets, limiting their applicability in data-scarce\nindustrial environments. While transfer learning and few-shot learning have\nshown potential, however, they are often constrained by the need for extensive\nfault datasets. This research introduces a unified framework leveraging a novel\nprogressive neural network (PNN) architecture designed to address these\nchallenges. The PNN sequentially estimates the fixed-size refined features of\nthe higher order with the help of all previously estimated features and appends\nthem to the feature set. This fixed-size feature output at each layer controls\nthe complexity of the PNN and makes it suitable for effective learning from\nsmall datasets. The framework's effectiveness is validated on eight datasets,\nincluding six open-source datasets, one in-house fault simulator, and one\nreal-world industrial dataset. The PNN achieves state-of-the-art performance in\nfault detection across varying dataset sizes and machinery types, highlighting\nsuperior generalization and classification capabilities.\n","date":"2025-03-24"}
{"id":"2503.18265","title":"Risk Management for Distributed Arbitrage Systems: Integrating\n  Artificial Intelligence","abstract":"  Effective risk management solutions become absolutely crucial when financial\nmarkets embrace distributed technology and decentralized financing (DeFi). This\nstudy offers a thorough survey and comparative analysis of the integration of\nartificial intelligence (AI) in risk management for distributed arbitrage\nsystems. We examine several modern caching techniques namely in memory caching,\ndistributed caching, and proxy caching and their functions in enhancing\nperformance in decentralized settings. Through literature review we examine the\nutilization of AI techniques for alleviating risks related to market\nvolatility, liquidity challenges, operational failures, regulatory compliance,\nand security threats. This comparison research evaluates various case studies\nfrom prominent DeFi technologies, emphasizing critical performance metrics like\nlatency reduction, load balancing, and system resilience. Additionally, we\nexamine the problems and trade offs associated with these technologies,\nemphasizing their effects on consistency, scalability, and fault tolerance. By\nmeticulously analyzing real world applications, specifically centering on the\nAave platform as our principal case study, we illustrate how the purposeful\namalgamation of AI with contemporary caching methodologies has revolutionized\nrisk management in distributed arbitrage systems.\n","date":"2025-03-24"}
{"id":"2503.18267","title":"Enhancing Dataset Distillation via Non-Critical Region Refinement","abstract":"  Dataset distillation has become a popular method for compressing large\ndatasets into smaller, more efficient representations while preserving critical\ninformation for model training. Data features are broadly categorized into two\ntypes: instance-specific features, which capture unique, fine-grained details\nof individual examples, and class-general features, which represent shared,\nbroad patterns across a class. However, previous approaches often struggle to\nbalance these features-some focus solely on class-general patterns, neglecting\nfiner instance details, while others prioritize instance-specific features,\noverlooking the shared characteristics essential for class-level understanding.\nIn this paper, we introduce the Non-Critical Region Refinement Dataset\nDistillation (NRR-DD) method, which preserves instance-specific details and\nfine-grained regions in synthetic data while enriching non-critical regions\nwith class-general information. This approach enables models to leverage all\npixel information, capturing both feature types and enhancing overall\nperformance. Additionally, we present Distance-Based Representative (DBR)\nknowledge transfer, which eliminates the need for soft labels in training by\nrelying on the distance between synthetic data predictions and one-hot encoded\nlabels. Experimental results show that NRR-DD achieves state-of-the-art\nperformance on both small- and large-scale datasets. Furthermore, by storing\nonly two distances per instance, our method delivers comparable results across\nvarious settings. The code is available at\nhttps:\/\/github.com\/tmtuan1307\/NRR-DD.\n","date":"2025-03-24"}
{"id":"2503.18273","title":"Analyzing Islamophobic Discourse Using Semi-Coded Terms and LLMs","abstract":"  Islamophobia started evolving into a global phenomenon by attracting\nfollowers across the globe, particularly in Western societies. Thus,\nunderstanding Islamophobia's global spread and online dissemination is crucial.\nThis paper performs a large-scale analysis of specialized, semi-coded\nIslamophobic terms such as (muzrat, pislam, mudslime, mohammedan, muzzies)\nfloated on extremist social platforms, i.e., 4Chan, Gab, Telegram, etc. First,\nwe use large language models (LLMs) to show their ability to understand these\nterms. Second, using Google Perspective API, we also find that Islamophobic\ntext is more toxic compared to other kinds of hate speech. Finally, we use BERT\ntopic modeling approach to extract different topics and Islamophobic discourse\non these social platforms. Our findings indicate that LLMs understand these\nOut-Of-Vocabulary (OOV) slurs; however, measures are still required to control\nsuch discourse. Our topic modeling also indicates that Islamophobic text is\nfound across various political, conspiratorial, and far-right movements and is\nparticularly directed against Muslim immigrants. Taken altogether, we performed\nthe first study on Islamophobic semi-coded terms and shed a global light on\nIslamophobia.\n","date":"2025-03-24"}
{"id":"2503.18275","title":"GI-SLAM: Gaussian-Inertial SLAM","abstract":"  3D Gaussian Splatting (3DGS) has recently emerged as a powerful\nrepresentation of geometry and appearance for dense Simultaneous Localization\nand Mapping (SLAM). Through rapid, differentiable rasterization of 3D\nGaussians, many 3DGS SLAM methods achieve near real-time rendering and\naccelerated training. However, these methods largely overlook inertial data,\nwitch is a critical piece of information collected from the inertial\nmeasurement unit (IMU). In this paper, we present GI-SLAM, a novel\ngaussian-inertial SLAM system which consists of an IMU-enhanced camera tracking\nmodule and a realistic 3D Gaussian-based scene representation for mapping. Our\nmethod introduces an IMU loss that seamlessly integrates into the deep learning\nframework underpinning 3D Gaussian Splatting SLAM, effectively enhancing the\naccuracy, robustness and efficiency of camera tracking. Moreover, our SLAM\nsystem supports a wide range of sensor configurations, including monocular,\nstereo, and RGBD cameras, both with and without IMU integration. Our method\nachieves competitive performance compared with existing state-of-the-art\nreal-time methods on the EuRoC and TUM-RGBD datasets.\n","date":"2025-03-24"}
{"id":"2503.18278","title":"TopV: Compatible Token Pruning with Inference Time Optimization for Fast\n  and Low-Memory Multimodal Vision Language Model","abstract":"  Vision-Language Models (VLMs) demand substantial computational resources\nduring inference, largely due to the extensive visual input tokens for\nrepresenting visual information. Previous studies have noted that visual tokens\ntend to receive less attention than text tokens, suggesting their lower\nimportance during inference and potential for pruning. However, their methods\nencounter several challenges: reliance on greedy heuristic criteria for token\nimportance and incompatibility with FlashAttention and KV cache. To address\nthese issues, we introduce \\textbf{TopV}, a compatible \\textbf{TO}ken\n\\textbf{P}runing with inference Time Optimization for fast and low-memory\n\\textbf{V}LM, achieving efficient pruning without additional training or\nfine-tuning. Instead of relying on attention scores, we formulate token pruning\nas an optimization problem, accurately identifying important visual tokens\nwhile remaining compatible with FlashAttention. Additionally, since we only\nperform this pruning once during the prefilling stage, it effectively reduces\nKV cache size. Our optimization framework incorporates a visual-aware cost\nfunction considering factors such as Feature Similarity, Relative Spatial\nDistance, and Absolute Central Distance, to measure the importance of each\nsource visual token, enabling effective pruning of low-importance tokens.\nExtensive experiments demonstrate that our method outperforms previous token\npruning methods, validating the effectiveness and efficiency of our approach.\n","date":"2025-03-24"}
{"id":"2503.18282","title":"TrackID3x3: A Dataset and Algorithm for Multi-Player Tracking with\n  Identification and Pose Estimation in 3x3 Basketball Full-court Videos","abstract":"  Multi-object tracking, player identification, and pose estimation are\nfundamental components of sports analytics, essential for analyzing player\nmovements, performance, and tactical strategies. However, existing datasets and\nmethodologies primarily target mainstream team sports such as soccer and\nconventional 5-on-5 basketball, often overlooking scenarios involving\nfixed-camera setups commonly used at amateur levels, less mainstream sports, or\ndatasets that explicitly incorporate pose annotations. In this paper, we\npropose the TrackID3x3 dataset, the first publicly available comprehensive\ndataset specifically designed for multi-player tracking, player identification,\nand pose estimation in 3x3 basketball scenarios. The dataset comprises three\ndistinct subsets (Indoor fixed-camera, Outdoor fixed-camera, and Drone camera\nfootage), capturing diverse full-court camera perspectives and environments. We\nalso introduce the Track-ID task, a simplified variant of the game state\nreconstruction task that excludes field detection and focuses exclusively on\nfixed-camera scenarios. To evaluate performance, we propose a baseline\nalgorithm called Track-ID algorithm, tailored to assess tracking and\nidentification quality. Furthermore, our benchmark experiments, utilizing\nrecent multi-object tracking algorithms (e.g., BoT-SORT-ReID) and top-down pose\nestimation methods (HRNet, RTMPose, and SwinPose), demonstrate robust results\nand highlight remaining challenges. Our dataset and evaluation benchmarks\nprovide a solid foundation for advancing automated analytics in 3x3 basketball.\nDataset and code will be available at\nhttps:\/\/github.com\/open-starlab\/TrackID3x3.\n","date":"2025-03-24"}
{"id":"2503.18283","title":"Voxel-based Point Cloud Geometry Compression with Space-to-Channel\n  Context","abstract":"  Voxel-based methods are among the most efficient for point cloud geometry\ncompression, particularly with dense point clouds. However, they face\nlimitations due to a restricted receptive field, especially when handling\nhigh-bit depth point clouds. To overcome this issue, we introduce a stage-wise\nSpace-to-Channel (S2C) context model for both dense point clouds and low-level\nsparse point clouds. This model utilizes a channel-wise autoregressive strategy\nto effectively integrate neighborhood information at a coarse resolution. For\nhigh-level sparse point clouds, we further propose a level-wise S2C context\nmodel that addresses resolution limitations by incorporating Geometry Residual\nCoding (GRC) for consistent-resolution cross-level prediction. Additionally, we\nuse the spherical coordinate system for its compact representation and enhance\nour GRC approach with a Residual Probability Approximation (RPA) module, which\nfeatures a large kernel size. Experimental results show that our S2C context\nmodel not only achieves bit savings while maintaining or improving\nreconstruction quality but also reduces computational complexity compared to\nstate-of-the-art voxel-based compression methods.\n","date":"2025-03-24"}
{"id":"2503.18284","title":"Byzantine-Resilient Over-the-Air Federated Learning under Zero-Trust\n  Architecture","abstract":"  Over-the-air computation (AirComp) has emerged as an essential approach for\nenabling communication-efficient federated learning (FL) over wireless\nnetworks. Nonetheless, the inherent analog transmission mechanism in\nAirComp-based FL (AirFL) intensifies challenges posed by potential Byzantine\nattacks. In this paper, we propose a novel Byzantine-robust FL paradigm for\nover-the-air transmissions, referred to as federated learning with secure\nadaptive clustering (FedSAC). FedSAC aims to protect a portion of the devices\nfrom attacks through zero trust architecture (ZTA) based Byzantine\nidentification and adaptive device clustering. By conducting a one-step\nconvergence analysis, we theoretically characterize the convergence behavior\nwith different device clustering mechanisms and uneven aggregation weighting\nfactors for each device. Building upon our analytical results, we formulate a\njoint optimization problem for the clustering and weighting factors in each\ncommunication round. To facilitate the targeted optimization, we propose a\ndynamic Byzantine identification method using historical reputation based on\nZTA. Furthermore, we introduce a sequential clustering method, transforming the\njoint optimization into a weighting optimization problem without sacrificing\nthe optimality. To optimize the weighting, we capitalize on the penalty\nconvex-concave procedure (P-CCP) to obtain a stationary solution. Numerical\nresults substantiate the superiority of the proposed FedSAC over existing\nmethods in terms of both test accuracy and convergence rate.\n","date":"2025-03-24"}
{"id":"2503.18286","title":"CO-SPY: Combining Semantic and Pixel Features to Detect Synthetic Images\n  by AI","abstract":"  With the rapid advancement of generative AI, it is now possible to synthesize\nhigh-quality images in a few seconds. Despite the power of these technologies,\nthey raise significant concerns regarding misuse. Current efforts to\ndistinguish between real and AI-generated images may lack generalization, being\neffective for only certain types of generative models and susceptible to\npost-processing techniques like JPEG compression. To overcome these\nlimitations, we propose a novel framework, Co-Spy, that first enhances existing\nsemantic features (e.g., the number of fingers in a hand) and artifact features\n(e.g., pixel value differences), and then adaptively integrates them to achieve\nmore general and robust synthetic image detection. Additionally, we create\nCo-Spy-Bench, a comprehensive dataset comprising 5 real image datasets and 22\nstate-of-the-art generative models, including the latest models like FLUX. We\nalso collect 50k synthetic images in the wild from the Internet to enable\nevaluation in a more practical setting. Our extensive evaluations demonstrate\nthat our detector outperforms existing methods under identical training\nconditions, achieving an average accuracy improvement of approximately 11% to\n34%. The code is available at https:\/\/github.com\/Megum1\/Co-Spy.\n","date":"2025-03-24"}
{"id":"2503.18288","title":"Sun-Shine: A Large Language Model for Tibetan Culture","abstract":"  Tibetan, a minority language in China, features a highly intricate\ngrammatical structure, characterized by four verb tenses and a tense system\nwith frequent irregularities, contributing to its extensive inflectional\ndiversity. Recently, advances in Large Language Models (LLMs) have transformed\nthe paradigm in many domains. Despite the success in other fields, current LLMs\noften fall short in catering to the needs of domain experts like Tibetans, and\nthe potential of LLMs for Tibetan culture is under-explored. The intrinsic\nreasons are the immense and intricate nature of Tibetan culture as well as the\nnecessity for higher granularity and richness in knowledge. Simultaneously, the\ncomplexity and uniqueness of its grammatical structure, coupled with its status\nas a minority ethnic language, contribute to data scarcity, which remains a\nfundamental challenge. To alleviate these issues, we introduce Llama-Sunshine\n(Sun-Shine), the first large language model for Tibetan culture, which is\nexpert in various Tibetan language processing tasks. Sun-Shine incorporates\nstate-of-the-art model architectures optimized for Tibetan's linguistic\nfeatures. We also propose TIB-STC, a comprehensive dataset comprising diverse\nTibetan texts such as literature, religious scripts, news, and conversational\ndata, which is also the first large-scale dataset for Tibetan culture. Though\ncomprehensive experiments, Sun-Shine not only demonstrates a higher level of\nknowledge expertise for Tibetan culture but also gains preliminary embodied\nintelligence capabilities in Tibetan language processing tasks, like language\nmodeling, text classification, machine translation, and syntactic analysis.\nMoreover, it excels in low-resource scenarios, showcasing strong generalization\ncapabilities.\n","date":"2025-03-24"}
{"id":"2503.18290","title":"When is dataset cartography ineffective? Using training dynamics does\n  not improve robustness against Adversarial SQuAD","abstract":"  In this paper, I investigate the effectiveness of dataset cartography for\nextractive question answering on the SQuAD dataset. I begin by analyzing\nannotation artifacts in SQuAD and evaluate the impact of two adversarial\ndatasets, AddSent and AddOneSent, on an ELECTRA-small model. Using training\ndynamics, I partition SQuAD into easy-to-learn, ambiguous, and hard-to-learn\nsubsets. I then compare the performance of models trained on these subsets to\nthose trained on randomly selected samples of equal size. Results show that\ntraining on cartography-based subsets does not improve generalization to the\nSQuAD validation set or the AddSent adversarial set. While the hard-to-learn\nsubset yields a slightly higher F1 score on the AddOneSent dataset, the overall\ngains are limited. These findings suggest that dataset cartography provides\nlittle benefit for adversarial robustness in SQuAD-style QA tasks. I conclude\nby comparing these results to prior findings on SNLI and discuss possible\nreasons for the observed differences.\n","date":"2025-03-24"}
{"id":"2503.18293","title":"Fact-checking AI-generated news reports: Can LLMs catch their own lies?","abstract":"  In this paper, we evaluate the ability of Large Language Models (LLMs) to\nassess the veracity of claims in ''news reports'' generated by themselves or\nother LLMs. Our goal is to determine whether LLMs can effectively fact-check\ntheir own content, using methods similar to those used to verify claims made by\nhumans. Our findings indicate that LLMs are more effective at assessing claims\nin national or international news stories than in local news stories, better at\nevaluating static information than dynamic information, and better at verifying\ntrue claims compared to false ones. We hypothesize that this disparity arises\nbecause the former types of claims are better represented in the training data.\nAdditionally, we find that incorporating retrieved results from a search engine\nin a Retrieval-Augmented Generation (RAG) setting significantly reduces the\nnumber of claims an LLM cannot assess. However, this approach also increases\nthe occurrence of incorrect assessments, partly due to irrelevant or\nlow-quality search results. This diagnostic study highlights the need for\nfuture research on fact-checking machine-generated reports to prioritize\nimproving the precision and relevance of retrieved information to better\nsupport fact-checking efforts. Furthermore, claims about dynamic events and\nlocal news may require human-in-the-loop fact-checking systems to ensure\naccuracy and reliability.\n","date":"2025-03-24"}
{"id":"2503.18294","title":"LGPS: A Lightweight GAN-Based Approach for Polyp Segmentation in\n  Colonoscopy Images","abstract":"  Colorectal cancer (CRC) is a major global cause of cancer-related deaths,\nwith early polyp detection and removal during colonoscopy being crucial for\nprevention. While deep learning methods have shown promise in polyp\nsegmentation, challenges such as high computational costs, difficulty in\nsegmenting small or low-contrast polyps, and limited generalizability across\ndatasets persist. To address these issues, we propose LGPS, a lightweight\nGAN-based framework for polyp segmentation. LGPS incorporates three key\ninnovations: (1) a MobileNetV2 backbone enhanced with modified residual blocks\nand Squeeze-and-Excitation (ResE) modules for efficient feature extraction; (2)\nConvolutional Conditional Random Fields (ConvCRF) for precise boundary\nrefinement; and (3) a hybrid loss function combining Binary Cross-Entropy,\nWeighted IoU Loss, and Dice Loss to address class imbalance and enhance\nsegmentation accuracy. LGPS is validated on five benchmark datasets and\ncompared with state-of-the-art(SOTA) methods. On the largest and challenging\nPolypGen test dataset, LGPS achieves a Dice of 0.7299 and an IoU of 0.7867,\noutperformed all SOTA works and demonstrating robust generalization. With only\n1.07 million parameters, LGPS is 17 times smaller than the smallest existing\nmodel, making it highly suitable for real-time clinical applications. Its\nlightweight design and strong performance underscore its potential for\nimproving early CRC diagnosis. Code is available at\nhttps:\/\/github.com\/Falmi\/LGPS\/.\n","date":"2025-03-24"}
{"id":"2503.18296","title":"Surgical Action Planning with Large Language Models","abstract":"  In robot-assisted minimally invasive surgery, we introduce the Surgical\nAction Planning (SAP) task, which generates future action plans from visual\ninputs to address the absence of intraoperative predictive planning in current\nintelligent applications. SAP shows great potential for enhancing\nintraoperative guidance and automating procedures. However, it faces challenges\nsuch as understanding instrument-action relationships and tracking surgical\nprogress. Large Language Models (LLMs) show promise in understanding surgical\nvideo content but remain underexplored for predictive decision-making in SAP,\nas they focus mainly on retrospective analysis. Challenges like data privacy,\ncomputational demands, and modality-specific constraints further highlight\nsignificant research gaps. To tackle these challenges, we introduce LLM-SAP, a\nLarge Language Models-based Surgical Action Planning framework that predicts\nfuture actions and generates text responses by interpreting natural language\nprompts of surgical goals. The text responses potentially support surgical\neducation, intraoperative decision-making, procedure documentation, and skill\nanalysis. LLM-SAP integrates two novel modules: the Near-History Focus Memory\nModule (NHF-MM) for modeling historical states and the prompts factory for\naction planning. We evaluate LLM-SAP on our constructed CholecT50-SAP dataset\nusing models like Qwen2.5 and Qwen2-VL, demonstrating its effectiveness in\nnext-action prediction. Pre-trained LLMs are tested in a zero-shot setting, and\nsupervised fine-tuning (SFT) with LoRA is implemented. Our experiments show\nthat Qwen2.5-72B-SFT surpasses Qwen2.5-72B with a 19.3% higher accuracy.\n","date":"2025-03-24"}
{"id":"2503.18297","title":"Image-to-Text for Medical Reports Using Adaptive Co-Attention and\n  Triple-LSTM Module","abstract":"  Medical report generation requires specialized expertise that general large\nmodels often fail to accurately capture. Moreover, the inherent repetition and\nsimilarity in medical data make it difficult for models to extract meaningful\nfeatures, resulting in a tendency to overfit. So in this paper, we propose a\nmultimodal model, Co-Attention Triple-LSTM Network (CA-TriNet), a deep learning\nmodel that combines transformer architectures with a Multi-LSTM network. Its\nCo-Attention module synergistically links a vision transformer with a text\ntransformer to better differentiate medical images with similarities, augmented\nby an adaptive weight operator to catch and amplify image labels with minor\nsimilarities. Furthermore, its Triple-LSTM module refines generated sentences\nusing targeted image objects. Extensive evaluations over three public datasets\nhave demonstrated that CA-TriNet outperforms state-of-the-art models in terms\nof comprehensive ability, even pre-trained large language models on some\nmetrics.\n","date":"2025-03-24"}
{"id":"2503.18302","title":"DiffMove: Group Mobility Tendency Enhanced Trajectory Recovery via\n  Diffusion Model","abstract":"  In the real world, trajectory data is often sparse and incomplete due to low\ncollection frequencies or limited device coverage. Trajectory recovery aims to\nrecover these missing trajectory points, making the trajectories denser and\nmore complete. However, this task faces two key challenges: 1) The excessive\nsparsity of individual trajectories makes it difficult to effectively leverage\nhistorical information for recovery; 2) Sparse trajectories make it harder to\ncapture complex individual mobility preferences. To address these challenges,\nwe propose a novel method called DiffMove. Firstly, we harness crowd wisdom for\ntrajectory recovery. Specifically, we construct a group tendency graph using\nthe collective trajectories of all users and then integrate the group mobility\ntrends into the location representations via graph embedding. This solves the\nchallenge of sparse trajectories being unable to rely on individual historical\ntrajectories for recovery. Secondly, we capture individual mobility preferences\nfrom both historical and current perspectives. Finally, we integrate group\nmobility tendencies and individual preferences into the spatiotemporal\ndistribution of the trajectory to recover high-quality trajectories. Extensive\nexperiments on two real-world datasets demonstrate that DiffMove outperforms\nexisting state-of-the-art methods. Further analysis validates the robustness of\nour method.\n","date":"2025-03-24"}
{"id":"2503.18303","title":"How to Capture and Study Conversations Between Research Participants and\n  ChatGPT: GPT for Researchers (g4r.org)","abstract":"  As large language models (LLMs) like ChatGPT become increasingly integrated\ninto our everyday lives--from customer service and education to creative work\nand personal productivity--understanding how people interact with these AI\nsystems has become a pressing issue. Despite the widespread use of LLMs,\nresearchers lack standardized tools for systematically studying people's\ninteractions with LLMs. To address this issue, we introduce GPT for Researchers\n(G4R), or g4r.org, a free website that researchers can use to easily create and\nintegrate a GPT Interface into their studies. At g4r.org, researchers can (1)\nenable their study participants to interact with GPT (such as ChatGPT), (2)\ncustomize GPT Interfaces to guide participants' interactions with GPT (e.g.,\nset constraints on topics or adjust GPT's tone or response style), and (3)\ncapture participants' interactions with GPT by downloading data on messages\nexchanged between participants and GPT. By facilitating study participants'\ninteractions with GPT and providing detailed data on these interactions, G4R\ncan support research on topics such as consumer interactions with AI agents or\nLLMs, AI-assisted decision-making, and linguistic patterns in human-AI\ncommunication. With this goal in mind, we provide a step-by-step guide to using\nG4R at g4r.org.\n","date":"2025-03-24"}
{"id":"2503.18309","title":"Efficient Transformed Gaussian Process State-Space Models for\n  Non-Stationary High-Dimensional Dynamical Systems","abstract":"  Gaussian process state-space models (GPSSMs) have emerged as a powerful\nframework for modeling dynamical systems, offering interpretable uncertainty\nquantification and inherent regularization. However, existing GPSSMs face\nsignificant challenges in handling high-dimensional, non-stationary systems due\nto computational inefficiencies, limited scalability, and restrictive\nstationarity assumptions. In this paper, we propose an efficient transformed\nGaussian process state-space model (ETGPSSM) to address these limitations. Our\napproach leverages a single shared Gaussian process (GP) combined with\nnormalizing flows and Bayesian neural networks, enabling efficient modeling of\ncomplex, high-dimensional state transitions while preserving scalability. To\naddress the lack of closed-form expressions for the implicit process in the\ntransformed GP, we follow its generative process and introduce an efficient\nvariational inference algorithm, aided by the ensemble Kalman filter (EnKF), to\nenable computationally tractable learning and inference. Extensive empirical\nevaluations on synthetic and real-world datasets demonstrate the superior\nperformance of our ETGPSSM in system dynamics learning, high-dimensional state\nestimation, and time-series forecasting, outperforming existing GPSSMs and\nneural network-based methods in both accuracy and computational efficiency.\n","date":"2025-03-24"}
{"id":"2503.18312","title":"Diff-Palm: Realistic Palmprint Generation with Polynomial Creases and\n  Intra-Class Variation Controllable Diffusion Models","abstract":"  Palmprint recognition is significantly limited by the lack of large-scale\npublicly available datasets. Previous methods have adopted B\\'ezier curves to\nsimulate the palm creases, which then serve as input for conditional GANs to\ngenerate realistic palmprints. However, without employing real data\nfine-tuning, the performance of the recognition model trained on these\nsynthetic datasets would drastically decline, indicating a large gap between\ngenerated and real palmprints. This is primarily due to the utilization of an\ninaccurate palm crease representation and challenges in balancing intra-class\nvariation with identity consistency. To address this, we introduce a\npolynomial-based palm crease representation that provides a new palm crease\ngeneration mechanism more closely aligned with the real distribution. We also\npropose the palm creases conditioned diffusion model with a novel intra-class\nvariation control method. By applying our proposed $K$-step noise-sharing\nsampling, we are able to synthesize palmprint datasets with large intra-class\nvariation and high identity consistency. Experimental results show that, for\nthe first time, recognition models trained solely on our synthetic datasets,\nwithout any fine-tuning, outperform those trained on real datasets.\nFurthermore, our approach achieves superior recognition performance as the\nnumber of generated identities increases.\n","date":"2025-03-24"}
{"id":"2503.18313","title":"DeepFund: Will LLM be Professional at Fund Investment? A Live Arena\n  Perspective","abstract":"  Large Language Models (LLMs) have demonstrated impressive capabilities across\nvarious domains, but their effectiveness in financial decision making,\nparticularly in fund investment, remains inadequately evaluated. Current\nbenchmarks primarily assess LLMs understanding of financial documents rather\nthan their ability to manage assets or analyze trading opportunities in dynamic\nmarket conditions. A critical limitation in existing evaluation methodologies\nis the backtesting approach, which suffers from information leakage when LLMs\nare evaluated on historical data they may have encountered during pretraining.\nThis paper introduces DeepFund, a comprehensive platform for evaluating LLM\nbased trading strategies in a simulated live environment. Our approach\nimplements a multi agent framework where LLMs serve as both analysts and\nmanagers, creating a realistic simulation of investment decision making. The\nplatform employs a forward testing methodology that mitigates information\nleakage by evaluating models on market data released after their training\ncutoff dates. We provide a web interface that visualizes model performance\nacross different market conditions and investment parameters, enabling detailed\ncomparative analysis. Through DeepFund, we aim to provide a more accurate and\nfair assessment of LLMs capabilities in fund investment, offering insights into\ntheir potential real world applications in financial markets.\n","date":"2025-03-24"}
{"id":"2503.18314","title":"LoTUS: Large-Scale Machine Unlearning with a Taste of Uncertainty","abstract":"  We present LoTUS, a novel Machine Unlearning (MU) method that eliminates the\ninfluence of training samples from pre-trained models, avoiding retraining from\nscratch. LoTUS smooths the prediction probabilities of the model up to an\ninformation-theoretic bound, mitigating its over-confidence stemming from data\nmemorization. We evaluate LoTUS on Transformer and ResNet18 models against\neight baselines across five public datasets. Beyond established MU benchmarks,\nwe evaluate unlearning on ImageNet1k, a large-scale dataset, where retraining\nis impractical, simulating real-world conditions. Moreover, we introduce the\nnovel Retrain-Free Jensen-Shannon Divergence (RF-JSD) metric to enable\nevaluation under real-world conditions. The experimental results show that\nLoTUS outperforms state-of-the-art methods in terms of both efficiency and\neffectiveness. Code: https:\/\/github.com\/cspartalis\/LoTUS.\n","date":"2025-03-24"}
{"id":"2503.18317","title":"Improved Rates of Differentially Private Nonconvex-Strongly-Concave\n  Minimax Optimization","abstract":"  In this paper, we study the problem of (finite sum) minimax optimization in\nthe Differential Privacy (DP) model. Unlike most of the previous studies on the\n(strongly) convex-concave settings or loss functions satisfying the\nPolyak-Lojasiewicz condition, here we mainly focus on the\nnonconvex-strongly-concave one, which encapsulates many models in deep learning\nsuch as deep AUC maximization. Specifically, we first analyze a DP version of\nStochastic Gradient Descent Ascent (SGDA) and show that it is possible to get a\nDP estimator whose $l_2$-norm of the gradient for the empirical risk function\nis upper bounded by $\\tilde{O}(\\frac{d^{1\/4}}{({n\\epsilon})^{1\/2}})$, where $d$\nis the model dimension and $n$ is the sample size. We then propose a new method\nwith less gradient noise variance and improve the upper bound to\n$\\tilde{O}(\\frac{d^{1\/3}}{(n\\epsilon)^{2\/3}})$, which matches the best-known\nresult for DP Empirical Risk Minimization with non-convex loss. We also\ndiscussed several lower bounds of private minimax optimization. Finally,\nexperiments on AUC maximization, generative adversarial networks, and temporal\ndifference learning with real-world data support our theoretical analysis.\n","date":"2025-03-24"}
{"id":"2503.18319","title":"A New Stochastic Approximation Method for Gradient-based Simulated\n  Parameter Estimation","abstract":"  This paper tackles the challenge of parameter calibration in stochastic\nmodels, particularly in scenarios where the likelihood function is unavailable\nin an analytical form. We introduce a gradient-based simulated parameter\nestimation framework, which employs a multi-time scale stochastic approximation\nalgorithm. This approach effectively addresses the ratio bias that arises in\nboth maximum likelihood estimation and posterior density estimation problems.\nThe proposed algorithm enhances estimation accuracy and significantly reduces\ncomputational costs, as demonstrated through extensive numerical experiments.\nOur work extends the GSPE framework to handle complex models such as hidden\nMarkov models and variational inference-based problems, offering a robust\nsolution for parameter estimation in challenging stochastic environments.\n","date":"2025-03-24"}
{"id":"2503.18320","title":"Bridging Writing Manner Gap in Visual Instruction Tuning by Creating\n  LLM-aligned Instructions","abstract":"  In the realm of Large Multi-modal Models (LMMs), the instruction quality\nduring the visual instruction tuning stage significantly influences the\nperformance of modality alignment. In this paper, we assess the instruction\nquality from a unique perspective termed \\textbf{Writing Manner}, which\nencompasses the selection of vocabulary, grammar and sentence structure to\nconvey specific semantics. We argue that there exists a substantial writing\nmanner gap between the visual instructions and the base Large Language Models\n(LLMs) within LMMs. This gap forces the pre-trained base LLMs to deviate from\ntheir original writing styles, leading to capability degradation of both base\nLLMs and LMMs. To bridge the writing manner gap while preserving the original\nsemantics, we propose directly leveraging the base LLM to align the writing\nmanner of soft-format visual instructions with that of the base LLM itself,\nresulting in novel LLM-aligned instructions. The manual writing manner\nevaluation results demonstrate that our approach successfully minimizes the\nwriting manner gap. By utilizing LLM-aligned instructions, the baseline models\nLLaVA-7B and QwenVL demonstrate enhanced resistance to hallucinations and\nnon-trivial comprehensive improvements across all $15$ visual and language\nbenchmarks.\n","date":"2025-03-24"}
{"id":"2503.18324","title":"Plug-and-Play Interpretable Responsible Text-to-Image Generation via\n  Dual-Space Multi-facet Concept Control","abstract":"  Ethical issues around text-to-image (T2I) models demand a comprehensive\ncontrol over the generative content. Existing techniques addressing these\nissues for responsible T2I models aim for the generated content to be fair and\nsafe (non-violent\/explicit). However, these methods remain bounded to handling\nthe facets of responsibility concepts individually, while also lacking in\ninterpretability. Moreover, they often require alteration to the original\nmodel, which compromises the model performance. In this work, we propose a\nunique technique to enable responsible T2I generation by simultaneously\naccounting for an extensive range of concepts for fair and safe content\ngeneration in a scalable manner. The key idea is to distill the target T2I\npipeline with an external plug-and-play mechanism that learns an interpretable\ncomposite responsible space for the desired concepts, conditioned on the target\nT2I pipeline. We use knowledge distillation and concept whitening to enable\nthis. At inference, the learned space is utilized to modulate the generative\ncontent. A typical T2I pipeline presents two plug-in points for our approach,\nnamely; the text embedding space and the diffusion model latent space. We\ndevelop modules for both points and show the effectiveness of our approach with\na range of strong results.\n","date":"2025-03-24"}
{"id":"2503.18325","title":"Towards Training-free Anomaly Detection with Vision and Language\n  Foundation Models","abstract":"  Anomaly detection is valuable for real-world applications, such as industrial\nquality inspection. However, most approaches focus on detecting local\nstructural anomalies while neglecting compositional anomalies incorporating\nlogical constraints. In this paper, we introduce LogSAD, a novel multi-modal\nframework that requires no training for both Logical and Structural Anomaly\nDetection. First, we propose a match-of-thought architecture that employs\nadvanced large multi-modal models (i.e. GPT-4V) to generate matching proposals,\nformulating interests and compositional rules of thought for anomaly detection.\nSecond, we elaborate on multi-granularity anomaly detection, consisting of\npatch tokens, sets of interests, and composition matching with vision and\nlanguage foundation models. Subsequently, we present a calibration module to\nalign anomaly scores from different detectors, followed by integration\nstrategies for the final decision. Consequently, our approach addresses both\nlogical and structural anomaly detection within a unified framework and\nachieves state-of-the-art results without the need for training, even when\ncompared to supervised approaches, highlighting its robustness and\neffectiveness. Code is available at https:\/\/github.com\/zhang0jhon\/LogSAD.\n","date":"2025-03-24"}
{"id":"2503.18328","title":"TensoFlow: Tensorial Flow-based Sampler for Inverse Rendering","abstract":"  Inverse rendering aims to recover scene geometry, material properties, and\nlighting from multi-view images. Given the complexity of light-surface\ninteractions, importance sampling is essential for the evaluation of the\nrendering equation, as it reduces variance and enhances the efficiency of Monte\nCarlo sampling. Existing inverse rendering methods typically use pre-defined\nnon-learnable importance samplers in prior manually, struggling to effectively\nmatch the spatially and directionally varied integrand and resulting in high\nvariance and suboptimal performance. To address this limitation, we propose the\nconcept of learning a spatially and directionally aware importance sampler for\nthe rendering equation to accurately and flexibly capture the unconstrained\ncomplexity of a typical scene. We further formulate TensoFlow, a generic\napproach for sampler learning in inverse rendering, enabling to closely match\nthe integrand of the rendering equation spatially and directionally.\nConcretely, our sampler is parameterized by normalizing flows, allowing both\ndirectional sampling of incident light and probability density function (PDF)\ninference. To capture the characteristics of the sampler spatially, we learn a\ntensorial representation of the scene space, which imposes spatial conditions,\ntogether with reflected direction, leading to spatially and directionally aware\nsampling distributions. Our model can be optimized by minimizing the difference\nbetween the integrand and our normalizing flow. Extensive experiments validate\nthe superiority of TensoFlow over prior alternatives on both synthetic and\nreal-world benchmarks.\n","date":"2025-03-24"}
{"id":"2503.18331","title":"Optimizing Influence Campaigns: Nudging under Bounded Confidence","abstract":"  Influence campaigns in online social networks are often run by organizations,\npolitical parties, and nation states to influence large audiences. These\ncampaigns are employed through the use of agents in the network that share\npersuasive content. Yet, their impact might be minimal if the audiences remain\nunswayed, often due to the bounded confidence phenomenon, where only a narrow\nspectrum of viewpoints can influence them. Here we show that to persuade under\nbounded confidence, an agent must nudge its targets to gradually shift their\nopinions. Using a control theory approach, we show how to construct an agent's\nnudging policy under the bounded confidence opinion dynamics model and also how\nto select targets for multiple agents in an influence campaign on a social\nnetwork. Simulations on real Twitter networks show that a multi-agent nudging\npolicy can shift the mean opinion, decrease opinion polarization, or even\nincrease it. We find that our nudging based policies outperform other common\ntechniques that do not consider the bounded confidence effect. Finally, we show\nhow to craft prompts for large language models, such as ChatGPT, to generate\ntext-based content for real nudging policies. This illustrates the practical\nfeasibility of our approach, allowing one to go from mathematical nudging\npolicies to real social media content.\n","date":"2025-03-24"}
{"id":"2503.18334","title":"Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language\n  Models","abstract":"  Test-time adaptation (TTA) of visual language models has recently attracted\nsignificant attention as a solution to the performance degradation caused by\ndistribution shifts in downstream tasks. However, existing cache-based TTA\nmethods have certain limitations. They mainly rely on the accuracy of cached\nfeature labels, and the presence of noisy pseudo-labels can cause these\nfeatures to deviate from their true distribution. This makes cache retrieval\nmethods based on similarity matching highly sensitive to outliers or extreme\nsamples. Moreover, current methods lack effective mechanisms to model class\ndistributions, which limits their ability to fully exploit the potential of\ncached information. To address these challenges, we introduce a comprehensive\nand reliable caching mechanism and propose a novel zero-shot TTA method called\n\"Cache, Residual, Gaussian\" (CRG). This method not only employs learnable\nresidual parameters to better align positive and negative visual prototypes\nwith text prototypes, thereby optimizing the quality of cached features, but\nalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically model\nintra-class feature distributions, further mitigating the impact of noisy\nfeatures. Experimental results on 13 benchmarks demonstrate that CRG\noutperforms state-of-the-art TTA methods, showcasing exceptional robustness and\nadaptability.\n","date":"2025-03-24"}
{"id":"2503.18337","title":"Coeff-Tuning: A Graph Filter Subspace View for Tuning Attention-Based\n  Large Models","abstract":"  Transformer-based large pre-trained models have shown remarkable\ngeneralization ability, and various parameter-efficient fine-tuning (PEFT)\nmethods have been proposed to customize these models on downstream tasks with\nminimal computational and memory budgets. Previous PEFT methods are primarily\ndesigned from a tensor-decomposition perspective that tries to effectively tune\nthe linear transformation by finding the smallest subset of parameters to\ntrain. Our study adopts an orthogonal view by representing the attention\noperation as a graph convolution and formulating the multi-head attention maps\nas a convolutional filter subspace, with each attention map as a subspace\nelement. In this paper, we propose to tune the large pre-trained transformers\nby learning a small set of combination coefficients that construct a more\nexpressive filter subspace from the original multi-head attention maps. We show\nanalytically and experimentally that the tuned filter subspace can effectively\nexpand the feature space of the multi-head attention and further enhance the\ncapacity of transformers. We further stabilize the fine-tuning with a residual\nparameterization of the tunable subspace coefficients, and enhance the\ngeneralization with a regularization design by directly applying dropout on the\ntunable coefficient during training. The tunable coefficients take a tiny\nnumber of parameters and can be combined with previous PEFT methods in a\nplug-and-play manner. Extensive experiments show that our approach achieves\nsuperior performances than PEFT baselines with neglectable additional\nparameters.\n","date":"2025-03-24"}
{"id":"2503.18338","title":"SPMTrack: Spatio-Temporal Parameter-Efficient Fine-Tuning with Mixture\n  of Experts for Scalable Visual Tracking","abstract":"  Most state-of-the-art trackers adopt one-stream paradigm, using a single\nVision Transformer for joint feature extraction and relation modeling of\ntemplate and search region images. However, relation modeling between different\nimage patches exhibits significant variations. For instance, background regions\ndominated by target-irrelevant information require reduced attention\nallocation, while foreground, particularly boundary areas, need to be be\nemphasized. A single model may not effectively handle all kinds of relation\nmodeling simultaneously. In this paper, we propose a novel tracker called\nSPMTrack based on mixture-of-experts tailored for visual tracking task (TMoE),\ncombining the capability of multiple experts to handle diverse relation\nmodeling more flexibly. Benefiting from TMoE, we extend relation modeling from\nimage pairs to spatio-temporal context, further improving tracking accuracy\nwith minimal increase in model parameters. Moreover, we employ TMoE as a\nparameter-efficient fine-tuning method, substantially reducing trainable\nparameters, which enables us to train SPMTrack of varying scales efficiently\nand preserve the generalization ability of pretrained models to achieve\nsuperior performance. We conduct experiments on seven datasets, and\nexperimental results demonstrate that our method significantly outperforms\ncurrent state-of-the-art trackers. The source code is available at\nhttps:\/\/github.com\/WenRuiCai\/SPMTrack.\n","date":"2025-03-24"}
{"id":"2503.18339","title":"GranQ: Granular Zero-Shot Quantization with Unified Layer-Channel\n  Awareness","abstract":"  Zero-shot quantization (ZSQ) enables neural network compression without\ntraining data, which is crucial in restricted data access environments.\nHowever, existing ZSQ methods suffer from significant activation loss in\nlow-bit environments owing to their coarse-grained scaling strategy. To address\nthis issue, we propose GranQ, a novel ZSQ approach that leverages layer-channel\nawareness to minimize the quantization error. Unlike conventional layer- or\nchannel-wise quantization, GranQ dynamically adjusts quantization granularity\nby considering both layer- and channel-level activation distributions. This\nenables fine-grained quantization while minimizing activation distortion.\nAdditionally, we introduce vectorized activation quantization, which enables\nefficient parallel computation and reduces computational overhead while\npreserving accuracy. GranQ achieves superior performance compared with those of\nstate-of-the-art ZSQ methods that employ quantization-aware training. With\nthese findings, we anticipate that GranQ will inspire novel research directions\nbeyond conventional ZSQ approaches focused on data generation and model\ntraining.\n","date":"2025-03-24"}
{"id":"2503.18341","title":"PS-EIP: Robust Photometric Stereo Based on Event Interval Profile","abstract":"  Recently, the energy-efficient photometric stereo method using an event\ncamera has been proposed to recover surface normals from events triggered by\nchanges in logarithmic Lambertian reflections under a moving directional light\nsource. However, EventPS treats each event interval independently, making it\nsensitive to noise, shadows, and non-Lambertian reflections. This paper\nproposes Photometric Stereo based on Event Interval Profile (PS-EIP), a robust\nmethod that recovers pixelwise surface normals from a time-series profile of\nevent intervals. By exploiting the continuity of the profile and introducing an\noutlier detection method based on profile shape, our approach enhances\nrobustness against outliers from shadows and specular reflections. Experiments\nusing real event data from 3D-printed objects demonstrate that PS-EIP\nsignificantly improves robustness to outliers compared to EventPS's\ndeep-learning variant, EventPS-FCN, without relying on deep learning.\n","date":"2025-03-24"}
{"id":"2503.18347","title":"Latent Embedding Adaptation for Human Preference Alignment in Diffusion\n  Planners","abstract":"  This work addresses the challenge of personalizing trajectories generated in\nautomated decision-making systems by introducing a resource-efficient approach\nthat enables rapid adaptation to individual users' preferences. Our method\nleverages a pretrained conditional diffusion model with Preference Latent\nEmbeddings (PLE), trained on a large, reward-free offline dataset. The PLE\nserves as a compact representation for capturing specific user preferences. By\nadapting the pretrained model using our proposed preference inversion method,\nwhich directly optimizes the learnable PLE, we achieve superior alignment with\nhuman preferences compared to existing solutions like Reinforcement Learning\nfrom Human Feedback (RLHF) and Low-Rank Adaptation (LoRA). To better reflect\npractical applications, we create a benchmark experiment using real human\npreferences on diverse, high-reward trajectories.\n","date":"2025-03-24"}
{"id":"2503.18349","title":"Human-Object Interaction with Vision-Language Model Guided Relative\n  Movement Dynamics","abstract":"  Human-Object Interaction (HOI) is vital for advancing simulation, animation,\nand robotics, enabling the generation of long-term, physically plausible\nmotions in 3D environments. However, existing methods often fall short of\nachieving physics realism and supporting diverse types of interactions. To\naddress these challenges, this paper introduces a unified Human-Object\nInteraction framework that provides unified control over interactions with\nstatic scenes and dynamic objects using language commands. The interactions\nbetween human and object parts can always be described as the continuous stable\nRelative Movement Dynamics (RMD) between human and object parts. By leveraging\nthe world knowledge and scene perception capabilities of Vision-Language Models\n(VLMs), we translate language commands into RMD diagrams, which are used to\nguide goal-conditioned reinforcement learning for sequential interaction with\nobjects. Our framework supports long-horizon interactions among dynamic,\narticulated, and static objects. To support the training and evaluation of our\nframework, we present a new dataset named Interplay, which includes multi-round\ntask plans generated by VLMs, covering both static and dynamic HOI tasks.\nExtensive experiments demonstrate that our proposed framework can effectively\nhandle a wide range of HOI tasks, showcasing its ability to maintain long-term,\nmulti-round transitions. For more details, please refer to our project webpage:\nhttps:\/\/rmd-hoi.github.io\/.\n","date":"2025-03-24"}
{"id":"2503.18352","title":"Diffusion-4K: Ultra-High-Resolution Image Synthesis with Latent\n  Diffusion Models","abstract":"  In this paper, we present Diffusion-4K, a novel framework for direct\nultra-high-resolution image synthesis using text-to-image diffusion models. The\ncore advancements include: (1) Aesthetic-4K Benchmark: addressing the absence\nof a publicly available 4K image synthesis dataset, we construct Aesthetic-4K,\na comprehensive benchmark for ultra-high-resolution image generation. We\ncurated a high-quality 4K dataset with carefully selected images and captions\ngenerated by GPT-4o. Additionally, we introduce GLCM Score and Compression\nRatio metrics to evaluate fine details, combined with holistic measures such as\nFID, Aesthetics and CLIPScore for a comprehensive assessment of\nultra-high-resolution images. (2) Wavelet-based Fine-tuning: we propose a\nwavelet-based fine-tuning approach for direct training with photorealistic 4K\nimages, applicable to various latent diffusion models, demonstrating its\neffectiveness in synthesizing highly detailed 4K images. Consequently,\nDiffusion-4K achieves impressive performance in high-quality image synthesis\nand text prompt adherence, especially when powered by modern large-scale\ndiffusion models (e.g., SD3-2B and Flux-12B). Extensive experimental results\nfrom our benchmark demonstrate the superiority of Diffusion-4K in\nultra-high-resolution image synthesis.\n","date":"2025-03-24"}
{"id":"2503.18358","title":"Cost-Sensitive Learning for Long-Tailed Temporal Action Segmentation","abstract":"  Temporal action segmentation in untrimmed procedural videos aims to densely\nlabel frames into action classes. These videos inherently exhibit long-tailed\ndistributions, where actions vary widely in frequency and duration. In temporal\naction segmentation approaches, we identified a bi-level learning bias. This\nbias encompasses (1) a class-level bias, stemming from class imbalance favoring\nhead classes, and (2) a transition-level bias arising from variations in\ntransitions, prioritizing commonly observed transitions. As a remedy, we\nintroduce a constrained optimization problem to alleviate both biases. We\ndefine learning states for action classes and their associated transitions and\nintegrate them into the optimization process. We propose a novel cost-sensitive\nloss function formulated as a weighted cross-entropy loss, with weights\nadaptively adjusted based on the learning state of actions and their\ntransitions. Experiments on three challenging temporal segmentation benchmarks\nand various frameworks demonstrate the effectiveness of our approach, resulting\nin significant improvements in both per-class frame-wise and segment-wise\nperformance.\n","date":"2025-03-24"}
{"id":"2503.18359","title":"Context-Enhanced Memory-Refined Transformer for Online Action Detection","abstract":"  Online Action Detection (OAD) detects actions in streaming videos using past\nobservations. State-of-the-art OAD approaches model past observations and their\ninteractions with an anticipated future. The past is encoded using short- and\nlong-term memories to capture immediate and long-range dependencies, while\nanticipation compensates for missing future context. We identify a\ntraining-inference discrepancy in existing OAD methods that hinders learning\neffectiveness. The training uses varying lengths of short-term memory, while\ninference relies on a full-length short-term memory. As a remedy, we propose a\nContext-enhanced Memory-Refined Transformer (CMeRT). CMeRT introduces a\ncontext-enhanced encoder to improve frame representations using additional\nnear-past context. It also features a memory-refined decoder to leverage\nnear-future generation to enhance performance. CMeRT achieves state-of-the-art\nin online detection and anticipation on THUMOS'14, CrossTask, and\nEPIC-Kitchens-100.\n","date":"2025-03-24"}
{"id":"2503.18360","title":"J&H: Evaluating the Robustness of Large Language Models Under\n  Knowledge-Injection Attacks in Legal Domain","abstract":"  As the scale and capabilities of Large Language Models (LLMs) increase, their\napplications in knowledge-intensive fields such as legal domain have garnered\nwidespread attention. However, it remains doubtful whether these LLMs make\njudgments based on domain knowledge for reasoning. If LLMs base their judgments\nsolely on specific words or patterns, rather than on the underlying logic of\nthe language, the ''LLM-as-judges'' paradigm poses substantial risks in the\nreal-world applications. To address this question, we propose a method of legal\nknowledge injection attacks for robustness testing, thereby inferring whether\nLLMs have learned legal knowledge and reasoning logic. In this paper, we\npropose J&H: an evaluation framework for detecting the robustness of LLMs under\nknowledge injection attacks in the legal domain. The aim of the framework is to\nexplore whether LLMs perform deductive reasoning when accomplishing legal\ntasks. To further this aim, we have attacked each part of the reasoning logic\nunderlying these tasks (major premise, minor premise, and conclusion\ngeneration). We have collected mistakes that legal experts might make in\njudicial decisions in the real world, such as typos, legal synonyms, inaccurate\nexternal legal statutes retrieval. However, in real legal practice, legal\nexperts tend to overlook these mistakes and make judgments based on logic.\nHowever, when faced with these errors, LLMs are likely to be misled by\ntypographical errors and may not utilize logic in their judgments. We conducted\nknowledge injection attacks on existing general and domain-specific LLMs.\nCurrent LLMs are not robust against the attacks employed in our experiments. In\naddition we propose and compare several methods to enhance the knowledge\nrobustness of LLMs.\n","date":"2025-03-24"}
{"id":"2503.18361","title":"NeRFPrior: Learning Neural Radiance Field as a Prior for Indoor Scene\n  Reconstruction","abstract":"  Recently, it has shown that priors are vital for neural implicit functions to\nreconstruct high-quality surfaces from multi-view RGB images. However, current\npriors require large-scale pre-training, and merely provide geometric clues\nwithout considering the importance of color. In this paper, we present\nNeRFPrior, which adopts a neural radiance field as a prior to learn signed\ndistance fields using volume rendering for surface reconstruction. Our NeRF\nprior can provide both geometric and color clues, and also get trained fast\nunder the same scene without additional data. Based on the NeRF prior, we are\nenabled to learn a signed distance function (SDF) by explicitly imposing a\nmulti-view consistency constraint on each ray intersection for surface\ninference. Specifically, at each ray intersection, we use the density in the\nprior as a coarse geometry estimation, while using the color near the surface\nas a clue to check its visibility from another view angle. For the textureless\nareas where the multi-view consistency constraint does not work well, we\nfurther introduce a depth consistency loss with confidence weights to infer the\nSDF. Our experimental results outperform the state-of-the-art methods under the\nwidely used benchmarks.\n","date":"2025-03-24"}
{"id":"2503.18363","title":"MonoInstance: Enhancing Monocular Priors via Multi-view Instance\n  Alignment for Neural Rendering and Reconstruction","abstract":"  Monocular depth priors have been widely adopted by neural rendering in\nmulti-view based tasks such as 3D reconstruction and novel view synthesis.\nHowever, due to the inconsistent prediction on each view, how to more\neffectively leverage monocular cues in a multi-view context remains a\nchallenge. Current methods treat the entire estimated depth map\nindiscriminately, and use it as ground truth supervision, while ignoring the\ninherent inaccuracy and cross-view inconsistency in monocular priors. To\nresolve these issues, we propose MonoInstance, a general approach that explores\nthe uncertainty of monocular depths to provide enhanced geometric priors for\nneural rendering and reconstruction. Our key insight lies in aligning each\nsegmented instance depths from multiple views within a common 3D space, thereby\ncasting the uncertainty estimation of monocular depths into a density measure\nwithin noisy point clouds. For high-uncertainty areas where depth priors are\nunreliable, we further introduce a constraint term that encourages the\nprojected instances to align with corresponding instance masks on nearby views.\nMonoInstance is a versatile strategy which can be seamlessly integrated into\nvarious multi-view neural rendering frameworks. Our experimental results\ndemonstrate that MonoInstance significantly improves the performance in both\nreconstruction and novel view synthesis under various benchmarks.\n","date":"2025-03-24"}
{"id":"2503.18364","title":"MaSS13K: A Matting-level Semantic Segmentation Benchmark","abstract":"  High-resolution semantic segmentation is essential for applications such as\nimage editing, bokeh imaging, AR\/VR, etc. Unfortunately, existing datasets\noften have limited resolution and lack precise mask details and boundaries. In\nthis work, we build a large-scale, matting-level semantic segmentation dataset,\nnamed MaSS13K, which consists of 13,348 real-world images, all at 4K\nresolution. MaSS13K provides high-quality mask annotations of a number of\nobjects, which are categorized into seven categories: human, vegetation,\nground, sky, water, building, and others. MaSS13K features precise masks, with\nan average mask complexity 20-50 times higher than existing semantic\nsegmentation datasets. We consequently present a method specifically designed\nfor high-resolution semantic segmentation, namely MaSSFormer, which employs an\nefficient pixel decoder that aggregates high-level semantic features and\nlow-level texture features across three stages, aiming to produce\nhigh-resolution masks with minimal computational cost. Finally, we propose a\nnew learning paradigm, which integrates the high-quality masks of the seven\ngiven categories with pseudo labels from new classes, enabling MaSSFormer to\ntransfer its accurate segmentation capability to other classes of objects. Our\nproposed MaSSFormer is comprehensively evaluated on the MaSS13K benchmark\ntogether with 14 representative segmentation models. We expect that our\nmeticulously annotated MaSS13K dataset and the MaSSFormer model can facilitate\nthe research of high-resolution and high-quality semantic segmentation.\nDatasets and codes can be found at https:\/\/github.com\/xiechenxi99\/MaSS13K.\n","date":"2025-03-24"}
{"id":"2503.18368","title":"MoST: Efficient Monarch Sparse Tuning for 3D Representation Learning","abstract":"  We introduce Monarch Sparse Tuning (MoST), the first reparameterization-based\nparameter-efficient fine-tuning (PEFT) method tailored for 3D representation\nlearning. Unlike existing adapter-based and prompt-tuning 3D PEFT methods, MoST\nintroduces no additional inference overhead and is compatible with many 3D\nrepresentation learning backbones. At its core, we present a new family of\nstructured matrices for 3D point clouds, Point Monarch, which can capture local\ngeometric features of irregular points while offering high expressiveness. MoST\nreparameterizes the dense update weight matrices as our sparse Point Monarch\nmatrices, significantly reducing parameters while retaining strong performance.\nExperiments on various backbones show that MoST is simple, effective, and\nhighly generalizable. It captures local features in point clouds, achieving\nstate-of-the-art results on multiple benchmarks, e.g., 97.5% acc. on\nScanObjectNN (PB_50_RS) and 96.2% on ModelNet40 classification, while it can\nalso combine with other matrix decompositions (e.g., Low-rank, Kronecker) to\nfurther reduce parameters.\n","date":"2025-03-24"}
{"id":"2503.18370","title":"DiffusedWrinkles: A Diffusion-Based Model for Data-Driven Garment\n  Animation","abstract":"  We present a data-driven method for learning to generate animations of 3D\ngarments using a 2D image diffusion model. In contrast to existing methods,\ntypically based on fully connected networks, graph neural networks, or\ngenerative adversarial networks, which have difficulties to cope with\nparametric garments with fine wrinkle detail, our approach is able to\nsynthesize high-quality 3D animations for a wide variety of garments and body\nshapes, while being agnostic to the garment mesh topology. Our key idea is to\nrepresent 3D garment deformations as a 2D layout-consistent texture that\nencodes 3D offsets with respect to a parametric garment template. Using this\nrepresentation, we encode a large dataset of garments simulated in various\nmotions and shapes and train a novel conditional diffusion model that is able\nto synthesize high-quality pose-shape-and-design dependent 3D garment\ndeformations. Since our model is generative, we can synthesize various\nplausible deformations for a given target pose, shape, and design.\nAdditionally, we show that we can further condition our model using an existing\ngarment state, which enables the generation of temporally coherent sequences.\n","date":"2025-03-24"}
{"id":"2503.18371","title":"Do Your Best and Get Enough Rest for Continual Learning","abstract":"  According to the forgetting curve theory, we can enhance memory retention by\nlearning extensive data and taking adequate rest. This means that in order to\neffectively retain new knowledge, it is essential to learn it thoroughly and\nensure sufficient rest so that our brain can memorize without forgetting. The\nmain takeaway from this theory is that learning extensive data at once\nnecessitates sufficient rest before learning the same data again. This aspect\nof human long-term memory retention can be effectively utilized to address the\ncontinual learning of neural networks. Retaining new knowledge for a long\nperiod of time without catastrophic forgetting is the critical problem of\ncontinual learning. Therefore, based on Ebbinghaus' theory, we introduce the\nview-batch model that adjusts the learning schedules to optimize the recall\ninterval between retraining the same samples. The proposed view-batch model\nallows the network to get enough rest to learn extensive knowledge from the\nsame samples with a recall interval of sufficient length. To this end, we\nspecifically present two approaches: 1) a replay method that guarantees the\noptimal recall interval, and 2) a self-supervised learning that acquires\nextensive knowledge from a single training sample at a time. We empirically\nshow that these approaches of our method are aligned with the forgetting curve\ntheory, which can enhance long-term memory. In our experiments, we also\ndemonstrate that our method significantly improves many state-of-the-art\ncontinual learning methods in various protocols and scenarios. We open-source\nthis project at https:\/\/github.com\/hankyul2\/ViewBatchModel.\n","date":"2025-03-24"}
{"id":"2503.18375","title":"ALWNN Empowered Automatic Modulation Classification: Conquering\n  Complexity and Scarce Sample Conditions","abstract":"  In Automatic Modulation Classification (AMC), deep learning methods have\nshown remarkable performance, offering significant advantages over traditional\napproaches and demonstrating their vast potential. Nevertheless, notable\ndrawbacks, particularly in their high demands for storage, computational\nresources, and large-scale labeled data, which limit their practical\napplication in real-world scenarios. To tackle this issue, this paper\ninnovatively proposes an automatic modulation classification model based on the\nAdaptive Lightweight Wavelet Neural Network (ALWNN) and the few-shot framework\n(MALWNN). The ALWNN model, by integrating the adaptive wavelet neural network\nand depth separable convolution, reduces the number of model parameters and\ncomputational complexity. The MALWNN framework, using ALWNN as an encoder and\nincorporating prototype network technology, decreases the model's dependence on\nthe quantity of samples. Simulation results indicate that this model performs\nremarkably well on mainstream datasets. Moreover, in terms of Floating Point\nOperations Per Second (FLOPS) and Normalized Multiply - Accumulate Complexity\n(NMACC), ALWNN significantly reduces computational complexity compared to\nexisting methods. This is further validated by real-world system tests on USRP\nand Raspberry Pi platforms. Experiments with MALWNN show its superior\nperformance in few-shot learning scenarios compared to other algorithms.\n","date":"2025-03-24"}
{"id":"2503.18377","title":"Maximum Redundancy Pruning: A Principle-Driven Layerwise Sparsity\n  Allocation for LLMs","abstract":"  Large language models (LLMs) have demonstrated impressive capabilities, but\ntheir enormous size poses significant challenges for deployment in real-world\napplications. To address this issue, researchers have sought to apply network\npruning techniques to LLMs. A critical challenge in pruning is allocation the\nsparsity for each layer. Recent sparsity allocation methods is often based on\nheuristics or search that can easily lead to suboptimal performance. In this\npaper, we conducted an extensive investigation into various LLMs and revealed\nthree significant discoveries: (1) the layerwise pruning sensitivity (LPS) of\nLLMs is highly non-uniform, (2) the choice of pruning metric affects LPS, and\n(3) the performance of a sparse model is related to the uniformity of its\nlayerwise redundancy level. Based on these observations, we propose that the\nlayerwise sparsity of LLMs should adhere to three principles:\n\\emph{non-uniformity}, \\emph{pruning metric dependency}, and \\emph{uniform\nlayerwise redundancy level} in the pruned model. To this end, we proposed\nMaximum Redundancy Pruning (MRP), an iterative pruning algorithm that prunes in\nthe most redundant layers (\\emph{i.e.}, those with the highest non-outlier\nratio) at each iteration. The achieved layerwise sparsity aligns with the\noutlined principles. We conducted extensive experiments on publicly available\nLLMs, including the LLaMA2 and OPT, across various benchmarks. Experimental\nresults validate the effectiveness of MRP, demonstrating its superiority over\nprevious methods.\n","date":"2025-03-24"}
{"id":"2503.18378","title":"Exploring State Space Model in Wavelet Domain: An Infrared and Visible\n  Image Fusion Network via Wavelet Transform and State Space Model","abstract":"  Deep learning techniques have revolutionized the infrared and visible image\nfusion (IVIF), showing remarkable efficacy on complex scenarios. However,\ncurrent methods do not fully combine frequency domain features with global\nsemantic information, which will result in suboptimal extraction of global\nfeatures across modalities and insufficient preservation of local texture\ndetails. To address these issues, we propose Wavelet-Mamba (W-Mamba), which\nintegrates wavelet transform with the state-space model (SSM). Specifically, we\nintroduce Wavelet-SSM module, which incorporates wavelet-based frequency domain\nfeature extraction and global information extraction through SSM, thereby\neffectively capturing both global and local features. Additionally, we propose\na cross-modal feature attention modulation, which facilitates efficient\ninteraction and fusion between different modalities. The experimental results\nindicate that our method achieves both visually compelling results and superior\nperformance compared to current state-of-the-art methods. Our code is available\nat https:\/\/github.com\/Lmmh058\/W-Mamba.\n","date":"2025-03-24"}
{"id":"2503.18382","title":"PP-FormulaNet: Bridging Accuracy and Efficiency in Advanced Formula\n  Recognition","abstract":"  Formula recognition is an important task in document intelligence. It\ninvolves converting mathematical expressions from document images into\nstructured symbolic formats that computers can easily work with. LaTeX is the\nmost common format used for this purpose. In this work, we present\nPP-FormulaNet, a state-of-the-art formula recognition model that excels in both\naccuracy and efficiency. To meet the diverse needs of applications, we have\ndeveloped two specialized models: PP-FormulaNet-L, tailored for high-accuracy\nscenarios, and PP-FormulaNet-S, optimized for high-efficiency contexts. Our\nextensive evaluations reveal that PP-FormulaNet-L attains accuracy levels that\nsurpass those of prominent models such as UniMERNet by a significant 6%.\nConversely, PP-FormulaNet-S operates at speeds that are over 16 times faster.\nThese advancements facilitate seamless integration of PP-FormulaNet into a\nbroad spectrum of document processing environments that involve intricate\nmathematical formulas. Furthermore, we introduce a Formula Mining System, which\nis capable of extracting a vast amount of high-quality formula data. This\nsystem further enhances the robustness and applicability of our formula\nrecognition model. Code and models are publicly available at\nPaddleOCR(https:\/\/github.com\/PaddlePaddle\/PaddleOCR) and\nPaddleX(https:\/\/github.com\/PaddlePaddle\/PaddleX).\n","date":"2025-03-24"}
{"id":"2503.18384","title":"LiDAR Remote Sensing Meets Weak Supervision: Concepts, Methods, and\n  Perspectives","abstract":"  LiDAR (Light Detection and Ranging) enables rapid and accurate acquisition of\nthree-dimensional spatial data, widely applied in remote sensing areas such as\nsurface mapping, environmental monitoring, urban modeling, and forestry\ninventory. LiDAR remote sensing primarily includes data interpretation and\nLiDAR-based inversion. However, LiDAR interpretation typically relies on dense\nand precise annotations, which are costly and time-consuming. Similarly, LiDAR\ninversion depends on scarce supervisory signals and expensive field surveys for\nannotations. To address this challenge, weakly supervised learning has gained\nsignificant attention in recent years, with many methods emerging to tackle\nLiDAR remote sensing tasks using incomplete, inaccurate, and inexact\nannotations, as well as annotations from other domains. Existing review\narticles treat LiDAR interpretation and inversion as separate tasks. This\nreview, for the first time, adopts a unified weakly supervised learning\nperspective to systematically examine research on both LiDAR interpretation and\ninversion. We summarize the latest advancements, provide a comprehensive review\nof the development and application of weakly supervised techniques in LiDAR\nremote sensing, and discuss potential future research directions in this field.\n","date":"2025-03-24"}
{"id":"2503.18385","title":"RoCA: Robust Contrastive One-class Time Series Anomaly Detection with\n  Contaminated Data","abstract":"  The accumulation of time-series signals and the absence of labels make\ntime-series Anomaly Detection (AD) a self-supervised task of deep learning.\nMethods based on normality assumptions face the following three limitations:\n(1) A single assumption could hardly characterize the whole normality or lead\nto some deviation. (2) Some assumptions may go against the principle of AD. (3)\nTheir basic assumption is that the training data is uncontaminated (free of\nanomalies), which is unrealistic in practice, leading to a decline in\nrobustness. This paper proposes a novel robust approach, RoCA, which is the\nfirst to address all of the above three challenges, as far as we are aware. It\nfuses the separated assumptions of one-class classification and contrastive\nlearning in a single training process to characterize a more complete so-called\nnormality. Additionally, it monitors the training data and computes a carefully\ndesigned anomaly score throughout the training process. This score helps\nidentify latent anomalies, which are then used to define the classification\nboundary, inspired by the concept of outlier exposure. The performance on AIOps\ndatasets improved by 6% compared to when contamination was not considered\n(COCA). On two large and high-dimensional multivariate datasets, the\nperformance increased by 5% to 10%. RoCA achieves the highest average\nperformance on both univariate and multivariate datasets. The source code is\navailable at https:\/\/github.com\/ruiking04\/RoCA.\n","date":"2025-03-24"}
{"id":"2503.18386","title":"Resource-Efficient Motion Control for Video Generation via Dynamic Mask\n  Guidance","abstract":"  Recent advances in diffusion models bring new vitality to visual content\ncreation. However, current text-to-video generation models still face\nsignificant challenges such as high training costs, substantial data\nrequirements, and difficulties in maintaining consistency between given text\nand motion of the foreground object. To address these challenges, we propose\nmask-guided video generation, which can control video generation through mask\nmotion sequences, while requiring limited training data. Our model enhances\nexisting architectures by incorporating foreground masks for precise\ntext-position matching and motion trajectory control. Through mask motion\nsequences, we guide the video generation process to maintain consistent\nforeground objects throughout the sequence. Additionally, through a first-frame\nsharing strategy and autoregressive extension approach, we achieve more stable\nand longer video generation. Extensive qualitative and quantitative experiments\ndemonstrate that this approach excels in various video generation tasks, such\nas video editing and generating artistic videos, outperforming previous methods\nin terms of consistency and quality. Our generated results can be viewed in the\nsupplementary materials.\n","date":"2025-03-24"}
{"id":"2503.18387","title":"Manipulation and the AI Act: Large Language Model Chatbots and the\n  Danger of Mirrors","abstract":"  Large Language Model chatbots are increasingly taking the form and visage of\nhuman beings, adapting human faces, names, voices, personalities, and quirks,\nincluding those of celebrities and well-known political figures. Personifying\nAI chatbots could foreseeably increase their trust with users. However, it\ncould also make them more capable of manipulation, by creating the illusion of\na close and intimate relationship with an artificial entity. The European\nCommission has finalized the AI Act, with the EU Parliament making amendments\nbanning manipulative and deceptive AI systems that cause significant harm to\nusers. Although the AI Act covers harms that accumulate over time, it is\nunlikely to prevent harms associated with prolonged discussions with AI\nchatbots. Specifically, a chatbot could reinforce a person's negative emotional\nstate over weeks, months, or years through negative feedback loops, prolonged\nconversations, or harmful recommendations, contributing to a user's\ndeteriorating mental health.\n","date":"2025-03-24"}
{"id":"2503.18391","title":"Finite-Time Bounds for Two-Time-Scale Stochastic Approximation with\n  Arbitrary Norm Contractions and Markovian Noise","abstract":"  Two-time-scale Stochastic Approximation (SA) is an iterative algorithm with\napplications in reinforcement learning and optimization. Prior finite time\nanalysis of such algorithms has focused on fixed point iterations with mappings\ncontractive under Euclidean norm. Motivated by applications in reinforcement\nlearning, we give the first mean square bound on non linear two-time-scale SA\nwhere the iterations have arbitrary norm contractive mappings and Markovian\nnoise. We show that the mean square error decays at a rate of $O(1\/n^{2\/3})$ in\nthe general case, and at a rate of $O(1\/n)$ in a special case where the slower\ntimescale is noiseless. Our analysis uses the generalized Moreau envelope to\nhandle the arbitrary norm contractions and solutions of Poisson equation to\ndeal with the Markovian noise. By analyzing the SSP Q-Learning algorithm, we\ngive the first $O(1\/n)$ bound for an algorithm for asynchronous control of MDPs\nunder the average reward criterion. We also obtain a rate of $O(1\/n)$ for\nQ-Learning with Polyak-averaging and provide an algorithm for learning\nGeneralized Nash Equilibrium (GNE) for strongly monotone games which converges\nat a rate of $O(1\/n^{2\/3})$.\n","date":"2025-03-24"}
{"id":"2503.18393","title":"PDDM: Pseudo Depth Diffusion Model for RGB-PD Semantic Segmentation\n  Based in Complex Indoor Scenes","abstract":"  The integration of RGB and depth modalities significantly enhances the\naccuracy of segmenting complex indoor scenes, with depth data from RGB-D\ncameras playing a crucial role in this improvement. However, collecting an\nRGB-D dataset is more expensive than an RGB dataset due to the need for\nspecialized depth sensors. Aligning depth and RGB images also poses challenges\ndue to sensor positioning and issues like missing data and noise. In contrast,\nPseudo Depth (PD) from high-precision depth estimation algorithms can eliminate\nthe dependence on RGB-D sensors and alignment processes, as well as provide\neffective depth information and show significant potential in semantic\nsegmentation. Therefore, to explore the practicality of utilizing pseudo depth\ninstead of real depth for semantic segmentation, we design an RGB-PD\nsegmentation pipeline to integrate RGB and pseudo depth and propose a Pseudo\nDepth Aggregation Module (PDAM) for fully exploiting the informative clues\nprovided by the diverse pseudo depth maps. The PDAM aggregates multiple pseudo\ndepth maps into a single modality, making it easily adaptable to other RGB-D\nsegmentation methods. In addition, the pre-trained diffusion model serves as a\nstrong feature extractor for RGB segmentation tasks, but multi-modal\ndiffusion-based segmentation methods remain unexplored. Therefore, we present a\nPseudo Depth Diffusion Model (PDDM) that adopts a large-scale text-image\ndiffusion model as a feature extractor and a simple yet effective fusion\nstrategy to integrate pseudo depth. To verify the applicability of pseudo depth\nand our PDDM, we perform extensive experiments on the NYUv2 and SUNRGB-D\ndatasets. The experimental results demonstrate that pseudo depth can\neffectively enhance segmentation performance, and our PDDM achieves\nstate-of-the-art performance, outperforming other methods by +6.98 mIoU on\nNYUv2 and +2.11 mIoU on SUNRGB-D.\n","date":"2025-03-24"}
{"id":"2503.18394","title":"Solving Situation Puzzles with Large Language Model and External\n  Reformulation","abstract":"  In recent years, large language models (LLMs) have shown an impressive\nability to perform arithmetic and symbolic reasoning tasks. However, we found\nthat LLMs (e.g., ChatGPT) cannot perform well on reasoning that requires\nmultiple rounds of dialogue, especially when solving situation puzzles.\nSpecifically, LLMs intend to ask very detailed questions focusing on a specific\naspect or same\/similar questions after several rounds of Q&As. To help LLMs get\nout of the above dilemma, we propose a novel external reformulation\nmethodology, where the situation puzzle will be reformulated after several\nrounds of Q&A or when the LLMs raise an incorrect guess. Experiments show\nsuperior performance (e.g., win rate, number of question\/guess attempts) of our\nmethod than directly using LLMs for solving situation puzzles, highlighting the\npotential of strategic problem reformulation to enhance the reasoning\ncapabilities of LLMs in complex interactive scenarios.\n","date":"2025-03-24"}
{"id":"2503.18395","title":"PRECTR: A Synergistic Framework for Integrating Personalized Search\n  Relevance Matching and CTR Prediction","abstract":"  The two primary tasks in the search recommendation system are search\nrelevance matching and click-through rate (CTR) prediction -- the former\nfocuses on seeking relevant items for user queries whereas the latter forecasts\nwhich item may better match user interest. Prior research typically develops\ntwo models to predict the CTR and search relevance separately, then ranking\ncandidate items based on the fusion of the two outputs. However, such a\ndivide-and-conquer paradigm creates the inconsistency between different models.\nMeanwhile, the search relevance model mainly concentrates on the degree of\nobjective text matching while neglecting personalized differences among\ndifferent users, leading to restricted model performance. To tackle these\nissues, we propose a unified Personalized Search RElevance Matching and CTR\nPrediction Fusion Model(PRECTR). Specifically, based on the conditional\nprobability fusion mechanism, PRECTR integrates the CTR prediction and search\nrelevance matching into one framework to enhance the interaction and\nconsistency of the two modules. However, directly optimizing CTR binary\nclassification loss may bring challenges to the fusion model's convergence and\nindefinitely promote the exposure of items with high CTR, regardless of their\nsearch relevance. Hence, we further introduce two-stage training and semantic\nconsistency regularization to accelerate the model's convergence and restrain\nthe recommendation of irrelevant items. Finally, acknowledging that different\nusers may have varied relevance preferences, we assessed current users'\nrelevance preferences by analyzing past users' preferences for similar queries\nand tailored incentives for different candidate items accordingly. Extensive\nexperimental results on our production dataset and online A\/B testing\ndemonstrate the effectiveness and superiority of our proposed PRECTR method.\n","date":"2025-03-24"}
{"id":"2503.18402","title":"DashGaussian: Optimizing 3D Gaussian Splatting in 200 Seconds","abstract":"  3D Gaussian Splatting (3DGS) renders pixels by rasterizing Gaussian\nprimitives, where the rendering resolution and the primitive number, concluded\nas the optimization complexity, dominate the time cost in primitive\noptimization. In this paper, we propose DashGaussian, a scheduling scheme over\nthe optimization complexity of 3DGS that strips redundant complexity to\naccelerate 3DGS optimization. Specifically, we formulate 3DGS optimization as\nprogressively fitting 3DGS to higher levels of frequency components in the\ntraining views, and propose a dynamic rendering resolution scheme that largely\nreduces the optimization complexity based on this formulation. Besides, we\nargue that a specific rendering resolution should cooperate with a proper\nprimitive number for a better balance between computing redundancy and fitting\nquality, where we schedule the growth of the primitives to synchronize with the\nrendering resolution. Extensive experiments show that our method accelerates\nthe optimization of various 3DGS backbones by 45.7% on average while preserving\nthe rendering quality.\n","date":"2025-03-24"}
{"id":"2503.18403","title":"Knowledge Graph Enhanced Generative Multi-modal Models for\n  Class-Incremental Learning","abstract":"  Continual learning in computer vision faces the critical challenge of\ncatastrophic forgetting, where models struggle to retain prior knowledge while\nadapting to new tasks. Although recent studies have attempted to leverage the\ngeneralization capabilities of pre-trained models to mitigate overfitting on\ncurrent tasks, models still tend to forget details of previously learned\ncategories as tasks progress, leading to misclassification. To address these\nlimitations, we introduce a novel Knowledge Graph Enhanced Generative\nMulti-modal model (KG-GMM) that builds an evolving knowledge graph throughout\nthe learning process. Our approach utilizes relationships within the knowledge\ngraph to augment the class labels and assigns different relations to similar\ncategories to enhance model differentiation. During testing, we propose a\nKnowledge Graph Augmented Inference method that locates specific categories by\nanalyzing relationships within the generated text, thereby reducing the loss of\ndetailed information about old classes when learning new knowledge and\nalleviating forgetting. Experiments demonstrate that our method effectively\nleverages relational information to help the model correct mispredictions,\nachieving state-of-the-art results in both conventional CIL and few-shot CIL\nsettings, confirming the efficacy of knowledge graphs at preserving knowledge\nin the continual learning scenarios.\n","date":"2025-03-24"}
{"id":"2503.18405","title":"Offline Meteorology-Pollution Coupling Global Air Pollution Forecasting\n  Model with Bilinear Pooling","abstract":"  Air pollution has become a major threat to human health, making accurate\nforecasting crucial for pollution control. Traditional physics-based models\nforecast global air pollution by coupling meteorology and pollution processes,\nusing either online or offline methods depending on whether fully integrated\nwith meteorological models and run simultaneously. However, the high\ncomputational demands of both methods severely limit real-time prediction\nefficiency. Existing deep learning (DL) solutions employ online coupling\nstrategies for global air pollution forecasting, which finetune pollution\nforecasting based on pretrained atmospheric models, requiring substantial\ntraining resources. This study pioneers a DL-based offline coupling framework\nthat utilizes bilinear pooling to achieve offline coupling between\nmeteorological fields and pollutants. The proposed model requires only 13% of\nthe parameters of DL-based online coupling models while achieving competitive\nperformance. Compared with the state-of-the-art global air pollution\nforecasting model CAMS, our approach demonstrates superiority in 63% variables\nacross all forecast time steps and 85% variables in predictions exceeding 48\nhours. This work pioneers experimental validation of the effectiveness of\nmeteorological fields in DL-based global air pollution forecasting,\ndemonstrating that offline coupling meteorological fields with pollutants can\nachieve a 15% relative reduction in RMSE across all pollution variables. The\nresearch establishes a new paradigm for real-time global air pollution warning\nsystems and delivers critical technical support for developing more efficient\nand comprehensive AI-powered global atmospheric forecasting frameworks.\n","date":"2025-03-24"}
{"id":"2503.18406","title":"Instruct-CLIP: Improving Instruction-Guided Image Editing with Automated\n  Data Refinement Using Contrastive Learning","abstract":"  Although natural language instructions offer an intuitive way to guide\nautomated image editing, deep-learning models often struggle to achieve\nhigh-quality results, largely due to the difficulty of creating large,\nhigh-quality training datasets. To do this, previous approaches have typically\nrelied on text-to-image (T2I) generative models to produce pairs of original\nand edited images that simulate the input\/output of an instruction-guided\nimage-editing model. However, these image pairs often fail to align with the\nspecified edit instructions due to the limitations of T2I models, which\nnegatively impacts models trained on such datasets. To address this, we present\nInstruct-CLIP (I-CLIP), a selfsupervised method that learns the semantic\nchanges between original and edited images to refine and better align the\ninstructions in existing datasets. Furthermore, we adapt Instruct-CLIP to\nhandle noisy latent images and diffusion timesteps so that it can be used to\ntrain latent diffusion models (LDMs) and efficiently enforce alignment between\nthe edit instruction and the image changes in latent space at any step of the\ndiffusion pipeline. We use Instruct-CLIP to correct the InstructPix2Pix dataset\nand get over 120K refined samples we then use to fine-tune their model, guided\nby our novel I-CLIP-based loss function. The resulting model can produce edits\nthat are more aligned with the given instructions. Our code and dataset are\navailable at https:\/\/github.com\/SherryXTChen\/Instruct-CLIP.git.\n","date":"2025-03-24"}
{"id":"2503.18407","title":"VTD-CLIP: Video-to-Text Discretization via Prompting CLIP","abstract":"  Vision-language models bridge visual and linguistic understanding and have\nproven to be powerful for video recognition tasks. Existing approaches\nprimarily rely on parameter-efficient fine-tuning of image-text pre-trained\nmodels, yet they often suffer from limited interpretability and poor\ngeneralization due to inadequate temporal modeling. To address these, we\npropose a simple yet effective video-to-text discretization framework. Our\nmethod repurposes the frozen text encoder to construct a visual codebook from\nvideo class labels due to the many-to-one contrastive alignment between visual\nand textual embeddings in multimodal pretraining. This codebook effectively\ntransforms temporal visual data into textual tokens via feature lookups and\noffers interpretable video representations through explicit video modeling.\nThen, to enhance robustness against irrelevant or noisy frames, we introduce a\nconfidence-aware fusion module that dynamically weights keyframes by assessing\ntheir semantic relevance via the codebook. Furthermore, our method incorporates\nlearnable text prompts to conduct adaptive codebook updates. Extensive\nexperiments on HMDB-51, UCF-101, SSv2, and Kinetics-400 have validated the\nsuperiority of our approach, achieving more competitive improvements over\nstate-of-the-art methods. The code will be publicly available at\nhttps:\/\/github.com\/isxinxin\/VTD-CLIP.\n","date":"2025-03-24"}
{"id":"2503.18408","title":"Fast and Physically-based Neural Explicit Surface for Relightable Human\n  Avatars","abstract":"  Efficiently modeling relightable human avatars from sparse-view videos is\ncrucial for AR\/VR applications. Current methods use neural implicit\nrepresentations to capture dynamic geometry and reflectance, which incur high\ncosts due to the need for dense sampling in volume rendering. To overcome these\nchallenges, we introduce Physically-based Neural Explicit Surface (PhyNES),\nwhich employs compact neural material maps based on the Neural Explicit Surface\n(NES) representation. PhyNES organizes human models in a compact 2D space,\nenhancing material disentanglement efficiency. By connecting Signed Distance\nFields to explicit surfaces, PhyNES enables efficient geometry inference around\na parameterized human shape model. This approach models dynamic geometry,\ntexture, and material maps as 2D neural representations, enabling efficient\nrasterization. PhyNES effectively captures physical surface attributes under\nvarying illumination, enabling real-time physically-based rendering.\nExperiments show that PhyNES achieves relighting quality comparable to SOTA\nmethods while significantly improving rendering speed, memory efficiency, and\nreconstruction quality.\n","date":"2025-03-24"}
{"id":"2503.18414","title":"U-REPA: Aligning Diffusion U-Nets to ViTs","abstract":"  Representation Alignment (REPA) that aligns Diffusion Transformer (DiT)\nhidden-states with ViT visual encoders has proven highly effective in DiT\ntraining, demonstrating superior convergence properties, but it has not been\nvalidated on the canonical diffusion U-Net architecture that shows faster\nconvergence compared to DiTs. However, adapting REPA to U-Net architectures\npresents unique challenges: (1) different block functionalities necessitate\nrevised alignment strategies; (2) spatial-dimension inconsistencies emerge from\nU-Net's spatial downsampling operations; (3) space gaps between U-Net and ViT\nhinder the effectiveness of tokenwise alignment. To encounter these challenges,\nwe propose U-REPA, a representation alignment paradigm that bridges U-Net\nhidden states and ViT features as follows: Firstly, we propose via observation\nthat due to skip connection, the middle stage of U-Net is the best alignment\noption. Secondly, we propose upsampling of U-Net features after passing them\nthrough MLPs. Thirdly, we observe difficulty when performing tokenwise\nsimilarity alignment, and further introduces a manifold loss that regularizes\nthe relative similarity between samples. Experiments indicate that the\nresulting U-REPA could achieve excellent generation quality and greatly\naccelerates the convergence speed. With CFG guidance interval, U-REPA could\nreach $FID<1.5$ in 200 epochs or 1M iterations on ImageNet 256 $\\times$ 256,\nand needs only half the total epochs to perform better than REPA. Codes are\navailable at https:\/\/github.com\/YuchuanTian\/U-REPA.\n","date":"2025-03-24"}
{"id":"2503.18419","title":"Generative AI in Knowledge Work: Design Implications for Data Navigation\n  and Decision-Making","abstract":"  Our study of 20 knowledge workers revealed a common challenge: the difficulty\nof synthesizing unstructured information scattered across multiple platforms to\nmake informed decisions. Drawing on their vision of an ideal knowledge\nsynthesis tool, we developed Yodeai, an AI-enabled system, to explore both the\nopportunities and limitations of AI in knowledge work. Through a user study\nwith 16 product managers, we identified three key requirements for Generative\nAI in knowledge work: adaptable user control, transparent collaboration\nmechanisms, and the ability to integrate background knowledge with external\ninformation. However, we also found significant limitations, including\noverreliance on AI, user isolation, and contextual factors outside the AI's\nreach. As AI tools become increasingly prevalent in professional settings, we\npropose design principles that emphasize adaptability to diverse workflows,\naccountability in personal and collaborative contexts, and context-aware\ninteroperability to guide the development of human-centered AI systems for\nproduct managers and knowledge workers.\n","date":"2025-03-24"}
{"id":"2503.18420","title":"Panorama Generation From NFoV Image Done Right","abstract":"  Generating 360-degree panoramas from narrow field of view (NFoV) image is a\npromising computer vision task for Virtual Reality (VR) applications. Existing\nmethods mostly assess the generated panoramas with InceptionNet or CLIP based\nmetrics, which tend to perceive the image quality and is \\textbf{not suitable\nfor evaluating the distortion}. In this work, we first propose a\ndistortion-specific CLIP, named Distort-CLIP to accurately evaluate the\npanorama distortion and discover the \\textbf{``visual cheating''} phenomenon in\nprevious works (\\ie, tending to improve the visual results by sacrificing\ndistortion accuracy). This phenomenon arises because prior methods employ a\nsingle network to learn the distinct panorama distortion and content completion\nat once, which leads the model to prioritize optimizing the latter. To address\nthe phenomenon, we propose \\textbf{PanoDecouple}, a decoupled diffusion model\nframework, which decouples the panorama generation into distortion guidance and\ncontent completion, aiming to generate panoramas with both accurate distortion\nand visual appeal. Specifically, we design a DistortNet for distortion guidance\nby imposing panorama-specific distortion prior and a modified condition\nregistration mechanism; and a ContentNet for content completion by imposing\nperspective image information. Additionally, a distortion correction loss\nfunction with Distort-CLIP is introduced to constrain the distortion\nexplicitly. The extensive experiments validate that PanoDecouple surpasses\nexisting methods both in distortion and visual metrics.\n","date":"2025-03-24"}
{"id":"2503.18421","title":"4DGC: Rate-Aware 4D Gaussian Compression for Efficient Streamable\n  Free-Viewpoint Video","abstract":"  3D Gaussian Splatting (3DGS) has substantial potential for enabling\nphotorealistic Free-Viewpoint Video (FVV) experiences. However, the vast number\nof Gaussians and their associated attributes poses significant challenges for\nstorage and transmission. Existing methods typically handle dynamic 3DGS\nrepresentation and compression separately, neglecting motion information and\nthe rate-distortion (RD) trade-off during training, leading to performance\ndegradation and increased model redundancy. To address this gap, we propose\n4DGC, a novel rate-aware 4D Gaussian compression framework that significantly\nreduces storage size while maintaining superior RD performance for FVV.\nSpecifically, 4DGC introduces a motion-aware dynamic Gaussian representation\nthat utilizes a compact motion grid combined with sparse compensated Gaussians\nto exploit inter-frame similarities. This representation effectively handles\nlarge motions, preserving quality and reducing temporal redundancy.\nFurthermore, we present an end-to-end compression scheme that employs\ndifferentiable quantization and a tiny implicit entropy model to compress the\nmotion grid and compensated Gaussians efficiently. The entire framework is\njointly optimized using a rate-distortion trade-off. Extensive experiments\ndemonstrate that 4DGC supports variable bitrates and consistently outperforms\nexisting methods in RD performance across multiple datasets.\n","date":"2025-03-24"}
{"id":"2503.18422","title":"Breaking the Encoder Barrier for Seamless Video-Language Understanding","abstract":"  Most Video-Large Language Models (Video-LLMs) adopt an encoder-decoder\nframework, where a vision encoder extracts frame-wise features for processing\nby a language model. However, this approach incurs high computational costs,\nintroduces resolution biases, and struggles to capture fine-grained multimodal\ninteractions. To overcome these limitations, we propose ELVA, an encoder-free\nVideo-LLM that directly models nuanced video-language interactions without\nrelying on a vision encoder. ELVA employs token merging to construct a\nbottom-up hierarchical representation and incorporates a video guidance\nsupervisor for direct spatiotemporal representation learning. Additionally, a\nhybrid-resolution mechanism strategically integrates high- and low-resolution\nframes as inputs to achieve an optimal balance between performance and\nefficiency. With only 7M publicly available video-text pairs, ELVA achieves\nperformance on par with encoder-based Video-LLMs while reducing FLOPs by up to\n95\\% and inference latency by 92\\%, offering a scalable and efficient solution\nfor real-time video understanding.\n","date":"2025-03-24"}
{"id":"2503.18429","title":"Teller: Real-Time Streaming Audio-Driven Portrait Animation with\n  Autoregressive Motion Generation","abstract":"  In this work, we introduce the first autoregressive framework for real-time,\naudio-driven portrait animation, a.k.a, talking head. Beyond the challenge of\nlengthy animation times, a critical challenge in realistic talking head\ngeneration lies in preserving the natural movement of diverse body parts. To\nthis end, we propose Teller, the first streaming audio-driven protrait\nanimation framework with autoregressive motion generation. Specifically, Teller\nfirst decomposes facial and body detail animation into two components: Facial\nMotion Latent Generation (FMLG) based on an autoregressive transfromer, and\nmovement authenticity refinement using a Efficient Temporal Module\n(ETM).Concretely, FMLG employs a Residual VQ model to map the facial motion\nlatent from the implicit keypoint-based model into discrete motion tokens,\nwhich are then temporally sliced with audio embeddings. This enables the AR\ntranformer to learn real-time, stream-based mappings from audio to motion.\nFurthermore, Teller incorporate ETM to capture finer motion details. This\nmodule ensures the physical consistency of body parts and accessories, such as\nneck muscles and earrings, improving the realism of these movements. Teller is\ndesigned to be efficient, surpassing the inference speed of diffusion-based\nmodels (Hallo 20.93s vs. Teller 0.92s for one second video generation), and\nachieves a real-time streaming performance of up to 25 FPS. Extensive\nexperiments demonstrate that our method outperforms recent audio-driven\nportrait animation models, especially in small movements, as validated by human\nevaluations with a significant margin in quality and realism.\n","date":"2025-03-24"}
{"id":"2503.18430","title":"CQ-DINO: Mitigating Gradient Dilution via Category Queries for Vast\n  Vocabulary Object Detection","abstract":"  With the exponential growth of data, traditional object detection methods are\nincreasingly struggling to handle vast vocabulary object detection tasks\neffectively. We analyze two key limitations of classification-based detectors:\npositive gradient dilution, where rare positive categories receive insufficient\nlearning signals, and hard negative gradient dilution, where discriminative\ngradients are overwhelmed by numerous easy negatives. To address these\nchallenges, we propose CQ-DINO, a category query-based object detection\nframework that reformulates classification as a contrastive task between object\nqueries and learnable category queries. Our method introduces image-guided\nquery selection, which reduces the negative space by adaptively retrieving\ntop-K relevant categories per image via cross-attention, thereby rebalancing\ngradient distributions and facilitating implicit hard example mining.\nFurthermore, CQ-DINO flexibly integrates explicit hierarchical category\nrelationships in structured datasets (e.g., V3Det) or learns implicit category\ncorrelations via self-attention in generic datasets (e.g., COCO). Experiments\ndemonstrate that CQ-DINO achieves superior performance on the challenging V3Det\nbenchmark (surpassing previous methods by 2.1% AP) while maintaining\ncompetitiveness in COCO. Our work provides a scalable solution for real-world\ndetection systems requiring wide category coverage. The dataset and code will\nbe publicly at https:\/\/github.com\/RedAIGC\/CQ-DINO.\n","date":"2025-03-24"}
{"id":"2503.18432","title":"Teaching LLMs for Step-Level Automatic Math Correction via Reinforcement\n  Learning","abstract":"  Automatic math correction aims to check students' solutions to mathematical\nproblems via artificial intelligence technologies. Most existing studies focus\non judging the final answer at the problem level, while they ignore detailed\nfeedback on each step in a math problem-solving process, which requires\nabilities of semantic understanding and reasoning. In this paper, we propose a\nreinforcement learning (RL)-based method to boost large language model (LLM)\nfor step-level automatic math correction, named StepAMC. Particularly, we\nconvert the step-level automatic math correction within the text classification\ntask into an RL problem to enhance the reasoning capabilities of LLMs. Then, we\ndesign a space-constrained policy network to improve the stability of RL. Then,\nwe introduce a fine-grained reward network to convert the binary human feedback\ninto a continuous value. We conduct extensive experiments over two benchmark\ndatasets and the results show that our model outperforms the eleven strong\nbaselines.\n","date":"2025-03-24"}
{"id":"2503.18434","title":"A Simple yet Effective Layout Token in Large Language Models for\n  Document Understanding","abstract":"  Recent methods that integrate spatial layouts with text for document\nunderstanding in large language models (LLMs) have shown promising results. A\ncommonly used method is to represent layout information as text tokens and\ninterleave them with text content as inputs to the LLMs. However, such a method\nstill demonstrates limitations, as it requires additional position IDs for\ntokens that are used to represent layout information. Due to the constraint on\nmax position IDs, assigning them to layout information reduces those available\nfor text content, reducing the capacity for the model to learn from the text\nduring training, while also introducing a large number of potentially untrained\nposition IDs during long-context inference, which can hinder performance on\ndocument understanding tasks. To address these issues, we propose LayTokenLLM,\na simple yet effective method for document understanding. LayTokenLLM\nrepresents layout information as a single token per text segment and uses a\nspecialized positional encoding scheme. It shares position IDs between text and\nlayout tokens, eliminating the need for additional position IDs. This design\nmaintains the model's capacity to learn from text while mitigating long-context\nissues during inference. Furthermore, a novel pre-training objective called\nNext Interleaved Text and Layout Token Prediction (NTLP) is devised to enhance\ncross-modality learning between text and layout tokens. Extensive experiments\nshow that LayTokenLLM outperforms existing layout-integrated LLMs and MLLMs of\nsimilar scales on multi-page document understanding tasks, as well as most\nsingle-page tasks.\n","date":"2025-03-24"}
{"id":"2503.18435","title":"On the Perception Bottleneck of VLMs for Chart Understanding","abstract":"  Chart understanding requires models to effectively analyze and reason about\nnumerical data, textual elements, and complex visual components. Our\nobservations reveal that the perception capabilities of existing large\nvision-language models (LVLMs) constitute a critical bottleneck in this\nprocess. In this study, we delve into this perception bottleneck by decomposing\nit into two components: the vision encoder bottleneck, where the visual\nrepresentation may fail to encapsulate the correct information, and the\nextraction bottleneck, where the language model struggles to extract the\nnecessary information from the provided visual representations. Through\ncomprehensive experiments, we find that (1) the information embedded within\nvisual representations is substantially richer than what is typically captured\nby linear extractors, such as the widely used retrieval accuracy metric; (2)\nWhile instruction tuning effectively enhances the extraction capability of\nLVLMs, the vision encoder remains a critical bottleneck, demanding focused\nattention and improvement. Therefore, we further enhance the visual encoder to\nmitigate the vision encoder bottleneck under a contrastive learning framework.\nEmpirical results demonstrate that our approach significantly mitigates the\nperception bottleneck and improves the ability of LVLMs to comprehend charts.\nCode is publicly available at https:\/\/github.com\/hkust-nlp\/Vision4Chart.\n","date":"2025-03-24"}
{"id":"2503.18436","title":"Distributionally Robust Federated Learning: An ADMM Algorithm","abstract":"  Federated learning (FL) aims to train machine learning (ML) models\ncollaboratively using decentralized data, bypassing the need for centralized\ndata aggregation. Standard FL models often assume that all data come from the\nsame unknown distribution. However, in practical situations, decentralized data\nfrequently exhibit heterogeneity. We propose a novel FL model, Distributionally\nRobust Federated Learning (DRFL), that applies distributionally robust\noptimization to overcome the challenges posed by data heterogeneity and\ndistributional ambiguity. We derive a tractable reformulation for DRFL and\ndevelop a novel solution method based on the alternating direction method of\nmultipliers (ADMM) algorithm to solve this problem. Our experimental results\ndemonstrate that DRFL outperforms standard FL models under data heterogeneity\nand ambiguity.\n","date":"2025-03-24"}
{"id":"2503.18438","title":"ReconDreamer++: Harmonizing Generative and Reconstructive Models for\n  Driving Scene Representation","abstract":"  Combining reconstruction models with generative models has emerged as a\npromising paradigm for closed-loop simulation in autonomous driving. For\nexample, ReconDreamer has demonstrated remarkable success in rendering\nlarge-scale maneuvers. However, a significant gap remains between the generated\ndata and real-world sensor observations, particularly in terms of fidelity for\nstructured elements, such as the ground surface. To address these challenges,\nwe propose ReconDreamer++, an enhanced framework that significantly improves\nthe overall rendering quality by mitigating the domain gap and refining the\nrepresentation of the ground surface. Specifically, ReconDreamer++ introduces\nthe Novel Trajectory Deformable Network (NTDNet), which leverages learnable\nspatial deformation mechanisms to bridge the domain gap between synthesized\nnovel views and original sensor observations. Moreover, for structured elements\nsuch as the ground surface, we preserve geometric prior knowledge in 3D\nGaussians, and the optimization process focuses on refining appearance\nattributes while preserving the underlying geometric structure. Experimental\nevaluations conducted on multiple datasets (Waymo, nuScenes, PandaSet, and\nEUVS) confirm the superior performance of ReconDreamer++. Specifically, on\nWaymo, ReconDreamer++ achieves performance comparable to Street Gaussians for\nthe original trajectory while significantly outperforming ReconDreamer on novel\ntrajectories. In particular, it achieves substantial improvements, including a\n6.1% increase in NTA-IoU, a 23. 0% improvement in FID, and a remarkable 4.5%\ngain in the ground surface metric NTL-IoU, highlighting its effectiveness in\naccurately reconstructing structured elements such as the road surface.\n","date":"2025-03-24"}
{"id":"2503.18445","title":"Benchmarking Multi-modal Semantic Segmentation under Sensor Failures:\n  Missing and Noisy Modality Robustness","abstract":"  Multi-modal semantic segmentation (MMSS) addresses the limitations of\nsingle-modality data by integrating complementary information across\nmodalities. Despite notable progress, a significant gap persists between\nresearch and real-world deployment due to variability and uncertainty in\nmulti-modal data quality. Robustness has thus become essential for practical\nMMSS applications. However, the absence of standardized benchmarks for\nevaluating robustness hinders further advancement. To address this, we first\nsurvey existing MMSS literature and categorize representative methods to\nprovide a structured overview. We then introduce a robustness benchmark that\nevaluates MMSS models under three scenarios: Entire-Missing Modality (EMM),\nRandom-Missing Modality (RMM), and Noisy Modality (NM). From a probabilistic\nstandpoint, we model modality failure under two conditions: (1) all damaged\ncombinations are equally probable; (2) each modality fails independently\nfollowing a Bernoulli distribution. Based on these, we propose four\nmetrics-$mIoU^{Avg}_{EMM}$, $mIoU^{E}_{EMM}$, $mIoU^{Avg}_{RMM}$, and\n$mIoU^{E}_{RMM}$-to assess model robustness under EMM and RMM. This work\nprovides the first dedicated benchmark for MMSS robustness, offering new\ninsights and tools to advance the field. Source code is available at\nhttps:\/\/github.com\/Chenfei-Liao\/Multi-Modal-Semantic-Segmentation-Robustness-Benchmark.\n","date":"2025-03-24"}
{"id":"2503.18446","title":"Latent Space Super-Resolution for Higher-Resolution Image Generation\n  with Diffusion Models","abstract":"  In this paper, we propose LSRNA, a novel framework for higher-resolution\n(exceeding 1K) image generation using diffusion models by leveraging\nsuper-resolution directly in the latent space. Existing diffusion models\nstruggle with scaling beyond their training resolutions, often leading to\nstructural distortions or content repetition. Reference-based methods address\nthe issues by upsampling a low-resolution reference to guide higher-resolution\ngeneration. However, they face significant challenges: upsampling in latent\nspace often causes manifold deviation, which degrades output quality. On the\nother hand, upsampling in RGB space tends to produce overly smoothed outputs.\nTo overcome these limitations, LSRNA combines Latent space Super-Resolution\n(LSR) for manifold alignment and Region-wise Noise Addition (RNA) to enhance\nhigh-frequency details. Our extensive experiments demonstrate that integrating\nLSRNA outperforms state-of-the-art reference-based methods across various\nresolutions and metrics, while showing the critical role of latent space\nupsampling in preserving detail and sharpness. The code is available at\nhttps:\/\/github.com\/3587jjh\/LSRNA.\n","date":"2025-03-24"}
{"id":"2503.18454","title":"InPO: Inversion Preference Optimization with Reparametrized DDIM for\n  Efficient Diffusion Model Alignment","abstract":"  Without using explicit reward, direct preference optimization (DPO) employs\npaired human preference data to fine-tune generative models, a method that has\ngarnered considerable attention in large language models (LLMs). However,\nexploration of aligning text-to-image (T2I) diffusion models with human\npreferences remains limited. In comparison to supervised fine-tuning, existing\nmethods that align diffusion model suffer from low training efficiency and\nsubpar generation quality due to the long Markov chain process and the\nintractability of the reverse process. To address these limitations, we\nintroduce DDIM-InPO, an efficient method for direct preference alignment of\ndiffusion models. Our approach conceptualizes diffusion model as a single-step\ngenerative model, allowing us to fine-tune the outputs of specific latent\nvariables selectively. In order to accomplish this objective, we first assign\nimplicit rewards to any latent variable directly via a reparameterization\ntechnique. Then we construct an Inversion technique to estimate appropriate\nlatent variables for preference optimization. This modification process enables\nthe diffusion model to only fine-tune the outputs of latent variables that have\na strong correlation with the preference dataset. Experimental results indicate\nthat our DDIM-InPO achieves state-of-the-art performance with just 400 steps of\nfine-tuning, surpassing all preference aligning baselines for T2I diffusion\nmodels in human preference evaluation tasks.\n","date":"2025-03-24"}
{"id":"2503.18458","title":"StableGS: A Floater-Free Framework for 3D Gaussian Splatting","abstract":"  Recent years have witnessed remarkable success of 3D Gaussian Splatting\n(3DGS) in novel view synthesis, surpassing prior differentiable rendering\nmethods in both quality and efficiency. However, its training process suffers\nfrom coupled opacity-color optimization that frequently converges to local\nminima, producing floater artifacts that degrade visual fidelity. We present\nStableGS, a framework that eliminates floaters through cross-view depth\nconsistency constraints while introducing a dual-opacity GS model to decouple\ngeometry and material properties of translucent objects. To further enhance\nreconstruction quality in weakly-textured regions, we integrate DUSt3R depth\nestimation, significantly improving geometric stability. Our method\nfundamentally addresses 3DGS training instabilities, outperforming existing\nstate-of-the-art methods across open-source datasets.\n","date":"2025-03-24"}
{"id":"2503.18459","title":"Hiding Images in Diffusion Models by Editing Learned Score Functions","abstract":"  Hiding data using neural networks (i.e., neural steganography) has achieved\nremarkable success across both discriminative classifiers and generative\nadversarial networks. However, the potential of data hiding in diffusion models\nremains relatively unexplored. Current methods exhibit limitations in achieving\nhigh extraction accuracy, model fidelity, and hiding efficiency due primarily\nto the entanglement of the hiding and extraction processes with multiple\ndenoising diffusion steps. To address these, we describe a simple yet effective\napproach that embeds images at specific timesteps in the reverse diffusion\nprocess by editing the learned score functions. Additionally, we introduce a\nparameter-efficient fine-tuning method that combines gradient-based parameter\nselection with low-rank adaptation to enhance model fidelity and hiding\nefficiency. Comprehensive experiments demonstrate that our method extracts\nhigh-quality images at human-indistinguishable levels, replicates the original\nmodel behaviors at both sample and population levels, and embeds images orders\nof magnitude faster than prior methods. Besides, our method naturally supports\nmulti-recipient scenarios through independent extraction channels.\n","date":"2025-03-24"}
{"id":"2503.18460","title":"ModiGen: A Large Language Model-Based Workflow for Multi-Task Modelica\n  Code Generation","abstract":"  Modelica is a widely adopted language for simulating complex physical\nsystems, yet effective model creation and optimization require substantial\ndomain expertise. Although large language models (LLMs) have demonstrated\npromising capabilities in code generation, their application to modeling\nremains largely unexplored. To address this gap, we have developed benchmark\ndatasets specifically designed to evaluate the performance of LLMs in\ngenerating Modelica component models and test cases. Our evaluation reveals\nsubstantial limitations in current LLMs, as the generated code often fails to\nsimulate successfully. To overcome these challenges, we propose a specialized\nworkflow that integrates supervised fine-tuning, graph retrieval-augmented\ngeneration, and feedback optimization to improve the accuracy and reliability\nof Modelica code generation. The evaluation results demonstrate significant\nperformance gains: the maximum improvement in pass@1 reached 0.3349 for the\ncomponent generation task and 0.2457 for the test case generation task. This\nresearch underscores the potential of LLMs to advance intelligent modeling\ntools and offers valuable insights for future developments in system modeling\nand engineering applications.\n","date":"2025-03-24"}
{"id":"2503.18461","title":"MuMA: 3D PBR Texturing via Multi-Channel Multi-View Generation and\n  Agentic Post-Processing","abstract":"  Current methods for 3D generation still fall short in physically based\nrendering (PBR) texturing, primarily due to limited data and challenges in\nmodeling multi-channel materials. In this work, we propose MuMA, a method for\n3D PBR texturing through Multi-channel Multi-view generation and Agentic\npost-processing. Our approach features two key innovations: 1) We opt to model\nshaded and albedo appearance channels, where the shaded channels enables the\nintegration intrinsic decomposition modules for material properties. 2)\nLeveraging multimodal large language models, we emulate artists' techniques for\nmaterial assessment and selection. Experiments demonstrate that MuMA achieves\nsuperior results in visual quality and material fidelity compared to existing\nmethods.\n","date":"2025-03-24"}
{"id":"2503.18462","title":"PALATE: Peculiar Application of the Law of Total Expectation to Enhance\n  the Evaluation of Deep Generative Models","abstract":"  Deep generative models (DGMs) have caused a paradigm shift in the field of\nmachine learning, yielding noteworthy advancements in domains such as image\nsynthesis, natural language processing, and other related areas. However, a\ncomprehensive evaluation of these models that accounts for the trichotomy\nbetween fidelity, diversity, and novelty in generated samples remains a\nformidable challenge. A recently introduced solution that has emerged as a\npromising approach in this regard is the Feature Likelihood Divergence (FLD), a\nmethod that offers a theoretically motivated practical tool, yet also exhibits\nsome computational challenges. In this paper, we propose PALATE, a novel\nenhancement to the evaluation of DGMs that addresses limitations of existing\nmetrics. Our approach is based on a peculiar application of the law of total\nexpectation to random variables representing accessible real data. When\ncombined with the MMD baseline metric and DINOv2 feature extractor, PALATE\noffers a holistic evaluation framework that matches or surpasses\nstate-of-the-art solutions while providing superior computational efficiency\nand scalability to large-scale datasets. Through a series of experiments, we\ndemonstrate the effectiveness of the PALATE enhancement, contributing a\ncomputationally efficient, holistic evaluation approach that advances the field\nof DGMs assessment, especially in detecting sample memorization and evaluating\ngeneralization capabilities.\n","date":"2025-03-24"}
{"id":"2503.18463","title":"SIT-FER: Integration of Semantic-, Instance-, Text-level Information for\n  Semi-supervised Facial Expression Recognition","abstract":"  Semi-supervised deep facial expression recognition (SS-DFER) has gained\nincreasingly research interest due to the difficulty in accessing sufficient\nlabeled data in practical settings. However, existing SS-DFER methods mainly\nutilize generated semantic-level pseudo-labels for supervised learning, the\nunreliability of which compromises their performance and undermines the\npractical utility. In this paper, we propose a novel SS-DFER framework that\nsimultaneously incorporates semantic, instance, and text-level information to\ngenerate high-quality pseudo-labels. Specifically, for the unlabeled data,\nconsidering the comprehensive knowledge within the textual descriptions and\ninstance representations, we respectively calculate the similarities between\nthe facial vision features and the corresponding textual and instance features\nto obtain the probabilities at the text- and instance-level. Combining with the\nsemantic-level probability, these three-level probabilities are elaborately\naggregated to gain the final pseudo-labels. Furthermore, to enhance the\nutilization of one-hot labels for the labeled data, we also incorporate text\nembeddings excavated from textual descriptions to co-supervise model training,\nenabling facial visual features to exhibit semantic correlations in the text\nspace. Experiments on three datasets demonstrate that our method significantly\noutperforms current state-of-the-art SS-DFER methods and even exceeds fully\nsupervised baselines. The code will be available at\nhttps:\/\/github.com\/PatrickStarL\/SIT-FER.\n","date":"2025-03-24"}
{"id":"2503.18469","title":"CFReID: Continual Few-shot Person Re-Identification","abstract":"  Real-world surveillance systems are dynamically evolving, requiring a person\nRe-identification model to continuously handle newly incoming data from various\ndomains. To cope with these dynamics, Lifelong ReID (LReID) has been proposed\nto learn and accumulate knowledge across multiple domains incrementally.\nHowever, LReID models need to be trained on large-scale labeled data for each\nunseen domain, which are typically inaccessible due to privacy and cost\nconcerns. In this paper, we propose a new paradigm called Continual Few-shot\nReID (CFReID), which requires models to be incrementally trained using few-shot\ndata and tested on all seen domains. Under few-shot conditions, CFREID faces\ntwo core challenges: 1) learning knowledge from few-shot data of unseen domain,\nand 2) avoiding catastrophic forgetting of seen domains. To tackle these two\nchallenges, we propose a Stable Distribution Alignment (SDA) framework from\nfeature distribution perspective. Specifically, our SDA is composed of two\nmodules, i.e., Meta Distribution Alignment (MDA) and Prototype-based Few-shot\nAdaptation (PFA). To support the study of CFReID, we establish an evaluation\nbenchmark for CFReID on five publicly available ReID datasets. Extensive\nexperiments demonstrate that our SDA can enhance the few-shot learning and\nanti-forgetting capabilities under few-shot conditions. Notably, our approach,\nusing only 5\\% of the data, i.e., 32 IDs, significantly outperforms LReID's\nstate-of-the-art performance, which requires 700 to 1,000 IDs.\n","date":"2025-03-24"}
{"id":"2503.18470","title":"MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse","abstract":"  We present MetaSpatial, the first reinforcement learning (RL)-based framework\ndesigned to enhance 3D spatial reasoning in vision-language models (VLMs),\nenabling real-time 3D scene generation without the need for hard-coded\noptimizations. MetaSpatial addresses two core challenges: (i) the lack of\ninternalized 3D spatial reasoning in VLMs, which limits their ability to\ngenerate realistic layouts, and (ii) the inefficiency of traditional supervised\nfine-tuning (SFT) for layout generation tasks, as perfect ground truth\nannotations are unavailable. Our key innovation is a multi-turn RL-based\noptimization mechanism that integrates physics-aware constraints and rendered\nimage evaluations, ensuring generated 3D layouts are coherent, physically\nplausible, and aesthetically consistent. Methodologically, MetaSpatial\nintroduces an adaptive, iterative reasoning process, where the VLM refines\nspatial arrangements over multiple turns by analyzing rendered outputs,\nimproving scene coherence progressively. Empirical evaluations demonstrate that\nMetaSpatial significantly enhances the spatial consistency and formatting\nstability of various scale models. Post-training, object placements are more\nrealistic, aligned, and functionally coherent, validating the effectiveness of\nRL for 3D spatial reasoning in metaverse, AR\/VR, digital twins, and game\ndevelopment applications. Our code, data, and training pipeline are publicly\navailable at https:\/\/github.com\/PzySeere\/MetaSpatial.\n","date":"2025-03-24"}
{"id":"2503.18471","title":"Words as Bridges: Exploring Computational Support for Cross-Disciplinary\n  Translation Work","abstract":"  Scholars often explore literature outside of their home community of study.\nThis exploration process is frequently hampered by field-specific jargon. Past\ncomputational work often focuses on supporting translation work by removing\njargon through simplification and summarization; here, we explore a different\napproach that preserves jargon as useful bridges to new conceptual spaces.\nSpecifically, we cast different scholarly domains as different language-using\ncommunities, and explore how to adapt techniques from unsupervised\ncross-lingual alignment of word embeddings to explore conceptual alignments\nbetween domain-specific word embedding spaces.We developed a prototype\ncross-domain search engine that uses aligned domain-specific embeddings to\nsupport conceptual exploration, and tested this prototype in two case studies.\nWe discuss qualitative insights into the promises and pitfalls of this approach\nto translation work, and suggest design insights for future interfaces that\nprovide computational support for cross-domain information seeking.\n","date":"2025-03-24"}
{"id":"2503.18476","title":"Global-Local Tree Search in VLMs for 3D Indoor Scene Generation","abstract":"  Large Vision-Language Models (VLMs), such as GPT-4, have achieved remarkable\nsuccess across various fields. However, there are few studies on 3D indoor\nscene generation with VLMs. This paper considers this task as a planning\nproblem subject to spatial and layout common sense constraints. To solve the\nproblem with a VLM, we propose a new global-local tree search algorithm.\nGlobally, the method places each object sequentially and explores multiple\nplacements during each placement process, where the problem space is\nrepresented as a tree. To reduce the depth of the tree, we decompose the scene\nstructure hierarchically, i.e. room level, region level, floor object level,\nand supported object level. The algorithm independently generates the floor\nobjects in different regions and supported objects placed on different floor\nobjects. Locally, we also decompose the sub-task, the placement of each object,\ninto multiple steps. The algorithm searches the tree of problem space. To\nleverage the VLM model to produce positions of objects, we discretize the\ntop-down view space as a dense grid and fill each cell with diverse emojis to\nmake to cells distinct. We prompt the VLM with the emoji grid and the VLM\nproduces a reasonable location for the object by describing the position with\nthe name of emojis. The quantitative and qualitative experimental results\nillustrate our approach generates more plausible 3D scenes than\nstate-of-the-art approaches. Our source code is available at\nhttps:\/\/github.com\/dw-dengwei\/TreeSearchGen .\n","date":"2025-03-24"}
{"id":"2503.18478","title":"Video-XL-Pro: Reconstructive Token Compression for Extremely Long Video\n  Understanding","abstract":"  Despite advanced token compression techniques, existing multimodal large\nlanguage models (MLLMs) still struggle with hour-long video understanding. In\nthis work, we propose Video-XL-Pro, an efficient method for extremely long\nvideo understanding, built upon Reconstructive Compression of Tokens (ReCoT), a\nlearnable module that leverages self-supervised learning to generate\ncomprehensive and compact video tokens. ReCoT introduces two key components:\n(i) Dynamic Token Synthesizer (DTS): DTS generates pseudo-video tokens from\nstatic image tokens by learning intra-token relationships, which are then used\nin masked video modeling. (ii) Semantic-Guided Masking (SGM): SGM adaptively\nmasks redundant visual tokens to facilitate more effective reconstructive\nlearning. To improve training efficiency in MLLMs fine-tuning, we introduce a\nvideo-specific dataset pruning strategy and design a simple yet Query-aware\nSelector that enables the model to precisely locate query-relevant video\ntokens. With only 3B parameters, Video-XL-Pro outperforms most 7B models\ntrained on larger datasets across multiple long video understanding benchmarks.\nMoreover, it can process over 8K frames on a single A100 GPU while maintaining\nhigh-quality performance.\n","date":"2025-03-24"}
{"id":"2503.18483","title":"Explaining Domain Shifts in Language: Concept erasing for Interpretable\n  Image Classification","abstract":"  Concept-based models can map black-box representations to\nhuman-understandable concepts, which makes the decision-making process more\ntransparent and then allows users to understand the reason behind predictions.\nHowever, domain-specific concepts often impact the final predictions, which\nsubsequently undermine the model generalization capabilities, and prevent the\nmodel from being used in high-stake applications. In this paper, we propose a\nnovel Language-guided Concept-Erasing (LanCE) framework. In particular, we\nempirically demonstrate that pre-trained vision-language models (VLMs) can\napproximate distinct visual domain shifts via domain descriptors while\nprompting large Language Models (LLMs) can easily simulate a wide range of\ndescriptors of unseen visual domains. Then, we introduce a novel plug-in domain\ndescriptor orthogonality (DDO) regularizer to mitigate the impact of these\ndomain-specific concepts on the final predictions. Notably, the DDO regularizer\nis agnostic to the design of concept-based models and we integrate it into\nseveral prevailing models. Through evaluation of domain generalization on four\nstandard benchmarks and three newly introduced benchmarks, we demonstrate that\nDDO can significantly improve the out-of-distribution (OOD) generalization over\nthe previous state-of-the-art concept-based models.Our code is available at\nhttps:\/\/github.com\/joeyz0z\/LanCE.\n","date":"2025-03-24"}
{"id":"2503.18484","title":"PM4Bench: A Parallel Multilingual Multi-Modal Multi-task Benchmark for\n  Large Vision Language Model","abstract":"  Existing multilingual benchmarks for Large Vision Language Models (LVLMs)\nsuffer from limitations including language-specific content biases, disjointed\nmultimodal input formats, and a lack of safety evaluation. To address these\ngaps, we propose PM4Bench, the first Parallel Multilingual Multi-Modal\nMulti-task Benchmark for LVLMs. PM4Bench features a parallel corpus design\nacross 10 languages, enabling fair and accurate cross-lingual comparisons. It\nincludes the vision setting where text and queries are embedded in images,\nrequiring LVLMs to simultaneously \"see\", \"read\", and \"think\", aligning with\nreal-world applications. Additionally, PM\\textsuperscript{4}Bench incorporates\nsafety evaluations, addressing critical oversight in existing multilingual\nbenchmarks. Using PM4Bench, we evaluate 11 mainstream LVLMs, revealing\nsignificant cross-linguistic performance disparities, particularly in vision\nsettings, and identifying OCR capability as a key determinant of these\nimbalances. We will release PM4Bench at https:\/\/github.com\/opendatalab\/PM4Bench .\n","date":"2025-03-24"}
{"id":"2503.18485","title":"Whispering in Amharic: Fine-tuning Whisper for Low-resource Language","abstract":"  This work explores fine-tuning OpenAI's Whisper automatic speech recognition\n(ASR) model for Amharic, a low-resource language, to improve transcription\naccuracy. While the foundational Whisper model struggles with Amharic due to\nlimited representation in its training data, we fine-tune it using datasets\nlike Mozilla Common Voice, FLEURS, and the BDU-speech dataset. The\nbest-performing model, Whispersmall-am, significantly improves when finetuned\non a mix of existing FLEURS data and new, unseen Amharic datasets. Training\nsolely on new data leads to poor performance, but combining it with FLEURS data\nreinforces the model, enabling better specialization in Amharic. We also\ndemonstrate that normalizing Amharic homophones significantly enhances Word\nError Rate (WER) and Bilingual Evaluation Understudy (BLEU) scores. This study\nunderscores the importance of fine-tuning strategies and dataset composition\nfor improving ASR in low-resource languages, providing insights for future\nAmharic speech recognition research.\n","date":"2025-03-24"}
{"id":"2503.18487","title":"Large Language Models powered Network Attack Detection: Architecture,\n  Opportunities and Case Study","abstract":"  Network attack detection is a pivotal technology to identify network anomaly\nand classify malicious traffic. Large Language Models (LLMs) are trained on a\nvast corpus of text, have amassed remarkable capabilities of\ncontext-understanding and commonsense knowledge. This has opened up a new door\nfor network threat detection. Researchers have already initiated discussions\nregarding the application of LLMs on specific cyber-security tasks.\nUnfortunately, there is still a lack of comprehensive elaboration how to mine\nLLMs' potentials in network threat detections, as well as the opportunities and\nchallenges. In this paper, we mainly focus on the classification of malicious\ntraffic from the perspective of LLMs' capability. We present a holistic view of\nthe architecture of LLM-powered network attack detection, including\nPre-training, Fine-tuning, and Detection. Especially, by exploring the\nknowledge and capabilities of LLM, we identify three distinct roles LLM can act\nin network attack detection: \\textit{Classifier, Encoder, and Predictor}. For\neach of them, the modeling paradigm, opportunities and challenges are\nelaborated. Finally, we present our design on LLM-powered DDoS detection as a\ncase study. The proposed framework attains accurate detection on carpet bombing\nDDoS by exploiting LLMs' capabilities in contextual mining. The evaluation\nshows its efficacy, exhibiting a nearly $35$\\% improvement compared to existing\nsystems.\n","date":"2025-03-24"}
{"id":"2503.18491","title":"MAGIC-VQA: Multimodal And Grounded Inference with Commonsense Knowledge\n  for Visual Question Answering","abstract":"  Visual Question Answering (VQA) requires reasoning across visual and textual\nmodalities, yet Large Vision-Language Models (LVLMs) often lack integrated\ncommonsense knowledge, limiting their robustness in real-world scenarios. To\naddress this, we introduce MAGIC-VQA, a novel framework that enhances VQA by\nsystematically integrating commonsense knowledge with LVLMs. MAGIC-VQA employs\na three-stage process: (1) Explicit Knowledge Integration from external\nsources, (2) By-Type Post-Processing for contextual refinement, and (3)\nImplicit Knowledge Augmentation using a Graph Neural Network (GNN) for\nstructured reasoning. While GNNs bring greater depth to structured inference,\nthey enable superior relational inference beyond LVLMs. MAGIC-VQA bridges a key\ngap by unifying commonsensse knowledge with LVLM-driven reasoning, eliminating\nthe need for extensive pre-training or complex prompt tuning. Our framework\nachieves state-of-the-art performance on benchmark datasets, significantly\nimproving commonsense reasoning in VQA.\n","date":"2025-03-24"}
{"id":"2503.18492","title":"Safeguarding Mobile GUI Agent via Logic-based Action Verification","abstract":"  Large Foundation Models (LFMs) have unlocked new possibilities in\nhuman-computer interaction, particularly with the rise of mobile Graphical User\nInterface (GUI) Agents capable of interpreting GUIs. These agents promise to\nrevolutionize mobile computing by allowing users to automate complex mobile\ntasks through simple natural language instructions. However, the inherent\nprobabilistic nature of LFMs, coupled with the ambiguity and context-dependence\nof mobile tasks, makes LFM-based automation unreliable and prone to errors. To\naddress this critical challenge, we introduce VeriSafe Agent (VSA): a formal\nverification system that serves as a logically grounded safeguard for Mobile\nGUI Agents. VSA is designed to deterministically ensure that an agent's actions\nstrictly align with user intent before conducting an action. At its core, VSA\nintroduces a novel autoformalization technique that translates natural language\nuser instructions into a formally verifiable specification, expressed in our\ndomain-specific language (DSL). This enables runtime, rule-based verification,\nallowing VSA to detect and prevent erroneous actions executing an action,\neither by providing corrective feedback or halting unsafe behavior. To the best\nof our knowledge, VSA is the first attempt to bring the rigor of formal\nverification to GUI agent. effectively bridging the gap between LFM-driven\nautomation and formal software verification. We implement VSA using\noff-the-shelf LLM services (GPT-4o) and evaluate its performance on 300 user\ninstructions across 18 widely used mobile apps. The results demonstrate that\nVSA achieves 94.3%-98.33% accuracy in verifying agent actions, representing a\nsignificant 20.4%-25.6% improvement over existing LLM-based verification\nmethods, and consequently increases the GUI agent's task completion rate by\n90%-130%.\n","date":"2025-03-24"}
{"id":"2503.18494","title":"Verbal Process Supervision Elicits Better Coding Agents","abstract":"  The emergence of large language models and their applications as AI agents\nhave significantly advanced state-of-the-art code generation benchmarks,\ntransforming modern software engineering tasks. However, even with test-time\ncomputed reasoning models, these systems still struggle with complex software\nengineering challenges. This work introduces CURA, a code understanding and\nreasoning agent system enhanced with verbal process supervision (VPS),\nachieving a 3.65\\% improvement over baseline models on challenging benchmarks\nlike BigCodeBench. Furthermore, CURA, when paired with the o3-mini model and\nVPS techniques, attains state-of-the-art performance. This work represents a\nstep forward in integrating reasoning-driven architectures with LLM-based code\ngeneration, enabling agentic reasoning for language models to solve complex\nsoftware engineering tasks.\n","date":"2025-03-24"}
{"id":"2503.18497","title":"Statistically Testing Training Data for Unwanted Error Patterns using\n  Rule-Oriented Regression","abstract":"  Artificial intelligence models trained from data can only be as good as the\nunderlying data is. Biases in training data propagating through to the output\nof a machine learning model are a well-documented and well-understood\nphenomenon, but the machinery to prevent these undesired effects is much less\ndeveloped. Efforts to ensure data is clean during collection, such as using\nbias-aware sampling, are most effective when the entity controlling data\ncollection also trains the AI. In cases where the data is already available,\nhow do we find out if the data was already manipulated, i.e., ``poisoned'', so\nthat an undesired behavior would be trained into a machine learning model? This\nis a challenge fundamentally different to (just) improving approximation\naccuracy or efficiency, and we provide a method to test training data for\nflaws, to establish a trustworthy ground-truth for a subsequent training of\nmachine learning models (of any kind). Unlike the well-studied problem of\napproximating data using fuzzy rules that are generated from the data, our\nmethod hinges on a prior definition of rules to happen before seeing the data\nto be tested. Therefore, the proposed method can also discover hidden error\npatterns, which may also have substantial influence. Our approach extends the\nabilities of conventional statistical testing by letting the ``test-condition''\nbe any Boolean condition to describe a pattern in the data, whose presence we\nwish to determine. The method puts fuzzy inference into a regression model, to\nget the best of the two: explainability from fuzzy logic with statistical\nproperties and diagnostics from the regression, and finally also being\napplicable to ``small data'', hence not requiring large datasets as deep\nlearning methods do. We provide an open source implementation for demonstration\nand experiments.\n","date":"2025-03-24"}
{"id":"2503.18500","title":"Learning a Class of Mixed Linear Regressions: Global Convergence under\n  General Data Conditions","abstract":"  Mixed linear regression (MLR) has attracted increasing attention because of\nits great theoretical and practical importance in capturing nonlinear\nrelationships by utilizing a mixture of linear regression sub-models. Although\nconsiderable efforts have been devoted to the learning problem of such systems,\ni.e., estimating data labels and identifying model parameters, most existing\ninvestigations employ the offline algorithm, impose the strict independent and\nidentically distributed (i.i.d.) or persistent excitation (PE) conditions on\nthe regressor data, and provide local convergence results only. In this paper,\nwe investigate the recursive estimation and data clustering problems for a\nclass of stochastic MLRs with two components. To address this inherently\nnonconvex optimization problem, we propose a novel two-step recursive\nidentification algorithm to estimate the true parameters, where the direction\nvector and the scaling coefficient of the unknown parameters are estimated by\nthe least squares and the expectation-maximization (EM) principles,\nrespectively. Under a general data condition, which is much weaker than the\ntraditional i.i.d. and PE conditions, we establish the global convergence and\nthe convergence rate of the proposed identification algorithm for the first\ntime. Furthermore, we prove that, without any excitation condition on the\nregressor data, the data clustering performance including the cumulative\nmis-classification error and the within-cluster error can be optimal\nasymptotically. Finally, we provide a numerical example to illustrate the\nperformance of the proposed learning algorithm.\n","date":"2025-03-24"}
{"id":"2503.18502","title":"Autoregressive Language Models for Knowledge Base Population: A case\n  study in the space mission domain","abstract":"  Knowledge base population KBP plays a crucial role in populating and\nmaintaining knowledge bases up-to-date in organizations by leveraging domain\ncorpora. Motivated by the increasingly large context windows supported by large\nlanguage models, we propose to fine-tune an autoregressive language model for\nend-toend KPB. Our case study involves the population of a space mission\nknowledge graph. To fine-tune the model we generate a dataset for end-to-end\nKBP tapping into existing domain resources. Our case study shows that\nfine-tuned language models of limited size can achieve competitive and even\nhigher accuracy than larger models in the KBP task. Smaller models specialized\nfor KBP offer affordable deployment and lower-cost inference. Moreover, KBP\nspecialist models do not require the ontology to be included in the prompt,\nallowing for more space in the context for additional input text or output\nserialization.\n","date":"2025-03-24"}
{"id":"2503.18503","title":"Deterministic Certification of Graph Neural Networks against Graph\n  Poisoning Attacks with Arbitrary Perturbations","abstract":"  Graph neural networks (GNNs) are becoming the de facto method to learn on the\ngraph data and have achieved the state-of-the-art on node and graph\nclassification tasks. However, recent works show GNNs are vulnerable to\ntraining-time poisoning attacks -- marginally perturbing edges, nodes, or\/and\nnode features of training graph(s) can largely degrade GNNs' testing\nperformance. Most previous defenses against graph poisoning attacks are\nempirical and are soon broken by adaptive \/ stronger ones. A few provable\ndefenses provide robustness guarantees, but have large gaps when applied in\npractice: 1) restrict the attacker on only one type of perturbation; 2) design\nfor a particular GNN architecture or task; and 3) robustness guarantees are not\n100\\% accurate.\n  In this work, we bridge all these gaps by developing PGNNCert, the first\ncertified defense of GNNs against poisoning attacks under arbitrary (edge,\nnode, and node feature) perturbations with deterministic robustness guarantees.\nExtensive evaluations on multiple node and graph classification datasets and\nGNNs demonstrate the effectiveness of PGNNCert to provably defend against\narbitrary poisoning perturbations. PGNNCert is also shown to significantly\noutperform the state-of-the-art certified defenses against edge perturbation or\nnode perturbation during GNN training.\n","date":"2025-03-24"}
{"id":"2503.18507","title":"Can Text-to-Video Generation help Video-Language Alignment?","abstract":"  Recent video-language alignment models are trained on sets of videos, each\nwith an associated positive caption and a negative caption generated by large\nlanguage models. A problem with this procedure is that negative captions may\nintroduce linguistic biases, i.e., concepts are seen only as negatives and\nnever associated with a video. While a solution would be to collect videos for\nthe negative captions, existing databases lack the fine-grained variations\nneeded to cover all possible negatives. In this work, we study whether\nsynthetic videos can help to overcome this issue. Our preliminary analysis with\nmultiple generators shows that, while promising on some tasks, synthetic videos\nharm the performance of the model on others. We hypothesize this issue is\nlinked to noise (semantic and visual) in the generated videos and develop a\nmethod, SynViTA, that accounts for those. SynViTA dynamically weights the\ncontribution of each synthetic video based on how similar its target caption is\nw.r.t. the real counterpart. Moreover, a semantic consistency loss makes the\nmodel focus on fine-grained differences across captions, rather than\ndifferences in video appearance. Experiments show that, on average, SynViTA\nimproves over existing methods on VideoCon test sets and SSv2-Temporal,\nSSv2-Events, and ATP-Hard benchmarks, being a first promising step for using\nsynthetic videos when learning video-language models.\n","date":"2025-03-24"}
{"id":"2503.18509","title":"Neuro-symbolic Weak Supervision: Theory and Semantics","abstract":"  Weak supervision allows machine learning models to learn from limited or\nnoisy labels, but it introduces challenges in interpretability and reliability\n- particularly in multi-instance partial label learning (MI-PLL), where models\nmust resolve both ambiguous labels and uncertain instance-label mappings. We\npropose a semantics for neuro-symbolic framework that integrates Inductive\nLogic Programming (ILP) to improve MI-PLL by providing structured relational\nconstraints that guide learning. Within our semantic characterization, ILP\ndefines a logical hypothesis space for label transitions, clarifies classifier\nsemantics, and establishes interpretable performance standards. This hybrid\napproach improves robustness, transparency, and accountability in weakly\nsupervised settings, ensuring neural predictions align with domain knowledge.\nBy embedding weak supervision into a logical framework, we enhance both\ninterpretability and learning, making weak supervision more suitable for\nreal-world, high-stakes applications.\n","date":"2025-03-24"}
{"id":"2503.18511","title":"Global Convergence of Continual Learning on Non-IID Data","abstract":"  Continual learning, which aims to learn multiple tasks sequentially, has\ngained extensive attention. However, most existing work focuses on empirical\nstudies, and the theoretical aspect remains under-explored. Recently, a few\ninvestigations have considered the theory of continual learning only for linear\nregressions, establishes the results based on the strict independent and\nidentically distributed (i.i.d.) assumption and the persistent excitation on\nthe feature data that may be difficult to verify or guarantee in practice. To\novercome this fundamental limitation, in this paper, we provide a general and\ncomprehensive theoretical analysis for continual learning of regression models.\nBy utilizing the stochastic Lyapunov function and martingale estimation\ntechniques, we establish the almost sure convergence results of continual\nlearning under a general data condition for the first time. Additionally,\nwithout any excitation condition imposed on the data, the convergence rates for\nthe forgetting and regret metrics are provided.\n","date":"2025-03-24"}
{"id":"2503.18512","title":"Uncertainty-guided Perturbation for Image Super-Resolution Diffusion\n  Model","abstract":"  Diffusion-based image super-resolution methods have demonstrated significant\nadvantages over GAN-based approaches, particularly in terms of perceptual\nquality. Building upon a lengthy Markov chain, diffusion-based methods possess\nremarkable modeling capacity, enabling them to achieve outstanding performance\nin real-world scenarios. Unlike previous methods that focus on modifying the\nnoise schedule or sampling process to enhance performance, our approach\nemphasizes the improved utilization of LR information. We find that different\nregions of the LR image can be viewed as corresponding to different timesteps\nin a diffusion process, where flat areas are closer to the target HR\ndistribution but edge and texture regions are farther away. In these flat\nareas, applying a slight noise is more advantageous for the reconstruction. We\nassociate this characteristic with uncertainty and propose to apply uncertainty\nestimate to guide region-specific noise level control, a technique we refer to\nas Uncertainty-guided Noise Weighting. Pixels with lower uncertainty (i.e.,\nflat regions) receive reduced noise to preserve more LR information, therefore\nimproving performance. Furthermore, we modify the network architecture of\nprevious methods to develop our Uncertainty-guided Perturbation\nSuper-Resolution (UPSR) model. Extensive experimental results demonstrate that,\ndespite reduced model size and training overhead, the proposed UWSR method\noutperforms current state-of-the-art methods across various datasets, both\nquantitatively and qualitatively.\n","date":"2025-03-24"}
{"id":"2503.18513","title":"LookCloser: Frequency-aware Radiance Field for Tiny-Detail Scene","abstract":"  Humans perceive and comprehend their surroundings through information\nspanning multiple frequencies. In immersive scenes, people naturally scan their\nenvironment to grasp its overall structure while examining fine details of\nobjects that capture their attention. However, current NeRF frameworks\nprimarily focus on modeling either high-frequency local views or the broad\nstructure of scenes with low-frequency information, which is limited to\nbalancing both. We introduce FA-NeRF, a novel frequency-aware framework for\nview synthesis that simultaneously captures the overall scene structure and\nhigh-definition details within a single NeRF model. To achieve this, we propose\na 3D frequency quantification method that analyzes the scene's frequency\ndistribution, enabling frequency-aware rendering. Our framework incorporates a\nfrequency grid for fast convergence and querying, a frequency-aware feature\nre-weighting strategy to balance features across different frequency contents.\nExtensive experiments show that our method significantly outperforms existing\napproaches in modeling entire scenes while preserving fine details. Project\npage: https:\/\/coscatter.github.io\/LookCloser\/\n","date":"2025-03-24"}
{"id":"2503.18526","title":"SciClaims: An End-to-End Generative System for Biomedical Claim Analysis","abstract":"  Validating key claims in scientific literature, particularly in biomedical\nresearch, is essential for ensuring accuracy and advancing knowledge. This\nprocess is critical in sectors like the pharmaceutical industry, where rapid\nscientific progress requires automation and deep domain expertise. However,\ncurrent solutions have significant limitations. They lack end-to-end pipelines\nencompassing all claim extraction, evidence retrieval, and verification steps;\nrely on complex NLP and information retrieval pipelines prone to multiple\nfailure points; and often fail to provide clear, user-friendly justifications\nfor claim verification outcomes. To address these challenges, we introduce\nSciClaims, an advanced system powered by state-of-the-art large language models\n(LLMs) that seamlessly integrates the entire scientific claim analysis process.\nSciClaims outperforms previous approaches in both claim extraction and\nverification without requiring additional fine-tuning, setting a new benchmark\nfor automated scientific claim analysis.\n","date":"2025-03-24"}
{"id":"2503.18527","title":"AIM2PC: Aerial Image to 3D Building Point Cloud Reconstruction","abstract":"  Three-dimensional urban reconstruction of buildings from single-view images\nhas attracted significant attention over the past two decades. However, recent\nmethods primarily focus on rooftops from aerial images, often overlooking\nessential geometrical details. Additionally, there is a notable lack of\ndatasets containing complete 3D point clouds for entire buildings, along with\nchallenges in obtaining reliable camera pose information for aerial images.\nThis paper addresses these challenges by presenting a novel methodology, AIM2PC\n, which utilizes our generated dataset that includes complete 3D point clouds\nand determined camera poses. Our approach takes features from a single aerial\nimage as input and concatenates them with essential additional conditions, such\nas binary masks and Sobel edge maps, to enable more edge-aware reconstruction.\nBy incorporating a point cloud diffusion model based on Centered denoising\nDiffusion Probabilistic Models (CDPM), we project these concatenated features\nonto the partially denoised point cloud using our camera poses at each\ndiffusion step. The proposed method is able to reconstruct the complete 3D\nbuilding point cloud, including wall information and demonstrates superior\nperformance compared to existing baseline techniques. To allow further\ncomparisons with our methodology the dataset has been made available at\nhttps:\/\/github.com\/Soulaimene\/AIM2PCDataset\n","date":"2025-03-24"}
{"id":"2503.18528","title":"k-NN as a Simple and Effective Estimator of Transferability","abstract":"  How well can one expect transfer learning to work in a new setting where the\ndomain is shifted, the task is different, and the architecture changes? Many\ntransfer learning metrics have been proposed to answer this question. But how\naccurate are their predictions in a realistic new setting? We conducted an\nextensive evaluation involving over 42,000 experiments comparing 23\ntransferability metrics across 16 different datasets to assess their ability to\npredict transfer performance. Our findings reveal that none of the existing\nmetrics perform well across the board. However, we find that a simple k-nearest\nneighbor evaluation -- as is commonly used to evaluate feature quality for\nself-supervision -- not only surpasses existing metrics, but also offers better\ncomputational efficiency and ease of implementation.\n","date":"2025-03-24"}
{"id":"2503.18531","title":"Parental Guidance: Efficient Lifelong Learning through Evolutionary\n  Distillation","abstract":"  Developing robotic agents that can perform well in diverse environments while\nshowing a variety of behaviors is a key challenge in AI and robotics.\nTraditional reinforcement learning (RL) methods often create agents that\nspecialize in narrow tasks, limiting their adaptability and diversity. To\novercome this, we propose a preliminary, evolution-inspired framework that\nincludes a reproduction module, similar to natural species reproduction,\nbalancing diversity and specialization. By integrating RL, imitation learning\n(IL), and a coevolutionary agent-terrain curriculum, our system evolves agents\ncontinuously through complex tasks. This approach promotes adaptability,\ninheritance of useful traits, and continual learning. Agents not only refine\ninherited skills but also surpass their predecessors. Our initial experiments\nshow that this method improves exploration efficiency and supports open-ended\nlearning, offering a scalable solution where sparse reward coupled with diverse\nterrain environments induces a multi-task setting.\n","date":"2025-03-24"}
{"id":"2503.18533","title":"MMCR: Advancing Visual Language Model in Multimodal Multi-Turn\n  Contextual Reasoning","abstract":"  Compared to single-turn dialogue, multi-turn dialogue involving multiple\nimages better aligns with the needs of real-world human-AI interactions.\nAdditionally, as training data, it provides richer contextual reasoning\ninformation, thereby guiding the model to achieve better performance. However,\nexisting vision-language models (VLMs) primarily rely on single-turn dialogue\ntraining and evaluation benchmarks. In this paper, following the\ncharacteristics of human dialogue, such as focused topics and concise, clear\ncontent, we present MMCR (Multimodal Multi-turn Contextual Reasoning), a novel\ndataset comprising: (1) MMCR-310k -- the largest multi-image multi-turn\ninstruction tuning dataset with 310K contextual dialogues, each covering 1-4\nimages and 4 or 8 dialogue turns; and (2) MMCR-Bench -- a diagnostic benchmark\nfeaturing dialogues, spanning 8 domains (Humanities, Natural, Science,\nEducation, etc.) and 40 sub-topics. Extensive evaluations demonstrate that\nmodels fine-tuned with MMCR-310k achieve 5.2\\% higher contextual accuracy on\nMMCR-Bench, while showing consistent improvements on existing benchmarks\n(+1.1\\% on AI2D, +1.2\\% on MMMU and MMVet). MMCR and prompt engineering will be\nreleased publicly.\n","date":"2025-03-24"}
{"id":"2503.18536","title":"DiN: Diffusion Model for Robust Medical VQA with Semantic Noisy Labels","abstract":"  Medical Visual Question Answering (Med-VQA) systems benefit the\ninterpretation of medical images containing critical clinical information.\nHowever, the challenge of noisy labels and limited high-quality datasets\nremains underexplored. To address this, we establish the first benchmark for\nnoisy labels in Med-VQA by simulating human mislabeling with semantically\ndesigned noise types. More importantly, we introduce the DiN framework, which\nleverages a diffusion model to handle noisy labels in Med-VQA. Unlike the\ndominant classification-based VQA approaches that directly predict answers, our\nAnswer Diffuser (AD) module employs a coarse-to-fine process, refining answer\ncandidates with a diffusion model for improved accuracy. The Answer Condition\nGenerator (ACG) further enhances this process by generating task-specific\nconditional information via integrating answer embeddings with fused\nimage-question features. To address label noise, our Noisy Label\nRefinement(NLR) module introduces a robust loss function and dynamic answer\nadjustment to further boost the performance of the AD module.\n","date":"2025-03-24"}
{"id":"2503.18539","title":"Natural Language Processing for Electronic Health Records in\n  Scandinavian Languages: Norwegian, Swedish, and Danish","abstract":"  Background: Clinical natural language processing (NLP) refers to the use of\ncomputational methods for extracting, processing, and analyzing unstructured\nclinical text data, and holds a huge potential to transform healthcare in\nvarious clinical tasks. Objective: The study aims to perform a systematic\nreview to comprehensively assess and analyze the state-of-the-art NLP methods\nfor the mainland Scandinavian clinical text. Method: A literature search was\nconducted in various online databases including PubMed, ScienceDirect, Google\nScholar, ACM digital library, and IEEE Xplore between December 2022 and\nFebruary 2024. Further, relevant references to the included articles were also\nused to solidify our search. The final pool includes articles that conducted\nclinical NLP in the mainland Scandinavian languages and were published in\nEnglish between 2010 and 2024. Results: Out of the 113 articles, 18% (n=21)\nfocus on Norwegian clinical text, 64% (n=72) on Swedish, 10% (n=11) on Danish,\nand 8% (n=9) focus on more than one language. Generally, the review identified\npositive developments across the region despite some observable gaps and\ndisparities between the languages. There are substantial disparities in the\nlevel of adoption of transformer-based models. In essential tasks such as\nde-identification, there is significantly less research activity focusing on\nNorwegian and Danish compared to Swedish text. Further, the review identified a\nlow level of sharing resources such as data, experimentation code, pre-trained\nmodels, and rate of adaptation and transfer learning in the region. Conclusion:\nThe review presented a comprehensive assessment of the state-of-the-art\nClinical NLP for electronic health records (EHR) text in mainland Scandinavian\nlanguages and, highlighted the potential barriers and challenges that hinder\nthe rapid advancement of the field in the region.\n","date":"2025-03-24"}
{"id":"2503.18540","title":"HiRes-FusedMIM: A High-Resolution RGB-DSM Pre-trained Model for\n  Building-Level Remote Sensing Applications","abstract":"  Recent advances in self-supervised learning have led to the development of\nfoundation models that have significantly advanced performance in various\ncomputer vision tasks. However, despite their potential, these models often\noverlook the crucial role of high-resolution digital surface models (DSMs) in\nunderstanding urban environments, particularly for building-level analysis,\nwhich is essential for applications like digital twins. To address this gap, we\nintroduce HiRes-FusedMIM, a novel pre-trained model specifically designed to\nleverage the rich information contained within high-resolution RGB and DSM\ndata. HiRes-FusedMIM utilizes a dual-encoder simple masked image modeling\n(SimMIM) architecture with a multi-objective loss function that combines\nreconstruction and contrastive objectives, enabling it to learn powerful, joint\nrepresentations from both modalities. We conducted a comprehensive evaluation\nof HiRes-FusedMIM on a diverse set of downstream tasks, including\nclassification, semantic segmentation, and instance segmentation. Our results\ndemonstrate that: 1) HiRes-FusedMIM outperforms previous state-of-the-art\ngeospatial methods on several building-related datasets, including WHU Aerial\nand LoveDA, demonstrating its effectiveness in capturing and leveraging\nfine-grained building information; 2) Incorporating DSMs during pre-training\nconsistently improves performance compared to using RGB data alone,\nhighlighting the value of elevation information for building-level analysis; 3)\nThe dual-encoder architecture of HiRes-FusedMIM, with separate encoders for RGB\nand DSM data, significantly outperforms a single-encoder model on the Vaihingen\nsegmentation task, indicating the benefits of learning specialized\nrepresentations for each modality. To facilitate further research and\napplications in this direction, we will publicly release the trained model\nweights.\n","date":"2025-03-24"}
{"id":"2503.18541","title":"UniPCGC: Towards Practical Point Cloud Geometry Compression via an\n  Efficient Unified Approach","abstract":"  Learning-based point cloud compression methods have made significant progress\nin terms of performance. However, these methods still encounter challenges\nincluding high complexity, limited compression modes, and a lack of support for\nvariable rate, which restrict the practical application of these methods. In\norder to promote the development of practical point cloud compression, we\npropose an efficient unified point cloud geometry compression framework, dubbed\nas UniPCGC. It is a lightweight framework that supports lossy compression,\nlossless compression, variable rate and variable complexity. First, we\nintroduce the Uneven 8-Stage Lossless Coder (UELC) in the lossless mode, which\nallocates more computational complexity to groups with higher coding\ndifficulty, and merges groups with lower coding difficulty. Second, Variable\nRate and Complexity Module (VRCM) is achieved in the lossy mode through joint\nadoption of a rate modulation module and dynamic sparse convolution. Finally,\nthrough the dynamic combination of UELC and VRCM, we achieve lossy compression,\nlossless compression, variable rate and complexity within a unified framework.\nCompared to the previous state-of-the-art method, our method achieves a\ncompression ratio (CR) gain of 8.1\\% on lossless compression, and a Bjontegaard\nDelta Rate (BD-Rate) gain of 14.02\\% on lossy compression, while also\nsupporting variable rate and variable complexity.\n","date":"2025-03-24"}
{"id":"2503.18542","title":"An Identity and Interaction Based Network Forensic Analysis","abstract":"  In todays landscape of increasing electronic crime, network forensics plays a\npivotal role in digital investigations. It aids in understanding which systems\nto analyse and as a supplement to support evidence found through more\ntraditional computer based investigations. However, the nature and\nfunctionality of the existing Network Forensic Analysis Tools (NFATs) fall\nshort compared to File System Forensic Analysis Tools (FS FATs) in providing\nusable data. The analysis tends to focus upon IP addresses, which are not\nsynonymous with user identities, a point of significant interest to\ninvestigators. This paper presents several experiments designed to create a\nnovel NFAT approach that can identify users and understand how they are using\nnetwork based applications whilst the traffic remains encrypted. The\nexperiments build upon the prior art and investigate how effective this\napproach is in classifying users and their actions. Utilising an in-house\ndataset composed of 50 million packers, the experiments are formed of three\nincremental developments that assist in improving performance. Building upon\nthe successful experiments, a proposed NFAT interface is presented to\nillustrate the ease at which investigators would be able to ask relevant\nquestions of user interactions. The experiments profiled across 27 users, has\nyielded an average 93.3% True Positive Identification Rate (TPIR), with 41% of\nusers experiencing 100% TPIR. Skype, Wikipedia and Hotmail services achieved a\nnotably high level of recognition performance. The study has developed and\nevaluated an approach to analyse encrypted network traffic more effectively\nthrough the modelling of network traffic and to subsequently visualise these\ninteractions through a novel network forensic analysis tool.\n","date":"2025-03-24"}
{"id":"2503.18544","title":"Distilling Stereo Networks for Performant and Efficient Leaner Networks","abstract":"  Knowledge distillation has been quite popular in vision for tasks like\nclassification and segmentation however not much work has been done for\ndistilling state-of-the-art stereo matching methods despite their range of\napplications. One of the reasons for its lack of use in stereo matching\nnetworks is due to the inherent complexity of these networks, where a typical\nnetwork is composed of multiple two- and three-dimensional modules. In this\nwork, we systematically combine the insights from state-of-the-art stereo\nmethods with general knowledge-distillation techniques to develop a joint\nframework for stereo networks distillation with competitive results and faster\ninference. Moreover, we show, via a detailed empirical analysis, that\ndistilling knowledge from the stereo network requires careful design of the\ncomplete distillation pipeline starting from backbone to the right selection of\ndistillation points and corresponding loss functions. This results in the\nstudent networks that are not only leaner and faster but give excellent\nperformance . For instance, our student network while performing better than\nthe performance oriented methods like PSMNet [1], CFNet [2], and LEAStereo [3])\non benchmark SceneFlow dataset, is 8x, 5x, and 8x faster respectively.\nFurthermore, compared to speed oriented methods having inference time less than\n100ms, our student networks perform better than all the tested methods. In\naddition, our student network also shows better generalization capabilities\nwhen tested on unseen datasets like ETH3D and Middlebury.\n","date":"2025-03-24"}
{"id":"2503.18548","title":"Benchmarking Post-Hoc Unknown-Category Detection in Food Recognition","abstract":"  Food recognition models often struggle to distinguish between seen and unseen\nsamples, frequently misclassifying samples from unseen categories by assigning\nthem an in-distribution (ID) label. This misclassification presents significant\nchallenges when deploying these models in real-world applications, particularly\nwithin automatic dietary assessment systems, where incorrect labels can lead to\ncascading errors throughout the system. Ideally, such models should prompt the\nuser when an unknown sample is encountered, allowing for corrective action.\nGiven no prior research exploring food recognition in real-world settings, in\nthis work we conduct an empirical analysis of various post-hoc\nout-of-distribution (OOD) detection methods for fine-grained food recognition.\nOur findings indicate that virtual logit matching (ViM) performed the best\noverall, likely due to its combination of logits and feature-space\nrepresentations. Additionally, our work reinforces prior notions in the OOD\ndomain, noting that models with higher ID accuracy performed better across the\nevaluated OOD detection methods. Furthermore, transformer-based architectures\nconsistently outperformed convolution-based models in detecting OOD samples\nacross various methods.\n","date":"2025-03-24"}
{"id":"2503.18549","title":"RLCAD: Reinforcement Learning Training Gym for Revolution Involved CAD\n  Command Sequence Generation","abstract":"  A CAD command sequence is a typical parametric design paradigm in 3D CAD\nsystems where a model is constructed by overlaying 2D sketches with operations\nsuch as extrusion, revolution, and Boolean operations. Although there is\ngrowing academic interest in the automatic generation of command sequences,\nexisting methods and datasets only support operations such as 2D sketching,\nextrusion,and Boolean operations. This limitation makes it challenging to\nrepresent more complex geometries. In this paper, we present a reinforcement\nlearning (RL) training environment (gym) built on a CAD geometric engine. Given\nan input boundary representation (B-Rep) geometry, the policy network in the RL\nalgorithm generates an action. This action, along with previously generated\nactions, is processed within the gym to produce the corresponding CAD geometry,\nwhich is then fed back into the policy network. The rewards, determined by the\ndifference between the generated and target geometries within the gym, are used\nto update the RL network. Our method supports operations beyond sketches,\nBoolean, and extrusion, including revolution operations. With this training\ngym, we achieve state-of-the-art (SOTA) quality in generating command sequences\nfrom B-Rep geometries. In addition, our method can significantly improve the\nefficiency of command sequence generation by a factor of 39X compared with the\nprevious training gym.\n","date":"2025-03-24"}
{"id":"2503.18551","title":"Discriminative protein sequence modelling with Latent Space Diffusion","abstract":"  We explore a framework for protein sequence representation learning that\ndecomposes the task between manifold learning and distributional modelling.\nSpecifically we present a Latent Space Diffusion architecture which combines a\nprotein sequence autoencoder with a denoising diffusion model operating on its\nlatent space. We obtain a one-parameter family of learned representations from\nthe diffusion model, along with the autoencoder's latent representation. We\npropose and evaluate two autoencoder architectures: a homogeneous model forcing\namino acids of the same type to be identically distributed in the latent space,\nand an inhomogeneous model employing a noise-based variant of masking. As a\nbaseline we take a latent space learned by masked language modelling, and\nevaluate discriminative capability on a range of protein property prediction\ntasks. Our finding is twofold: the diffusion models trained on both our\nproposed variants display higher discriminative power than the one trained on\nthe masked language model baseline, none of the diffusion representations\nachieve the performance of the masked language model embeddings themselves.\n","date":"2025-03-24"}
{"id":"2503.18552","title":"EvAnimate: Event-conditioned Image-to-Video Generation for Human\n  Animation","abstract":"  Conditional human animation transforms a static reference image into a\ndynamic sequence by applying motion cues such as poses. These motion cues are\ntypically derived from video data but are susceptible to limitations including\nlow temporal resolution, motion blur, overexposure, and inaccuracies under\nlow-light conditions. In contrast, event cameras provide data streams with\nexceptionally high temporal resolution, a wide dynamic range, and inherent\nresistance to motion blur and exposure issues. In this work, we propose\nEvAnimate, a framework that leverages event streams as motion cues to animate\nstatic human images. Our approach employs a specialized event representation\nthat transforms asynchronous event streams into 3-channel slices with\ncontrollable slicing rates and appropriate slice density, ensuring\ncompatibility with diffusion models. Subsequently, a dual-branch architecture\ngenerates high-quality videos by harnessing the inherent motion dynamics of the\nevent streams, thereby enhancing both video quality and temporal consistency.\nSpecialized data augmentation strategies further enhance cross-person\ngeneralization. Finally, we establish a new benchmarking, including simulated\nevent data for training and validation, and a real-world event dataset\ncapturing human actions under normal and extreme scenarios. The experiment\nresults demonstrate that EvAnimate achieves high temporal fidelity and robust\nperformance in scenarios where traditional video-derived cues fall short.\n","date":"2025-03-24"}
{"id":"2503.18553","title":"ATARS: An Aerial Traffic Atomic Activity Recognition and Temporal\n  Segmentation Dataset","abstract":"  Traffic Atomic Activity which describes traffic patterns for topological\nintersection dynamics is a crucial topic for the advancement of intelligent\ndriving systems. However, existing atomic activity datasets are collected from\nan egocentric view, which cannot support the scenarios where traffic activities\nin an entire intersection are required. Moreover, existing datasets only\nprovide video-level atomic activity annotations, which require exhausting\nefforts to manually trim the videos for recognition and limit their\napplications to untrimmed videos. To bridge this gap, we introduce the Aerial\nTraffic Atomic Activity Recognition and Segmentation (ATARS) dataset, the first\naerial dataset designed for multi-label atomic activity analysis. We offer\natomic activity labels for each frame, which accurately record the intervals\nfor traffic activities. Moreover, we propose a novel task, Multi-label Temporal\nAtomic Activity Recognition, enabling the study of accurate temporal\nlocalization for atomic activity and easing the burden of manual video trimming\nfor recognition. We conduct extensive experiments to evaluate existing\nstate-of-the-art models on both atomic activity recognition and temporal atomic\nactivity segmentation. The results highlight the unique challenges of our ATARS\ndataset, such as recognizing extremely small objects' activities. We further\nprovide comprehensive discussion analyzing these challenges and offer valuable\ninsights for future direction to improve recognizing atomic activity in aerial\nview. Our source code and dataset are available at\nhttps:\/\/github.com\/magecliff96\/ATARS\/\n","date":"2025-03-24"}
{"id":"2503.18556","title":"Instruction-Aligned Visual Attention for Mitigating Hallucinations in\n  Large Vision-Language Models","abstract":"  Despite the significant success of Large Vision-Language models(LVLMs), these\nmodels still suffer hallucinations when describing images, generating answers\nthat include non-existent objects. It is reported that these models tend to\nover-focus on certain irrelevant image tokens that do not contain critical\ninformation for answering the question and distort the output. To address this,\nwe propose an Instruction-Aligned Visual Attention(IAVA) approach, which\nidentifies irrelevant tokens by comparing changes in attention weights under\ntwo different instructions. By applying contrastive decoding, we dynamically\nadjust the logits generated from original image tokens and irrelevant image\ntokens, reducing the model's over-attention to irrelevant information. The\nexperimental results demonstrate that IAVA consistently outperforms existing\ndecoding techniques on benchmarks such as MME, POPE, and TextVQA in mitigating\nobject hallucinations. Our IAVA approach is available online at\nhttps:\/\/github.com\/Lee-lab558\/IAVA.\n","date":"2025-03-24"}
{"id":"2503.18557","title":"LeanStereo: A Leaner Backbone based Stereo Network","abstract":"  Recently, end-to-end deep networks based stereo matching methods, mainly\nbecause of their performance, have gained popularity. However, this improvement\nin performance comes at the cost of increased computational and memory\nbandwidth requirements, thus necessitating specialized hardware (GPUs); even\nthen, these methods have large inference times compared to classical methods.\nThis limits their applicability in real-world applications. Although we desire\nhigh accuracy stereo methods albeit with reasonable inference time. To this\nend, we propose a fast end-to-end stereo matching method. Majority of this\nspeedup comes from integrating a leaner backbone. To recover the performance\nlost because of a leaner backbone, we propose to use learned attention weights\nbased cost volume combined with LogL1 loss for stereo matching. Using LogL1\nloss not only improves the overall performance of the proposed network but also\nleads to faster convergence. We do a detailed empirical evaluation of different\ndesign choices and show that our method requires 4x less operations and is also\nabout 9 to 14x faster compared to the state of the art methods like ACVNet [1],\nLEAStereo [2] and CFNet [3] while giving comparable performance.\n","date":"2025-03-24"}
{"id":"2503.18559","title":"AMD-Hummingbird: Towards an Efficient Text-to-Video Model","abstract":"  Text-to-Video (T2V) generation has attracted significant attention for its\nability to synthesize realistic videos from textual descriptions. However,\nexisting models struggle to balance computational efficiency and high visual\nquality, particularly on resource-limited devices, e.g.,iGPUs and mobile\nphones. Most prior work prioritizes visual fidelity while overlooking the need\nfor smaller, more efficient models suitable for real-world deployment. To\naddress this challenge, we propose a lightweight T2V framework, termed\nHummingbird, which prunes existing models and enhances visual quality through\nvisual feedback learning. Our approach reduces the size of the U-Net from 1.4\nbillion to 0.7 billion parameters, significantly improving efficiency while\npreserving high-quality video generation. Additionally, we introduce a novel\ndata processing pipeline that leverages Large Language Models (LLMs) and Video\nQuality Assessment (VQA) models to enhance the quality of both text prompts and\nvideo data. To support user-driven training and style customization, we\npublicly release the full training code, including data processing and model\ntraining. Extensive experiments show that our method achieves a 31X speedup\ncompared to state-of-the-art models such as VideoCrafter2, while also attaining\nthe highest overall score on VBench. Moreover, our method supports the\ngeneration of videos with up to 26 frames, addressing the limitations of\nexisting U-Net-based methods in long video generation. Notably, the entire\ntraining process requires only four GPUs, yet delivers performance competitive\nwith existing leading methods. Hummingbird presents a practical and efficient\nsolution for T2V generation, combining high performance, scalability, and\nflexibility for real-world applications.\n","date":"2025-03-24"}
{"id":"2503.18562","title":"Self-Reported Confidence of Large Language Models in Gastroenterology:\n  Analysis of Commercial, Open-Source, and Quantized Models","abstract":"  This study evaluated self-reported response certainty across several large\nlanguage models (GPT, Claude, Llama, Phi, Mistral, Gemini, Gemma, and Qwen)\nusing 300 gastroenterology board-style questions. The highest-performing models\n(GPT-o1 preview, GPT-4o, and Claude-3.5-Sonnet) achieved Brier scores of\n0.15-0.2 and AUROC of 0.6. Although newer models demonstrated improved\nperformance, all exhibited a consistent tendency towards overconfidence.\nUncertainty estimation presents a significant challenge to the safe use of LLMs\nin healthcare. Keywords: Large Language Models; Confidence Elicitation;\nArtificial Intelligence; Gastroenterology; Uncertainty Quantification\n","date":"2025-03-24"}
{"id":"2503.18565","title":"Distil-xLSTM: Learning Attention Mechanisms through Recurrent Structures","abstract":"  The current era of Natural Language Processing (NLP) is dominated by\nTransformer models. However, novel architectures relying on recurrent\nmechanisms, such as xLSTM and Mamba, have been proposed as alternatives to\nattention-based models. Although computation is done differently than with the\nattention mechanism mechanism, these recurrent models yield good results and\nsometimes even outperform state-of-the-art attention-based models. In this\nwork, we propose Distil-xLSTM, an xLSTM-based Small Language Model (SLM)\ntrained by distilling knowledge from a Large Language Model (LLM) that shows\npromising results while being compute and scale efficient. Our Distil-xLSTM\nfocuses on approximating a transformer-based model attention parametrization\nusing its recurrent sequence mixing components and shows good results with\nminimal training.\n","date":"2025-03-24"}
{"id":"2503.18567","title":"Advancing Cross-Organ Domain Generalization with Test-Time Style\n  Transfer and Diversity Enhancement","abstract":"  Deep learning has made significant progress in addressing challenges in\nvarious fields including computational pathology (CPath). However, due to the\ncomplexity of the domain shift problem, the performance of existing models will\ndegrade, especially when it comes to multi-domain or cross-domain tasks. In\nthis paper, we propose a Test-time style transfer (T3s) that uses a\nbidirectional mapping mechanism to project the features of the source and\ntarget domains into a unified feature space, enhancing the generalization\nability of the model. To further increase the style expression space, we\nintroduce a Cross-domain style diversification module (CSDM) to ensure the\northogonality between style bases. In addition, data augmentation and low-rank\nadaptation techniques are used to improve feature alignment and sensitivity,\nenabling the model to adapt to multi-domain inputs effectively. Our method has\ndemonstrated effectiveness on three unseen datasets.\n","date":"2025-03-24"}
{"id":"2503.18569","title":"Anchor-based oversampling for imbalanced tabular data via contrastive\n  and adversarial learning","abstract":"  Imbalanced data represent a distribution with more frequencies of one class\n(majority) than the other (minority). This phenomenon occurs across various\ndomains, such as security, medical care and human activity. In imbalanced\nlearning, classification algorithms are typically inclined to classify the\nmajority class accurately, resulting in artificially high accuracy rates. As a\nresult, many minority samples are mistakenly labelled as majority-class\ninstances, resulting in a bias that benefits the majority class. This study\npresents a framework based on boundary anchor samples to tackle the imbalance\nlearning challenge. First, we select and use anchor samples to train a\nmultilayer perceptron (MLP) classifier, which acts as a prior knowledge model\nand aids the adversarial and contrastive learning procedures. Then, we designed\na novel deep generative model called Anchor Stabilized Conditional Generative\nAdversarial Network or Anch-SCGAN in short. Anch-SCGAN is supported with two\ngenerators for the minority and majority classes and a discriminator\nincorporating additional class-specific information from the pre-trained\nfeature extractor MLP. In addition, we facilitate the generator's training\nprocedure in two ways. First, we define a new generator loss function based on\nreprocessed anchor samples and contrastive learning. Second, we apply a scoring\nstrategy to stabilize the adversarial training part in generators. We train\nAnch-SCGAN and further finetune it with anchor samples to improve the precision\nof the generated samples. Our experiments on 16 real-world imbalanced datasets\nillustrate that Anch-SCGAN outperforms the renowned methods in imbalanced\nlearning.\n","date":"2025-03-24"}
{"id":"2503.18570","title":"Dense Retrieval for Low Resource Languages -- the Case of Amharic\n  Language","abstract":"  This paper reports some difficulties and some results when using dense\nretrievers on Amharic, one of the low-resource languages spoken by 120 millions\npopulations. The efforts put and difficulties faced by University Addis Ababa\ntoward Amharic Information Retrieval will be developed during the presentation.\n","date":"2025-03-24"}
{"id":"2503.18571","title":"Parametric Dynamic Mode Decomposition with multi-linear interpolation\n  for prediction of thermal fields of Al2O3-water nanofluid flows at unseen\n  parameters","abstract":"  The study proposes a data-driven model which combines the Dynamic Mode\nDecomposition with multi-linear interpolation to predict the thermal fields of\nnanofluid flows at unseen Reynolds numbers (Re) and particle volume\nconcentrations ($\\epsilon$). The flow, considered for the study, is laminar and\nincompressible. The study employs an in-house Fortran-based solver to predict\nthe thermal fields of Al$_2$O$_3$-water nanofluid flow through a\ntwo-dimensional rectangular channel, with the bottom wall subjected to a\nuniform heat flux. The performance of two models operating in one- and\ntwo-dimensional parametric spaces are investigated. Initially, a DMD with\nlinear interpolation (DMD-LI) based solver is used for prediction of\ntemperature of the nanofluid at any Re $>$ 100. The DMD-LI based model,\npredicts temperature fields with a maximum percentage difference of just\n0.0273\\%, in comparison with the CFD-based solver at Re =960, and $\\epsilon$ =\n1.0\\%. The corresponding difference in the average Nusselt numbers is only\n0.39\\%. Following that a DMD with bi-linear interpolation (DMD-BLI) based\nsolver is used for prediction of temperature of the nanofluid at any Re $>$ 100\nand $\\epsilon$ $>$ 0.5\\%. The performance of two different ways of stacking the\ndata are also examined. When compared to the CFD-based model, the DMD-BLI-based\nmodel predicts the temperature fields with a maximum percentage difference of\n0.21 \\%, at Re = 800 and $\\epsilon$ = 1.35\\%. And the corresponding percentage\ndifference in the average Nusselt number prediction is only 6.08\\%. All the\nresults are reported in detail. Along side the important conclusions, the\nfuture scope of the study is also listed.\n","date":"2025-03-24"}
{"id":"2503.18572","title":"Identifying and Characterising Higher Order Interactions in Mobility\n  Networks Using Hypergraphs","abstract":"  Understanding human mobility is essential for applications ranging from urban\nplanning to public health. Traditional mobility models such as flow networks\nand colocation matrices capture only pairwise interactions between discrete\nlocations, overlooking higher-order relationships among locations (i.e.,\nmobility flow among two or more locations). To address this, we propose\nco-visitation hypergraphs, a model that leverages temporal observation windows\nto extract group interactions between locations from individual mobility\ntrajectory data. Using frequent pattern mining, our approach constructs\nhypergraphs that capture dynamic mobility behaviors across different spatial\nand temporal scales. We validate our method on a publicly available mobility\ndataset and demonstrate its effectiveness in analyzing city-scale mobility\npatterns, detecting shifts during external disruptions such as extreme weather\nevents, and examining how a location's connectivity (degree) relates to the\nnumber of points of interest (POIs) within it. Our results demonstrate that our\nhypergraph-based mobility analysis framework is a valuable tool with potential\napplications in diverse fields such as public health, disaster resilience, and\nurban planning.\n","date":"2025-03-24"}
{"id":"2503.18578","title":"Galaxy Walker: Geometry-aware VLMs For Galaxy-scale Understanding","abstract":"  Modern vision-language models (VLMs) develop patch embedding and convolution\nbackbone within vector space, especially Euclidean ones, at the very founding.\nWhen expanding VLMs to a galaxy scale for understanding astronomical phenomena,\nthe integration of spherical space for planetary orbits and hyperbolic spaces\nfor black holes raises two formidable challenges. a) The current pre-training\nmodel is confined to Euclidean space rather than a comprehensive geometric\nembedding. b) The predominant architecture lacks suitable backbones for\nanisotropic physical geometries. In this paper, we introduced Galaxy-Walker, a\ngeometry-aware VLM, for the universe-level vision understanding tasks. We\nproposed the geometry prompt that generates geometry tokens by random walks\nacross diverse spaces on a multi-scale physical graph, along with a geometry\nadapter that compresses and reshapes the space anisotropy in a\nmixture-of-experts manner. Extensive experiments demonstrate the effectiveness\nof our approach, with Galaxy-Walker achieving state-of-the-art performance in\nboth galaxy property estimation ($R^2$ scores up to $0.91$) and morphology\nclassification tasks (up to $+0.17$ F1 improvement in challenging features),\nsignificantly outperforming both domain-specific models and general-purpose\nVLMs.\n","date":"2025-03-24"}
{"id":"2503.18583","title":"Adapting Video Diffusion Models for Time-Lapse Microscopy","abstract":"  We present a domain adaptation of video diffusion models to generate highly\nrealistic time-lapse microscopy videos of cell division in HeLa cells. Although\nstate-of-the-art generative video models have advanced significantly for\nnatural videos, they remain underexplored in microscopy domains. To address\nthis gap, we fine-tune a pretrained video diffusion model on\nmicroscopy-specific sequences, exploring three conditioning strategies: (1)\ntext prompts derived from numeric phenotypic measurements (e.g., proliferation\nrates, migration speeds, cell-death frequencies), (2) direct numeric embeddings\nof phenotype scores, and (3) image-conditioned generation, where an initial\nmicroscopy frame is extended into a complete video sequence. Evaluation using\nbiologically meaningful morphological, proliferation, and migration metrics\ndemonstrates that fine-tuning substantially improves realism and accurately\ncaptures critical cellular behaviors such as mitosis and migration. Notably,\nthe fine-tuned model also generalizes beyond the training horizon, generating\ncoherent cell dynamics even in extended sequences. However, precisely\ncontrolling specific phenotypic characteristics remains challenging,\nhighlighting opportunities for future work to enhance conditioning methods. Our\nresults demonstrate the potential for domain-specific fine-tuning of generative\nvideo models to produce biologically plausible synthetic microscopy data,\nsupporting applications such as in-silico hypothesis testing and data\naugmentation.\n","date":"2025-03-24"}
{"id":"2503.18584","title":"A Universal Model Combining Differential Equations and Neural Networks\n  for Ball Trajectory Prediction","abstract":"  This paper presents a data driven universal ball trajectory prediction method\nintegrated with physics equations. Existing methods are designed for specific\nball types and struggle to generalize. This challenge arises from three key\nfactors. First, learning-based models require large datasets but suffer from\naccuracy drops in unseen scenarios. Second, physics-based models rely on\ncomplex formulas and detailed inputs, yet accurately obtaining ball states,\nsuch as spin, is often impractical. Third, integrating physical principles with\nneural networks to achieve high accuracy, fast inference, and strong\ngeneralization remains difficult. To address these issues, we propose an\ninnovative approach that incorporates physics-based equations and neural\nnetworks. We first derive three generalized physical formulas. Then, using a\nneural network and observed trajectory points, we infer certain parameters\nwhile fitting the remaining ones. These formulas enable precise trajectory\nprediction with minimal training data: only a few dozen samples. Extensive\nexperiments demonstrate our method superiority in generalization, real-time\nperformance, and accuracy.\n","date":"2025-03-24"}
{"id":"2503.18589","title":"Unified Uncertainty-Aware Diffusion for Multi-Agent Trajectory Modeling","abstract":"  Multi-agent trajectory modeling has primarily focused on forecasting future\nstates, often overlooking broader tasks like trajectory completion, which are\ncrucial for real-world applications such as correcting tracking data. Existing\nmethods also generally predict agents' states without offering any state-wise\nmeasure of uncertainty. Moreover, popular multi-modal sampling methods lack any\nerror probability estimates for each generated scene under the same prior\nobservations, making it difficult to rank the predictions during inference\ntime. We introduce U2Diff, a \\textbf{unified} diffusion model designed to\nhandle trajectory completion while providing state-wise \\textbf{uncertainty}\nestimates jointly. This uncertainty estimation is achieved by augmenting the\nsimple denoising loss with the negative log-likelihood of the predicted noise\nand propagating latent space uncertainty to the real state space. Additionally,\nwe incorporate a Rank Neural Network in post-processing to enable \\textbf{error\nprobability} estimation for each generated mode, demonstrating a strong\ncorrelation with the error relative to ground truth. Our method outperforms the\nstate-of-the-art solutions in trajectory completion and forecasting across four\nchallenging sports datasets (NBA, Basketball-U, Football-U, Soccer-U),\nhighlighting the effectiveness of uncertainty and error probability estimation.\nVideo at https:\/\/youtu.be\/ngw4D4eJToE\n","date":"2025-03-24"}
{"id":"2503.18592","title":"The Role of Artificial Intelligence in Enhancing Insulin Recommendations\n  and Therapy Outcomes","abstract":"  The growing worldwide incidence of diabetes requires more effective\napproaches for managing blood glucose levels. Insulin delivery systems have\nadvanced significantly, with artificial intelligence (AI) playing a key role in\nimproving their precision and adaptability. AI algorithms, particularly those\nbased on reinforcement learning, allow for personalised insulin dosing by\ncontinuously adapting to an individual's responses. Despite these advancements,\nchallenges such as data privacy, algorithm transparency, and accessibility\nstill need to be addressed. Continued progress and validation in AI-driven\ninsulin delivery systems promise to improve therapy outcomes further, offering\npeople more effective and individualised management of their diabetes. This\npaper presents an overview of current strategies, key challenges, and future\ndirections.\n","date":"2025-03-24"}
{"id":"2503.18594","title":"ClinText-SP and RigoBERTa Clinical: a new set of open resources for\n  Spanish Clinical NLP","abstract":"  We present a novel contribution to Spanish clinical natural language\nprocessing by introducing the largest publicly available clinical corpus,\nClinText-SP, along with a state-of-the-art clinical encoder language model,\nRigoBERTa Clinical. Our corpus was meticulously curated from diverse open\nsources, including clinical cases from medical journals and annotated corpora\nfrom shared tasks, providing a rich and diverse dataset that was previously\ndifficult to access. RigoBERTa Clinical, developed through domain-adaptive\npretraining on this comprehensive dataset, significantly outperforms existing\nmodels on multiple clinical NLP benchmarks. By publicly releasing both the\ndataset and the model, we aim to empower the research community with robust\nresources that can drive further advancements in clinical NLP and ultimately\ncontribute to improved healthcare applications.\n","date":"2025-03-24"}
{"id":"2503.18595","title":"Adaptive Unimodal Regulation for Balanced Multimodal Information\n  Acquisition","abstract":"  Sensory training during the early ages is vital for human development.\nInspired by this cognitive phenomenon, we observe that the early training stage\nis also important for the multimodal learning process, where dataset\ninformation is rapidly acquired. We refer to this stage as the prime learning\nwindow. However, based on our observation, this prime learning window in\nmultimodal learning is often dominated by information-sufficient modalities,\nwhich in turn suppresses the information acquisition of\ninformation-insufficient modalities. To address this issue, we propose\nInformation Acquisition Regulation (InfoReg), a method designed to balance\ninformation acquisition among modalities. Specifically, InfoReg slows down the\ninformation acquisition process of information-sufficient modalities during the\nprime learning window, which could promote information acquisition of\ninformation-insufficient modalities. This regulation enables a more balanced\nlearning process and improves the overall performance of the multimodal\nnetwork. Experiments show that InfoReg outperforms related multimodal\nimbalanced methods across various datasets, achieving superior model\nperformance. The code is available at\nhttps:\/\/github.com\/GeWu-Lab\/InfoReg_CVPR2025.\n","date":"2025-03-24"}
{"id":"2503.18596","title":"LinkAlign: Scalable Schema Linking for Real-World Large-Scale\n  Multi-Database Text-to-SQL","abstract":"  Schema linking is a critical bottleneck in achieving human-level performance\nin Text-to-SQL tasks, particularly in real-world large-scale multi-database\nscenarios. Addressing schema linking faces two major challenges: (1) Database\nRetrieval: selecting the correct database from a large schema pool in\nmulti-database settings, while filtering out irrelevant ones. (2) Schema Item\nGrounding: accurately identifying the relevant tables and columns from within a\nlarge and redundant schema for SQL generation. To address this, we introduce\nLinkAlign, a novel framework that can effectively adapt existing baselines to\nreal-world environments by systematically addressing schema linking. Our\nframework comprises three key steps: multi-round semantic enhanced retrieval\nand irrelevant information isolation for Challenge 1, and schema extraction\nenhancement for Challenge 2. We evaluate our method performance of schema\nlinking on the SPIDER and BIRD benchmarks, and the ability to adapt existing\nText-to-SQL models to real-world environments on the SPIDER 2.0-lite benchmark.\nExperiments show that LinkAlign outperforms existing baselines in\nmulti-database settings, demonstrating its effectiveness and robustness. On the\nother hand, our method ranks highest among models excluding those using long\nchain-of-thought reasoning LLMs. This work bridges the gap between current\nresearch and real-world scenarios, providing a practical solution for robust\nand scalable schema linking. The codes are available at\nhttps:\/\/github.com\/Satissss\/LinkAlign.\n","date":"2025-03-24"}
{"id":"2503.18599","title":"Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization","abstract":"  Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques.\n","date":"2025-03-24"}
{"id":"2503.18603","title":"LANGALIGN: Enhancing Non-English Language Models via Cross-Lingual\n  Embedding Alignment","abstract":"  While Large Language Models have gained attention, many service developers\nstill rely on embedding-based models due to practical constraints. In such\ncases, the quality of fine-tuning data directly impacts performance, and\nEnglish datasets are often used as seed data for training non-English models.\nIn this study, we propose LANGALIGN, which enhances target language processing\nby aligning English embedding vectors with those of the target language at the\ninterface between the language model and the task header. Experiments on\nKorean, Japanese, and Chinese demonstrate that LANGALIGN significantly improves\nperformance across all three languages. Additionally, we show that LANGALIGN\ncan be applied in reverse to convert target language data into a format that an\nEnglish-based model can process.\n","date":"2025-03-24"}
{"id":"2503.18607","title":"Reinforcement Learning in Switching Non-Stationary Markov Decision\n  Processes: Algorithms and Convergence Analysis","abstract":"  Reinforcement learning in non-stationary environments is challenging due to\nabrupt and unpredictable changes in dynamics, often causing traditional\nalgorithms to fail to converge. However, in many real-world cases,\nnon-stationarity has some structure that can be exploited to develop algorithms\nand facilitate theoretical analysis. We introduce one such structure, Switching\nNon-Stationary Markov Decision Processes (SNS-MDP), where environments switch\nover time based on an underlying Markov chain. Under a fixed policy, the value\nfunction of an SNS-MDP admits a closed-form solution determined by the Markov\nchain's statistical properties, and despite the inherent non-stationarity,\nTemporal Difference (TD) learning methods still converge to the correct value\nfunction. Furthermore, policy improvement can be performed, and it is shown\nthat policy iteration converges to the optimal policy. Moreover, since\nQ-learning converges to the optimal Q-function, it likewise yields the\ncorresponding optimal policy. To illustrate the practical advantages of\nSNS-MDPs, we present an example in communication networks where channel noise\nfollows a Markovian pattern, demonstrating how this framework can effectively\nguide decision-making in complex, time-varying contexts.\n","date":"2025-03-24"}
{"id":"2503.18608","title":"AutoBayes: A Compositional Framework for Generalized Variational\n  Inference","abstract":"  We introduce a new compositional framework for generalized variational\ninference, clarifying the different parts of a model, how they interact, and\nhow they compose. We explain that both exact Bayesian inference and the loss\nfunctions typical of variational inference (such as variational free energy and\nits generalizations) satisfy chain rules akin to that of reverse-mode automatic\ndifferentiation, and we advocate for exploiting this to build and optimize\nmodels accordingly. To this end, we construct a series of compositional tools:\nfor building models; for constructing their inversions; for attaching local\nloss functions; and for exposing parameters. Finally, we explain how the\nresulting parameterized statistical games may be optimized locally, too. We\nillustrate our framework with a number of classic examples, pointing to new\nareas of extensibility that are revealed.\n","date":"2025-03-24"}
{"id":"2503.18612","title":"Adventurer: Exploration with BiGAN for Deep Reinforcement Learning","abstract":"  Recent developments in deep reinforcement learning have been very successful\nin learning complex, previously intractable problems. Sample efficiency and\nlocal optimality, however, remain significant challenges. To address these\nchallenges, novelty-driven exploration strategies have emerged and shown\npromising potential. Unfortunately, no single algorithm outperforms all others\nin all tasks and most of them struggle with tasks with high-dimensional and\ncomplex observations. In this work, we propose Adventurer, a novelty-driven\nexploration algorithm that is based on Bidirectional Generative Adversarial\nNetworks (BiGAN), where BiGAN is trained to estimate state novelty.\nIntuitively, a generator that has been trained on the distribution of visited\nstates should only be able to generate a state coming from the distribution of\nvisited states. As a result, novel states using the generator to reconstruct\ninput states from certain latent representations would lead to larger\nreconstruction errors. We show that BiGAN performs well in estimating state\nnovelty for complex observations. This novelty estimation method can be\ncombined with intrinsic-reward-based exploration. Our empirical results show\nthat Adventurer produces competitive results on a range of popular benchmark\ntasks, including continuous robotic manipulation tasks (e.g. Mujoco robotics)\nand high-dimensional image-based tasks (e.g. Atari games).\n","date":"2025-03-24"}
{"id":"2503.18617","title":"Scaling Laws for Emulation of Stellar Spectra","abstract":"  Neural network-based emulators for the inference of stellar parameters and\nelemental abundances represent an increasingly popular methodology in modern\nspectroscopic surveys. However, these approaches are often constrained by their\nemulation precision and domain transfer capabilities. Greater generalizability\nhas previously been achieved only with significantly larger model\narchitectures, as demonstrated by Transformer-based models in natural language\nprocessing. This observation aligns with neural scaling laws, where model\nperformance predictably improves with increased model size, computational\nresources allocated to model training, and training data volume. In this study,\nwe demonstrate that these scaling laws also apply to Transformer-based spectral\nemulators in astronomy. Building upon our previous work with TransformerPayne\nand incorporating Maximum Update Parametrization techniques from natural\nlanguage models, we provide training guidelines for scaling models to achieve\noptimal performance. Our results show that within the explored parameter space,\nclear scaling relationships emerge. These findings suggest that optimal\ncomputational resource allocation requires balanced scaling. Specifically,\ngiven a tenfold increase in training compute, achieving an optimal seven-fold\nreduction in mean squared error necessitates an approximately 2.5-fold increase\nin dataset size and a 3.8-fold increase in model size. This study establishes a\nfoundation for developing spectral foundational models with enhanced domain\ntransfer capabilities.\n","date":"2025-03-24"}
{"id":"2503.18623","title":"Training-Free Personalization via Retrieval and Reasoning on\n  Fingerprints","abstract":"  Vision Language Models (VLMs) have lead to major improvements in multimodal\nreasoning, yet they still struggle to understand user-specific concepts.\nExisting personalization methods address this limitation but heavily rely on\ntraining procedures, that can be either costly or unpleasant to individual\nusers. We depart from existing work, and for the first time explore the\ntraining-free setting in the context of personalization. We propose a novel\nmethod, Retrieval and Reasoning for Personalization (R2P), leveraging internal\nknowledge of VLMs. First, we leverage VLMs to extract the concept fingerprint,\ni.e., key attributes uniquely defining the concept within its semantic class.\nWhen a query arrives, the most similar fingerprints are retrieved and scored\nvia chain-of-thought-reasoning. To reduce the risk of hallucinations, the\nscores are validated through cross-modal verification at the attribute level:\nin case of a discrepancy between the scores, R2P refines the concept\nassociation via pairwise multimodal matching, where the retrieved fingerprints\nand their images are directly compared with the query. We validate R2P on two\npublicly available benchmarks and a newly introduced dataset, Personal Concepts\nwith Visual Ambiguity (PerVA), for concept identification highlighting\nchallenges in visual ambiguity. R2P consistently outperforms state-of-the-art\napproaches on various downstream tasks across all benchmarks. Code will be\navailable upon acceptance.\n","date":"2025-03-24"}
{"id":"2503.18626","title":"Generative Dataset Distillation using Min-Max Diffusion Model","abstract":"  In this paper, we address the problem of generative dataset distillation that\nutilizes generative models to synthesize images. The generator may produce any\nnumber of images under a preserved evaluation time. In this work, we leverage\nthe popular diffusion model as the generator to compute a surrogate dataset,\nboosted by a min-max loss to control the dataset's diversity and\nrepresentativeness during training. However, the diffusion model is\ntime-consuming when generating images, as it requires an iterative generation\nprocess. We observe a critical trade-off between the number of image samples\nand the image quality controlled by the diffusion steps and propose Diffusion\nStep Reduction to achieve optimal performance. This paper details our\ncomprehensive method and its performance. Our model achieved $2^{nd}$ place in\nthe generative track of \\href{https:\/\/www.dd-challenge.com\/#\/}{The First\nDataset Distillation Challenge of ECCV2024}, demonstrating its superior\nperformance.\n","date":"2025-03-24"}
{"id":"2503.18627","title":"Dig2DIG: Dig into Diffusion Information Gains for Image Fusion","abstract":"  Image fusion integrates complementary information from multi-source images to\ngenerate more informative results. Recently, the diffusion model, which\ndemonstrates unprecedented generative potential, has been explored in image\nfusion. However, these approaches typically incorporate predefined multimodal\nguidance into diffusion, failing to capture the dynamically changing\nsignificance of each modality, while lacking theoretical guarantees. To address\nthis issue, we reveal a significant spatio-temporal imbalance in image\ndenoising; specifically, the diffusion model produces dynamic information gains\nin different image regions with denoising steps. Based on this observation, we\nDig into the Diffusion Information Gains (Dig2DIG) and theoretically derive a\ndiffusion-based dynamic image fusion framework that provably reduces the upper\nbound of the generalization error. Accordingly, we introduce diffusion\ninformation gains (DIG) to quantify the information contribution of each\nmodality at different denoising steps, thereby providing dynamic guidance\nduring the fusion process. Extensive experiments on multiple fusion scenarios\nconfirm that our method outperforms existing diffusion-based approaches in\nterms of both fusion quality and inference efficiency.\n","date":"2025-03-24"}
{"id":"2503.18629","title":"Towards Human-Understandable Multi-Dimensional Concept Discovery","abstract":"  Concept-based eXplainable AI (C-XAI) aims to overcome the limitations of\ntraditional saliency maps by converting pixels into human-understandable\nconcepts that are consistent across an entire dataset. A crucial aspect of\nC-XAI is completeness, which measures how well a set of concepts explains a\nmodel's decisions. Among C-XAI methods, Multi-Dimensional Concept Discovery\n(MCD) effectively improves completeness by breaking down the CNN latent space\ninto distinct and interpretable concept subspaces. However, MCD's explanations\ncan be difficult for humans to understand, raising concerns about their\npractical utility. To address this, we propose Human-Understandable\nMulti-dimensional Concept Discovery (HU-MCD). HU-MCD uses the Segment Anything\nModel for concept identification and implements a CNN-specific input masking\ntechnique to reduce noise introduced by traditional masking methods. These\nchanges to MCD, paired with the completeness relation, enable HU-MCD to enhance\nconcept understandability while maintaining explanation faithfulness. Our\nexperiments, including human subject studies, show that HU-MCD provides more\nprecise and reliable explanations than existing C-XAI methods. The code is\navailable at https:\/\/github.com\/grobruegge\/hu-mcd.\n","date":"2025-03-24"}
{"id":"2503.18631","title":"Robust Lane Detection with Wavelet-Enhanced Context Modeling and\n  Adaptive Sampling","abstract":"  Lane detection is critical for autonomous driving and ad-vanced driver\nassistance systems (ADAS). While recent methods like CLRNet achieve strong\nperformance, they struggle under adverse con-ditions such as extreme weather,\nillumination changes, occlusions, and complex curves. We propose a\nWavelet-Enhanced Feature Pyramid Net-work (WE-FPN) to address these challenges.\nA wavelet-based non-local block is integrated before the feature pyramid to\nimprove global context modeling, especially for occluded and curved lanes.\nAdditionally, we de-sign an adaptive preprocessing module to enhance lane\nvisibility under poor lighting. An attention-guided sampling strategy further\nreffnes spa-tial features, boosting accuracy on distant and curved lanes.\nExperiments on CULane and TuSimple demonstrate that our approach signiffcantly\noutperforms baselines in challenging scenarios, achieving better robust-ness\nand accuracy in real-world driving conditions.\n","date":"2025-03-24"}
{"id":"2503.18634","title":"Adaptive Machine Learning for Resource-Constrained Environments","abstract":"  The Internet of Things is an example domain where data is perpetually\ngenerated in ever-increasing quantities, reflecting the proliferation of\nconnected devices and the formation of continuous data streams over time.\nConsequently, the demand for ad-hoc, cost-effective machine learning solutions\nmust adapt to this evolving data influx. This study tackles the task of\noffloading in small gateways, exacerbated by their dynamic availability over\ntime. An approach leveraging CPU utilization metrics using online and continual\nmachine learning techniques is proposed to predict gateway availability. These\nmethods are compared to popular machine learning algorithms and a recent\ntime-series foundation model, Lag-Llama, for fine-tuned and zero-shot setups.\nTheir performance is benchmarked on a dataset of CPU utilization measurements\nover time from an IoT gateway and focuses on model metrics such as prediction\nerrors, training and inference times, and memory consumption. Our primary\nobjective is to study new efficient ways to predict CPU performance in IoT\nenvironments. Across various scenarios, our findings highlight that ensemble\nand online methods offer promising results for this task in terms of accuracy\nwhile maintaining a low resource footprint.\n","date":"2025-03-24"}
{"id":"2503.18635","title":"OCCO: LVM-guided Infrared and Visible Image Fusion Framework based on\n  Object-aware and Contextual COntrastive Learning","abstract":"  Image fusion is a crucial technique in the field of computer vision, and its\ngoal is to generate high-quality fused images and improve the performance of\ndownstream tasks. However, existing fusion methods struggle to balance these\ntwo factors. Achieving high quality in fused images may result in lower\nperformance in downstream visual tasks, and vice versa. To address this\ndrawback, a novel LVM (large vision model)-guided fusion framework with\nObject-aware and Contextual COntrastive learning is proposed, termed as OCCO.\nThe pre-trained LVM is utilized to provide semantic guidance, allowing the\nnetwork to focus solely on fusion tasks while emphasizing learning salient\nsemantic features in form of contrastive learning. Additionally, a novel\nfeature interaction fusion network is also designed to resolve information\nconflicts in fusion images caused by modality differences. By learning the\ndistinction between positive samples and negative samples in the latent feature\nspace (contextual space), the integrity of target information in fused image is\nimproved, thereby benefiting downstream performance. Finally, compared with\neight state-of-the-art methods on four datasets, the effectiveness of the\nproposed method is validated, and exceptional performance is also demonstrated\non downstream visual task.\n","date":"2025-03-24"}
{"id":"2503.18637","title":"Unbiasing through Textual Descriptions: Mitigating Representation Bias\n  in Video Benchmarks","abstract":"  We propose a new \"Unbiased through Textual Description (UTD)\" video benchmark\nbased on unbiased subsets of existing video classification and retrieval\ndatasets to enable a more robust assessment of video understanding\ncapabilities. Namely, we tackle the problem that current video benchmarks may\nsuffer from different representation biases, e.g., object bias or single-frame\nbias, where mere recognition of objects or utilization of only a single frame\nis sufficient for correct prediction. We leverage VLMs and LLMs to analyze and\ndebias benchmarks from such representation biases. Specifically, we generate\nframe-wise textual descriptions of videos, filter them for specific information\n(e.g. only objects) and leverage them to examine representation biases across\nthree dimensions: 1) concept bias - determining if a specific concept (e.g.,\nobjects) alone suffice for prediction; 2) temporal bias - assessing if temporal\ninformation contributes to prediction; and 3) common sense vs. dataset bias -\nevaluating whether zero-shot reasoning or dataset correlations contribute to\nprediction. We conduct a systematic analysis of 12 popular video classification\nand retrieval datasets and create new object-debiased test splits for these\ndatasets. Moreover, we benchmark 30 state-of-the-art video models on original\nand debiased splits and analyze biases in the models. To facilitate the future\ndevelopment of more robust video understanding benchmarks and models, we\nrelease: \"UTD-descriptions\", a dataset with our rich structured descriptions\nfor each dataset, and \"UTD-splits\", a dataset of object-debiased test splits.\n","date":"2025-03-24"}
{"id":"2503.18640","title":"LLGS: Unsupervised Gaussian Splatting for Image Enhancement and\n  Reconstruction in Pure Dark Environment","abstract":"  3D Gaussian Splatting has shown remarkable capabilities in novel view\nrendering tasks and exhibits significant potential for multi-view\noptimization.However, the original 3D Gaussian Splatting lacks color\nrepresentation for inputs in low-light environments. Simply using enhanced\nimages as inputs would lead to issues with multi-view consistency, and current\nsingle-view enhancement systems rely on pre-trained data, lacking scene\ngeneralization. These problems limit the application of 3D Gaussian Splatting\nin low-light conditions in the field of robotics, including high-fidelity\nmodeling and feature matching. To address these challenges, we propose an\nunsupervised multi-view stereoscopic system based on Gaussian Splatting, called\nLow-Light Gaussian Splatting (LLGS). This system aims to enhance images in\nlow-light environments while reconstructing the scene. Our method introduces a\ndecomposable Gaussian representation called M-Color, which separately\ncharacterizes color information for targeted enhancement. Furthermore, we\npropose an unsupervised optimization method with zero-knowledge priors, using\ndirection-based enhancement to ensure multi-view consistency. Experiments\nconducted on real-world datasets demonstrate that our system outperforms\nstate-of-the-art methods in both low-light enhancement and 3D Gaussian\nSplatting.\n","date":"2025-03-24"}
{"id":"2503.18641","title":"From Fragment to One Piece: A Survey on AI-Driven Graphic Design","abstract":"  This survey provides a comprehensive overview of the advancements in\nArtificial Intelligence in Graphic Design (AIGD), focusing on integrating AI\ntechniques to support design interpretation and enhance the creative process.\nWe categorize the field into two primary directions: perception tasks, which\ninvolve understanding and analyzing design elements, and generation tasks,\nwhich focus on creating new design elements and layouts. The survey covers\nvarious subtasks, including visual element perception and generation, aesthetic\nand semantic understanding, layout analysis, and generation. We highlight the\nrole of large language models and multimodal approaches in bridging the gap\nbetween localized visual features and global design intent. Despite significant\nprogress, challenges remain to understanding human intent, ensuring\ninterpretability, and maintaining control over multilayered compositions. This\nsurvey serves as a guide for researchers, providing information on the current\nstate of AIGD and potential future\ndirections\\footnote{https:\/\/github.com\/zhangtianer521\/excellent\\_Intelligent\\_graphic\\_design}.\n","date":"2025-03-24"}
{"id":"2503.18642","title":"Rethinking Glaucoma Calibration: Voting-Based Binocular and Metadata\n  Integration","abstract":"  Glaucoma is an incurable ophthalmic disease that damages the optic nerve,\nleads to vision loss, and ranks among the leading causes of blindness\nworldwide. Diagnosing glaucoma typically involves fundus photography, optical\ncoherence tomography (OCT), and visual field testing. However, the high cost of\nOCT often leads to reliance on fundus photography and visual field testing,\nboth of which exhibit inherent inter-observer variability. This stems from\nglaucoma being a multifaceted disease that influenced by various factors. As a\nresult, glaucoma diagnosis is highly subjective, emphasizing the necessity of\ncalibration, which aligns predicted probabilities with actual disease\nlikelihood. Proper calibration is essential to prevent overdiagnosis or\nmisdiagnosis, which are critical concerns for high-risk diseases. Although AI\nhas significantly improved diagnostic accuracy, overconfidence in models have\nworsen calibration performance. Recent study has begun focusing on calibration\nfor glaucoma. Nevertheless, previous study has not fully considered glaucoma's\nsystemic nature and the high subjectivity in its diagnostic process. To\novercome these limitations, we propose V-ViT (Voting-based ViT), a novel\nframework that enhances calibration by incorporating disease-specific\ncharacteristics. V-ViT integrates binocular data and metadata, reflecting the\nmulti-faceted nature of glaucoma diagnosis. Additionally, we introduce a MC\ndropout-based Voting System to address high subjectivity. Our approach achieves\nstate-of-the-art performance across all metrics, including accuracy,\ndemonstrating that our proposed methods are effective in addressing calibration\nissues. We validate our method using a custom dataset including binocular data.\n","date":"2025-03-24"}
{"id":"2503.18646","title":"ZeroLM: Data-Free Transformer Architecture Search for Language Models","abstract":"  Neural architecture search (NAS) provides a systematic framework for\nautomating the design of neural network architectures, yet its widespread\nadoption is hindered by prohibitive computational requirements. Existing\nzero-cost proxy methods, while reducing search overhead, demonstrate inadequate\nperformance in architecture ranking tasks, particularly for Transformer-based\nmodels where they often underperform simple parameter counting metrics. Current\nautomated proxy discovery approaches suffer from extended search times,\nsusceptibility to data overfitting, and structural complexity. This paper\nintroduces a novel zero-cost proxy methodology that quantifies model capacity\nthrough efficient weight statistics computation while decomposing Transformer\narchitectures into functionally distinct sub-modules, thereby optimizing the\nbalance of their contributions to overall performance. Our comprehensive\nevaluation demonstrates the superiority of this approach, achieving a\nSpearman's rho of 0.76 and Kendall's tau of 0.53 on the FlexiBERT benchmark.\nThe proposed method exhibits exceptional computational efficiency while\nmaintaining robust performance across diverse NAS benchmark tasks, offering a\npractical solution for large-scale architecture search.\n","date":"2025-03-24"}
{"id":"2503.18652","title":"Robust face recognition based on the wing loss and the $\\ell_1$\n  regularization","abstract":"  In recent years, sparse sampling techniques based on regression analysis have\nwitnessed extensive applications in face recognition research. Presently,\nnumerous sparse sampling models based on regression analysis have been explored\nby various researchers. Nevertheless, the recognition rates of the majority of\nthese models would be significantly decreased when confronted with highly\noccluded and highly damaged face images. In this paper, a new wing-constrained\nsparse coding model(WCSC) and its weighted version(WWCSC) are introduced, so as\nto deal with the face recognition problem in complex circumstances, where the\nalternating direction method of multipliers (ADMM) algorithm is employed to\nsolve the corresponding minimization problems. In addition, performances of the\nproposed method are examined based on the four well-known facial databases,\nnamely the ORL facial database, the Yale facial database, the AR facial\ndatabase and the FERET facial database. Also, compared to the other methods in\nthe literatures, the WWCSC has a very high recognition rate even in complex\nsituations where face images have high occlusion or high damage, which\nillustrates the robustness of the WWCSC method in facial recognition.\n","date":"2025-03-24"}
{"id":"2503.18658","title":"Leveraging Land Cover Priors for Isoprene Emission Super-Resolution","abstract":"  Remote sensing plays a crucial role in monitoring Earth's ecosystems, yet\nsatellite-derived data often suffer from limited spatial resolution,\nrestricting their applicability in atmospheric modeling and climate research.\nIn this work, we propose a deep learning-based Super-Resolution (SR) framework\nthat leverages land cover information to enhance the spatial accuracy of\nBiogenic Volatile Organic Compounds (BVOCs) emissions, with a particular focus\non isoprene. Our approach integrates land cover priors as emission drivers,\ncapturing spatial patterns more effectively than traditional methods. We\nevaluate the model's performance across various climate conditions and analyze\nstatistical correlations between isoprene emissions and key environmental\ninformation such as cropland and tree cover data. Additionally, we assess the\ngeneralization capabilities of our SR model by applying it to unseen climate\nzones and geographical regions. Experimental results demonstrate that\nincorporating land cover data significantly improves emission SR accuracy,\nparticularly in heterogeneous landscapes. This study contributes to atmospheric\nchemistry and climate modeling by providing a cost-effective, data-driven\napproach to refining BVOC emission maps. The proposed method enhances the\nusability of satellite-based emissions data, supporting applications in air\nquality forecasting, climate impact assessments, and environmental studies.\n","date":"2025-03-24"}
{"id":"2503.18665","title":"Boosting Virtual Agent Learning and Reasoning: A Step-wise,\n  Multi-dimensional, and Generalist Reward Model with Benchmark","abstract":"  The development of Generalist Virtual Agents (GVAs) powered by Multimodal\nLarge Language Models (MLLMs) has shown significant promise in autonomous task\nexecution. However, current training paradigms face critical limitations,\nincluding reliance on outcome supervision and labor-intensive human\nannotations. To address these challenges, we propose Similar, a Step-wise\nMulti-dimensional Generalist Reward Model, which offers fine-grained signals\nfor agent training and can choose better action for inference-time scaling.\nSpecifically, we begin by systematically defining five dimensions for\nevaluating agent actions. Building on this framework, we design an MCTS-P\nalgorithm to automatically collect and annotate step-wise, five-dimensional\nagent execution data. Using this data, we train Similar with the Triple-M\nstrategy. Furthermore, we introduce the first benchmark in the virtual agent\ndomain for step-wise, multi-dimensional reward model training and evaluation,\nnamed SRM. This benchmark consists of two components: SRMTrain, which serves as\nthe training set for Similar, and SRMEval, a manually selected test set for\nevaluating the reward model. Experimental results demonstrate that Similar,\nthrough its step-wise, multi-dimensional assessment and synergistic gain,\nprovides GVAs with effective intermediate signals during both training and\ninference-time scaling. The code is available at\nhttps:\/\/github.com\/Galery23\/Similar-v1.\n","date":"2025-03-24"}
{"id":"2503.18666","title":"AgentSpec: Customizable Runtime Enforcement for Safe and Reliable LLM\n  Agents","abstract":"  Agents built on LLMs are increasingly deployed across diverse domains,\nautomating complex decision-making and task execution. However, their autonomy\nintroduces safety risks, including security vulnerabilities, legal violations,\nand unintended harmful actions. Existing mitigation methods, such as\nmodel-based safeguards and early enforcement strategies, fall short in\nrobustness, interpretability, and adaptability. To address these challenges, we\npropose AgentSpec, a lightweight domain-specific language for specifying and\nenforcing runtime constraints on LLM agents. With AgentSpec, users define\nstructured rules that incorporate triggers, predicates, and enforcement\nmechanisms, ensuring agents operate within predefined safety boundaries. We\nimplement AgentSpec across multiple domains, including code execution, embodied\nagents, and autonomous driving, demonstrating its adaptability and\neffectiveness. Our evaluation shows that AgentSpec successfully prevents unsafe\nexecutions in over 90% of code agent cases, eliminates all hazardous actions in\nembodied agent tasks, and enforces 100% compliance by autonomous vehicles\n(AVs). Despite its strong safety guarantees, AgentSpec remains computationally\nlightweight, with overheads in milliseconds. By combining interpretability,\nmodularity, and efficiency, AgentSpec provides a practical and scalable\nsolution for enforcing LLM agent safety across diverse applications. We also\nautomate the generation of rules using LLMs and assess their effectiveness. Our\nevaluation shows that the rules generated by OpenAI o1 achieve a precision of\n95.56% and recall of 70.96% for embodied agents, successfully identifying\n87.26% of the risky code, and prevent AVs from breaking laws in 5 out of 8\nscenarios.\n","date":"2025-03-24"}
{"id":"2503.18668","title":"Geometric Preference Elicitation for Minimax Regret Optimization in\n  Uncertainty Matroids","abstract":"  This paper presents an efficient preference elicitation framework for\nuncertain matroid optimization, where precise weight information is\nunavailable, but insights into possible weight values are accessible. The core\ninnovation of our approach lies in its ability to systematically elicit user\npreferences, aligning the optimization process more closely with\ndecision-makers' objectives. By incrementally querying preferences between\npairs of elements, we iteratively refine the parametric uncertainty regions,\nleveraging the structural properties of matroids. Our method aims to achieve\nthe exact optimum by reducing regret with a few elicitation rounds.\nAdditionally, our approach avoids the computation of Minimax Regret and the use\nof Linear programming solvers at every iteration, unlike previous methods.\nExperimental results on four standard matroids demonstrate that our method\nreaches optimality more quickly and with fewer preference queries than existing\ntechniques.\n","date":"2025-03-24"}
{"id":"2503.18670","title":"Deep learning-based identification of precipitation clouds from all-sky\n  camera data for observatory safety","abstract":"  For monitoring the night sky conditions, wide-angle all-sky cameras are used\nin most astronomical observatories to monitor the sky cloudiness. In this\nmanuscript, we apply a deep-learning approach for automating the identification\nof precipitation clouds in all-sky camera data as a cloud warning system. We\nconstruct our original training and test sets using the all-sky camera image\narchive of the Iranian National Observatory (INO). The training and test set\nimages are labeled manually based on their potential rainfall and their\ndistribution in the sky. We train our model on a set of roughly 2445 images\ntaken by the INO all-sky camera through the deep learning method based on the\nEfficientNet network. Our model reaches an average accuracy of 99\\% in\ndetermining the cloud rainfall's potential and an accuracy of 96\\% for cloud\ncoverage. To enable a comprehensive comparison and evaluate the performance of\nalternative architectures for the task, we additionally trained three models\nLeNet, DeiT, and AlexNet. This approach can be used for early warning of\nincoming dangerous clouds toward telescopes and harnesses the power of deep\nlearning to automatically analyze vast amounts of all-sky camera data and\naccurately identify precipitation clouds formations. Our trained model can be\ndeployed for real-time analysis, enabling the rapid identification of potential\nthreats, and offering a scaleable solution that can improve our ability to\nsafeguard telescopes and instruments in observatories. This is important now\nthat numerous small and medium-sized telescopes are increasingly integrated\nwith smart control systems to reduce manual operation.\n","date":"2025-03-24"}
{"id":"2503.18671","title":"Structure-Aware Correspondence Learning for Relative Pose Estimation","abstract":"  Relative pose estimation provides a promising way for achieving\nobject-agnostic pose estimation. Despite the success of existing 3D\ncorrespondence-based methods, the reliance on explicit feature matching suffers\nfrom small overlaps in visible regions and unreliable feature estimation for\ninvisible regions. Inspired by humans' ability to assemble two object parts\nthat have small or no overlapping regions by considering object structure, we\npropose a novel Structure-Aware Correspondence Learning method for Relative\nPose Estimation, which consists of two key modules. First, a structure-aware\nkeypoint extraction module is designed to locate a set of kepoints that can\nrepresent the structure of objects with different shapes and appearance, under\nthe guidance of a keypoint based image reconstruction loss. Second, a\nstructure-aware correspondence estimation module is designed to model the\nintra-image and inter-image relationships between keypoints to extract\nstructure-aware features for correspondence estimation. By jointly leveraging\nthese two modules, the proposed method can naturally estimate 3D-3D\ncorrespondences for unseen objects without explicit feature matching for\nprecise relative pose estimation. Experimental results on the CO3D, Objaverse\nand LineMOD datasets demonstrate that the proposed method significantly\noutperforms prior methods, i.e., with 5.7{\\deg}reduction in mean angular error\non the CO3D dataset.\n","date":"2025-03-24"}
{"id":"2503.18672","title":"Feature Calibration enhanced Parameter Synthesis for CLIP-based\n  Class-incremental Learning","abstract":"  Class-incremental Learning (CIL) enables models to continuously learn new\nclass knowledge while memorizing previous classes, facilitating their\nadaptation and evolution in dynamic environments. Traditional CIL methods are\nmainly based on visual features, which limits their ability to handle complex\nscenarios. In contrast, Vision-Language Models (VLMs) show promising potential\nto promote CIL by integrating pretrained knowledge with textual features.\nHowever, previous methods make it difficult to overcome catastrophic forgetting\nwhile preserving the generalization capabilities of VLMs. To tackle these\nchallenges, we propose Feature Calibration enhanced Parameter Synthesis (FCPS)\nin this paper. Specifically, our FCPS employs a specific parameter adjustment\nmechanism to iteratively refine the proportion of original visual features\nparticipating in the final class determination, ensuring the model's\nfoundational generalization capabilities. Meanwhile, parameter integration\nacross different tasks achieves a balance between learning new class knowledge\nand retaining old knowledge. Experimental results on popular benchmarks (e.g.,\nCIFAR100 and ImageNet100) validate the superiority of the proposed method.\n","date":"2025-03-24"}
{"id":"2503.18673","title":"Any6D: Model-free 6D Pose Estimation of Novel Objects","abstract":"  We introduce Any6D, a model-free framework for 6D object pose estimation that\nrequires only a single RGB-D anchor image to estimate both the 6D pose and size\nof unknown objects in novel scenes. Unlike existing methods that rely on\ntextured 3D models or multiple viewpoints, Any6D leverages a joint object\nalignment process to enhance 2D-3D alignment and metric scale estimation for\nimproved pose accuracy. Our approach integrates a render-and-compare strategy\nto generate and refine pose hypotheses, enabling robust performance in\nscenarios with occlusions, non-overlapping views, diverse lighting conditions,\nand large cross-environment variations. We evaluate our method on five\nchallenging datasets: REAL275, Toyota-Light, HO3D, YCBINEOAT, and LM-O,\ndemonstrating its effectiveness in significantly outperforming state-of-the-art\nmethods for novel object pose estimation. Project page:\nhttps:\/\/taeyeop.com\/any6d\n","date":"2025-03-24"}
{"id":"2503.18674","title":"Human Motion Unlearning","abstract":"  We introduce the task of human motion unlearning to prevent the synthesis of\ntoxic animations while preserving the general text-to-motion generative\nperformance. Unlearning toxic motions is challenging as those can be generated\nfrom explicit text prompts and from implicit toxic combinations of safe motions\n(e.g., ``kicking\" is ``loading and swinging a leg\"). We propose the first\nmotion unlearning benchmark by filtering toxic motions from the large and\nrecent text-to-motion datasets of HumanML3D and Motion-X. We propose baselines,\nby adapting state-of-the-art image unlearning techniques to process\nspatio-temporal signals. Finally, we propose a novel motion unlearning model\nbased on Latent Code Replacement, which we dub LCR. LCR is training-free and\nsuitable to the discrete latent spaces of state-of-the-art text-to-motion\ndiffusion models. LCR is simple and consistently outperforms baselines\nqualitatively and quantitatively. Project page:\n\\href{https:\/\/www.pinlab.org\/hmu}{https:\/\/www.pinlab.org\/hmu}.\n","date":"2025-03-24"}
{"id":"2503.18676","title":"Feature Qualification by Deep Nets: A Constructive Approach","abstract":"  The great success of deep learning has stimulated avid research activities in\nverifying the power of depth in theory, a common consensus of which is that\ndeep net are versatile in approximating and learning numerous functions. Such a\nversatility certainly enhances the understanding of the power of depth, but\nmakes it difficult to judge which data features are crucial in a specific\nlearning task. This paper proposes a constructive approach to equip deep nets\nfor the feature qualification purpose. Using the product-gate nature and\nlocalized approximation property of deep nets with sigmoid activation (deep\nsigmoid nets), we succeed in constructing a linear deep net operator that\npossesses optimal approximation performance in approximating smooth and radial\nfunctions. Furthermore, we provide theoretical evidences that the constructed\ndeep net operator is capable of qualifying multiple features such as the\nsmoothness and radialness of the target functions.\n","date":"2025-03-24"}
{"id":"2503.18678","title":"NullSwap: Proactive Identity Cloaking Against Deepfake Face Swapping","abstract":"  Suffering from performance bottlenecks in passively detecting high-quality\nDeepfake images due to the advancement of generative models, proactive\nperturbations offer a promising approach to disabling Deepfake manipulations by\ninserting signals into benign images. However, existing proactive perturbation\napproaches remain unsatisfactory in several aspects: 1) visual degradation due\nto direct element-wise addition; 2) limited effectiveness against face swapping\nmanipulation; 3) unavoidable reliance on white- and grey-box settings to\ninvolve generative models during training. In this study, we analyze the\nessence of Deepfake face swapping and argue the necessity of protecting source\nidentities rather than target images, and we propose NullSwap, a novel\nproactive defense approach that cloaks source image identities and nullifies\nface swapping under a pure black-box scenario. We design an Identity Extraction\nmodule to obtain facial identity features from the source image, while a\nPerturbation Block is then devised to generate identity-guided perturbations\naccordingly. Meanwhile, a Feature Block extracts shallow-level image features,\nwhich are then fused with the perturbation in the Cloaking Block for image\nreconstruction. Furthermore, to ensure adaptability across different identity\nextractors in face swapping algorithms, we propose Dynamic Loss Weighting to\nadaptively balance identity losses. Experiments demonstrate the outstanding\nability of our approach to fool various identity recognition models,\noutperforming state-of-the-art proactive perturbations in preventing face\nswapping models from generating images with correct source identities.\n","date":"2025-03-24"}
{"id":"2503.18680","title":"ArchSeek: Retrieving Architectural Case Studies Using Vision-Language\n  Models","abstract":"  Efficiently searching for relevant case studies is critical in architectural\ndesign, as designers rely on precedent examples to guide or inspire their\nongoing projects. However, traditional text-based search tools struggle to\ncapture the inherently visual and complex nature of architectural knowledge,\noften leading to time-consuming and imprecise exploration. This paper\nintroduces ArchSeek, an innovative case study search system with recommendation\ncapability, tailored for architecture design professionals. Powered by the\nvisual understanding capabilities from vision-language models and cross-modal\nembeddings, it enables text and image queries with fine-grained control, and\ninteraction-based design case recommendations. It offers architects a more\nefficient, personalized way to discover design inspirations, with potential\napplications across other visually driven design fields. The source code is\navailable at https:\/\/github.com\/danruili\/ArchSeek.\n","date":"2025-03-24"}
{"id":"2503.18681","title":"Commander-GPT: Fully Unleashing the Sarcasm Detection Capability of\n  Multi-Modal Large Language Models","abstract":"  Sarcasm detection, as a crucial research direction in the field of Natural\nLanguage Processing (NLP), has attracted widespread attention. Traditional\nsarcasm detection tasks have typically focused on single-modal approaches\n(e.g., text), but due to the implicit and subtle nature of sarcasm, such\nmethods often fail to yield satisfactory results. In recent years, researchers\nhave shifted the focus of sarcasm detection to multi-modal approaches. However,\neffectively leveraging multi-modal information to accurately identify sarcastic\ncontent remains a challenge that warrants further exploration. Leveraging the\npowerful integrated processing capabilities of Multi-Modal Large Language\nModels (MLLMs) for various information sources, we propose an innovative\nmulti-modal Commander-GPT framework. Inspired by military strategy, we first\ndecompose the sarcasm detection task into six distinct sub-tasks. A central\ncommander (decision-maker) then assigns the best-suited large language model to\naddress each specific sub-task. Ultimately, the detection results from each\nmodel are aggregated to identify sarcasm. We conducted extensive experiments on\nMMSD and MMSD 2.0, utilizing four multi-modal large language models and six\nprompting strategies. Our experiments demonstrate that our approach achieves\nstate-of-the-art performance, with a 19.3% improvement in F1 score, without\nnecessitating fine-tuning or ground-truth rationales.\n","date":"2025-03-24"}
{"id":"2503.18682","title":"Hardware-Rasterized Ray-Based Gaussian Splatting","abstract":"  We present a novel, hardware rasterized rendering approach for ray-based 3D\nGaussian Splatting (RayGS), obtaining both fast and high-quality results for\nnovel view synthesis. Our work contains a mathematically rigorous and\ngeometrically intuitive derivation about how to efficiently estimate all\nrelevant quantities for rendering RayGS models, structured with respect to\nstandard hardware rasterization shaders. Our solution is the first enabling\nrendering RayGS models at sufficiently high frame rates to support\nquality-sensitive applications like Virtual and Mixed Reality. Our second\ncontribution enables alias-free rendering for RayGS, by addressing MIP-related\nissues arising when rendering diverging scales during training and testing. We\ndemonstrate significant performance gains, across different benchmark scenes,\nwhile retaining state-of-the-art appearance quality of RayGS.\n","date":"2025-03-24"}
{"id":"2503.18684","title":"Efficient Continual Adaptation of Pretrained Robotic Policy with Online\n  Meta-Learned Adapters","abstract":"  Continual adaptation is essential for general autonomous agents. For example,\na household robot pretrained with a repertoire of skills must still adapt to\nunseen tasks specific to each household. Motivated by this, building upon\nparameter-efficient fine-tuning in language models, prior works have explored\nlightweight adapters to adapt pretrained policies, which can preserve learned\nfeatures from the pretraining phase and demonstrate good adaptation\nperformances. However, these approaches treat task learning separately,\nlimiting knowledge transfer between tasks. In this paper, we propose Online\nMeta-Learned adapters (OMLA). Instead of applying adapters directly, OMLA can\nfacilitate knowledge transfer from previously learned tasks to current learning\ntasks through a novel meta-learning objective. Extensive experiments in both\nsimulated and real-world environments demonstrate that OMLA can lead to better\nadaptation performances compared to the baseline methods. The project link:\nhttps:\/\/ricky-zhu.github.io\/OMLA\/.\n","date":"2025-03-24"}
{"id":"2503.18693","title":"TARDIS: Mitigating Temporal Misalignment via Representation Steering","abstract":"  Language models often struggle with temporal misalignment, performance\ndegradation caused by shifts in the temporal distribution of data. Continuously\nupdating models to avoid degradation is expensive. Can models be adapted\nwithout updating model weights? We present TARDIS, an unsupervised\nrepresentation editing method that addresses this challenge. TARDIS extracts\nsteering vectors from unlabeled data and adjusts the model's representations to\nbetter align with the target time period's distribution. Our experiments reveal\nthat TARDIS enhances downstream task performance without the need for\nfine-tuning, can mitigate temporal misalignment even when exact target time\nperiod data is unavailable, and remains efficient even when the temporal\ninformation of the target data points is unknown at inference time.\n","date":"2025-03-24"}
{"id":"2503.18695","title":"OCRT: Boosting Foundation Models in the Open World with\n  Object-Concept-Relation Triad","abstract":"  Although foundation models (FMs) claim to be powerful, their generalization\nability significantly decreases when faced with distribution shifts, weak\nsupervision, or malicious attacks in the open world. On the other hand, most\ndomain generalization or adversarial fine-tuning methods are task-related or\nmodel-specific, ignoring the universality in practical applications and the\ntransferability between FMs. This paper delves into the problem of generalizing\nFMs to the out-of-domain data. We propose a novel framework, the\nObject-Concept-Relation Triad (OCRT), that enables FMs to extract sparse,\nhigh-level concepts and intricate relational structures from raw visual inputs.\nThe key idea is to bind objects in visual scenes and a set of object-centric\nrepresentations through unsupervised decoupling and iterative refinement. To be\nspecific, we project the object-centric representations onto a semantic concept\nspace that the model can readily interpret and estimate their importance to\nfilter out irrelevant elements. Then, a concept-based graph, which has a\nflexible degree, is constructed to incorporate the set of concepts and their\ncorresponding importance, enabling the extraction of high-order factors from\ninformative concepts and facilitating relational reasoning among these\nconcepts. Extensive experiments demonstrate that OCRT can substantially boost\nthe generalizability and robustness of SAM and CLIP across multiple downstream\ntasks.\n","date":"2025-03-24"}
{"id":"2503.18702","title":"Unsupervised Acquisition of Discrete Grammatical Categories","abstract":"  This article presents experiments performed using a computational laboratory\nenvironment for language acquisition experiments. It implements a multi-agent\nsystem consisting of two agents: an adult language model and a daughter\nlanguage model that aims to learn the mother language. Crucially, the daughter\nagent does not have access to the internal knowledge of the mother language\nmodel but only to the language exemplars the mother agent generates. These\nexperiments illustrate how this system can be used to acquire abstract\ngrammatical knowledge. We demonstrate how statistical analyses of patterns in\nthe input data corresponding to grammatical categories yield discrete\ngrammatical rules. These rules are subsequently added to the grammatical\nknowledge of the daughter language model. To this end, hierarchical\nagglomerative cluster analysis was applied to the utterances consecutively\ngenerated by the mother language model. It is argued that this procedure can be\nused to acquire structures resembling grammatical categories proposed by\nlinguists for natural languages. Thus, it is established that non-trivial\ngrammatical knowledge has been acquired. Moreover, the parameter configuration\nof this computational laboratory environment determined using training data\ngenerated by the mother language model is validated in a second experiment with\na test set similarly resulting in the acquisition of non-trivial categories.\n","date":"2025-03-24"}
{"id":"2503.18703","title":"Channel Consistency Prior and Self-Reconstruction Strategy Based\n  Unsupervised Image Deraining","abstract":"  Recently, deep image deraining models based on paired datasets have made a\nseries of remarkable progress. However, they cannot be well applied in\nreal-world applications due to the difficulty of obtaining real paired datasets\nand the poor generalization performance. In this paper, we propose a novel\nChannel Consistency Prior and Self-Reconstruction Strategy Based Unsupervised\nImage Deraining framework, CSUD, to tackle the aforementioned challenges.\nDuring training with unpaired data, CSUD is capable of generating high-quality\npseudo clean and rainy image pairs which are used to enhance the performance of\nderaining network. Specifically, to preserve more image background details\nwhile transferring rain streaks from rainy images to the unpaired clean images,\nwe propose a novel Channel Consistency Loss (CCLoss) by introducing the Channel\nConsistency Prior (CCP) of rain streaks into training process, thereby ensuring\nthat the generated pseudo rainy images closely resemble the real ones.\nFurthermore, we propose a novel Self-Reconstruction (SR) strategy to alleviate\nthe redundant information transfer problem of the generator, further improving\nthe deraining performance and the generalization capability of our method.\nExtensive experiments on multiple synthetic and real-world datasets demonstrate\nthat the deraining performance of CSUD surpasses other state-of-the-art\nunsupervised methods and CSUD exhibits superior generalization capability.\n","date":"2025-03-24"}
{"id":"2503.18705","title":"Benchmarking Burst Super-Resolution for Polarization Images: Noise\n  Dataset and Analysis","abstract":"  Snapshot polarization imaging calculates polarization states from linearly\npolarized subimages. To achieve this, a polarization camera employs a double\nBayer-patterned sensor to capture both color and polarization. It demonstrates\nlow light efficiency and low spatial resolution, resulting in increased noise\nand compromised polarization measurements. Although burst super-resolution\neffectively reduces noise and enhances spatial resolution, applying it to\npolarization imaging poses challenges due to the lack of tailored datasets and\nreliable ground truth noise statistics. To address these issues, we introduce\nPolarNS and PolarBurstSR, two innovative datasets developed specifically for\npolarization imaging. PolarNS provides characterization of polarization noise\nstatistics, facilitating thorough analysis, while PolarBurstSR functions as a\nbenchmark for burst super-resolution in polarization images. These datasets,\ncollected under various real-world conditions, enable comprehensive evaluation.\nAdditionally, we present a model for analyzing polarization noise to quantify\nnoise propagation, tested on a large dataset captured in a darkroom\nenvironment. As part of our application, we compare the latest burst\nsuper-resolution models, highlighting the advantages of training tailored to\npolarization compared to RGB-based methods. This work establishes a benchmark\nfor polarization burst super-resolution and offers critical insights into noise\npropagation, thereby enhancing polarization image reconstruction.\n","date":"2025-03-24"}
{"id":"2503.18706","title":"Energy-Efficient Dynamic Training and Inference for GNN-Based Network\n  Modeling","abstract":"  Efficient network modeling is essential for resource optimization and network\nplanning in next-generation large-scale complex networks. Traditional\napproaches, such as queuing theory-based modeling and packet-based simulators,\ncan be inefficient due to the assumption made and the computational expense,\nrespectively. To address these challenges, we propose an innovative\nenergy-efficient dynamic orchestration of Graph Neural Networks (GNN) based\nmodel training and inference framework for context-aware network modeling and\npredictions. We have developed a low-complexity solution framework, QAG, that\nis a Quantum approximation optimization (QAO) algorithm for Adaptive\norchestration of GNN-based network modeling. We leverage the tripartite graph\nmodel to represent a multi-application system with many compute nodes.\nThereafter, we apply the constrained graph-cutting using QAO to find the\nfeasible energy-efficient configurations of the GNN-based model and deploying\nthem on the available compute nodes to meet the network modeling application\nrequirements. The proposed QAG scheme closely matches the optimum and offers\natleast a 50% energy saving while meeting the application requirements with 60%\nlower churn-rate.\n","date":"2025-03-24"}
{"id":"2503.18709","title":"Revisiting Automatic Data Curation for Vision Foundation Models in\n  Digital Pathology","abstract":"  Vision foundation models (FMs) are accelerating the development of digital\npathology algorithms and transforming biomedical research. These models learn,\nin a self-supervised manner, to represent histological features in highly\nheterogeneous tiles extracted from whole-slide images (WSIs) of real-world\npatient samples. The performance of these FMs is significantly influenced by\nthe size, diversity, and balance of the pre-training data. However, data\nselection has been primarily guided by expert knowledge at the WSI level,\nfocusing on factors such as disease classification and tissue types, while\nlargely overlooking the granular details available at the tile level. In this\npaper, we investigate the potential of unsupervised automatic data curation at\nthe tile-level, taking into account 350 million tiles. Specifically, we apply\nhierarchical clustering trees to pre-extracted tile embeddings, allowing us to\nsample balanced datasets uniformly across the embedding space of the pretrained\nFM. We further identify these datasets are subject to a trade-off between size\nand balance, potentially compromising the quality of representations learned by\nFMs, and propose tailored batch sampling strategies to mitigate this effect. We\ndemonstrate the effectiveness of our method through improved performance on a\ndiverse range of clinically relevant downstream tasks.\n","date":"2025-03-24"}
{"id":"2503.18711","title":"Accenture-NVS1: A Novel View Synthesis Dataset","abstract":"  This paper introduces ACC-NVS1, a specialized dataset designed for research\non Novel View Synthesis specifically for airborne and ground imagery. Data for\nACC-NVS1 was collected in Austin, TX and Pittsburgh, PA in 2023 and 2024. The\ncollection encompasses six diverse real-world scenes captured from both\nairborne and ground cameras, resulting in a total of 148,000 images. ACC-NVS1\naddresses challenges such as varying altitudes and transient objects. This\ndataset is intended to supplement existing datasets, providing additional\nresources for comprehensive research, rather than serving as a benchmark.\n","date":"2025-03-24"}
{"id":"2503.18712","title":"LLaVAction: evaluating and training multi-modal large language models\n  for action recognition","abstract":"  Understanding human behavior requires measuring behavioral actions. Due to\nits complexity, behavior is best mapped onto a rich, semantic structure such as\nlanguage. The recent development of multi-modal large language models (MLLMs)\nis a promising candidate for a wide range of action understanding tasks. In\nthis work, we focus on evaluating and then improving MLLMs to perform action\nrecognition. We reformulate EPIC-KITCHENS-100, one of the largest and most\nchallenging egocentric action datasets, to the form of video multiple question\nanswering (EPIC-KITCHENS-100-MQA). We show that when we sample difficult\nincorrect answers as distractors, leading MLLMs struggle to recognize the\ncorrect actions. We propose a series of methods that greatly improve the MLLMs'\nability to perform action recognition, achieving state-of-the-art on both the\nEPIC-KITCHENS-100 validation set, as well as outperforming GPT-4o by 21 points\nin accuracy on EPIC-KITCHENS-100-MQA. Lastly, we show improvements on other\naction-related video benchmarks such as EgoSchema, PerceptionTest,\nLongVideoBench, VideoMME and MVBench, suggesting that MLLMs are a promising\npath forward for complex action tasks. Code and models are available at:\nhttps:\/\/github.com\/AdaptiveMotorControlLab\/LLaVAction.\n","date":"2025-03-24"}
{"id":"2503.18718","title":"GS-Marker: Generalizable and Robust Watermarking for 3D Gaussian\n  Splatting","abstract":"  In the Generative AI era, safeguarding 3D models has become increasingly\nurgent. While invisible watermarking is well-established for 2D images with\nencoder-decoder frameworks, generalizable and robust solutions for 3D remain\nelusive. The main difficulty arises from the renderer between the 3D encoder\nand 2D decoder, which disrupts direct gradient flow and complicates training.\nExisting 3D methods typically rely on per-scene iterative optimization,\nresulting in time inefficiency and limited generalization. In this work, we\npropose a single-pass watermarking approach for 3D Gaussian Splatting (3DGS), a\nwell-known yet underexplored representation for watermarking. We identify two\nmajor challenges: (1) ensuring effective training generalized across diverse 3D\nmodels, and (2) reliably extracting watermarks from free-view renderings, even\nunder distortions. Our framework, named GS-Marker, incorporates a 3D encoder to\nembed messages, distortion layers to enhance resilience against various\ndistortions, and a 2D decoder to extract watermarks from renderings. A key\ninnovation is the Adaptive Marker Control mechanism that adaptively perturbs\nthe initially optimized 3DGS, escaping local minima and improving both training\nstability and convergence. Extensive experiments show that GS-Marker\noutperforms per-scene training approaches in terms of decoding accuracy and\nmodel fidelity, while also significantly reducing computation time.\n","date":"2025-03-24"}
{"id":"2503.18719","title":"Boosting Resolution Generalization of Diffusion Transformers with\n  Randomized Positional Encodings","abstract":"  Resolution generalization in image generation tasks enables the production of\nhigher-resolution images with lower training resolution overhead. However, a\nsignificant challenge in resolution generalization, particularly in the widely\nused Diffusion Transformers, lies in the mismatch between the positional\nencodings encountered during testing and those used during training. While\nexisting methods have employed techniques such as interpolation, extrapolation,\nor their combinations, none have fully resolved this issue. In this paper, we\npropose a novel two-dimensional randomized positional encodings (RPE-2D)\nframework that focuses on learning positional order of image patches instead of\nthe specific distances between them, enabling seamless high- and low-resolution\nimage generation without requiring high- and low-resolution image training.\nSpecifically, RPE-2D independently selects positions over a broader range along\nboth the horizontal and vertical axes, ensuring that all position encodings are\ntrained during the inference phase, thus improving resolution generalization.\nAdditionally, we propose a random data augmentation technique to enhance the\nmodeling of position order. To address the issue of image cropping caused by\nthe augmentation, we introduce corresponding micro-conditioning to enable the\nmodel to perceive the specific cropping patterns. On the ImageNet dataset, our\nproposed RPE-2D achieves state-of-the-art resolution generalization\nperformance, outperforming existing competitive methods when trained at a\nresolution of $256 \\times 256$ and inferred at $384 \\times 384$ and $512 \\times\n512$, as well as when scaling from $512 \\times 512$ to $768 \\times 768$ and\n$1024 \\times 1024$. And it also exhibits outstanding capabilities in\nlow-resolution image generation, multi-stage training acceleration and\nmulti-resolution inheritance.\n","date":"2025-03-24"}
{"id":"2503.18721","title":"Differentially Private Joint Independence Test","abstract":"  Identification of joint dependence among more than two random vectors plays\nan important role in many statistical applications, where the data may contain\nsensitive or confidential information. In this paper, we consider the the\nd-variable Hilbert-Schmidt independence criterion (dHSIC) in the context of\ndifferential privacy. Given the limiting distribution of the empirical estimate\nof dHSIC is complicated Gaussian chaos, constructing tests in the non-privacy\nregime is typically based on permutation and bootstrap. To detect joint\ndependence in privacy, we propose a dHSIC-based testing procedure by employing\na differentially private permutation methodology. Our method enjoys privacy\nguarantee, valid level and pointwise consistency, while the bootstrap\ncounterpart suffers inconsistent power. We further investigate the uniform\npower of the proposed test in dHSIC metric and $L_2$ metric, indicating that\nthe proposed test attains the minimax optimal power across different privacy\nregimes. As a byproduct, our results also contain the pointwise and uniform\npower of the non-private permutation dHSIC, addressing an unsolved question\nremained in Pfister et al. (2018).\n","date":"2025-03-24"}
{"id":"2503.18725","title":"FG$^2$: Fine-Grained Cross-View Localization by Fine-Grained Feature\n  Matching","abstract":"  We propose a novel fine-grained cross-view localization method that estimates\nthe 3 Degrees of Freedom pose of a ground-level image in an aerial image of the\nsurroundings by matching fine-grained features between the two images. The pose\nis estimated by aligning a point plane generated from the ground image with a\npoint plane sampled from the aerial image. To generate the ground points, we\nfirst map ground image features to a 3D point cloud. Our method then learns to\nselect features along the height dimension to pool the 3D points to a\nBird's-Eye-View (BEV) plane. This selection enables us to trace which feature\nin the ground image contributes to the BEV representation. Next, we sample a\nset of sparse matches from computed point correspondences between the two point\nplanes and compute their relative pose using Procrustes alignment. Compared to\nthe previous state-of-the-art, our method reduces the mean localization error\nby 28% on the VIGOR cross-area test set. Qualitative results show that our\nmethod learns semantically consistent matches across ground and aerial views\nthrough weakly supervised learning from the camera pose.\n","date":"2025-03-24"}
{"id":"2503.18730","title":"Predicting the Road Ahead: A Knowledge Graph based Foundation Model for\n  Scene Understanding in Autonomous Driving","abstract":"  The autonomous driving field has seen remarkable advancements in various\ntopics, such as object recognition, trajectory prediction, and motion planning.\nHowever, current approaches face limitations in effectively comprehending the\ncomplex evolutions of driving scenes over time. This paper proposes FM4SU, a\nnovel methodology for training a symbolic foundation model (FM) for scene\nunderstanding in autonomous driving. It leverages knowledge graphs (KGs) to\ncapture sensory observation along with domain knowledge such as road topology,\ntraffic rules, or complex interactions between traffic participants. A bird's\neye view (BEV) symbolic representation is extracted from the KG for each\ndriving scene, including the spatio-temporal information among the objects\nacross the scenes. The BEV representation is serialized into a sequence of\ntokens and given to pre-trained language models (PLMs) for learning an inherent\nunderstanding of the co-occurrence among driving scene elements and generating\npredictions on the next scenes. We conducted a number of experiments using the\nnuScenes dataset and KG in various scenarios. The results demonstrate that\nfine-tuned models achieve significantly higher accuracy in all tasks. The\nfine-tuned T5 model achieved a next scene prediction accuracy of 86.7%. This\npaper concludes that FM4SU offers a promising foundation for developing more\ncomprehensive models for scene understanding in autonomous driving.\n","date":"2025-03-24"}
{"id":"2503.18731","title":"Thermalizer: Stable autoregressive neural emulation of spatiotemporal\n  chaos","abstract":"  Autoregressive surrogate models (or \\textit{emulators}) of spatiotemporal\nsystems provide an avenue for fast, approximate predictions, with broad\napplications across science and engineering. At inference time, however, these\nmodels are generally unable to provide predictions over long time rollouts due\nto accumulation of errors leading to diverging trajectories. In essence,\nemulators operate out of distribution, and controlling the online distribution\nquickly becomes intractable in large-scale settings. To address this\nfundamental issue, and focusing on time-stationary systems admitting an\ninvariant measure, we leverage diffusion models to obtain an implicit estimator\nof the score of this invariant measure. We show that this model of the score\nfunction can be used to stabilize autoregressive emulator rollouts by applying\non-the-fly denoising during inference, a process we call\n\\textit{thermalization}. Thermalizing an emulator rollout is shown to extend\nthe time horizon of stable predictions by an order of magnitude in complex\nsystems exhibiting turbulent and chaotic behavior, opening up a novel\napplication of diffusion models in the context of neural emulation.\n","date":"2025-03-24"}
{"id":"2503.18742","title":"SFDLA: Source-Free Document Layout Analysis","abstract":"  Document Layout Analysis (DLA) is a fundamental task in document\nunderstanding. However, existing DLA and adaptation methods often require\naccess to large-scale source data and target labels. This requirements severely\nlimiting their real-world applicability, particularly in privacy-sensitive and\nresource-constrained domains, such as financial statements, medical records,\nand proprietary business documents. According to our observation, directly\ntransferring source-domain fine-tuned models on target domains often results in\na significant performance drop (Avg. -32.64%). In this work, we introduce\nSource-Free Document Layout Analysis (SFDLA), aiming for adapting a pre-trained\nsource DLA models to an unlabeled target domain, without access to any source\ndata. To address this challenge, we establish the first SFDLA benchmark,\ncovering three major DLA datasets for geometric- and content-aware adaptation.\nFurthermore, we propose Document Layout Analysis Adapter (DLAdapter), a novel\nframework that is designed to improve source-free adaptation across document\ndomains. Our method achieves a +4.21% improvement over the source-only baseline\nand a +2.26% gain over existing source-free methods from PubLayNet to\nDocLayNet. We believe this work will inspire the DLA community to further\ninvestigate source-free document understanding. To support future research of\nthe community, the benchmark, models, and code will be publicly available at\nhttps:\/\/github.com\/s3setewe\/sfdla-DLAdapter.\n","date":"2025-03-24"}
{"id":"2503.18746","title":"Linguistics-aware Masked Image Modeling for Self-supervised Scene Text\n  Recognition","abstract":"  Text images are unique in their dual nature, encompassing both visual and\nlinguistic information. The visual component encompasses structural and\nappearance-based features, while the linguistic dimension incorporates\ncontextual and semantic elements. In scenarios with degraded visual quality,\nlinguistic patterns serve as crucial supplements for comprehension,\nhighlighting the necessity of integrating both aspects for robust scene text\nrecognition (STR). Contemporary STR approaches often use language models or\nsemantic reasoning modules to capture linguistic features, typically requiring\nlarge-scale annotated datasets. Self-supervised learning, which lacks\nannotations, presents challenges in disentangling linguistic features related\nto the global context. Typically, sequence contrastive learning emphasizes the\nalignment of local features, while masked image modeling (MIM) tends to exploit\nlocal structures to reconstruct visual patterns, resulting in limited\nlinguistic knowledge. In this paper, we propose a Linguistics-aware Masked\nImage Modeling (LMIM) approach, which channels the linguistic information into\nthe decoding process of MIM through a separate branch. Specifically, we design\na linguistics alignment module to extract vision-independent features as\nlinguistic guidance using inputs with different visual appearances. As features\nextend beyond mere visual structures, LMIM must consider the global context to\nachieve reconstruction. Extensive experiments on various benchmarks\nquantitatively demonstrate our state-of-the-art performance, and attention\nvisualizations qualitatively show the simultaneous capture of both visual and\nlinguistic information.\n","date":"2025-03-24"}
{"id":"2503.18748","title":"Simulation-Driven Balancing of Competitive Game Levels with\n  Reinforcement Learning","abstract":"  The balancing process for game levels in competitive two-player contexts\ninvolves a lot of manual work and testing, particularly for non-symmetrical\ngame levels. In this work, we frame game balancing as a procedural content\ngeneration task and propose an architecture for automatically balancing of\ntile-based levels within the PCGRL framework (procedural content generation via\nreinforcement learning). Our architecture is divided into three parts: (1) a\nlevel generator, (2) a balancing agent, and (3) a reward modeling simulation.\nThrough repeated simulations, the balancing agent receives rewards for\nadjusting the level towards a given balancing objective, such as equal win\nrates for all players. To this end, we propose new swap-based representations\nto improve the robustness of playability, thereby enabling agents to balance\ngame levels more effectively and quickly compared to traditional PCGRL. By\nanalyzing the agent's swapping behavior, we can infer which tile types have the\nmost impact on the balance. We validate our approach in the Neural MMO (NMMO)\nenvironment in a competitive two-player scenario. In this extended conference\npaper, we present improved results, explore the applicability of the method to\nvarious forms of balancing beyond equal balancing, compare the performance to\nanother search-based approach, and discuss the application of existing fairness\nmetrics to game balancing.\n","date":"2025-03-24"}
{"id":"2503.18751","title":"Construction Identification and Disambiguation Using BERT: A Case Study\n  of NPN","abstract":"  Construction Grammar hypothesizes that knowledge of a language consists\nchiefly of knowledge of form-meaning pairs (''constructions'') that include\nvocabulary, general grammar rules, and even idiosyncratic patterns. Recent work\nhas shown that transformer language models represent at least some\nconstructional patterns, including ones where the construction is rare overall.\nIn this work, we probe BERT's representation of the form and meaning of a minor\nconstruction of English, the NPN (noun-preposition-noun) construction --\nexhibited in such expressions as face to face and day to day -- which is known\nto be polysemous. We construct a benchmark dataset of semantically annotated\ncorpus instances (including distractors that superficially resemble the\nconstruction). With this dataset, we train and evaluate probing classifiers.\nThey achieve decent discrimination of the construction from distractors, as\nwell as sense disambiguation among true instances of the construction,\nrevealing that BERT embeddings carry indications of the construction's\nsemantics. Moreover, artificially permuting the word order of true construction\ninstances causes them to be rejected, indicating sensitivity to matters of\nform. We conclude that BERT does latently encode at least some knowledge of the\nNPN construction going beyond a surface syntactic pattern and lexical cues.\n","date":"2025-03-24"}
{"id":"2503.18752","title":"Robust Tube-based Control Strategy for Vision-guided Autonomous Vehicles","abstract":"  A robust control strategy for autonomous vehicles can improve system\nstability, enhance riding comfort, and prevent driving accidents. This paper\npresents a novel interpolation tube-based constrained iterative linear\nquadratic regulator (itube-CILQR) algorithm for autonomous\ncomputer-vision-based vehicle lane-keeping. The goal of the algorithm is to\nenhance robustness during high-speed cornering on tight turns. The advantages\nof itube-CILQR over the standard tube-approach include reduced system\nconservatism and increased computational speed. Numerical and vision-based\nexperiments were conducted to examine the feasibility of the proposed\nalgorithm. The proposed itube-CILQR algorithm is better suited to vehicle\nlane-keeping than variational CILQR-based methods and model predictive control\n(MPC) approaches using a classical interior-point solver. Specifically, in\nevaluation experiments, itube-CILQR achieved an average runtime of 3.16 ms to\ngenerate a control signal to guide a self-driving vehicle; itube-MPC typically\nrequired a 4.67-times longer computation time to complete the same task.\nMoreover, the influence of conservatism on system behavior was investigated by\nexploring the interpolation variable trajectories derived from the proposed\nitube-CILQR algorithm during lane-keeping maneuvers.\n","date":"2025-03-24"}
{"id":"2503.18753","title":"Self-Supervised Learning based on Transformed Image Reconstruction for\n  Equivariance-Coherent Feature Representation","abstract":"  The equivariant behaviour of features is essential in many computer vision\ntasks, yet popular self-supervised learning (SSL) methods tend to constrain\nequivariance by design. We propose a self-supervised learning approach where\nthe system learns transformations independently by reconstructing images that\nhave undergone previously unseen transformations. Specifically, the model is\ntasked to reconstruct intermediate transformed images, e.g. translated or\nrotated images, without prior knowledge of these transformations. This\nauxiliary task encourages the model to develop equivariance-coherent features\nwithout relying on predefined transformation rules. To this end, we apply\ntransformations to the input image, generating an image pair, and then split\nthe extracted features into two sets per image. One set is used with a usual\nSSL loss encouraging invariance, the other with our loss based on the auxiliary\ntask to reconstruct the intermediate transformed images. Our loss and the SSL\nloss are linearly combined with weighted terms. Evaluating on synthetic tasks\nwith natural images, our proposed method strongly outperforms all competitors,\nregardless of whether they are designed to learn equivariance. Furthermore,\nwhen trained alongside augmentation-based methods as the invariance tasks, such\nas iBOT or DINOv2, we successfully learn a balanced combination of invariant\nand equivariant features. Our approach performs strong on a rich set of\nrealistic computer vision downstream tasks, almost always improving over all\nbaselines.\n","date":"2025-03-24"}
{"id":"2503.18754","title":"Dynamically Learning to Integrate in Recurrent Neural Networks","abstract":"  Learning to remember over long timescales is fundamentally challenging for\nrecurrent neural networks (RNNs). While much prior work has explored why RNNs\nstruggle to learn long timescales and how to mitigate this, we still lack a\nclear understanding of the dynamics involved when RNNs learn long timescales\nvia gradient descent. Here we build a mathematical theory of the learning\ndynamics of linear RNNs trained to integrate white noise. We show that when the\ninitial recurrent weights are small, the dynamics of learning are described by\na low-dimensional system that tracks a single outlier eigenvalue of the\nrecurrent weights. This reveals the precise manner in which the long timescale\nassociated with white noise integration is learned. We extend our analyses to\nRNNs learning a damped oscillatory filter, and find rich dynamical equations\nfor the evolution of a conjugate pair of outlier eigenvalues. Taken together,\nour analyses build a rich mathematical framework for studying dynamical\nlearning problems salient for both machine learning and neuroscience.\n","date":"2025-03-24"}
{"id":"2503.18755","title":"EgoSurgery-HTS: A Dataset for Egocentric Hand-Tool Segmentation in Open\n  Surgery Videos","abstract":"  Egocentric open-surgery videos capture rich, fine-grained details essential\nfor accurately modeling surgical procedures and human behavior in the operating\nroom. A detailed, pixel-level understanding of hands and surgical tools is\ncrucial for interpreting a surgeon's actions and intentions. We introduce\nEgoSurgery-HTS, a new dataset with pixel-wise annotations and a benchmark suite\nfor segmenting surgical tools, hands, and interacting tools in egocentric\nopen-surgery videos. Specifically, we provide a labeled dataset for (1) tool\ninstance segmentation of 14 distinct surgical tools, (2) hand instance\nsegmentation, and (3) hand-tool segmentation to label hands and the tools they\nmanipulate. Using EgoSurgery-HTS, we conduct extensive evaluations of\nstate-of-the-art segmentation methods and demonstrate significant improvements\nin the accuracy of hand and hand-tool segmentation in egocentric open-surgery\nvideos compared to existing datasets. The dataset will be released at\nhttps:\/\/github.com\/Fujiry0\/EgoSurgery.\n","date":"2025-03-24"}
{"id":"2503.18756","title":"Local Interference: Removing Interference Bias in Semi-Parametric Causal\n  Models","abstract":"  Interference bias is a major impediment to identifying causal effects in\nreal-world settings. For example, vaccination reduces the transmission of a\nvirus in a population such that everyone benefits -- even those who are not\ntreated. This is a source of bias that must be accounted for if one wants to\nlearn the true effect of a vaccine on an individual's immune system. Previous\napproaches addressing interference bias require strong domain knowledge in the\nform of a graphical interaction network fully describing interference between\nunits. Moreover, they place additional constraints on the form the interference\ncan take, such as restricting to linear outcome models, and assuming that\ninterference experienced by a unit does not depend on the unit's covariates.\nOur work addresses these shortcomings. We first provide and justify a novel\ndefinition of causal models with local interference. We prove that the True\nAverage Causal Effect, a measure of causality where interference has been\nremoved, can be identified in certain semi-parametric models satisfying this\ndefinition. These models allow for non-linearity, and also for interference to\ndepend on a unit's covariates. An analytic estimand for the True Average Causal\nEffect is given in such settings. We further prove that the True Average Causal\nEffect cannot be identified in arbitrary models with local interference,\nshowing that identification requires semi-parametric assumptions. Finally, we\nprovide an empirical validation of our method on both simulated and real-world\ndatasets.\n","date":"2025-03-24"}
{"id":"2503.18758","title":"On the Optimality of Single-label and Multi-label Neural Network\n  Decoders","abstract":"  We investigate the design of two neural network (NN) architectures recently\nproposed as decoders for forward error correction: the so-called single-label\nNN (SLNN) and multi-label NN (MLNN) decoders. These decoders have been reported\nto achieve near-optimal codeword- and bit-wise performance, respectively.\nResults in the literature show near-optimality for a variety of short codes. In\nthis paper, we analytically prove that certain SLNN and MLNN architectures can,\nin fact, always realize optimal decoding, regardless of the code. These optimal\narchitectures and their binary weights are shown to be defined by the codebook,\ni.e., no training or network optimization is required. Our proposed\narchitectures are in fact not NNs, but a different way of implementing the\nmaximum likelihood decoding rule. Optimal performance is numerically\ndemonstrated for Hamming $(7,4)$, Polar $(16,8)$, and BCH $(31,21)$ codes. The\nresults show that our optimal architectures are less complex than the SLNN and\nMLNN architectures proposed in the literature, which in fact only achieve\nnear-optimal performance. Extension to longer codes is still hindered by the\ncurse of dimensionality. Therefore, even though SLNN and MLNN can perform\nmaximum likelihood decoding, such architectures cannot be used for medium and\nlong codes.\n","date":"2025-03-24"}
{"id":"2503.18760","title":"Synthetic Function Demonstrations Improve Generation in Low-Resource\n  Programming Languages","abstract":"  A key consideration when training an LLM is whether the target language is\nmore or less resourced, whether this is English compared to Welsh, or Python\ncompared to Excel. Typical training data for programming languages consist of\nreal program demonstrations coupled with human-written comments. Here we\npresent novel approaches to the creation of such data for low resource\nprogramming languages. We generate fully-synthetic, textbook-quality\ndemonstrations of common library functions in an example domain of Excel\nformulas, using a teacher model. We then finetune an underperforming student\nmodel, and show improvement on 2 question-answering datasets recast into the\nExcel domain. We show advantages of finetuning over standard, off-the-shelf RAG\napproaches, which can offer only modest improvement due to the unfamiliar\ntarget domain.\n","date":"2025-03-24"}
{"id":"2503.18762","title":"Mechanistic Interpretability of Fine-Tuned Vision Transformers on\n  Distorted Images: Decoding Attention Head Behavior for Transparent and\n  Trustworthy AI","abstract":"  Mechanistic interpretability improves the safety, reliability, and robustness\nof large AI models. This study examined individual attention heads in vision\ntransformers (ViTs) fine tuned on distorted 2D spectrogram images containing\nnon relevant content (axis labels, titles, color bars). By introducing\nextraneous features, the study analyzed how transformer components processed\nunrelated information, using mechanistic interpretability to debug issues and\nreveal insights into transformer architectures. Attention maps assessed head\ncontributions across layers. Heads in early layers (1 to 3) showed minimal task\nimpact with ablation increased MSE loss slightly ({\\mu}=0.11%, {\\sigma}=0.09%),\nindicating focus on less critical low level features. In contrast, deeper heads\n(e.g., layer 6) caused a threefold higher loss increase ({\\mu}=0.34%,\n{\\sigma}=0.02%), demonstrating greater task importance. Intermediate layers (6\nto 11) exhibited monosemantic behavior, attending exclusively to chirp regions.\nSome early heads (1 to 4) were monosemantic but non task relevant (e.g. text\ndetectors, edge or corner detectors). Attention maps distinguished monosemantic\nheads (precise chirp localization) from polysemantic heads (multiple irrelevant\nregions). These findings revealed functional specialization in ViTs, showing\nhow heads processed relevant vs. extraneous information. By decomposing\ntransformers into interpretable components, this work enhanced model\nunderstanding, identified vulnerabilities, and advanced safer, more transparent\nAI.\n","date":"2025-03-24"}
{"id":"2503.18767","title":"Good Keypoints for the Two-View Geometry Estimation Problem","abstract":"  Local features are essential to many modern downstream applications.\nTherefore, it is of interest to determine the properties of local features that\ncontribute to the downstream performance for a better design of feature\ndetectors and descriptors. In our work, we propose a new theoretical model for\nscoring feature points (keypoints) in the context of the two-view geometry\nestimation problem. The model determines two properties that a good keypoint\nfor solving the homography estimation problem should have: be repeatable and\nhave a small expected measurement error. This result provides key insights into\nwhy maximizing the number of correspondences doesn't always lead to better\nhomography estimation accuracy. We use the developed model to design a method\nthat detects keypoints that benefit the homography estimation introducing the\nBounded NeSS-ST (BoNeSS-ST) keypoint detector. The novelty of BoNeSS-ST comes\nfrom strong theoretical foundations, a more accurate keypoint scoring due to\nsubpixel refinement and a cost designed for superior robustness to low saliency\nkeypoints. As a result, BoNeSS-ST outperforms prior self-supervised local\nfeature detectors in both planar homography and epipolar geometry estimation\nproblems.\n","date":"2025-03-24"}
{"id":"2503.18769","title":"AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and\n  Symbolic Reasoning","abstract":"  This paper presents AlphaSpace, a novel methodology designed to enhance the\nspatial reasoning capabilities of language models for robotic manipulation in\n3D Cartesian space. AlphaSpace employs a hierarchical semantics-based\ntokenization strategy that encodes spatial information at both coarse and\nfine-grained levels. Our approach represents objects with their attributes,\npositions, and height information through structured tokens, enabling precise\nspatial reasoning without relying on traditional vision-based embeddings. This\napproach enables LLMs to accurately manipulate objects by positioning them at\nspecific (x, y, z) coordinates. Experimental results suggest that AlphaSpace\ndemonstrates promising potential for improving manipulation tasks, achieving a\ntotal accuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude\n3.5 Sonnet. These results demonstrate the potential of structured spatial\nencoding for manipulation tasks and warrant further exploration.\n","date":"2025-03-24"}
{"id":"2503.18773","title":"BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with\n  Low-Bit KV Cache","abstract":"  The growing adoption of long-context Large Language Models (LLMs) has\nintroduced significant memory and computational challenges in autoregressive\ndecoding due to the expanding Key-Value (KV) cache. KV cache quantization has\nemerged as a promising solution, with prior work showing that 4-bit or even\n2-bit quantization can maintain model accuracy while reducing memory costs.\nHowever, despite these benefits, preliminary implementations for the low-bit KV\ncache struggle to deliver the expected speedup due to quantization and\ndequantization overheads and the lack of Tensor Cores utilization. In this\nwork, we propose BitDecoding, a GPU-optimized framework that unlocks Tensor\nCores for efficient decoding with low-bit KV cache. Efficiently leveraging\nTensor Cores for low-bit KV cache is challenging due to the dynamic nature of\nKV cache generation at each decoding step. BitDecoding addresses these\nchallenges with a Tensor Cores-Centric BitFusion Scheme that ensures data\nlayout compatibility to enable high utilization of Tensor Cores. Additionally,\nBitDecoding incorporates a warp-efficient parallel decoding kernel and a\nfine-grained asynchronous pipeline, minimizing dequantization overhead and\nimproving computational efficiency. Experiments show that BitDecoding achieves\nup to 7.5x speedup on RTX 4090, 4.8x on A100, and 8.9x on H100, compared to\nFP16 FlashDecoding-v2. It also outperforms the state-of-the-art low-bit KV\ncache implementation (QServe) by up to 4.3x. On LLaMA-3.1-8B with a 128K\nsequence length, BitDecoding reduces single-batch decoding latency by 3x,\ndemonstrating its effectiveness in long-context generation scenarios. The code\nis available at https:\/\/github.com\/DD-DuDa\/BitDecoding.\n","date":"2025-03-24"}
{"id":"2503.18778","title":"The case for delegated AI autonomy for Human AI teaming in healthcare","abstract":"  In this paper we propose an advanced approach to integrating artificial\nintelligence (AI) into healthcare: autonomous decision support. This approach\nallows the AI algorithm to act autonomously for a subset of patient cases\nwhilst serving a supportive role in other subsets of patient cases based on\ndefined delegation criteria. By leveraging the complementary strengths of both\nhumans and AI, it aims to deliver greater overall performance than existing\nhuman-AI teaming models. It ensures safe handling of patient cases and\npotentially reduces clinician review time, whilst being mindful of AI tool\nlimitations. After setting the approach within the context of current human-AI\nteaming models, we outline the delegation criteria and apply them to a specific\nAI-based tool used in histopathology. The potential impact of the approach and\nthe regulatory requirements for its successful implementation are then\ndiscussed.\n","date":"2025-03-24"}
{"id":"2503.18783","title":"Frequency Dynamic Convolution for Dense Image Prediction","abstract":"  While Dynamic Convolution (DY-Conv) has shown promising performance by\nenabling adaptive weight selection through multiple parallel weights combined\nwith an attention mechanism, the frequency response of these weights tends to\nexhibit high similarity, resulting in high parameter costs but limited\nadaptability. In this work, we introduce Frequency Dynamic Convolution\n(FDConv), a novel approach that mitigates these limitations by learning a fixed\nparameter budget in the Fourier domain. FDConv divides this budget into\nfrequency-based groups with disjoint Fourier indices, enabling the construction\nof frequency-diverse weights without increasing the parameter cost. To further\nenhance adaptability, we propose Kernel Spatial Modulation (KSM) and Frequency\nBand Modulation (FBM). KSM dynamically adjusts the frequency response of each\nfilter at the spatial level, while FBM decomposes weights into distinct\nfrequency bands in the frequency domain and modulates them dynamically based on\nlocal content. Extensive experiments on object detection, segmentation, and\nclassification validate the effectiveness of FDConv. We demonstrate that when\napplied to ResNet-50, FDConv achieves superior performance with a modest\nincrease of +3.6M parameters, outperforming previous methods that require\nsubstantial increases in parameter budgets (e.g., CondConv +90M, KW +76.5M).\nMoreover, FDConv seamlessly integrates into a variety of architectures,\nincluding ConvNeXt, Swin-Transformer, offering a flexible and efficient\nsolution for modern vision tasks. The code is made publicly available at\nhttps:\/\/github.com\/Linwei-Chen\/FDConv.\n","date":"2025-03-24"}
{"id":"2503.18784","title":"Leveraging Perturbation Robustness to Enhance Out-of-Distribution\n  Detection","abstract":"  Out-of-distribution (OOD) detection is the task of identifying inputs that\ndeviate from the training data distribution. This capability is essential for\nsafely deploying deep computer vision models in open-world environments. In\nthis work, we propose a post-hoc method, Perturbation-Rectified OOD detection\n(PRO), based on the insight that prediction confidence for OOD inputs is more\nsusceptible to reduction under perturbation than in-distribution (IND) inputs.\nBased on the observation, we propose an adversarial score function that\nsearches for the local minimum scores near the original inputs by applying\ngradient descent. This procedure enhances the separability between IND and OOD\nsamples. Importantly, the approach improves OOD detection performance without\ncomplex modifications to the underlying model architectures. We conduct\nextensive experiments using the OpenOOD benchmark~\\cite{yang2022openood}. Our\napproach further pushes the limit of softmax-based OOD detection and is the\nleading post-hoc method for small-scale models. On a CIFAR-10 model with\nadversarial training, PRO effectively detects near-OOD inputs, achieving a\nreduction of more than 10\\% on FPR@95 compared to state-of-the-art methods.\n","date":"2025-03-24"}
{"id":"2503.18785","title":"LGI-DETR: Local-Global Interaction for UAV Object Detection","abstract":"  UAV has been widely used in various fields. However, most of the existing\nobject detectors used in drones are not end-to-end and require the design of\nvarious complex components and careful fine-tuning. Most of the existing\nend-to-end object detectors are designed for natural scenes. It is not ideal to\napply them directly to UAV images. In order to solve the above challenges, we\ndesign an local-global information interaction DETR for UAVs, namely LGI-DETR.\nCross-layer bidirectional low-level and high-level feature information\nenhancement, this fusion method is effective especially in the field of small\nobjection detection. At the initial stage of encoder, we propose a local\nspatial enhancement module (LSE), which enhances the low-level rich local\nspatial information into the high-level feature, and reduces the loss of local\ninformation in the transmission process of high-level information. At the final\nstage of the encoder, we propose a novel global information injection module\n(GII) designed to integrate rich high-level global semantic representations\nwith low-level feature maps. This hierarchical fusion mechanism effectively\naddresses the inherent limitations of local receptive fields by propagating\ncontextual information across the feature hierarchy. Experimental results on\ntwo challenging UAV image object detection benchmarks, VisDrone2019 and UAVDT,\nshow that our proposed model outperforms the SOTA model. Compared to the\nbaseline model, AP and AP50 improved by 1.9% and 2.4%, respectively.\n","date":"2025-03-24"}
{"id":"2503.18787","title":"Sample-Efficient Reinforcement Learning of Koopman eNMPC","abstract":"  Reinforcement learning (RL) can be used to tune data-driven (economic)\nnonlinear model predictive controllers ((e)NMPCs) for optimal performance in a\nspecific control task by optimizing the dynamic model or parameters in the\npolicy's objective function or constraints, such as state bounds. However, the\nsample efficiency of RL is crucial, and to improve it, we combine a model-based\nRL algorithm with our published method that turns Koopman (e)NMPCs into\nautomatically differentiable policies. We apply our approach to an eNMPC case\nstudy of a continuous stirred-tank reactor (CSTR) model from the literature.\nThe approach outperforms benchmark methods, i.e., data-driven eNMPCs using\nmodels based on system identification without further RL tuning of the\nresulting policy, and neural network controllers trained with model-based RL,\nby achieving superior control performance and higher sample efficiency.\nFurthermore, utilizing partial prior knowledge about the system dynamics via\nphysics-informed learning further increases sample efficiency.\n","date":"2025-03-24"}
{"id":"2503.18792","title":"REALM: A Dataset of Real-World LLM Use Cases","abstract":"  Large Language Models, such as the GPT series, have driven significant\nindustrial applications, leading to economic and societal transformations.\nHowever, a comprehensive understanding of their real-world applications remains\nlimited. To address this, we introduce REALM, a dataset of over 94,000 LLM use\ncases collected from Reddit and news articles. REALM captures two key\ndimensions: the diverse applications of LLMs and the demographics of their\nusers. It categorizes LLM applications and explores how users' occupations\nrelate to the types of applications they use. By integrating real-world data,\nREALM offers insights into LLM adoption across different domains, providing a\nfoundation for future research on their evolving societal roles. A dedicated\ndashboard https:\/\/realm-e7682.web.app\/ presents the data.\n","date":"2025-03-24"}
{"id":"2503.18794","title":"NexusGS: Sparse View Synthesis with Epipolar Depth Priors in 3D Gaussian\n  Splatting","abstract":"  Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) have noticeably\nadvanced photo-realistic novel view synthesis using images from densely spaced\ncamera viewpoints. However, these methods struggle in few-shot scenarios due to\nlimited supervision. In this paper, we present NexusGS, a 3DGS-based approach\nthat enhances novel view synthesis from sparse-view images by directly\nembedding depth information into point clouds, without relying on complex\nmanual regularizations. Exploiting the inherent epipolar geometry of 3DGS, our\nmethod introduces a novel point cloud densification strategy that initializes\n3DGS with a dense point cloud, reducing randomness in point placement while\npreventing over-smoothing and overfitting. Specifically, NexusGS comprises\nthree key steps: Epipolar Depth Nexus, Flow-Resilient Depth Blending, and\nFlow-Filtered Depth Pruning. These steps leverage optical flow and camera poses\nto compute accurate depth maps, while mitigating the inaccuracies often\nassociated with optical flow. By incorporating epipolar depth priors, NexusGS\nensures reliable dense point cloud coverage and supports stable 3DGS training\nunder sparse-view conditions. Experiments demonstrate that NexusGS\nsignificantly enhances depth accuracy and rendering quality, surpassing\nstate-of-the-art methods by a considerable margin. Furthermore, we validate the\nsuperiority of our generated point clouds by substantially boosting the\nperformance of competing methods. Project page:\nhttps:\/\/usmizuki.github.io\/NexusGS\/.\n","date":"2025-03-24"}
{"id":"2503.18803","title":"Change3D: Revisiting Change Detection and Captioning from A Video\n  Modeling Perspective","abstract":"  In this paper, we present Change3D, a framework that reconceptualizes the\nchange detection and captioning tasks through video modeling. Recent methods\nhave achieved remarkable success by regarding each pair of bi-temporal images\nas separate frames. They employ a shared-weight image encoder to extract\nspatial features and then use a change extractor to capture differences between\nthe two images. However, image feature encoding, being a task-agnostic process,\ncannot attend to changed regions effectively. Furthermore, different change\nextractors designed for various change detection and captioning tasks make it\ndifficult to have a unified framework. To tackle these challenges, Change3D\nregards the bi-temporal images as comprising two frames akin to a tiny video.\nBy integrating learnable perception frames between the bi-temporal images, a\nvideo encoder enables the perception frames to interact with the images\ndirectly and perceive their differences. Therefore, we can get rid of the\nintricate change extractors, providing a unified framework for different change\ndetection and captioning tasks. We verify Change3D on multiple tasks,\nencompassing change detection (including binary change detection, semantic\nchange detection, and building damage assessment) and change captioning, across\neight standard benchmarks. Without bells and whistles, this simple yet\neffective framework can achieve superior performance with an ultra-light video\nmodel comprising only ~6%-13% of the parameters and ~8%-34% of the FLOPs\ncompared to state-of-the-art methods. We hope that Change3D could be an\nalternative to 2D-based models and facilitate future research.\n","date":"2025-03-24"}
{"id":"2503.18807","title":"Streaming Federated Learning with Markovian Data","abstract":"  Federated learning (FL) is now recognized as a key framework for\ncommunication-efficient collaborative learning. Most theoretical and empirical\nstudies, however, rely on the assumption that clients have access to\npre-collected data sets, with limited investigation into scenarios where\nclients continuously collect data. In many real-world applications,\nparticularly when data is generated by physical or biological processes, client\ndata streams are often modeled by non-stationary Markov processes. Unlike\nstandard i.i.d. sampling, the performance of FL with Markovian data streams\nremains poorly understood due to the statistical dependencies between client\nsamples over time. In this paper, we investigate whether FL can still support\ncollaborative learning with Markovian data streams. Specifically, we analyze\nthe performance of Minibatch SGD, Local SGD, and a variant of Local SGD with\nmomentum. We answer affirmatively under standard assumptions and smooth\nnon-convex client objectives: the sample complexity is proportional to the\ninverse of the number of clients with a communication complexity comparable to\nthe i.i.d. scenario. However, the sample complexity for Markovian data streams\nremains higher than for i.i.d. sampling.\n","date":"2025-03-24"}
{"id":"2503.18808","title":"CRCL: Causal Representation Consistency Learning for Anomaly Detection\n  in Surveillance Videos","abstract":"  Video Anomaly Detection (VAD) remains a fundamental yet formidable task in\nthe video understanding community, with promising applications in areas such as\ninformation forensics and public safety protection. Due to the rarity and\ndiversity of anomalies, existing methods only use easily collected regular\nevents to model the inherent normality of normal spatial-temporal patterns in\nan unsupervised manner. Previous studies have shown that existing unsupervised\nVAD models are incapable of label-independent data offsets (e.g., scene\nchanges) in real-world scenarios and may fail to respond to light anomalies due\nto the overgeneralization of deep neural networks. Inspired by causality\nlearning, we argue that there exist causal factors that can adequately\ngeneralize the prototypical patterns of regular events and present significant\ndeviations when anomalous instances occur. In this regard, we propose Causal\nRepresentation Consistency Learning (CRCL) to implicitly mine potential\nscene-robust causal variable in unsupervised video normality learning.\nSpecifically, building on the structural causal models, we propose\nscene-debiasing learning and causality-inspired normality learning to strip\naway entangled scene bias in deep representations and learn causal video\nnormality, respectively. Extensive experiments on benchmarks validate the\nsuperiority of our method over conventional deep representation learning.\nMoreover, ablation studies and extension validation show that the CRCL can cope\nwith label-independent biases in multi-scene settings and maintain stable\nperformance with only limited training data available.\n","date":"2025-03-24"}
{"id":"2503.18809","title":"Classical Planning with LLM-Generated Heuristics: Challenging the State\n  of the Art with Python Code","abstract":"  In recent years, large language models (LLMs) have shown remarkable\ncapabilities in various artificial intelligence problems. However, they fail to\nplan reliably, even when prompted with a detailed definition of the planning\ntask. Attempts to improve their planning capabilities, such as chain-of-thought\nprompting, fine-tuning, and explicit \"reasoning\" still yield incorrect plans\nand usually fail to generalize to larger tasks. In this paper, we show how to\nuse LLMs to generate correct plans, even for out-of-distribution tasks of\nincreasing size. For a given planning domain, we ask an LLM to generate several\ndomain-dependent heuristic functions in the form of Python code, evaluate them\non a set of training tasks within a greedy best-first search, and choose the\nstrongest one. The resulting LLM-generated heuristics solve many more unseen\ntest tasks than state-of-the-art domain-independent heuristics for classical\nplanning. They are even competitive with the strongest learning algorithm for\ndomain-dependent planning. These findings are especially remarkable given that\nour proof-of-concept implementation is based on an unoptimized Python planner\nand the baselines all build upon highly optimized C++ code. In some domains,\nthe LLM-generated heuristics expand fewer states than the baselines, revealing\nthat they are not only efficiently computable, but sometimes even more\ninformative than the state-of-the-art heuristics. Overall, our results show\nthat sampling a set of planning heuristic function programs can significantly\nimprove the planning capabilities of LLMs.\n","date":"2025-03-24"}
{"id":"2503.18812","title":"SKDU at De-Factify 4.0: Vision Transformer with Data Augmentation for\n  AI-Generated Image Detection","abstract":"  The aim of this work is to explore the potential of pre-trained\nvision-language models, e.g. Vision Transformers (ViT), enhanced with advanced\ndata augmentation strategies for the detection of AI-generated images. Our\napproach leverages a fine-tuned ViT model trained on the Defactify-4.0 dataset,\nwhich includes images generated by state-of-the-art models such as Stable\nDiffusion 2.1, Stable Diffusion XL, Stable Diffusion 3, DALL-E 3, and\nMidJourney. We employ perturbation techniques like flipping, rotation, Gaussian\nnoise injection, and JPEG compression during training to improve model\nrobustness and generalisation. The experimental results demonstrate that our\nViT-based pipeline achieves state-of-the-art performance, significantly\noutperforming competing methods on both validation and test datasets.\n","date":"2025-03-24"}
{"id":"2503.18813","title":"Defeating Prompt Injections by Design","abstract":"  Large Language Models (LLMs) are increasingly deployed in agentic systems\nthat interact with an external environment. However, LLM agents are vulnerable\nto prompt injection attacks when handling untrusted data. In this paper we\npropose CaMeL, a robust defense that creates a protective system layer around\nthe LLM, securing it even when underlying models may be susceptible to attacks.\nTo operate, CaMeL explicitly extracts the control and data flows from the\n(trusted) query; therefore, the untrusted data retrieved by the LLM can never\nimpact the program flow. To further improve security, CaMeL relies on a notion\nof a capability to prevent the exfiltration of private data over unauthorized\ndata flows. We demonstrate effectiveness of CaMeL by solving $67\\%$ of tasks\nwith provable security in AgentDojo [NeurIPS 2024], a recent agentic security\nbenchmark.\n","date":"2025-03-24"}
{"id":"2503.18814","title":"Towards Responsible AI Music: an Investigation of Trustworthy Features\n  for Creative Systems","abstract":"  Generative AI is radically changing the creative arts, by fundamentally\ntransforming the way we create and interact with cultural artefacts. While\noffering unprecedented opportunities for artistic expression and\ncommercialisation, this technology also raises ethical, societal, and legal\nconcerns. Key among these are the potential displacement of human creativity,\ncopyright infringement stemming from vast training datasets, and the lack of\ntransparency, explainability, and fairness mechanisms. As generative systems\nbecome pervasive in this domain, responsible design is crucial. Whilst previous\nwork has tackled isolated aspects of generative systems (e.g., transparency,\nevaluation, data), we take a comprehensive approach, grounding these efforts\nwithin the Ethics Guidelines for Trustworthy Artificial Intelligence produced\nby the High-Level Expert Group on AI appointed by the European Commission - a\nframework for designing responsible AI systems across seven macro requirements.\nFocusing on generative music AI, we illustrate how these requirements can be\ncontextualised for the field, addressing trustworthiness across multiple\ndimensions and integrating insights from the existing literature. We further\npropose a roadmap for operationalising these contextualised requirements,\nemphasising interdisciplinary collaboration and stakeholder engagement. Our\nwork provides a foundation for designing and evaluating responsible music\ngeneration systems, calling for collaboration among AI experts, ethicists,\nlegal scholars, and artists. This manuscript is accompanied by a website:\nhttps:\/\/amresearchlab.github.io\/raim-framework\/.\n","date":"2025-03-24"}
{"id":"2503.18816","title":"Learning Multi-Robot Coordination through Locality-Based Factorized\n  Multi-Agent Actor-Critic Algorithm","abstract":"  In this work, we present a novel cooperative multi-agent reinforcement\nlearning method called \\textbf{Loc}ality based \\textbf{Fac}torized\n\\textbf{M}ulti-Agent \\textbf{A}ctor-\\textbf{C}ritic (Loc-FACMAC). Existing\nstate-of-the-art algorithms, such as FACMAC, rely on global reward information,\nwhich may not accurately reflect the quality of individual robots' actions in\ndecentralized systems. We integrate the concept of locality into critic\nlearning, where strongly related robots form partitions during training. Robots\nwithin the same partition have a greater impact on each other, leading to more\nprecise policy evaluation. Additionally, we construct a dependency graph to\ncapture the relationships between robots, facilitating the partitioning\nprocess. This approach mitigates the curse of dimensionality and prevents\nrobots from using irrelevant information. Our method improves existing\nalgorithms by focusing on local rewards and leveraging partition-based learning\nto enhance training efficiency and performance. We evaluate the performance of\nLoc-FACMAC in three environments: Hallway, Multi-cartpole, and\nBounded-Cooperative-Navigation. We explore the impact of partition sizes on the\nperformance and compare the result with baseline MARL algorithms such as LOMAQ,\nFACMAC, and QMIX. The experiments reveal that, if the locality structure is\ndefined properly, Loc-FACMAC outperforms these baseline algorithms up to 108\\%,\nindicating that exploiting the locality structure in the actor-critic framework\nimproves the MARL performance.\n","date":"2025-03-24"}
{"id":"2503.18817","title":"Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal\n  Representations","abstract":"  Prior research on out-of-distribution detection (OoDD) has primarily focused\non single-modality models. Recently, with the advent of large-scale pretrained\nvision-language models such as CLIP, OoDD methods utilizing such multi-modal\nrepresentations through zero-shot and prompt learning strategies have emerged.\nHowever, these methods typically involve either freezing the pretrained weights\nor only partially tuning them, which can be suboptimal for downstream datasets.\nIn this paper, we highlight that multi-modal fine-tuning (MMFT) can achieve\nnotable OoDD performance. Despite some recent works demonstrating the impact of\nfine-tuning methods for OoDD, there remains significant potential for\nperformance improvement. We investigate the limitation of na\\\"ive fine-tuning\nmethods, examining why they fail to fully leverage the pretrained knowledge.\nOur empirical analysis suggests that this issue could stem from the modality\ngap within in-distribution (ID) embeddings. To address this, we propose a\ntraining objective that enhances cross-modal alignment by regularizing the\ndistances between image and text embeddings of ID data. This adjustment helps\nin better utilizing pretrained textual information by aligning similar\nsemantics from different modalities (i.e., text and image) more closely in the\nhyperspherical representation space. We theoretically demonstrate that the\nproposed regularization corresponds to the maximum likelihood estimation of an\nenergy-based model on a hypersphere. Utilizing ImageNet-1k OoD benchmark\ndatasets, we show that our method, combined with post-hoc OoDD approaches\nleveraging pretrained knowledge (e.g., NegLabel), significantly outperforms\nexisting methods, achieving state-of-the-art OoDD performance and leading ID\naccuracy.\n","date":"2025-03-24"}
{"id":"2503.18825","title":"EconEvals: Benchmarks and Litmus Tests for LLM Agents in Unknown\n  Environments","abstract":"  We develop benchmarks for LLM agents that act in, learn from, and strategize\nin unknown environments, the specifications of which the LLM agent must learn\nover time from deliberate exploration. Our benchmarks consist of\ndecision-making tasks derived from key problems in economics. To forestall\nsaturation, the benchmark tasks are synthetically generated with scalable\ndifficulty levels. Additionally, we propose litmus tests, a new kind of\nquantitative measure for LLMs and LLM agents. Unlike benchmarks, litmus tests\nquantify differences in character, values, and tendencies of LLMs and LLM\nagents, by considering their behavior when faced with tradeoffs (e.g.,\nefficiency versus equality) where there is no objectively right or wrong\nbehavior. Overall, our benchmarks and litmus tests assess the abilities and\ntendencies of LLM agents in tackling complex economic problems in diverse\nsettings spanning procurement, scheduling, task allocation, and pricing --\napplications that should grow in importance as such agents are further\nintegrated into the economy.\n","date":"2025-03-24"}
{"id":"2503.18826","title":"Interpretable and Fair Mechanisms for Abstaining Classifiers","abstract":"  Abstaining classifiers have the option to refrain from providing a prediction\nfor instances that are difficult to classify. The abstention mechanism is\ndesigned to trade off the classifier's performance on the accepted data while\nensuring a minimum number of predictions. In this setting, often fairness\nconcerns arise when the abstention mechanism solely reduces errors for the\nmajority groups of the data, resulting in increased performance differences\nacross demographic groups. While there exist a bunch of methods that aim to\nreduce discrimination when abstaining, there is no mechanism that can do so in\nan explainable way. In this paper, we fill this gap by introducing\nInterpretable and Fair Abstaining Classifier IFAC, an algorithm that can reject\npredictions both based on their uncertainty and their unfairness. By rejecting\npossibly unfair predictions, our method reduces error and positive decision\nrate differences across demographic groups of the non-rejected data. Since the\nunfairness-based rejections are based on an interpretable-by-design method,\ni.e., rule-based fairness checks and situation testing, we create a transparent\nprocess that can empower human decision-makers to review the unfair predictions\nand make more just decisions for them. This explainable aspect is especially\nimportant in light of recent AI regulations, mandating that any high-risk\ndecision task should be overseen by human experts to reduce discrimination\nrisks.\n","date":"2025-03-24"}
{"id":"2503.18830","title":"DAGait: Generalized Skeleton-Guided Data Alignment for Gait Recognition","abstract":"  Gait recognition is emerging as a promising and innovative area within the\nfield of computer vision, widely applied to remote person identification.\nAlthough existing gait recognition methods have achieved substantial success in\ncontrolled laboratory datasets, their performance often declines significantly\nwhen transitioning to wild datasets.We argue that the performance gap can be\nprimarily attributed to the spatio-temporal distribution inconsistencies\npresent in wild datasets, where subjects appear at varying angles, positions,\nand distances across the frames. To achieve accurate gait recognition in the\nwild, we propose a skeleton-guided silhouette alignment strategy, which uses\nprior knowledge of the skeletons to perform affine transformations on the\ncorresponding silhouettes.To the best of our knowledge, this is the first study\nto explore the impact of data alignment on gait recognition. We conducted\nextensive experiments across multiple datasets and network architectures, and\nthe results demonstrate the significant advantages of our proposed alignment\nstrategy.Specifically, on the challenging Gait3D dataset, our method achieved\nan average performance improvement of 7.9% across all evaluated networks.\nFurthermore, our method achieves substantial improvements on cross-domain\ndatasets, with accuracy improvements of up to 24.0%.\n","date":"2025-03-24"}
{"id":"2503.18836","title":"Dual-domain Multi-path Self-supervised Diffusion Model for Accelerated\n  MRI Reconstruction","abstract":"  Magnetic resonance imaging (MRI) is a vital diagnostic tool, but its\ninherently long acquisition times reduce clinical efficiency and patient\ncomfort. Recent advancements in deep learning, particularly diffusion models,\nhave improved accelerated MRI reconstruction. However, existing diffusion\nmodels' training often relies on fully sampled data, models incur high\ncomputational costs, and often lack uncertainty estimation, limiting their\nclinical applicability. To overcome these challenges, we propose a novel\nframework, called Dual-domain Multi-path Self-supervised Diffusion Model\n(DMSM), that integrates a self-supervised dual-domain diffusion model training\nscheme, a lightweight hybrid attention network for the reconstruction diffusion\nmodel, and a multi-path inference strategy, to enhance reconstruction accuracy,\nefficiency, and explainability. Unlike traditional diffusion-based models, DMSM\neliminates the dependency on training from fully sampled data, making it more\npractical for real-world clinical settings. We evaluated DMSM on two human MRI\ndatasets, demonstrating that it achieves favorable performance over several\nsupervised and self-supervised baselines, particularly in preserving fine\nanatomical structures and suppressing artifacts under high acceleration\nfactors. Additionally, our model generates uncertainty maps that correlate\nreasonably well with reconstruction errors, offering valuable clinically\ninterpretable guidance and potentially enhancing diagnostic confidence.\n","date":"2025-03-24"}
{"id":"2503.18840","title":"Learning to segment anatomy and lesions from disparately labeled sources\n  in brain MRI","abstract":"  Segmenting healthy tissue structures alongside lesions in brain Magnetic\nResonance Images (MRI) remains a challenge for today's algorithms due to\nlesion-caused disruption of the anatomy and lack of jointly labeled training\ndatasets, where both healthy tissues and lesions are labeled on the same\nimages. In this paper, we propose a method that is robust to lesion-caused\ndisruptions and can be trained from disparately labeled training sets, i.e.,\nwithout requiring jointly labeled samples, to automatically segment both. In\ncontrast to prior work, we decouple healthy tissue and lesion segmentation in\ntwo paths to leverage multi-sequence acquisitions and merge information with an\nattention mechanism. During inference, an image-specific adaptation reduces\nadverse influences of lesion regions on healthy tissue predictions. During\ntraining, the adaptation is taken into account through meta-learning and\nco-training is used to learn from disparately labeled training images. Our\nmodel shows an improved performance on several anatomical structures and\nlesions on a publicly available brain glioblastoma dataset compared to the\nstate-of-the-art segmentation methods.\n","date":"2025-03-24"}
{"id":"2503.18841","title":"Unsupervised Detection of Fraudulent Transactions in E-commerce Using\n  Contrastive Learning","abstract":"  With the rapid development of e-commerce, e-commerce platforms are facing an\nincreasing number of fraud threats. Effectively identifying and preventing\nthese fraudulent activities has become a critical research problem. Traditional\nfraud detection methods typically rely on supervised learning, which requires\nlarge amounts of labeled data. However, such data is often difficult to obtain,\nand the continuous evolution of fraudulent activities further reduces the\nadaptability and effectiveness of traditional methods. To address this issue,\nthis study proposes an unsupervised e-commerce fraud detection algorithm based\non SimCLR. The algorithm leverages the contrastive learning framework to\neffectively detect fraud by learning the underlying representations of\ntransaction data in an unlabeled setting. Experimental results on the eBay\nplatform dataset show that the proposed algorithm outperforms traditional\nunsupervised methods such as K-means, Isolation Forest, and Autoencoders in\nterms of accuracy, precision, recall, and F1 score, demonstrating strong fraud\ndetection capabilities. The results confirm that the SimCLR-based unsupervised\nfraud detection method has broad application prospects in e-commerce platform\nsecurity, improving both detection accuracy and robustness. In the future, with\nthe increasing scale and diversity of datasets, the model's performance will\ncontinue to improve, and it could be integrated with real-time monitoring\nsystems to provide more efficient security for e-commerce platforms.\n","date":"2025-03-24"}
{"id":"2503.18842","title":"Three Kinds of AI Ethics","abstract":"  There is an overwhelming abundance of works in AI Ethics. This growth is\nchaotic because of how sudden it is, its volume, and its multidisciplinary\nnature. This makes difficult to keep track of debates, and to systematically\ncharacterize goals, research questions, methods, and expertise required by AI\nethicists. In this article, I show that the relation between AI and ethics can\nbe characterized in at least three ways, which correspond to three\nwell-represented kinds of AI ethics: ethics and AI; ethics in AI; ethics of AI.\nI elucidate the features of these three kinds of AI Ethics, characterize their\nresearch questions, and identify the kind of expertise that each kind needs. I\nalso show how certain criticisms to AI ethics are misplaced, as being done from\nthe point of view of one kind of AI ethics, to another kind with different\ngoals. All in all, this work sheds light on the nature of AI ethics, and sets\nthe groundwork for more informed discussions about the scope, methods, and\ntraining of AI ethicists.\n","date":"2025-03-24"}
{"id":"2503.18849","title":"Application of Physics-Informed Neural Networks for Solving the Inverse\n  Advection-Diffusion Problem to Localize Pollution Sources","abstract":"  This paper investigates the application of Physics-Informed Neural Networks\n(PINNs) for solving the inverse advection-diffusion problem to localize\npollution sources. The study focuses on optimizing neural network architectures\nto accurately model pollutant dispersion dynamics under diverse conditions,\nincluding scenarios with weak and strong winds and multiple pollution sources.\nVarious PINN configurations are evaluated, showing the strong dependence of\nsolution accuracy on hyperparameter selection. Recommendations for efficient\nPINN configurations are provided based on these comparisons. The approach is\ntested across multiple scenarios and validated using real-world data that\naccounts for atmospheric variability. The results demonstrate that the proposed\nmethodology achieves high accuracy in source localization, showcasing the\nstability and potential of PINNs for addressing environmental monitoring and\npollution management challenges under complex weather conditions.\n","date":"2025-03-24"}
{"id":"2503.18852","title":"Self-Organizing Graph Reasoning Evolves into a Critical State for\n  Continuous Discovery Through Structural-Semantic Dynamics","abstract":"  We report fundamental insights into how agentic graph reasoning systems\nspontaneously evolve toward a critical state that sustains continuous semantic\ndiscovery. By rigorously analyzing structural (Von Neumann graph entropy) and\nsemantic (embedding) entropy, we identify a subtle yet robust regime in which\nsemantic entropy persistently dominates over structural entropy. This interplay\nis quantified by a dimensionless Critical Discovery Parameter that stabilizes\nat a small negative value, indicating a consistent excess of semantic entropy.\nEmpirically, we observe a stable fraction (12%) of \"surprising\" edges, links\nbetween semantically distant concepts, providing evidence of long-range or\ncross-domain connections that drive continuous innovation. Concomitantly, the\nsystem exhibits scale-free and small-world topological features, alongside a\nnegative cross-correlation between structural and semantic measures,\nreinforcing the analogy to self-organized criticality. These results establish\nclear parallels with critical phenomena in physical, biological, and cognitive\ncomplex systems, revealing an entropy-based principle governing adaptability\nand continuous innovation. Crucially, semantic richness emerges as the\nunderlying driver of sustained exploration, despite not being explicitly used\nby the reasoning process. Our findings provide interdisciplinary insights and\npractical strategies for engineering intelligent systems with intrinsic\ncapacities for long-term discovery and adaptation, and offer insights into how\nmodel training strategies can be developed that reinforce critical discovery.\n","date":"2025-03-24"}
{"id":"2503.18853","title":"3DSwapping: Texture Swapping For 3D Object From Single Reference Image","abstract":"  3D texture swapping allows for the customization of 3D object textures,\nenabling efficient and versatile visual transformations in 3D editing. While no\ndedicated method exists, adapted 2D editing and text-driven 3D editing\napproaches can serve this purpose. However, 2D editing requires frame-by-frame\nmanipulation, causing inconsistencies across views, while text-driven 3D\nediting struggles to preserve texture characteristics from reference images. To\ntackle these challenges, we introduce 3DSwapping, a 3D texture swapping method\nthat integrates: 1) progressive generation, 2) view-consistency gradient\nguidance, and 3) prompt-tuned gradient guidance. To ensure view consistency,\nour progressive generation process starts by editing a single reference image\nand gradually propagates the edits to adjacent views. Our view-consistency\ngradient guidance further reinforces consistency by conditioning the generation\nmodel on feature differences between consistent and inconsistent outputs. To\npreserve texture characteristics, we introduce prompt-tuning-based gradient\nguidance, which learns a token that precisely captures the difference between\nthe reference image and the 3D object. This token then guides the editing\nprocess, ensuring more consistent texture preservation across views. Overall,\n3DSwapping integrates these novel strategies to achieve higher-fidelity texture\ntransfer while preserving structural coherence across multiple viewpoints.\nExtensive qualitative and quantitative evaluations confirm that our three novel\ncomponents enable convincing and effective 2D texture swapping for 3D objects.\nCode will be available upon acceptance.\n","date":"2025-03-24"}
{"id":"2503.18854","title":"MC-LLaVA: Multi-Concept Personalized Vision-Language Model","abstract":"  Current vision-language models (VLMs) show exceptional abilities across\ndiverse tasks, such as visual question answering. To enhance user experience,\nrecent studies investigate VLM personalization to understand user-provided\nconcepts. However, they mainly focus on single-concept personalization,\nneglecting the existence and interplay of multiple concepts, which limits\nreal-world applicability. This paper proposes the first multi-concept\npersonalization paradigm, MC-LLaVA. Specifically, MC-LLaVA employs a\nmulti-concept instruction tuning strategy, effectively integrating multiple\nconcepts in a single training step. To reduce the costs related to joint\ntraining, we propose a personalized textual prompt that uses visual token\ninformation to initialize concept tokens. Additionally, we introduce a\npersonalized visual prompt during inference, aggregating location confidence\nmaps for enhanced recognition and grounding capabilities. To advance\nmulti-concept personalization research, we further contribute a high-quality\ninstruction tuning dataset. We carefully collect images with multiple\ncharacters and objects from movies and manually generate question-answer\nsamples for multi-concept scenarios, featuring superior diversity.\nComprehensive qualitative and quantitative experiments demonstrate that\nMC-LLaVA can achieve impressive multi-concept personalized responses, paving\nthe way for VLMs to become better user-specific assistants. The code and\ndataset will be publicly available at https:\/\/github.com\/arctanxarc\/MC-LLaVA}.\n","date":"2025-03-24"}
{"id":"2503.18856","title":"MODIS: Multi-Omics Data Integration for Small and Unpaired Datasets","abstract":"  A key challenge today lies in the ability to efficiently handle multi-omics\ndata since such multimodal data may provide a more comprehensive overview of\nthe underlying processes in a system. Yet it comes with challenges: multi-omics\ndata are most often unpaired and only partially labeled, moreover only small\namounts of data are available in some situation such as rare diseases. We\npropose MODIS which stands for Multi-Omics Data Integration for Small and\nunpaired datasets, a semi supervised approach to account for these particular\nsettings. MODIS learns a probabilistic coupling of heterogeneous data\nmodalities and learns a shared latent space where modalities are aligned. We\nrely on artificial data to build controlled experiments to explore how much\nsupervision is needed for an accurate alignment of modalities, and how our\napproach enables dealing with new conditions for which few data are available.\nThe code is available athttps:\/\/github.com\/VILLOUTREIXLab\/MODIS.\n","date":"2025-03-24"}
{"id":"2503.18858","title":"Dynamics of Insect Paraintelligence: How a Mindless Colony of Ants\n  Meaningfully Moves a Beetle","abstract":"  In this work, a new concept called Vector Dissipation of Randomness (VDR) is\ndeveloped and formalized. It describes the mechanism by which complex\nmulticomponent systems transition from chaos to order through the filtering of\nrandom directions, accumulation of information in the environment, and\nself-organization of agents. VDR explains how individual random strategies can\nevolve into collective goaldirected behavior, leading to the emergence of an\nordered structure without centralized control. To test the proposed model, a\nnumerical simulation of the \"ant and beetle\" system was conducted, in which\nagents (ants) randomly choose movement directions, but through feedback\nmechanisms and filtering of weak strategies, they form a single coordinated\nvector of the beetles movement. VDR is a universal mechanism applicable to a\nwide range of self-organizing systems, including biological populations,\ndecentralized technological networks, sociological processes, and artificial\nintelligence algorithms. For the first time, an equation of the normalized\nemergence function in the processing of vector dissipation of randomness in the\nAnt and Beetle system has been formulated. The concept of paraintelligence was\nintroduced for the first time. Insect paraintelligence is interpreted as a\nrational functionality that is close to or equivalent to intelligent activity\nin the absence of reflexive consciousness and selfawareness.\n","date":"2025-03-24"}
{"id":"2503.18860","title":"HunyuanPortrait: Implicit Condition Control for Enhanced Portrait\n  Animation","abstract":"  We introduce HunyuanPortrait, a diffusion-based condition control method that\nemploys implicit representations for highly controllable and lifelike portrait\nanimation. Given a single portrait image as an appearance reference and video\nclips as driving templates, HunyuanPortrait can animate the character in the\nreference image by the facial expression and head pose of the driving videos.\nIn our framework, we utilize pre-trained encoders to achieve the decoupling of\nportrait motion information and identity in videos. To do so, implicit\nrepresentation is adopted to encode motion information and is employed as\ncontrol signals in the animation phase. By leveraging the power of stable video\ndiffusion as the main building block, we carefully design adapter layers to\ninject control signals into the denoising unet through attention mechanisms.\nThese bring spatial richness of details and temporal consistency.\nHunyuanPortrait also exhibits strong generalization performance, which can\neffectively disentangle appearance and motion under different image styles. Our\nframework outperforms existing methods, demonstrating superior temporal\nconsistency and controllability. Our project is available at\nhttps:\/\/kkakkkka.github.io\/HunyuanPortrait.\n","date":"2025-03-24"}
{"id":"2503.18862","title":"Exploring the Integration of Key-Value Attention Into Pure and Hybrid\n  Transformers for Semantic Segmentation","abstract":"  While CNNs were long considered state of the art for image processing, the\nintroduction of Transformer architectures has challenged this position. While\nachieving excellent results in image classification and segmentation,\nTransformers remain inherently reliant on large training datasets and remain\ncomputationally expensive. A newly introduced Transformer derivative named KV\nTransformer shows promising results in synthetic, NLP, and image classification\ntasks, while reducing complexity and memory usage. This is especially conducive\nto use cases where local inference is required, such as medical screening\napplications. We endeavoured to further evaluate the merit of KV Transformers\non semantic segmentation tasks, specifically in the domain of medical imaging.\nBy directly comparing traditional and KV variants of the same base\narchitectures, we provide further insight into the practical tradeoffs of\nreduced model complexity. We observe a notable reduction in parameter count and\nmultiply accumulate operations, while achieving similar performance from most\nof the KV variant models when directly compared to their QKV implementation.\n","date":"2025-03-24"}
{"id":"2503.18865","title":"Structuring Scientific Innovation: A Framework for Modeling and\n  Discovering Impactful Knowledge Combinations","abstract":"  The emergence of large language models offers new possibilities for\nstructured exploration of scientific knowledge. Rather than viewing scientific\ndiscovery as isolated ideas or content, we propose a structured approach that\nemphasizes the role of method combinations in shaping disruptive insights.\nSpecifically, we investigate how knowledge unit--especially those tied to\nmethodological design--can be modeled and recombined to yield research\nbreakthroughs. Our proposed framework addresses two key challenges. First, we\nintroduce a contrastive learning-based mechanism to identify distinguishing\nfeatures of historically disruptive method combinations within problem-driven\ncontexts. Second, we propose a reasoning-guided Monte Carlo search algorithm\nthat leverages the chain-of-thought capability of LLMs to identify promising\nknowledge recombinations for new problem statements.Empirical studies across\nmultiple domains show that the framework is capable of modeling the structural\ndynamics of innovation and successfully highlights combinations with high\ndisruptive potential. This research provides a new path for computationally\nguided scientific ideation grounded in structured reasoning and historical data\nmodeling.\n","date":"2025-03-24"}
{"id":"2503.18866","title":"Reasoning to Learn from Latent Thoughts","abstract":"  Compute scaling for language model (LM) pretraining has outpaced the growth\nof human-written texts, leading to concerns that data will become the\nbottleneck to LM scaling. To continue scaling pretraining in this\ndata-constrained regime, we propose that explicitly modeling and inferring the\nlatent thoughts that underlie the text generation process can significantly\nimprove pretraining data efficiency. Intuitively, our approach views web text\nas the compressed final outcome of a verbose human thought process and that the\nlatent thoughts contain important contextual knowledge and reasoning steps that\nare critical to data-efficient learning. We empirically demonstrate the\neffectiveness of our approach through data-constrained continued pretraining\nfor math. We first show that synthetic data approaches to inferring latent\nthoughts significantly improve data efficiency, outperforming training on the\nsame amount of raw data (5.7\\% $\\rightarrow$ 25.4\\% on MATH). Furthermore, we\ndemonstrate latent thought inference without a strong teacher, where an LM\nbootstraps its own performance by using an EM algorithm to iteratively improve\nthe capability of the trained LM and the quality of thought-augmented\npretraining data. We show that a 1B LM can bootstrap its performance across at\nleast three iterations and significantly outperform baselines trained on raw\ndata, with increasing gains from additional inference compute when performing\nthe E-step. The gains from inference scaling and EM iterations suggest new\nopportunities for scaling data-constrained pretraining.\n","date":"2025-03-24"}
{"id":"2503.18871","title":"Bootstrapped Model Predictive Control","abstract":"  Model Predictive Control (MPC) has been demonstrated to be effective in\ncontinuous control tasks. When a world model and a value function are\navailable, planning a sequence of actions ahead of time leads to a better\npolicy. Existing methods typically obtain the value function and the\ncorresponding policy in a model-free manner. However, we find that such an\napproach struggles with complex tasks, resulting in poor policy learning and\ninaccurate value estimation. To address this problem, we leverage the strengths\nof MPC itself. In this work, we introduce Bootstrapped Model Predictive Control\n(BMPC), a novel algorithm that performs policy learning in a bootstrapped\nmanner. BMPC learns a network policy by imitating an MPC expert, and in turn,\nuses this policy to guide the MPC process. Combined with model-based\nTD-learning, our policy learning yields better value estimation and further\nboosts the efficiency of MPC. We also introduce a lazy reanalyze mechanism,\nwhich enables computationally efficient imitation learning. Our method achieves\nsuperior performance over prior works on diverse continuous control tasks. In\nparticular, on challenging high-dimensional locomotion tasks, BMPC\nsignificantly improves data efficiency while also enhancing asymptotic\nperformance and training stability, with comparable training time and smaller\nnetwork sizes. Code is available at https:\/\/github.com\/wertyuilife2\/bmpc.\n","date":"2025-03-24"}
{"id":"2503.18872","title":"Curriculum Coarse-to-Fine Selection for High-IPC Dataset Distillation","abstract":"  Dataset distillation (DD) excels in synthesizing a small number of images per\nclass (IPC) but struggles to maintain its effectiveness in high-IPC settings.\nRecent works on dataset distillation demonstrate that combining distilled and\nreal data can mitigate the effectiveness decay. However, our analysis of the\ncombination paradigm reveals that the current one-shot and independent\nselection mechanism induces an incompatibility issue between distilled and real\nimages. To address this issue, we introduce a novel curriculum coarse-to-fine\nselection (CCFS) method for efficient high-IPC dataset distillation. CCFS\nemploys a curriculum selection framework for real data selection, where we\nleverage a coarse-to-fine strategy to select appropriate real data based on the\ncurrent synthetic dataset in each curriculum. Extensive experiments validate\nCCFS, surpassing the state-of-the-art by +6.6\\% on CIFAR-10, +5.8\\% on\nCIFAR-100, and +3.4\\% on Tiny-ImageNet under high-IPC settings. Notably, CCFS\nachieves 60.2\\% test accuracy on ResNet-18 with a 20\\% compression ratio of\nTiny-ImageNet, closely matching full-dataset training with only 0.3\\%\ndegradation. Code: https:\/\/github.com\/CYDaaa30\/CCFS.\n","date":"2025-03-24"}
{"id":"2503.18873","title":"Efficient Self-Supervised Adaptation for Medical Image Analysis","abstract":"  Self-supervised adaptation (SSA) improves foundation model transfer to\nmedical domains but is computationally prohibitive. Although parameter\nefficient fine-tuning methods such as LoRA have been explored for supervised\nadaptation, their effectiveness for SSA remains unknown. In this work, we\nintroduce efficient self-supervised adaptation (ESSA), a framework that applies\nparameter-efficient fine-tuning techniques to SSA with the aim of reducing\ncomputational cost and improving adaptation performance. Among the methods\ntested, Attention Projection Layer Adaptation (APLA) sets a new\nstate-of-the-art, consistently surpassing full-parameter SSA and supervised\nfine-tuning across diverse medical tasks, while reducing GPU memory by up to\n40.1% and increasing training throughput by 25.2%, all while maintaining\ninference efficiency.\n","date":"2025-03-24"}
{"id":"2503.18874","title":"A semantic communication-based workload-adjustable transceiver for\n  wireless AI-generated content (AIGC) delivery","abstract":"  With the significant advances in generative AI (GAI) and the proliferation of\nmobile devices, providing high-quality AI-generated content (AIGC) services via\nwireless networks is becoming the future direction. However, the primary\nchallenges of AIGC service delivery in wireless networks lie in unstable\nchannels, limited bandwidth resources, and unevenly distributed computational\nresources. In this paper, we employ semantic communication (SemCom) in\ndiffusion-based GAI models to propose a Resource-aware wOrkload-adjUstable\nTransceivEr (ROUTE) for AIGC delivery in dynamic wireless networks.\nSpecifically, to relieve the communication resource bottleneck, SemCom is\nutilized to prioritize semantic information of the generated content. Then, to\nimprove computational resource utilization in both edge and local and reduce\nAIGC semantic distortion in transmission, modified diffusion-based models are\napplied to adjust the computing workload and semantic density in cooperative\ncontent generation. Simulations verify the superiority of our proposed ROUTE in\nterms of latency and content quality compared to conventional AIGC approaches.\n","date":"2025-03-24"}
{"id":"2503.18878","title":"I Have Covered All the Bases Here: Interpreting Reasoning Features in\n  Large Language Models via Sparse Autoencoders","abstract":"  Large Language Models (LLMs) have achieved remarkable success in natural\nlanguage processing. Recent advances have led to the developing of a new class\nof reasoning LLMs; for example, open-source DeepSeek-R1 has achieved\nstate-of-the-art performance by integrating deep thinking and complex\nreasoning. Despite these impressive capabilities, the internal reasoning\nmechanisms of such models remain unexplored. In this work, we employ Sparse\nAutoencoders (SAEs), a method to learn a sparse decomposition of latent\nrepresentations of a neural network into interpretable features, to identify\nfeatures that drive reasoning in the DeepSeek-R1 series of models. First, we\npropose an approach to extract candidate ''reasoning features'' from SAE\nrepresentations. We validate these features through empirical analysis and\ninterpretability methods, demonstrating their direct correlation with the\nmodel's reasoning abilities. Crucially, we demonstrate that steering these\nfeatures systematically enhances reasoning performance, offering the first\nmechanistic account of reasoning in LLMs. Code available at\nhttps:\/\/github.com\/AIRI-Institute\/SAE-Reasoning\n","date":"2025-03-24"}
{"id":"2503.18880","title":"Seeing Speech and Sound: Distinguishing and Locating Audios in Visual\n  Scenes","abstract":"  We present a unified model capable of simultaneously grounding both spoken\nlanguage and non-speech sounds within a visual scene, addressing key\nlimitations in current audio-visual grounding models. Existing approaches are\ntypically limited to handling either speech or non-speech sounds independently,\nor at best, together but sequentially without mixing. This limitation prevents\nthem from capturing the complexity of real-world audio sources that are often\nmixed. Our approach introduces a 'mix-and-separate' framework with audio-visual\nalignment objectives that jointly learn correspondence and disentanglement\nusing mixed audio. Through these objectives, our model learns to produce\ndistinct embeddings for each audio type, enabling effective disentanglement and\ngrounding across mixed audio sources. Additionally, we created a new dataset to\nevaluate simultaneous grounding of mixed audio sources, demonstrating that our\nmodel outperforms prior methods. Our approach also achieves comparable or\nbetter performance in standard segmentation and cross-modal retrieval tasks,\nhighlighting the benefits of our mix-and-separate approach.\n","date":"2025-03-24"}
{"id":"2503.18883","title":"Efficient and Accurate Scene Text Recognition with Cascaded-Transformers","abstract":"  In recent years, vision transformers with text decoder have demonstrated\nremarkable performance on Scene Text Recognition (STR) due to their ability to\ncapture long-range dependencies and contextual relationships with high learning\ncapacity. However, the computational and memory demands of these models are\nsignificant, limiting their deployment in resource-constrained applications. To\naddress this challenge, we propose an efficient and accurate STR system.\nSpecifically, we focus on improving the efficiency of encoder models by\nintroducing a cascaded-transformers structure. This structure progressively\nreduces the vision token size during the encoding step, effectively eliminating\nredundant tokens and reducing computational cost. Our experimental results\nconfirm that our STR system achieves comparable performance to state-of-the-art\nbaselines while substantially decreasing computational requirements. In\nparticular, for large-models, the accuracy remains same, 92.77 to 92.68, while\ncomputational complexity is almost halved with our structure.\n","date":"2025-03-24"}
{"id":"2503.18886","title":"CFG-Zero*: Improved Classifier-Free Guidance for Flow Matching Models","abstract":"  Classifier-Free Guidance (CFG) is a widely adopted technique in\ndiffusion\/flow models to improve image fidelity and controllability. In this\nwork, we first analytically study the effect of CFG on flow matching models\ntrained on Gaussian mixtures where the ground-truth flow can be derived. We\nobserve that in the early stages of training, when the flow estimation is\ninaccurate, CFG directs samples toward incorrect trajectories. Building on this\nobservation, we propose CFG-Zero*, an improved CFG with two contributions: (a)\noptimized scale, where a scalar is optimized to correct for the inaccuracies in\nthe estimated velocity, hence the * in the name; and (b) zero-init, which\ninvolves zeroing out the first few steps of the ODE solver. Experiments on both\ntext-to-image (Lumina-Next, Stable Diffusion 3, and Flux) and text-to-video\n(Wan-2.1) generation demonstrate that CFG-Zero* consistently outperforms CFG,\nhighlighting its effectiveness in guiding Flow Matching models. (Code is\navailable at github.com\/WeichenFan\/CFG-Zero-star)\n","date":"2025-03-24"}
{"id":"2503.18888","title":"Toward building next-generation Geocoding systems: a systematic review","abstract":"  Geocoding systems are widely used in both scientific research for spatial\nanalysis and everyday life through location-based services. The quality of\ngeocoded data significantly impacts subsequent processes and applications,\nunderscoring the need for next-generation systems. In response to this demand,\nthis review first examines the evolving requirements for geocoding inputs and\noutputs across various scenarios these systems must address. It then provides a\ndetailed analysis of how to construct such systems by breaking them down into\nkey functional components and reviewing a broad spectrum of existing\napproaches, from traditional rule-based methods to advanced techniques in\ninformation retrieval, natural language processing, and large language models.\nFinally, we identify opportunities to improve next-generation geocoding systems\nin light of recent technological advances.\n","date":"2025-03-24"}
{"id":"2503.18891","title":"AgentDropout: Dynamic Agent Elimination for Token-Efficient and\n  High-Performance LLM-Based Multi-Agent Collaboration","abstract":"  Multi-agent systems (MAS) based on large language models (LLMs) have\ndemonstrated significant potential in collaborative problem-solving. However,\nthey still face substantial challenges of low communication efficiency and\nsuboptimal task performance, making the careful design of the agents'\ncommunication topologies particularly important. Inspired by the management\ntheory that roles in an efficient team are often dynamically adjusted, we\npropose AgentDropout, which identifies redundant agents and communication\nacross different communication rounds by optimizing the adjacency matrices of\nthe communication graphs and eliminates them to enhance both token efficiency\nand task performance. Compared to state-of-the-art methods, AgentDropout\nachieves an average reduction of 21.6% in prompt token consumption and 18.4% in\ncompletion token consumption, along with a performance improvement of 1.14 on\nthe tasks. Furthermore, the extended experiments demonstrate that AgentDropout\nachieves notable domain transferability and structure robustness, revealing its\nreliability and effectiveness. We release our code at\nhttps:\/\/github.com\/wangzx1219\/AgentDropout.\n","date":"2025-03-24"}
{"id":"2503.18892","title":"SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for\n  Open Base Models in the Wild","abstract":"  DeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can\nnaturally emerge through a simple reinforcement learning (RL) framework with\nrule-based rewards, where the training may directly start from the base\nmodels-a paradigm referred to as zero RL training. Most recent efforts to\nreproduce zero RL training have primarily focused on the Qwen2.5 model series,\nwhich may not be representative as we find the base models already exhibit\nstrong instruction-following and self-reflection abilities. In this work, we\ninvestigate zero RL training across 10 diverse base models, spanning different\nfamilies and sizes including LLama3-8B, Mistral-7B\/24B, DeepSeek-Math-7B,\nQwen2.5-math-7B, and all Qwen2.5 models from 0.5B to 32B. Leveraging several\nkey design strategies-such as adjusting format reward and controlling query\ndifficulty-we achieve substantial improvements in both reasoning accuracy and\nresponse length across most settings. However, by carefully monitoring the\ntraining dynamics, we observe that different base models exhibit distinct\npatterns during training. For instance, the increased response length does not\nalways correlate with the emergence of certain cognitive behaviors such as\nverification (i.e., the \"aha moment\"). Notably, we observe the \"aha moment\" for\nthe first time in small models not from the Qwen family. We share the key\ndesigns that enable successful zero RL training, along with our findings and\npractices. To facilitate further research, we open-source the code, models, and\nanalysis tools.\n","date":"2025-03-24"}
{"id":"2503.18893","title":"xKV: Cross-Layer SVD for KV-Cache Compression","abstract":"  Large Language Models (LLMs) with long context windows enable powerful\napplications but come at the cost of high memory consumption to store the Key\nand Value states (KV-Cache). Recent studies attempted to merge KV-cache from\nmultiple layers into shared representations, yet these approaches either\nrequire expensive pretraining or rely on assumptions of high per-token cosine\nsimilarity across layers which generally does not hold in practice. We find\nthat the dominant singular vectors are remarkably well-aligned across multiple\nlayers of the KV-Cache. Exploiting this insight, we propose xKV, a simple\npost-training method that applies Singular Value Decomposition (SVD) on the\nKV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers\ninto a shared low-rank subspace, significantly reducing KV-Cache sizes. Through\nextensive evaluations on the RULER long-context benchmark with widely-used LLMs\n(e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates\nthan state-of-the-art inter-layer technique while improving accuracy by 2.7%.\nMoreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA)\n(e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding\ntasks without performance degradation. These results highlight xKV's strong\ncapability and versatility in addressing memory bottlenecks for long-context\nLLM inference. Our code is publicly available at:\nhttps:\/\/github.com\/abdelfattah-lab\/xKV.\n","date":"2025-03-24"}
{"id":"2503.18896","title":"Calibration Bands for Mean Estimates within the Exponential Dispersion\n  Family","abstract":"  A statistical model is said to be calibrated if the resulting mean estimates\nperfectly match the true means of the underlying responses. Aiming for\ncalibration is often not achievable in practice as one has to deal with finite\nsamples of noisy observations. A weaker notion of calibration is\nauto-calibration. An auto-calibrated model satisfies that the expected value of\nthe responses being given the same mean estimate matches this estimate. Testing\nfor auto-calibration has only been considered recently in the literature and we\npropose a new approach based on calibration bands. Calibration bands denote a\nset of lower and upper bounds such that the probability that the true means lie\nsimultaneously inside those bounds exceeds some given confidence level. Such\nbands were constructed by Yang-Barber (2019) for sub-Gaussian distributions.\nDimitriadis et al. (2023) then introduced narrower bands for the Bernoulli\ndistribution and we use the same idea in order to extend the construction to\nthe entire exponential dispersion family that contains for example the\nbinomial, Poisson, negative binomial, gamma and normal distributions. Moreover,\nwe show that the obtained calibration bands allow us to construct various tests\nfor calibration and auto-calibration, respectively.\n","date":"2025-03-24"}
{"id":"2503.18897","title":"Online 3D Scene Reconstruction Using Neural Object Priors","abstract":"  This paper addresses the problem of reconstructing a scene online at the\nlevel of objects given an RGB-D video sequence. While current object-aware\nneural implicit representations hold promise, they are limited in online\nreconstruction efficiency and shape completion. Our main contributions to\nalleviate the above limitations are twofold. First, we propose a feature grid\ninterpolation mechanism to continuously update grid-based object-centric neural\nimplicit representations as new object parts are revealed. Second, we construct\nan object library with previously mapped objects in advance and leverage the\ncorresponding shape priors to initialize geometric object models in new videos,\nsubsequently completing them with novel views as well as synthesized past views\nto avoid losing original object details. Extensive experiments on synthetic\nenvironments from the Replica dataset, real-world ScanNet sequences and videos\ncaptured in our laboratory demonstrate that our approach outperforms\nstate-of-the-art neural implicit models for this task in terms of\nreconstruction accuracy and completeness.\n","date":"2025-03-24"}
{"id":"2503.18899","title":"Statistical Proof of Execution (SPEX)","abstract":"  Many real-world applications are increasingly incorporating automated\ndecision-making, driven by the widespread adoption of ML\/AI inference for\nplanning and guidance. This study examines the growing need for verifiable\ncomputing in autonomous decision-making. We formalize the problem of verifiable\ncomputing and introduce a sampling-based protocol that is significantly faster,\nmore cost-effective, and simpler than existing methods. Furthermore, we tackle\nthe challenges posed by non-determinism, proposing a set of strategies to\neffectively manage common scenarios.\n","date":"2025-03-24"}
{"id":"2503.18903","title":"Building Blocks for Robust and Effective Semi-Supervised Real-World\n  Object Detection","abstract":"  Semi-supervised object detection (SSOD) based on pseudo-labeling\nsignificantly reduces dependence on large labeled datasets by effectively\nleveraging both labeled and unlabeled data. However, real-world applications of\nSSOD often face critical challenges, including class imbalance, label noise,\nand labeling errors. We present an in-depth analysis of SSOD under real-world\nconditions, uncovering causes of suboptimal pseudo-labeling and key trade-offs\nbetween label quality and quantity. Based on our findings, we propose four\nbuilding blocks that can be seamlessly integrated into an SSOD framework. Rare\nClass Collage (RCC): a data augmentation method that enhances the\nrepresentation of rare classes by creating collages of rare objects. Rare Class\nFocus (RCF): a stratified batch sampling strategy that ensures a more balanced\nrepresentation of all classes during training. Ground Truth Label Correction\n(GLC): a label refinement method that identifies and corrects false, missing,\nand noisy ground truth labels by leveraging the consistency of teacher model\npredictions. Pseudo-Label Selection (PLS): a selection method for removing\nlow-quality pseudo-labeled images, guided by a novel metric estimating the\nmissing detection rate while accounting for class rarity. We validate our\nmethods through comprehensive experiments on autonomous driving datasets,\nresulting in up to 6% increase in SSOD performance. Overall, our investigation\nand novel, data-centric, and broadly applicable building blocks enable robust\nand effective SSOD in complex, real-world scenarios. Code is available at\nhttps:\/\/mos-ks.github.io\/publications.\n","date":"2025-03-24"}
{"id":"2503.18908","title":"FFN Fusion: Rethinking Sequential Computation in Large Language Models","abstract":"  We introduce FFN Fusion, an architectural optimization technique that reduces\nsequential computation in large language models by identifying and exploiting\nnatural opportunities for parallelization. Our key insight is that sequences of\nFeed-Forward Network (FFN) layers, particularly those remaining after the\nremoval of specific attention layers, can often be parallelized with minimal\naccuracy impact. We develop a principled methodology for identifying and fusing\nsuch sequences, transforming them into parallel operations that significantly\nreduce inference latency while preserving model behavior. Applying these\ntechniques to Llama-3.1-405B-Instruct, we create Llama-Nemotron-Ultra-253B-Base\n(Ultra-253B-Base), an efficient and soon-to-be publicly available model that\nachieves a 1.71X speedup in inference latency and 35X lower per-token cost\nwhile maintaining strong performance across benchmarks. Through extensive\nexperiments on models from 49B to 253B parameters, we demonstrate that FFN\nFusion becomes increasingly effective at larger scales and can complement\nexisting optimization techniques like quantization and pruning. Most\nintriguingly, we find that even full transformer blocks containing both\nattention and FFN layers can sometimes be parallelized, suggesting new\ndirections for neural architecture design.\n","date":"2025-03-24"}
{"id":"2503.18912","title":"Causal Links Between Anthropogenic Emissions and Air Pollution Dynamics\n  in Delhi","abstract":"  Air pollution poses significant health and environmental challenges,\nparticularly in rapidly urbanizing regions. Delhi-National Capital Region\nexperiences air pollution episodes due to complex interactions between\nanthropogenic emissions and meteorological conditions. Understanding the causal\ndrivers of key pollutants such as $PM_{2.5}$ and ground $O_3$ is crucial for\ndeveloping effective mitigation strategies. This study investigates the causal\nlinks of anthropogenic emissions on $PM_{2.5}$ and $O_3$ concentrations using\npredictive modeling and causal inference techniques. Integrating\nhigh-resolution air quality data from Jan 2018 to Aug 2023 across 32 monitoring\nstations, we develop predictive regression models that incorporate\nmeteorological variables (temperature and relative humidity), pollutant\nconcentrations ($NO_2, SO_2, CO$), and seasonal harmonic components to capture\nboth diurnal and annual cycles. Here, we show that reductions in anthropogenic\nemissions lead to significant decreases in $PM_{2.5}$ levels, whereas their\neffect on $O_3$ remains marginal and statistically insignificant. To address\nspatial heterogeneity, we employ Gaussian Process modeling. Further, we use\nGranger causality analysis and counterfactual simulation to establish direct\ncausal links. Validation using real-world data from the COVID-19 lockdown\nconfirms that reduced emissions led to a substantial drop in $PM_{2.5}$ but\nonly a slight, insignificant change in $O_3$. The findings highlight the\nnecessity of targeted emission reduction policies while emphasizing the need\nfor integrated strategies addressing both particulate and ozone pollution.\nThese insights are crucial for policymakers designing air pollution\ninterventions in other megacities, and offer a scalable methodology for\ntackling complex urban air pollution through data-driven decision-making.\n","date":"2025-03-24"}
{"id":"2503.18923","title":"Video SimpleQA: Towards Factuality Evaluation in Large Video Language\n  Models","abstract":"  Recent advancements in Large Video Language Models (LVLMs) have highlighted\ntheir potential for multi-modal understanding, yet evaluating their factual\ngrounding in video contexts remains a critical unsolved challenge. To address\nthis gap, we introduce Video SimpleQA, the first comprehensive benchmark\ntailored for factuality evaluation of LVLMs. Our work distinguishes from\nexisting video benchmarks through the following key features: 1) Knowledge\nrequired: demanding integration of external knowledge beyond the explicit\nnarrative; 2) Fact-seeking question: targeting objective, undisputed events or\nrelationships, avoiding subjective interpretation; 3) Definitive & short-form\nanswer: Answers are crafted as unambiguous and definitively correct in a short\nformat, enabling automated evaluation through LLM-as-a-judge frameworks with\nminimal scoring variance; 4) External-source verified: All annotations undergo\nrigorous validation against authoritative external references to ensure the\nreliability; 5) Temporal reasoning required: The annotated question types\nencompass both static single-frame understanding and dynamic temporal\nreasoning, explicitly evaluating LVLMs factuality under the long-context\ndependencies. We extensively evaluate 41 state-of-the-art LVLMs and summarize\nkey findings as follows: 1) Current LVLMs exhibit notable deficiencies in\nfactual adherence, particularly for open-source models. The best-performing\nmodel Gemini-1.5-Pro achieves merely an F-score of 54.4%; 2) Test-time compute\nparadigms show insignificant performance gains, revealing fundamental\nconstraints for enhancing factuality through post-hoc computation; 3)\nRetrieval-Augmented Generation demonstrates consistent improvements at the cost\nof additional inference time overhead, presenting a critical\nefficiency-performance trade-off.\n","date":"2025-03-24"}
{"id":"2503.18929","title":"Trajectory Balance with Asynchrony: Decoupling Exploration and Learning\n  for Fast, Scalable LLM Post-Training","abstract":"  Reinforcement learning (RL) is a critical component of large language model\n(LLM) post-training. However, existing on-policy algorithms used for\npost-training are inherently incompatible with the use of experience replay\nbuffers, which can be populated scalably by distributed off-policy actors to\nenhance exploration as compute increases. We propose efficiently obtaining this\nbenefit of replay buffers via Trajectory Balance with Asynchrony (TBA), a\nmassively scalable LLM RL system. In contrast to existing approaches, TBA uses\na larger fraction of compute on search, constantly generating off-policy data\nfor a central replay buffer. A training node simultaneously samples data from\nthis buffer based on reward or recency to update the policy using Trajectory\nBalance (TB), a diversity-seeking RL objective introduced for GFlowNets. TBA\noffers three key advantages: (1) decoupled training and search, speeding up\ntraining wall-clock time by 4x or more; (2) improved diversity through\nlarge-scale off-policy sampling; and (3) scalable search for sparse reward\nsettings. On mathematical reasoning, preference-tuning, and automated\nred-teaming (diverse and representative post-training tasks), TBA produces\nspeed and performance improvements over strong baselines.\n","date":"2025-03-24"}
{"id":"2503.18931","title":"CoMP: Continual Multimodal Pre-training for Vision Foundation Models","abstract":"  Pre-trained Vision Foundation Models (VFMs) provide strong visual\nrepresentations for a wide range of applications. In this paper, we continually\npre-train prevailing VFMs in a multimodal manner such that they can\neffortlessly process visual inputs of varying sizes and produce visual\nrepresentations that are more aligned with language representations, regardless\nof their original pre-training process. To this end, we introduce CoMP, a\ncarefully designed multimodal pre-training pipeline. CoMP uses a Continual\nRotary Position Embedding to support native resolution continual pre-training,\nand an Alignment Loss between visual and textual features through language\nprototypes to align multimodal representations. By three-stage training, our\nVFMs achieve remarkable improvements not only in multimodal understanding but\nalso in other downstream tasks such as classification and segmentation.\nRemarkably, CoMP-SigLIP achieves scores of 66.7 on ChartQA and 75.9 on DocVQA\nwith a 0.5B LLM, while maintaining an 87.4% accuracy on ImageNet-1K and a 49.5\nmIoU on ADE20K under frozen chunk evaluation.\n","date":"2025-03-24"}
{"id":"2503.18933","title":"SyncVP: Joint Diffusion for Synchronous Multi-Modal Video Prediction","abstract":"  Predicting future video frames is essential for decision-making systems, yet\nRGB frames alone often lack the information needed to fully capture the\nunderlying complexities of the real world. To address this limitation, we\npropose a multi-modal framework for Synchronous Video Prediction (SyncVP) that\nincorporates complementary data modalities, enhancing the richness and accuracy\nof future predictions. SyncVP builds on pre-trained modality-specific diffusion\nmodels and introduces an efficient spatio-temporal cross-attention module to\nenable effective information sharing across modalities. We evaluate SyncVP on\nstandard benchmark datasets, such as Cityscapes and BAIR, using depth as an\nadditional modality. We furthermore demonstrate its generalization to other\nmodalities on SYNTHIA with semantic information and ERA5-Land with climate\ndata. Notably, SyncVP achieves state-of-the-art performance, even in scenarios\nwhere only one modality is present, demonstrating its robustness and potential\nfor a wide range of applications.\n","date":"2025-03-24"}
{"id":"2503.18938","title":"AdaWorld: Learning Adaptable World Models with Latent Actions","abstract":"  World models aim to learn action-controlled prediction models and have proven\nessential for the development of intelligent agents. However, most existing\nworld models rely heavily on substantial action-labeled data and costly\ntraining, making it challenging to adapt to novel environments with\nheterogeneous actions through limited interactions. This limitation can hinder\ntheir applicability across broader domains. To overcome this challenge, we\npropose AdaWorld, an innovative world model learning approach that enables\nefficient adaptation. The key idea is to incorporate action information during\nthe pretraining of world models. This is achieved by extracting latent actions\nfrom videos in a self-supervised manner, capturing the most critical\ntransitions between frames. We then develop an autoregressive world model that\nconditions on these latent actions. This learning paradigm enables highly\nadaptable world models, facilitating efficient transfer and learning of new\nactions even with limited interactions and finetuning. Our comprehensive\nexperiments across multiple environments demonstrate that AdaWorld achieves\nsuperior performance in both simulation quality and visual planning.\n","date":"2025-03-24"}
{"id":"2503.18940","title":"Training-free Diffusion Acceleration with Bottleneck Sampling","abstract":"  Diffusion models have demonstrated remarkable capabilities in visual content\ngeneration but remain challenging to deploy due to their high computational\ncost during inference. This computational burden primarily arises from the\nquadratic complexity of self-attention with respect to image or video\nresolution. While existing acceleration methods often compromise output quality\nor necessitate costly retraining, we observe that most diffusion models are\npre-trained at lower resolutions, presenting an opportunity to exploit these\nlow-resolution priors for more efficient inference without degrading\nperformance. In this work, we introduce Bottleneck Sampling, a training-free\nframework that leverages low-resolution priors to reduce computational overhead\nwhile preserving output fidelity. Bottleneck Sampling follows a high-low-high\ndenoising workflow: it performs high-resolution denoising in the initial and\nfinal stages while operating at lower resolutions in intermediate steps. To\nmitigate aliasing and blurring artifacts, we further refine the resolution\ntransition points and adaptively shift the denoising timesteps at each stage.\nWe evaluate Bottleneck Sampling on both image and video generation tasks, where\nextensive experiments demonstrate that it accelerates inference by up to\n3$\\times$ for image generation and 2.5$\\times$ for video generation, all while\nmaintaining output quality comparable to the standard full-resolution sampling\nprocess across multiple evaluation metrics.\n","date":"2025-03-24"}
{"id":"2503.18941","title":"Exploring Training and Inference Scaling Laws in Generative Retrieval","abstract":"  Generative retrieval has emerged as a novel paradigm that leverages large\nlanguage models (LLMs) to autoregressively generate document identifiers.\nAlthough promising, the mechanisms that underpin its performance and\nscalability remain largely unclear. We conduct a systematic investigation of\ntraining and inference scaling laws in generative retrieval, exploring how\nmodel size, training data scale, and inference-time compute jointly influence\nretrieval performance. To address the lack of suitable metrics, we propose a\nnovel evaluation measure inspired by contrastive entropy and generation loss,\nproviding a continuous performance signal that enables robust comparisons\nacross diverse generative retrieval methods. Our experiments show that\nn-gram-based methods demonstrate strong alignment with both training and\ninference scaling laws, especially when paired with larger LLMs. Furthermore,\nincreasing inference computation yields substantial performance gains,\nrevealing that generative retrieval can significantly benefit from higher\ncompute budgets at inference. Across these settings, LLaMA models consistently\noutperform T5 models, suggesting a particular advantage for larger decoder-only\nmodels in generative retrieval. Taken together, our findings underscore that\nmodel sizes, data availability, and inference computation interact to unlock\nthe full potential of generative retrieval, offering new insights for designing\nand optimizing future systems.\n","date":"2025-03-24"}
{"id":"2503.18942","title":"Video-T1: Test-Time Scaling for Video Generation","abstract":"  With the scale capability of increasing training data, model size, and\ncomputational cost, video generation has achieved impressive results in digital\ncreation, enabling users to express creativity across various domains.\nRecently, researchers in Large Language Models (LLMs) have expanded the scaling\nto test-time, which can significantly improve LLM performance by using more\ninference-time computation. Instead of scaling up video foundation models\nthrough expensive training costs, we explore the power of Test-Time Scaling\n(TTS) in video generation, aiming to answer the question: if a video generation\nmodel is allowed to use non-trivial amount of inference-time compute, how much\ncan it improve generation quality given a challenging text prompt. In this\nwork, we reinterpret the test-time scaling of video generation as a searching\nproblem to sample better trajectories from Gaussian noise space to the target\nvideo distribution. Specifically, we build the search space with test-time\nverifiers to provide feedback and heuristic algorithms to guide searching\nprocess. Given a text prompt, we first explore an intuitive linear search\nstrategy by increasing noise candidates at inference time. As full-step\ndenoising all frames simultaneously requires heavy test-time computation costs,\nwe further design a more efficient TTS method for video generation called\nTree-of-Frames (ToF) that adaptively expands and prunes video branches in an\nautoregressive manner. Extensive experiments on text-conditioned video\ngeneration benchmarks demonstrate that increasing test-time compute\nconsistently leads to significant improvements in the quality of videos.\nProject page: https:\/\/liuff19.github.io\/Video-T1\n","date":"2025-03-24"}
{"id":"2503.18943","title":"SlowFast-LLaVA-1.5: A Family of Token-Efficient Video Large Language\n  Models for Long-Form Video Understanding","abstract":"  We introduce SlowFast-LLaVA-1.5 (abbreviated as SF-LLaVA-1.5), a family of\nvideo large language models (LLMs) offering a token-efficient solution for\nlong-form video understanding. We incorporate the two-stream SlowFast mechanism\ninto a streamlined training pipeline, and perform joint video-image training on\na carefully curated data mixture of only publicly available datasets. Our\nprimary focus is on highly efficient model scales (1B and 3B), demonstrating\nthat even relatively small Video LLMs can achieve state-of-the-art performance\non video understanding, meeting the demand for mobile-friendly models.\nExperimental results demonstrate that SF-LLaVA-1.5 achieves superior\nperformance on a wide range of video and image tasks, with robust results at\nall model sizes (ranging from 1B to 7B). Notably, SF-LLaVA-1.5 achieves\nstate-of-the-art results in long-form video understanding (e.g., LongVideoBench\nand MLVU) and excels at small scales across various video benchmarks.\n","date":"2025-03-24"}
{"id":"2503.18944","title":"DINO in the Room: Leveraging 2D Foundation Models for 3D Segmentation","abstract":"  Vision foundation models (VFMs) trained on large-scale image datasets provide\nhigh-quality features that have significantly advanced 2D visual recognition.\nHowever, their potential in 3D vision remains largely untapped, despite the\ncommon availability of 2D images alongside 3D point cloud datasets. While\nsignificant research has been dedicated to 2D-3D fusion, recent\nstate-of-the-art 3D methods predominantly focus on 3D data, leaving the\nintegration of VFMs into 3D models underexplored. In this work, we challenge\nthis trend by introducing DITR, a simple yet effective approach that extracts\n2D foundation model features, projects them to 3D, and finally injects them\ninto a 3D point cloud segmentation model. DITR achieves state-of-the-art\nresults on both indoor and outdoor 3D semantic segmentation benchmarks. To\nenable the use of VFMs even when images are unavailable during inference, we\nfurther propose to distill 2D foundation models into a 3D backbone as a\npretraining task. By initializing the 3D backbone with knowledge distilled from\n2D VFMs, we create a strong basis for downstream 3D segmentation tasks,\nultimately boosting performance across various datasets.\n","date":"2025-03-24"}
{"id":"2503.18945","title":"Aether: Geometric-Aware Unified World Modeling","abstract":"  The integration of geometric reconstruction and generative modeling remains a\ncritical challenge in developing AI systems capable of human-like spatial\nreasoning. This paper proposes Aether, a unified framework that enables\ngeometry-aware reasoning in world models by jointly optimizing three core\ncapabilities: (1) 4D dynamic reconstruction, (2) action-conditioned video\nprediction, and (3) goal-conditioned visual planning. Through task-interleaved\nfeature learning, Aether achieves synergistic knowledge sharing across\nreconstruction, prediction, and planning objectives. Building upon video\ngeneration models, our framework demonstrates unprecedented synthetic-to-real\ngeneralization despite never observing real-world data during training.\nFurthermore, our approach achieves zero-shot generalization in both action\nfollowing and reconstruction tasks, thanks to its intrinsic geometric modeling.\nRemarkably, even without real-world data, its reconstruction performance is\ncomparable with or even better than that of domain-specific models.\nAdditionally, Aether employs camera trajectories as geometry-informed action\nspaces, enabling effective action-conditioned prediction and visual planning.\nWe hope our work inspires the community to explore new frontiers in\nphysically-reasonable world modeling and its applications.\n","date":"2025-03-24"}
{"id":"2503.18947","title":"Tuning-Free Amodal Segmentation via the Occlusion-Free Bias of\n  Inpainting Models","abstract":"  Amodal segmentation aims to predict segmentation masks for both the visible\nand occluded regions of an object. Most existing works formulate this as a\nsupervised learning problem, requiring manually annotated amodal masks or\nsynthetic training data. Consequently, their performance depends on the quality\nof the datasets, which often lack diversity and scale. This work introduces a\ntuning-free approach that repurposes pretrained diffusion-based inpainting\nmodels for amodal segmentation. Our approach is motivated by the\n\"occlusion-free bias\" of inpainting models, i.e., the inpainted objects tend to\nbe complete objects without occlusions. Specifically, we reconstruct the\noccluded regions of an object via inpainting and then apply segmentation, all\nwithout additional training or fine-tuning. Experiments on five datasets\ndemonstrate the generalizability and robustness of our approach. On average,\nour approach achieves 5.3% more accurate masks over the state-of-the-art.\n","date":"2025-03-24"}
{"id":"2503.18948","title":"Equivariant Image Modeling","abstract":"  Current generative models, such as autoregressive and diffusion approaches,\ndecompose high-dimensional data distribution learning into a series of simpler\nsubtasks. However, inherent conflicts arise during the joint optimization of\nthese subtasks, and existing solutions fail to resolve such conflicts without\nsacrificing efficiency or scalability. We propose a novel equivariant image\nmodeling framework that inherently aligns optimization targets across subtasks\nby leveraging the translation invariance of natural visual signals. Our method\nintroduces (1) column-wise tokenization which enhances translational symmetry\nalong the horizontal axis, and (2) windowed causal attention which enforces\nconsistent contextual relationships across positions. Evaluated on\nclass-conditioned ImageNet generation at 256x256 resolution, our approach\nachieves performance comparable to state-of-the-art AR models while using fewer\ncomputational resources. Systematic analysis demonstrates that enhanced\nequivariance reduces inter-task conflicts, significantly improving zero-shot\ngeneralization and enabling ultra-long image synthesis. This work establishes\nthe first framework for task-aligned decomposition in generative modeling,\noffering insights into efficient parameter sharing and conflict-free\noptimization. The code and models are publicly available at\nhttps:\/\/github.com\/drx-code\/EquivariantModeling.\n","date":"2025-03-24"}
{"id":"2503.18950","title":"Target-Aware Video Diffusion Models","abstract":"  We present a target-aware video diffusion model that generates videos from an\ninput image in which an actor interacts with a specified target while\nperforming a desired action. The target is defined by a segmentation mask and\nthe desired action is described via a text prompt. Unlike existing controllable\nimage-to-video diffusion models that often rely on dense structural or motion\ncues to guide the actor's movements toward the target, our target-aware model\nrequires only a simple mask to indicate the target, leveraging the\ngeneralization capabilities of pretrained models to produce plausible actions.\nThis makes our method particularly effective for human-object interaction (HOI)\nscenarios, where providing precise action guidance is challenging, and further\nenables the use of video diffusion models for high-level action planning in\napplications such as robotics. We build our target-aware model by extending a\nbaseline model to incorporate the target mask as an additional input. To\nenforce target awareness, we introduce a special token that encodes the\ntarget's spatial information within the text prompt. We then fine-tune the\nmodel with our curated dataset using a novel cross-attention loss that aligns\nthe cross-attention maps associated with this token with the input target mask.\nTo further improve performance, we selectively apply this loss to the most\nsemantically relevant transformer blocks and attention regions. Experimental\nresults show that our target-aware model outperforms existing solutions in\ngenerating videos where actors interact accurately with the specified targets.\nWe further demonstrate its efficacy in two downstream applications: video\ncontent creation and zero-shot 3D HOI motion synthesis.\n","date":"2025-03-24"}
{"id":"2503.18979","title":"Threshold Crossings as Tail Events for Catastrophic AI Risk","abstract":"  We analyse circumstances in which bifurcation-driven jumps in AI systems are\nassociated with emergent heavy-tailed outcome distributions. By analysing how a\ncontrol parameter's random fluctuations near a catastrophic threshold generate\nextreme outcomes, we demonstrate in what circumstances the probability of a\nsudden, large-scale, transition aligns closely with the tail probability of the\nresulting damage distribution. Our results contribute to research in\nmonitoring, mitigation and control of AI systems when seeking to manage\npotentially catastrophic AI risk.\n","date":"2025-03-23"}
{"id":"2503.18980","title":"CAE: Repurposing the Critic as an Explorer in Deep Reinforcement\n  Learning","abstract":"  Exploration remains a critical challenge in reinforcement learning, as many\nexisting methods either lack theoretical guarantees or fall short of practical\neffectiveness. In this paper, we introduce CAE, a lightweight algorithm that\nrepurposes the value networks in standard deep RL algorithms to drive\nexploration without introducing additional parameters. CAE utilizes any linear\nmulti-armed bandit technique and incorporates an appropriate scaling strategy,\nenabling efficient exploration with provable sub-linear regret bounds and\npractical stability. Notably, it is simple to implement, requiring only around\n10 lines of code. In complex tasks where learning an effective value network\nproves challenging, we propose CAE+, an extension of CAE that incorporates an\nauxiliary network. This extension increases the parameter count by less than 1%\nwhile maintaining implementation simplicity, adding only about 10 additional\nlines of code. Experiments on MuJoCo and MiniHack show that both CAE and CAE+\noutperform state-of-the-art baselines, bridging the gap between theoretical\nrigor and practical efficiency.\n","date":"2025-03-23"}
{"id":"2503.18981","title":"FedSKD: Aggregation-free Model-heterogeneous Federated Learning using\n  Multi-dimensional Similarity Knowledge Distillation","abstract":"  Federated learning (FL) enables privacy-preserving collaborative model\ntraining without direct data sharing. Model-heterogeneous FL (MHFL) extends\nthis paradigm by allowing clients to train personalized models with\nheterogeneous architectures tailored to their computational resources and\napplication-specific needs. However, existing MHFL methods predominantly rely\non centralized aggregation, which introduces scalability and efficiency\nbottlenecks, or impose restrictions requiring partially identical model\narchitectures across clients. While peer-to-peer (P2P) FL removes server\ndependence, it suffers from model drift and knowledge dilution, limiting its\neffectiveness in heterogeneous settings. To address these challenges, we\npropose FedSKD, a novel MHFL framework that facilitates direct knowledge\nexchange through round-robin model circulation, eliminating the need for\ncentralized aggregation while allowing fully heterogeneous model architectures\nacross clients. FedSKD's key innovation lies in multi-dimensional similarity\nknowledge distillation, which enables bidirectional cross-client knowledge\ntransfer at batch, pixel\/voxel, and region levels for heterogeneous models in\nFL. This approach mitigates catastrophic forgetting and model drift through\nprogressive reinforcement and distribution alignment while preserving model\nheterogeneity. Extensive evaluations on fMRI-based autism spectrum disorder\ndiagnosis and skin lesion classification demonstrate that FedSKD outperforms\nstate-of-the-art heterogeneous and homogeneous FL baselines, achieving superior\npersonalization (client-specific accuracy) and generalization\n(cross-institutional adaptability). These findings underscore FedSKD's\npotential as a scalable and robust solution for real-world medical federated\nlearning applications.\n","date":"2025-03-23"}
{"id":"2503.18982","title":"Generative Data Imputation for Sparse Learner Performance Data Using\n  Generative Adversarial Imputation Networks","abstract":"  Learner performance data collected by Intelligent Tutoring Systems (ITSs),\nsuch as responses to questions, is essential for modeling and predicting\nlearners' knowledge states. However, missing responses due to skips or\nincomplete attempts create data sparsity, challenging accurate assessment and\npersonalized instruction. To address this, we propose a generative imputation\napproach using Generative Adversarial Imputation Networks (GAIN). Our method\nfeatures a three-dimensional (3D) framework (learners, questions, and\nattempts), flexibly accommodating various sparsity levels. Enhanced by\nconvolutional neural networks and optimized with a least squares loss function,\nthe GAIN-based method aligns input and output dimensions to question-attempt\nmatrices along the learners' dimension. Extensive experiments using datasets\nfrom AutoTutor Adult Reading Comprehension (ARC), ASSISTments, and MATHia\ndemonstrate that our approach significantly outperforms tensor factorization\nand alternative GAN methods in imputation accuracy across different attempt\nscenarios. Bayesian Knowledge Tracing (BKT) further validates the effectiveness\nof the imputed data by estimating learning parameters: initial knowledge\n(P(L0)), learning rate (P(T)), guess rate (P(G)), and slip rate (P(S)). Results\nindicate the imputed data enhances model fit and closely mirrors original\ndistributions, capturing underlying learning behaviors reliably.\nKullback-Leibler (KL) divergence assessments confirm minimal divergence,\nshowing the imputed data preserves essential learning characteristics\neffectively. These findings underscore GAIN's capability as a robust imputation\ntool in ITSs, alleviating data sparsity and supporting adaptive, individualized\ninstruction, ultimately leading to more precise and responsive learner\nassessments and improved educational outcomes.\n","date":"2025-03-23"}
{"id":"2503.18983","title":"Confronting Catastrophic Risk: The International Obligation to Regulate\n  Artificial Intelligence","abstract":"  While artificial intelligence (AI) holds enormous promise, many experts in\nthe field are warning that there is a non-trivial chance that the development\nof AI poses an existential threat to humanity. Existing regulatory initiative\ndo not address this threat but merely instead focus on discrete AI-related\nrisks such as consumer safety, cybersecurity, data protection, and privacy. In\nthe absence of regulatory action to address the possible risk of human\nextinction by AI, the question arises: What legal obligations, if any, does\npublic international law impose on states to regulate its development. Grounded\nin the precautionary principle, we argue that there exists an international\nobligation to mitigate the threat of human extinction by AI. Often invoked in\nrelation to environmental regulation and the regulation of potentially harmful\ntechnologies, the principle holds that in situations where there is the\npotential for significant harm, even in the absence of full scientific\ncertainty, preventive measures should not be postponed if delayed action may\nresult in irreversible consequences. We argue that the precautionary principle\nis a general principle of international law and, therefore, that there is a\npositive obligation on states under the right to life within international\nhuman rights law to proactively take regulatory action to mitigate the\npotential existential risk of AI. This is significant because, if an\ninternational obligation to regulate the development of AI can be established\nunder international law, then the basic legal framework would be in place to\naddress this evolving threat.\n","date":"2025-03-23"}
{"id":"2503.18984","title":"The Misinterpretable Evidence Conveyed by Arbitrary Codes","abstract":"  Evidence Theory is a mathematical framework for handling imprecise reasoning\nin the context of a judge evaluating testimonies or a detective evaluating\ncues, rather than a gambler playing games of chance. In comparison to\nProbability Theory, it is better equipped to deal with ambiguous information\nand novel possibilities. Furthermore, arrival and evaluation of testimonies\nimplies a communication channel.\n  This paper explores the possibility of employing Evidence Theory to represent\narbitrary communication codes between and within living organisms. In this\npaper, different schemes are explored for living organisms incapable of\nanticipation, animals sufficiently sophisticated to be capable of\nextrapolation, and humans capable of reading one other's minds.\n","date":"2025-03-23"}
{"id":"2503.18985","title":"LoRA Subtraction for Drift-Resistant Space in Exemplar-Free Continual\n  Learning","abstract":"  In continual learning (CL), catastrophic forgetting often arises due to\nfeature drift. This challenge is particularly prominent in the exemplar-free\ncontinual learning (EFCL) setting, where samples from previous tasks cannot be\nretained, making it difficult to preserve prior knowledge. To address this\nissue, some EFCL methods aim to identify feature spaces that minimize the\nimpact on previous tasks while accommodating new ones. However, they rely on\nstatic features or outdated statistics stored from old tasks, which prevents\nthem from capturing the dynamic evolution of the feature space in CL, leading\nto performance degradation over time. In this paper, we introduce the\nDrift-Resistant Space (DRS), which effectively handles feature drifts without\nrequiring explicit feature modeling or the storage of previous tasks. A novel\nparameter-efficient fine-tuning approach called Low-Rank Adaptation Subtraction\n(LoRA-) is proposed to develop the DRS. This method subtracts the LoRA weights\nof old tasks from the initial pre-trained weight before processing new task\ndata to establish the DRS for model training. Therefore, LoRA- enhances\nstability, improves efficiency, and simplifies implementation. Furthermore,\nstabilizing feature drifts allows for better plasticity by learning with a\ntriplet loss. Our method consistently achieves state-of-the-art results,\nespecially for long task sequences, across multiple datasets.\n","date":"2025-03-23"}
{"id":"2503.18986","title":"SplitFrozen: Split Learning with Device-side Model Frozen for\n  Fine-Tuning LLM on Heterogeneous Resource-Constrained Devices","abstract":"  Fine-tuning large language models (LLMs) on private, on-device data can\nempower tailored personalized AI agents. However, fine-tuning LLMs on\nresource-constrained edge devices faces significant challenges, including\nexcessive computation overhead, device heterogeneity, and data imbalance. This\npaper proposes SplitFrozen, a split learning framework that enables efficient\nLLM fine-tuning by strategically freezing device-side model layers while\ncentralizing parameter-efficient fine-tuning on the server. Our framework\npartitions LLMs into device-side frozen layers and server-side fine-tuning\nlayers, where heterogeneous resource-constrained devices execute only forward\npropagation. To minimize server-side training costs, we integrate Low-Rank\nAdaptation (LoRA) into the server-side layers. A pipeline parallelism strategy\nfurther optimizes training efficiency by decoupling device-server computations\nand leveraging decomposed backward propagation. Experiments on GPT-2 with the\nMRPC, MNLI-matched, and SST-2 datasets demonstrate that SplitFrozen outperforms\nFedLoRA and SplitLoRA by 69.4\\% model accuracy under extremely imbalanced data,\nwhile reducing up to 86.8\\% device-side computations and 50.2\\% total training\ntime. Experiments also validate the scalability of SplitFrozen on content\ngeneration task using Llama-3.2 model on GSM8K dataset.\n","date":"2025-03-23"}
{"id":"2503.18987","title":"Balanced Direction from Multifarious Choices: Arithmetic Meta-Learning\n  for Domain Generalization","abstract":"  Domain generalization is proposed to address distribution shift, arising from\nstatistical disparities between training source and unseen target domains. The\nwidely used first-order meta-learning algorithms demonstrate strong performance\nfor domain generalization by leveraging the gradient matching theory, which\naims to establish balanced parameters across source domains to reduce\noverfitting to any particular domain. However, our analysis reveals that there\nare actually numerous directions to achieve gradient matching, with current\nmethods representing just one possible path. These methods actually overlook\nanother critical factor that the balanced parameters should be close to the\ncentroid of optimal parameters of each source domain. To address this, we\npropose a simple yet effective arithmetic meta-learning with\narithmetic-weighted gradients. This approach, while adhering to the principles\nof gradient matching, promotes a more precise balance by estimating the\ncentroid between domain-specific optimal parameters. Experimental results\nvalidate the effectiveness of our strategy.\n","date":"2025-03-23"}
{"id":"2503.18988","title":"SG-Tailor: Inter-Object Commonsense Relationship Reasoning for Scene\n  Graph Manipulation","abstract":"  Scene graphs capture complex relationships among objects, serving as strong\npriors for content generation and manipulation. Yet, reasonably manipulating\nscene graphs -- whether by adding nodes or modifying edges -- remains a\nchallenging and untouched task. Tasks such as adding a node to the graph or\nreasoning about a node's relationships with all others are computationally\nintractable, as even a single edge modification can trigger conflicts due to\nthe intricate interdependencies within the graph. To address these challenges,\nwe introduce SG-Tailor, an autoregressive model that predicts the conflict-free\nrelationship between any two nodes. SG-Tailor not only infers inter-object\nrelationships, including generating commonsense edges for newly added nodes but\nalso resolves conflicts arising from edge modifications to produce coherent,\nmanipulated graphs for downstream tasks. For node addition, the model queries\nthe target node and other nodes from the graph to predict the appropriate\nrelationships. For edge modification, SG-Tailor employs a Cut-And-Stitch\nstrategy to solve the conflicts and globally adjust the graph. Extensive\nexperiments demonstrate that SG-Tailor outperforms competing methods by a large\nmargin and can be seamlessly integrated as a plug-in module for scene\ngeneration and robotic manipulation tasks.\n","date":"2025-03-23"}
{"id":"2503.18989","title":"A Novel Hat-Shaped Device-Cloud Collaborative Inference Framework for\n  Large Language Models","abstract":"  Recent advancements in large language models (LLMs) have catalyzed a\nsubstantial surge in demand for LLM services. While traditional cloud-based LLM\nservices satisfy high-accuracy requirements, they fall short in meeting\ncritical demands for low delay and enhanced privacy. To address these\nlimitations, we propose HAT, a novel device-cloud collaborative inference\nframework that leverages the complementary strengths of U-shaped inference and\nspeculative decoding. HAT partitions the LLM into three submodels, and the\ninput and output submodels, stacked with a lightweight adapter network, are\ndeployed as a small language model (SLM) on each end device. Meanwhile, the\nmiddle submodel, encompassing the majority of the LLM's decoder layers, is\nhosted in the cloud to perform speculative decoding with on-device SLMs. During\ninference, HAT exchanges hidden states (rather than raw tokens) of input or\ndraft tokens between devices and the cloud, thereby incurring substantial\ncommunication delays. Besides, processing hidden states of long prompts will\nexacerbate computation delays in the cloud, further compromising inference\nefficiency. To improve efficiency, we introduce a prompt chunking mechanism\nthat segments long prompts into shorter chunks, enabling parallel transmission\nand processing. Furthermore, HAT is implemented to dynamically determine\noptimal chunk sizes for devices handling long prompts, thereby improving\noverall inference speed. Extensive experiments are conducted on a physical\ntestbed comprising 30 NVIDIA Jetson devices and a server with 8 NVIDIA A6000\nGPUs. Experimental results demonstrate that HAT achieves promising performance\nimprovements, reducing TTFT by 41% to 54% and TBT by 41% to 77% compared to the\nbaselines.\n","date":"2025-03-23"}
{"id":"2503.18991","title":"SRMIR: Shadow Reward Models Based on Introspective Reasoning for LLM\n  Alignment","abstract":"  Aligning large language models (LLMs) with human preferences and values is\nvital for application. However, current alignment methods face three main\nlimitations: (1) reliance on costly human annotation; (2) alignment tax; (3)\nshallow alignment vulnerable to jailbreak attacks. Additionally, current\nalignment datasets often suffer from uneven distributions, leading to\noverrepresentation of some topics and neglect of others. To address these\nissues, we propose SRMIR (Shadow Reward Models Based on Introspective\nReasoning), inspired by shadow models in membership inference attacks. We first\nconstruct a balanced safety Chain of Draft (CoD) dataset across $7$ harmful\ntypes with structured prompt leveraging the introspective reasoning\ncapabilities of LLMs, then train a set of specialized reward models to guide\npolicy optimization through Group Relative Policy Optimization (GRPO). We apply\ntwo strategies, linear combination and categorized approach, to integrate\nshadow reward models for policy optimization. By comparison, we find that the\nlatter achieves superior alignment despite higher computational costs.\nExperiments across several LLMs demonstrate SRMIR significantly outperforms\nexisting methods.\n","date":"2025-03-23"}
{"id":"2503.18994","title":"HH4AI: A methodological Framework for AI Human Rights impact assessment\n  under the EUAI ACT","abstract":"  This paper introduces the HH4AI Methodology, a structured approach to\nassessing the impact of AI systems on human rights, focusing on compliance with\nthe EU AI Act and addressing technical, ethical, and regulatory challenges. The\npaper highlights AIs transformative nature, driven by autonomy, data, and\ngoal-oriented design, and how the EU AI Act promotes transparency,\naccountability, and safety. A key challenge is defining and assessing\n\"high-risk\" AI systems across industries, complicated by the lack of\nuniversally accepted standards and AIs rapid evolution.\n  To address these challenges, the paper explores the relevance of ISO\/IEC and\nIEEE standards, focusing on risk management, data quality, bias mitigation, and\ngovernance. It proposes a Fundamental Rights Impact Assessment (FRIA)\nmethodology, a gate-based framework designed to isolate and assess risks\nthrough phases including an AI system overview, a human rights checklist, an\nimpact assessment, and a final output phase. A filtering mechanism tailors the\nassessment to the system's characteristics, targeting areas like\naccountability, AI literacy, data governance, and transparency.\n  The paper illustrates the FRIA methodology through a fictional case study of\nan automated healthcare triage service. The structured approach enables\nsystematic filtering, comprehensive risk assessment, and mitigation planning,\neffectively prioritizing critical risks and providing clear remediation\nstrategies. This promotes better alignment with human rights principles and\nenhances regulatory compliance.\n","date":"2025-03-23"}
{"id":"2503.18995","title":"LLMs in the Classroom: Outcomes and Perceptions of Questions Written\n  with the Aid of AI","abstract":"  We randomly deploy questions constructed with and without use of the LLM tool\nand gauge the ability of the students to correctly answer, as well as their\nability to correctly perceive the difference between human-authored and\nLLM-authored questions. In determining whether the questions written with the\naid of ChatGPT were consistent with the instructor's questions and source text,\nwe computed representative vectors of both the human and ChatGPT questions\nusing SBERT and compared cosine similarity to the course textbook. A\nnon-significant Mann-Whitney U test (z = 1.018, p = .309) suggests that\nstudents were unable to perceive whether questions were written with or without\nthe aid of ChatGPT. However, student scores on LLM-authored questions were\nalmost 9% lower (z = 2.702, p < .01). This result may indicate that either the\nAI questions were more difficult or that the students were more familiar with\nthe instructor's style of questions. Overall, the study suggests that while\nthere is potential for using LLM tools to aid in the construction of\nassessments, care must be taken to ensure that the questions are fair,\nwell-composed, and relevant to the course material.\n","date":"2025-03-23"}
{"id":"2503.18996","title":"Enhanced prediction of spine surgery outcomes using advanced machine\n  learning techniques and oversampling methods","abstract":"  The study proposes an advanced machine learning approach to predict spine\nsurgery outcomes by incorporating oversampling techniques and grid search\noptimization. A variety of models including GaussianNB, ComplementNB, KNN,\nDecision Tree, and optimized versions with RandomOverSampler and SMOTE were\ntested on a dataset of 244 patients, which included pre-surgical, psychometric,\nsocioeconomic, and analytical variables. The enhanced KNN models achieved up to\n76% accuracy and a 67% F1-score, while grid-search optimization further\nimproved performance. The findings underscore the potential of these advanced\ntechniques to aid healthcare professionals in decision-making, with future\nresearch needed to refine these models on larger and more diverse datasets.\n","date":"2025-03-23"}
{"id":"2503.18997","title":"Improving Food Image Recognition with Noisy Vision Transformer","abstract":"  Food image recognition is a challenging task in computer vision due to the\nhigh variability and complexity of food images. In this study, we investigate\nthe potential of Noisy Vision Transformers (NoisyViT) for improving food\nclassification performance. By introducing noise into the learning process,\nNoisyViT reduces task complexity and adjusts the entropy of the system, leading\nto enhanced model accuracy. We fine-tune NoisyViT on three benchmark datasets:\nFood2K (2,000 categories, ~1M images), Food-101 (101 categories, ~100K images),\nand CNFOOD-241 (241 categories, ~190K images). The performance of NoisyViT is\nevaluated against state-of-the-art food recognition models. Our results\ndemonstrate that NoisyViT achieves Top-1 accuracies of 95%, 99.5%, and 96.6% on\nFood2K, Food-101, and CNFOOD-241, respectively, significantly outperforming\nexisting approaches. This study underscores the potential of NoisyViT for\ndietary assessment, nutritional monitoring, and healthcare applications, paving\nthe way for future advancements in vision-based food computing. Code for\nreproducing NoisyViT for food recognition is available at NoisyViT_Food.\n","date":"2025-03-24"}
{"id":"2503.18998","title":"FACE: Few-shot Adapter with Cross-view Fusion for Cross-subject EEG\n  Emotion Recognition","abstract":"  Cross-subject EEG emotion recognition is challenged by significant\ninter-subject variability and intricately entangled intra-subject variability.\nExisting works have primarily addressed these challenges through domain\nadaptation or generalization strategies. However, they typically require\nextensive target subject data or demonstrate limited generalization performance\nto unseen subjects. Recent few-shot learning paradigms attempt to address these\nlimitations but often encounter catastrophic overfitting during\nsubject-specific adaptation with limited samples. This article introduces the\nfew-shot adapter with a cross-view fusion method called FACE for cross-subject\nEEG emotion recognition, which leverages dynamic multi-view fusion and\neffective subject-specific adaptation. Specifically, FACE incorporates a\ncross-view fusion module that dynamically integrates global brain connectivity\nwith localized patterns via subject-specific fusion weights to provide\ncomplementary emotional information. Moreover, the few-shot adapter module is\nproposed to enable rapid adaptation for unseen subjects while reducing\noverfitting by enhancing adapter structures with meta-learning. Experimental\nresults on three public EEG emotion recognition benchmarks demonstrate FACE's\nsuperior generalization performance over state-of-the-art methods. FACE\nprovides a practical solution for cross-subject scenarios with limited labeled\ndata.\n","date":"2025-03-24"}
{"id":"2503.18999","title":"Near-optimal Active Reconstruction","abstract":"  With the growing practical interest in vision-based tasks for autonomous\nsystems, the need for efficient and complex methods becomes increasingly\nlarger. In the rush to develop new methods with the aim to outperform the\ncurrent state of the art, an analysis of the underlying theory is often\nneglected and simply replaced with empirical evaluations in simulated or\nreal-world experiments. While such methods might yield favorable performance in\npractice, they are often less well understood, which prevents them from being\napplied in safety-critical systems. The goal of this work is to design an\nalgorithm for the Next Best View (NBV) problem in the context of active object\nreconstruction, for which we can provide qualitative performance guarantees\nwith respect to true optimality. To the best of our knowledge, no previous work\nin this field addresses such an analysis for their proposed methods. Based on\nexisting work on Gaussian process optimization, we rigorously derive sublinear\nbounds for the cumulative regret of our algorithm, which guarantees\nnear-optimality. Complementing this, we evaluate the performance of our\nalgorithm empirically within our simulation framework. We further provide\nadditional insights through an extensive study of potential objective functions\nand analyze the differences to the results of related work.\n","date":"2025-03-24"}
{"id":"2503.19001","title":"DisentTalk: Cross-lingual Talking Face Generation via Semantic\n  Disentangled Diffusion Model","abstract":"  Recent advances in talking face generation have significantly improved facial\nanimation synthesis. However, existing approaches face fundamental limitations:\n3DMM-based methods maintain temporal consistency but lack fine-grained regional\ncontrol, while Stable Diffusion-based methods enable spatial manipulation but\nsuffer from temporal inconsistencies. The integration of these approaches is\nhindered by incompatible control mechanisms and semantic entanglement of facial\nrepresentations. This paper presents DisentTalk, introducing a data-driven\nsemantic disentanglement framework that decomposes 3DMM expression parameters\ninto meaningful subspaces for fine-grained facial control. Building upon this\ndisentangled representation, we develop a hierarchical latent diffusion\narchitecture that operates in 3DMM parameter space, integrating region-aware\nattention mechanisms to ensure both spatial precision and temporal coherence.\nTo address the scarcity of high-quality Chinese training data, we introduce\nCHDTF, a Chinese high-definition talking face dataset. Extensive experiments\nshow superior performance over existing methods across multiple metrics,\nincluding lip synchronization, expression quality, and temporal consistency.\nProject Page: https:\/\/kangweiiliu.github.io\/DisentTalk.\n","date":"2025-03-24"}
{"id":"2503.19002","title":"Quantum Complex-Valued Self-Attention Model","abstract":"  The self-attention mechanism has revolutionized classical machine learning,\nyet its quantum counterpart remains underexplored in fully harnessing the\nrepresentational power of quantum states. Current quantum self-attention models\nexhibit a critical limitation by neglecting the indispensable phase information\ninherent in quantum systems when compressing attention weights into real-valued\noverlaps. To address this fundamental gap, we propose the Quantum\nComplex-Valued Self-Attention Model (QCSAM), the first framework that\nexplicitly leverages complex-valued similarities between quantum states to\ncapture both amplitude and phase relationships. Simultaneously, we enhance the\nstandard Linear Combination of Unitaries (LCUs) method by introducing a Complex\nLCUs (CLCUs) framework that natively supports complex-valued coefficients. This\nframework enables the weighting of corresponding quantum values using fixed\nquantum complex self-attention weights, while also supporting trainable\ncomplex-valued parameters for value aggregation and quantum multi-head\nattention. Experimental evaluations on MNIST and Fashion-MNIST demonstrate our\nmodel's superiority over recent quantum self-attention architectures including\nQKSAN, QSAN, and GQHAN, with multi-head configurations showing consistent\nadvantages over single-head variants. We systematically evaluate model\nscalability through qubit configurations ranging from 3 to 8 qubits and\nmulti-class classification tasks spanning 2 to 4 categories. Through\ncomprehensive ablation studies, we establish the critical advantage of\ncomplex-valued quantum attention weights over real-valued alternatives.\n","date":"2025-03-24"}
{"id":"2503.19005","title":"Foundation Model for Whole-Heart Segmentation: Leveraging\n  Student-Teacher Learning in Multi-Modal Medical Imaging","abstract":"  Whole-heart segmentation from CT and MRI scans is crucial for cardiovascular\ndisease analysis, yet existing methods struggle with modality-specific biases\nand the need for extensive labeled datasets. To address these challenges, we\npropose a foundation model for whole-heart segmentation using a self-supervised\nlearning (SSL) framework based on a student-teacher architecture. Our model is\npretrained on a large, unlabeled dataset of CT and MRI scans, leveraging the\nxLSTM backbone to capture long-range spatial dependencies and complex\nanatomical structures in 3D medical images. By incorporating multi-modal\npretraining, our approach ensures strong generalization across both CT and MRI\nmodalities, mitigating modality-specific variations and improving segmentation\naccuracy in diverse clinical settings. The use of large-scale unlabeled data\nsignificantly reduces the dependency on manual annotations, enabling robust\nperformance even with limited labeled data. We further introduce an\nxLSTM-UNet-based architecture for downstream whole-heart segmentation tasks,\ndemonstrating its effectiveness on few-label CT and MRI datasets. Our results\nvalidate the robustness and adaptability of the proposed model, highlighting\nits potential for advancing automated whole-heart segmentation in medical\nimaging.\n","date":"2025-03-24"}
{"id":"2503.19006","title":"Computational Thinking with Computer Vision: Developing AI Competency in\n  an Introductory Computer Science Course","abstract":"  Developing competency in artificial intelligence is becoming increasingly\ncrucial for computer science (CS) students at all levels of the CS curriculum.\nHowever, most previous research focuses on advanced CS courses, as traditional\nintroductory courses provide limited opportunities to develop AI skills and\nknowledge. This paper introduces an introductory CS course where students learn\ncomputational thinking through computer vision, a sub-field of AI, as an\napplication context. The course aims to achieve computational thinking outcomes\nalongside critical thinking outcomes that expose students to AI approaches and\ntheir societal implications. Through experiential activities such as individual\nprojects and reading discussions, our course seeks to balance technical\nlearning and critical thinking goals. Our evaluation, based on pre-and\npost-course surveys, shows an improved sense of belonging, self-efficacy, and\nAI ethics awareness among students. The results suggest that an AI-focused\ncontext can enhance participation and employability, student-selected projects\nsupport self-efficacy, and ethically grounded AI instruction can be effective\nfor interdisciplinary audiences. Students' discussions on reading assignments\ndemonstrated deep engagement with the complex challenges in today's AI\nlandscape. Finally, we share insights on scaling such courses for larger\ncohorts and improving the learning experience for introductory CS students.\n","date":"2025-03-24"}
{"id":"2503.19007","title":"Option Discovery Using LLM-guided Semantic Hierarchical Reinforcement\n  Learning","abstract":"  Large Language Models (LLMs) have shown remarkable promise in reasoning and\ndecision-making, yet their integration with Reinforcement Learning (RL) for\ncomplex robotic tasks remains underexplored. In this paper, we propose an\nLLM-guided hierarchical RL framework, termed LDSC, that leverages LLM-driven\nsubgoal selection and option reuse to enhance sample efficiency,\ngeneralization, and multi-task adaptability. Traditional RL methods often\nsuffer from inefficient exploration and high computational cost. Hierarchical\nRL helps with these challenges, but existing methods often fail to reuse\noptions effectively when faced with new tasks. To address these limitations, we\nintroduce a three-stage framework that uses LLMs for subgoal generation given\nnatural language description of the task, a reusable option learning and\nselection method, and an action-level policy, enabling more effective\ndecision-making across diverse tasks. By incorporating LLMs for subgoal\nprediction and policy guidance, our approach improves exploration efficiency\nand enhances learning performance. On average, LDSC outperforms the baseline by\n55.9\\% in average reward, demonstrating its effectiveness in complex RL\nsettings. More details and experiment videos could be found in\n\\href{https:\/\/raaslab.org\/projects\/LDSC\/}{this\nlink\\footnote{https:\/\/raaslab.org\/projects\/LDSC}}.\n","date":"2025-03-24"}
{"id":"2503.19009","title":"Video-ColBERT: Contextualized Late Interaction for Text-to-Video\n  Retrieval","abstract":"  In this work, we tackle the problem of text-to-video retrieval (T2VR).\nInspired by the success of late interaction techniques in text-document,\ntext-image, and text-video retrieval, our approach, Video-ColBERT, introduces a\nsimple and efficient mechanism for fine-grained similarity assessment between\nqueries and videos. Video-ColBERT is built upon 3 main components: a\nfine-grained spatial and temporal token-wise interaction, query and visual\nexpansions, and a dual sigmoid loss during training. We find that this\ninteraction and training paradigm leads to strong individual, yet compatible,\nrepresentations for encoding video content. These representations lead to\nincreases in performance on common text-to-video retrieval benchmarks compared\nto other bi-encoder methods.\n","date":"2025-03-24"}
{"id":"2503.19011","title":"RomanTex: Decoupling 3D-aware Rotary Positional Embedded Multi-Attention\n  Network for Texture Synthesis","abstract":"  Painting textures for existing geometries is a critical yet labor-intensive\nprocess in 3D asset generation. Recent advancements in text-to-image (T2I)\nmodels have led to significant progress in texture generation. Most existing\nresearch approaches this task by first generating images in 2D spaces using\nimage diffusion models, followed by a texture baking process to achieve UV\ntexture. However, these methods often struggle to produce high-quality textures\ndue to inconsistencies among the generated multi-view images, resulting in\nseams and ghosting artifacts. In contrast, 3D-based texture synthesis methods\naim to address these inconsistencies, but they often neglect 2D diffusion model\npriors, making them challenging to apply to real-world objects To overcome\nthese limitations, we propose RomanTex, a multiview-based texture generation\nframework that integrates a multi-attention network with an underlying 3D\nrepresentation, facilitated by our novel 3D-aware Rotary Positional Embedding.\nAdditionally, we incorporate a decoupling characteristic in the multi-attention\nblock to enhance the model's robustness in image-to-texture task, enabling\nsemantically-correct back-view synthesis. Furthermore, we introduce a\ngeometry-related Classifier-Free Guidance (CFG) mechanism to further improve\nthe alignment with both geometries and images. Quantitative and qualitative\nevaluations, along with comprehensive user studies, demonstrate that our method\nachieves state-of-the-art results in texture quality and consistency.\n","date":"2025-03-24"}
{"id":"2503.19012","title":"DiffV2IR: Visible-to-Infrared Diffusion Model via Vision-Language\n  Understanding","abstract":"  The task of translating visible-to-infrared images (V2IR) is inherently\nchallenging due to three main obstacles: 1) achieving semantic-aware\ntranslation, 2) managing the diverse wavelength spectrum in infrared imagery,\nand 3) the scarcity of comprehensive infrared datasets. Current leading methods\ntend to treat V2IR as a conventional image-to-image synthesis challenge, often\noverlooking these specific issues. To address this, we introduce DiffV2IR, a\nnovel framework for image translation comprising two key elements: a\nProgressive Learning Module (PLM) and a Vision-Language Understanding Module\n(VLUM). PLM features an adaptive diffusion model architecture that leverages\nmulti-stage knowledge learning to infrared transition from full-range to target\nwavelength. To improve V2IR translation, VLUM incorporates unified\nVision-Language Understanding. We also collected a large infrared dataset,\nIR-500K, which includes 500,000 infrared images compiled by various scenes and\nobjects under various environmental conditions. Through the combination of PLM,\nVLUM, and the extensive IR-500K dataset, DiffV2IR markedly improves the\nperformance of V2IR. Experiments validate DiffV2IR's excellence in producing\nhigh-quality translations, establishing its efficacy and broad applicability.\nThe code, dataset, and DiffV2IR model will be available at\nhttps:\/\/github.com\/LidongWang-26\/DiffV2IR.\n","date":"2025-03-24"}
{"id":"2503.19034","title":"Color Conditional Generation with Sliced Wasserstein Guidance","abstract":"  We propose SW-Guidance, a training-free approach for image generation\nconditioned on the color distribution of a reference image. While it is\npossible to generate an image with fixed colors by first creating an image from\na text prompt and then applying a color style transfer method, this approach\noften results in semantically meaningless colors in the generated image. Our\nmethod solves this problem by modifying the sampling process of a diffusion\nmodel to incorporate the differentiable Sliced 1-Wasserstein distance between\nthe color distribution of the generated image and the reference palette. Our\nmethod outperforms state-of-the-art techniques for color-conditional generation\nin terms of color similarity to the reference, producing images that not only\nmatch the reference colors but also maintain semantic coherence with the\noriginal text prompt. Our source code is available at\nhttps:\/\/github.com\/alobashev\/sw-guidance\/.\n","date":"2025-03-24"}
{"id":"2503.19037","title":"Evolutionary Policy Optimization","abstract":"  Despite its extreme sample inefficiency, on-policy reinforcement learning has\nbecome a fundamental tool in real-world applications. With recent advances in\nGPU-driven simulation, the ability to collect vast amounts of data for RL\ntraining has scaled exponentially. However, studies show that current on-policy\nmethods, such as PPO, fail to fully leverage the benefits of parallelized\nenvironments, leading to performance saturation beyond a certain scale. In\ncontrast, Evolutionary Algorithms (EAs) excel at increasing diversity through\nrandomization, making them a natural complement to RL. However, existing EvoRL\nmethods have struggled to gain widespread adoption due to their extreme sample\ninefficiency. To address these challenges, we introduce Evolutionary Policy\nOptimization (EPO), a novel policy gradient algorithm that combines the\nstrengths of EA and policy gradients. We show that EPO significantly improves\nperformance across diverse and challenging environments, demonstrating superior\nscalability with parallelized simulations.\n","date":"2025-03-24"}
{"id":"2503.19041","title":"LookAhead Tuning: Safer Language Models via Partial Answer Previews","abstract":"  Fine-tuning enables large language models (LLMs) to adapt to specific\ndomains, but often undermines their previously established safety alignment. To\nmitigate the degradation of model safety during fine-tuning, we introduce\nLookAhead Tuning, which comprises two simple, low-resource, and effective\ndata-driven methods that modify training data by previewing partial answer\nprefixes. Both methods aim to preserve the model's inherent safety mechanisms\nby minimizing perturbations to initial token distributions. Comprehensive\nexperiments demonstrate that LookAhead Tuning effectively maintains model\nsafety without sacrificing robust performance on downstream tasks. Our findings\nposition LookAhead Tuning as a reliable and efficient solution for the safe and\neffective adaptation of LLMs. Code is released at\nhttps:\/\/github.com\/zjunlp\/LookAheadTuning.\n","date":"2025-03-24"}
{"id":"2503.19043","title":"Enhancing Symbolic Regression with Quality-Diversity and\n  Physics-Inspired Constraints","abstract":"  This paper presents QDSR, an advanced symbolic Regression (SR) system that\nintegrates genetic programming (GP), a quality-diversity (QD) algorithm, and a\ndimensional analysis (DA) engine. Our method focuses on exact symbolic recovery\nof known expressions from datasets, with a particular emphasis on the\nFeynman-AI benchmark. On this widely used collection of 117 physics equations,\nQDSR achieves an exact recovery rate of 91.6~$\\%$, surpassing all previous SR\nmethods by over 20 percentage points. Our method also exhibits strong\nrobustness to noise. Beyond QD and DA, this high success rate results from a\nprofitable trade-off between vocabulary expressiveness and search space size:\nwe show that significantly expanding the vocabulary with precomputed meaningful\nvariables (e.g., dimensionless combinations and well-chosen scalar products)\noften reduces equation complexity, ultimately leading to better performance.\nAblation studies will also show that QD alone already outperforms the\nstate-of-the-art. This suggests that a simple integration of QD, by projecting\nindividuals onto a QD grid, can significantly boost performance in existing\nalgorithms, without requiring major system overhauls.\n","date":"2025-03-24"}
{"id":"2503.19046","title":"Learning Beamforming Codebooks for Active Sensing with Reconfigurable\n  Intelligent Surface","abstract":"  This paper explores the design of beamforming codebooks for the base station\n(BS) and for the reconfigurable intelligent surfaces (RISs) in an active\nsensing scheme for uplink localization, in which the mobile user transmits a\nsequence of pilots to the BS through reflection at the RISs, and the BS and the\nRISs are adaptively configured by carefully choosing BS beamforming codeword\nand RIS codewords from their respective codebooks in a sequential manner to\nprogressively focus onto the user. Most existing codebook designs for RIS are\nnot tailored for active sensing, by which we mean the choice of the next\ncodeword should depend on the measurements made so far, and the sequence of\ncodewords should dynamically focus reflection toward the user. Moreover, most\nexisting codeword selection methods rely on exhaustive search in beam training\nto identify the codeword with the highest signal-to-noise ratio (SNR), thus\nincurring substantial pilot overhead as the size of the codebook scales. This\npaper proposes a learning-based approach for codebook construction and for\ncodeword selection for active sensing. The proposed learning approach aims to\nlocate a target in the service area by recursively selecting a sequence of BS\nbeamforming codewords and RIS codewords from the respective codebooks as more\nmeasurements become available without exhaustive beam training. The codebook\ndesign and the codeword selection fuse key ideas from the vector quantized\nvariational autoencoder (VQ-VAE) and the long short-term memory (LSTM) network\nto learn respectively the discrete function space of the codebook and the\ntemporal dependencies between measurements.\n","date":"2025-03-24"}
{"id":"2503.19048","title":"Forecasting Labor Demand: Predicting JOLT Job Openings using Deep\n  Learning Model","abstract":"  This thesis studies the effectiveness of Long Short Term Memory model in\nforecasting future Job Openings and Labor Turnover Survey data in the United\nStates. Drawing on multiple economic indicators from various sources, the data\nare fed directly into LSTM model to predict JOLT job openings in subsequent\nperiods. The performance of the LSTM model is compared with conventional\nautoregressive approaches, including ARIMA, SARIMA, and Holt-Winters. Findings\nsuggest that the LSTM model outperforms these traditional models in predicting\nJOLT job openings, as it not only captures the dependent variables trends but\nalso harmonized with key economic factors. These results highlight the\npotential of deep learning techniques in capturing complex temporal\ndependencies in economic data, offering valuable insights for policymakers and\nstakeholders in developing data-driven labor market strategies\n","date":"2025-03-24"}
{"id":"2503.19050","title":"Mist: Efficient Distributed Training of Large Language Models via\n  Memory-Parallelism Co-Optimization","abstract":"  Various parallelism, such as data, tensor, and pipeline parallelism, along\nwith memory optimizations like activation checkpointing, redundancy\nelimination, and offloading, have been proposed to accelerate distributed\ntraining for Large Language Models. To find the best combination of these\ntechniques, automatic distributed training systems are proposed. However,\nexisting systems only tune a subset of optimizations, due to the lack of\noverlap awareness, inability to navigate the vast search space, and ignoring\nthe inter-microbatch imbalance, leading to sub-optimal performance. To address\nthese shortcomings, we propose Mist, a memory, overlap, and imbalance-aware\nautomatic distributed training system that comprehensively co-optimizes all\nmemory footprint reduction techniques alongside parallelism. Mist is based on\nthree key ideas: (1) fine-grained overlap-centric scheduling, orchestrating\noptimizations in an overlapped manner, (2) symbolic-based performance analysis\nthat predicts runtime and memory usage using symbolic expressions for fast\ntuning, and (3) imbalance-aware hierarchical tuning, decoupling the process\ninto an inter-stage imbalance and overlap aware Mixed Integer Linear\nProgramming problem and an intra-stage Dual-Objective Constrained Optimization\nproblem, and connecting them through Pareto frontier sampling. Our evaluation\nresults show that Mist achieves an average of 1.28$\\times$ (up to 1.73$\\times$)\nand 1.27$\\times$ (up to 2.04$\\times$) speedup compared to state-of-the-art\nmanual system Megatron-LM and state-of-the-art automatic system Aceso,\nrespectively.\n","date":"2025-03-24"}
{"id":"2503.19062","title":"Color Transfer with Modulated Flows","abstract":"  In this work, we introduce Modulated Flows (ModFlows), a novel approach for\ncolor transfer between images based on rectified flows. The primary goal of the\ncolor transfer is to adjust the colors of a target image to match the color\ndistribution of a reference image. Our technique is based on optimal transport\nand executes color transfer as an invertible transformation within the RGB\ncolor space. The ModFlows utilizes the bijective property of flows, enabling us\nto introduce a common intermediate color distribution and build a dataset of\nrectified flows. We train an encoder on this dataset to predict the weights of\na rectified model for new images. After training on a set of optimal transport\nplans, our approach can generate plans for new pairs of distributions without\nadditional fine-tuning. We additionally show that the trained encoder provides\nan image embedding, associated only with its color style. The presented method\nis capable of processing 4K images and achieves the state-of-the-art\nperformance in terms of content and style similarity. Our source code is\navailable at https:\/\/github.com\/maria-larchenko\/modflows\n","date":"2025-03-24"}
{"id":"2503.19065","title":"WikiAutoGen: Towards Multi-Modal Wikipedia-Style Article Generation","abstract":"  Knowledge discovery and collection are intelligence-intensive tasks that\ntraditionally require significant human effort to ensure high-quality outputs.\nRecent research has explored multi-agent frameworks for automating\nWikipedia-style article generation by retrieving and synthesizing information\nfrom the internet. However, these methods primarily focus on text-only\ngeneration, overlooking the importance of multimodal content in enhancing\ninformativeness and engagement. In this work, we introduce WikiAutoGen, a novel\nsystem for automated multimodal Wikipedia-style article generation. Unlike\nprior approaches, WikiAutoGen retrieves and integrates relevant images\nalongside text, enriching both the depth and visual appeal of generated\ncontent. To further improve factual accuracy and comprehensiveness, we propose\na multi-perspective self-reflection mechanism, which critically assesses\nretrieved content from diverse viewpoints to enhance reliability, breadth, and\ncoherence, etc. Additionally, we introduce WikiSeek, a benchmark comprising\nWikipedia articles with topics paired with both textual and image-based\nrepresentations, designed to evaluate multimodal knowledge generation on more\nchallenging topics. Experimental results show that WikiAutoGen outperforms\nprevious methods by 8%-29% on our WikiSeek benchmark, producing more accurate,\ncoherent, and visually enriched Wikipedia-style articles. We show some of our\ngenerated examples in https:\/\/wikiautogen.github.io\/ .\n","date":"2025-03-24"}
{"id":"2503.19066","title":"Accelerating Langevin Monte Carlo Sampling: A Large Deviations Analysis","abstract":"  Langevin algorithms are popular Markov chain Monte Carlo methods that are\noften used to solve high-dimensional large-scale sampling problems in machine\nlearning. The most classical Langevin Monte Carlo algorithm is based on the\noverdamped Langevin dynamics. There are many variants of Langevin dynamics that\noften show superior performance in practice. In this paper, we provide a\nunified approach to study the acceleration of the variants of the overdamped\nLangevin dynamics through the lens of large deviations theory. Numerical\nexperiments using both synthetic and real data are provided to illustrate the\nefficiency of these variants.\n","date":"2025-03-24"}
{"id":"2503.19067","title":"Clustering data by reordering them","abstract":"  Grouping elements into families to analyse them separately is a standard\nanalysis procedure in many areas of sciences. We propose herein a new algorithm\nbased on the simple idea that members from a family look like each other, and\ndon't resemble elements foreign to the family. After reordering the data\naccording to the distance between elements, the analysis is automatically\nperformed with easily-understandable parameters. Noise is explicitly taken into\naccount to deal with the variety of problems of a data-driven world. We applied\nthe algorithm to sort biomolecules conformations, gene sequences, cells,\nimages, and experimental conditions.\n","date":"2025-03-24"}
{"id":"2503.19068","title":"Minimum Volume Conformal Sets for Multivariate Regression","abstract":"  Conformal prediction provides a principled framework for constructing\npredictive sets with finite-sample validity. While much of the focus has been\non univariate response variables, existing multivariate methods either impose\nrigid geometric assumptions or rely on flexible but computationally expensive\napproaches that do not explicitly optimize prediction set volume. We propose an\noptimization-driven framework based on a novel loss function that directly\nlearns minimum-volume covering sets while ensuring valid coverage. This\nformulation naturally induces a new nonconformity score for conformal\nprediction, which adapts to the residual distribution and covariates. Our\napproach optimizes over prediction sets defined by arbitrary norm balls,\nincluding single and multi-norm formulations. Additionally, by jointly\noptimizing both the predictive model and predictive uncertainty, we obtain\nprediction sets that are tight, informative, and computationally efficient, as\ndemonstrated in our experiments on real-world datasets.\n","date":"2025-03-24"}
{"id":"2503.19069","title":"Detecting Arbitrary Planted Subgraphs in Random Graphs","abstract":"  The problems of detecting and recovering planted structures\/subgraphs in\nErd\\H{o}s-R\\'{e}nyi random graphs, have received significant attention over the\npast three decades, leading to many exciting results and mathematical\ntechniques. However, prior work has largely focused on specific ad hoc planted\nstructures and inferential settings, while a general theory has remained\nelusive. In this paper, we bridge this gap by investigating the detection of an\n\\emph{arbitrary} planted subgraph $\\Gamma = \\Gamma_n$ in an Erd\\H{o}s-R\\'{e}nyi\nrandom graph $\\mathcal{G}(n, q_n)$, where the edge probability within $\\Gamma$\nis $p_n$. We examine both the statistical and computational aspects of this\nproblem and establish the following results. In the dense regime, where the\nedge probabilities $p_n$ and $q_n$ are fixed, we tightly characterize the\ninformation-theoretic and computational thresholds for detecting $\\Gamma$, and\nprovide conditions under which a computational-statistical gap arises. Most\nnotably, these thresholds depend on $\\Gamma$ only through its number of edges,\nmaximum degree, and maximum subgraph density. Our lower and upper bounds are\ngeneral and apply to any value of $p_n$ and $q_n$ as functions of $n$.\nAccordingly, we also analyze the sparse regime where $q_n =\n\\Theta(n^{-\\alpha})$ and $p_n-q_n =\\Theta(q_n)$, with $\\alpha\\in[0,2]$, as well\nas the critical regime where $p_n=1-o(1)$ and $q_n = \\Theta(n^{-\\alpha})$, both\nof which have been widely studied, for specific choices of $\\Gamma$. For these\nregimes, we show that our bounds are tight for all planted subgraphs\ninvestigated in the literature thus far\\textemdash{}and many more. Finally, we\nidentify conditions under which detection undergoes sharp phase transition,\nwhere the boundaries at which algorithms succeed or fail shift abruptly as a\nfunction of $q_n$.\n","date":"2025-03-24"}
{"id":"2503.19070","title":"Graph-Level Label-Only Membership Inference Attack against Graph Neural\n  Networks","abstract":"  Graph neural networks (GNNs) are widely used for graph-structured data but\nare vulnerable to membership inference attacks (MIAs) in graph classification\ntasks, which determine if a graph was part of the training dataset, potentially\ncausing data leakage. Existing MIAs rely on prediction probability vectors, but\nthey become ineffective when only prediction labels are available. We propose a\nGraph-level Label-Only Membership Inference Attack (GLO-MIA), which is based on\nthe intuition that the target model's predictions on training data are more\nstable than those on testing data. GLO-MIA generates a set of perturbed graphs\nfor target graph by adding perturbations to its effective features and queries\nthe target model with the perturbed graphs to get their prediction labels,\nwhich are then used to calculate robustness score of the target graph. Finally,\nby comparing the robustness score with a predefined threshold, the membership\nof the target graph can be inferred correctly with high probability. Our\nevaluation on three datasets and four GNN models shows that GLO-MIA achieves an\nattack accuracy of up to 0.825, outperforming baseline work by 8.5% and closely\nmatching the performance of probability-based MIAs, even with only prediction\nlabels.\n","date":"2025-03-24"}
{"id":"2503.19074","title":"HingeRLC-GAN: Combating Mode Collapse with Hinge Loss and RLC\n  Regularization","abstract":"  Recent advances in Generative Adversarial Networks (GANs) have demonstrated\ntheir capability for producing high-quality images. However, a significant\nchallenge remains mode collapse, which occurs when the generator produces a\nlimited number of data patterns that do not reflect the diversity of the\ntraining dataset. This study addresses this issue by proposing a number of\narchitectural changes aimed at increasing the diversity and stability of GAN\nmodels. We start by improving the loss function with Wasserstein loss and\nGradient Penalty to better capture the full range of data variations. We also\ninvestigate various network architectures and conclude that ResNet\nsignificantly contributes to increased diversity. Building on these findings,\nwe introduce HingeRLC-GAN, a novel approach that combines RLC Regularization\nand the Hinge loss function. With a FID Score of 18 and a KID Score of 0.001,\nour approach outperforms existing methods by effectively balancing training\nstability and increased diversity.\n","date":"2025-03-24"}
{"id":"2503.19075","title":"The Case for \"Thick Evaluations\" of Cultural Representation in AI","abstract":"  Generative AI image models have been increasingly evaluated for their\n(in)ability to represent non-Western cultures. We argue that these evaluations\noperate through reductive ideals of representation, abstracted from how people\ndefine their own representation and neglecting the inherently interpretive and\ncontextual nature of cultural representation. In contrast to these 'thin'\nevaluations, we introduce the idea of 'thick evaluations': a more granular,\nsituated, and discursive measurement framework for evaluating representations\nof social worlds in AI images, steeped in communities' own understandings of\nrepresentation. We develop this evaluation framework through workshops in South\nAsia, by studying the 'thick' ways in which people interpret and assign meaning\nto images of their own cultures. We introduce practices for thicker evaluations\nof representation that expand the understanding of representation underpinning\nAI evaluations and by co-constructing metrics with communities, bringing\nmeasurement in line with the experiences of communities on the ground.\n","date":"2025-03-24"}
{"id":"2503.19081","title":"Paving the way for scientific foundation models: enhancing\n  generalization and robustness in PDEs with constraint-aware pre-training","abstract":"  Partial differential equations (PDEs) govern a wide range of physical\nsystems, but solving them efficiently remains a major challenge. The idea of a\nscientific foundation model (SciFM) is emerging as a promising tool for\nlearning transferable representations across diverse domains. However, SciFMs\nrequire large amounts of solution data, which may be scarce or computationally\nexpensive to generate. To maximize generalization while reducing data\ndependence, we propose incorporating PDE residuals into pre-training either as\nthe sole learning signal or in combination with data loss to compensate for\nlimited or infeasible training data. We evaluate this constraint-aware\npre-training across three key benchmarks: (i) generalization to new physics,\nwhere material properties, e.g., the diffusion coefficient, is shifted with\nrespect to the training distribution; (ii) generalization to entirely new PDEs,\nrequiring adaptation to different operators; and (iii) robustness against noisy\nfine-tuning data, ensuring stability in real-world applications. Our results\nshow that pre-training with PDE constraints significantly enhances\ngeneralization, outperforming models trained solely on solution data across all\nbenchmarks. These findings prove the effectiveness of our proposed\nconstraint-aware pre-training as a crucial component for SciFMs, providing a\nscalable approach to data-efficient, generalizable PDE solvers.\n","date":"2025-03-24"}
{"id":"2503.19083","title":"3D Structural Phenotype of the Optic Nerve Head at the Intersection of\n  Glaucoma and Myopia -- A Key to Improving Glaucoma Diagnosis in Myopic\n  Populations","abstract":"  Purpose: To characterize the 3D structural phenotypes of the optic nerve head\n(ONH) in patients with glaucoma, high myopia, and concurrent high myopia and\nglaucoma, and to evaluate their variations across these conditions.\nParticipants: A total of 685 optical coherence tomography (OCT) scans from 754\nsubjects of Singapore-Chinese ethnicity, including 256 healthy (H), 94 highly\nmyopic (HM), 227 glaucomatous (G), and 108 highly myopic with glaucoma (HMG)\ncases. Methods: We segmented the retinal and connective tissues from OCT\nvolumes and their boundary edges were converted into 3D point clouds. To\nclassify the 3D point clouds into four ONH conditions, i.e., H, HM, G, and HMG,\na specialized ensemble network was developed, consisting of an encoder to\ntransform high-dimensional input data into a compressed latent vector, a\ndecoder to reconstruct point clouds from the latent vector, and a classifier to\ncategorize the point clouds into the four ONH conditions. Results: The\nclassification network achieved high accuracy, distinguishing H, HM, G, and HMG\nclasses with a micro-average AUC of 0.92 $\\pm$ 0.03 on an independent test set.\nThe decoder effectively reconstructed point clouds, achieving a Chamfer loss of\n0.013 $\\pm$ 0.002. Dimensionality reduction clustered ONHs into four distinct\ngroups, revealing structural variations such as changes in retinal and\nconnective tissue thickness, tilting and stretching of the disc and scleral\ncanal opening, and alterations in optic cup morphology, including shallow or\ndeep excavation, across the four conditions. Conclusions: This study\ndemonstrated that ONHs exhibit distinct structural signatures across H, HM, G,\nand HMG conditions. The findings further indicate that ONH morphology provides\nsufficient information for classification into distinct clusters, with\nprincipal components capturing unique structural patterns within each group.\n","date":"2025-03-24"}
{"id":"2503.19090","title":"LLM-Based Insight Extraction for Contact Center Analytics and\n  Cost-Efficient Deployment","abstract":"  Large Language Models have transformed the Contact Center industry,\nmanifesting in enhanced self-service tools, streamlined administrative\nprocesses, and augmented agent productivity. This paper delineates our system\nthat automates call driver generation, which serves as the foundation for tasks\nsuch as topic modeling, incoming call classification, trend detection, and FAQ\ngeneration, delivering actionable insights for contact center agents and\nadministrators to consume. We present a cost-efficient LLM system design, with\n1) a comprehensive evaluation of proprietary, open-weight, and fine-tuned\nmodels and 2) cost-efficient strategies, and 3) the corresponding cost analysis\nwhen deployed in production environments.\n","date":"2025-03-24"}
{"id":"2503.19091","title":"High Probability Complexity Bounds of Trust-Region Stochastic Sequential\n  Quadratic Programming with Heavy-Tailed Noise","abstract":"  In this paper, we consider nonlinear optimization problems with a stochastic\nobjective and deterministic equality constraints. We propose a Trust-Region\nStochastic Sequential Quadratic Programming (TR-SSQP) method and establish its\nhigh-probability iteration complexity bounds for identifying first- and\nsecond-order $\\epsilon$-stationary points. In our algorithm, we assume that\nexact objective values, gradients, and Hessians are not directly accessible but\ncan be estimated via zeroth-, first-, and second-order probabilistic oracles.\nCompared to existing complexity studies of SSQP methods that rely on a\nzeroth-order oracle with sub-exponential tail noise (i.e., light-tailed) and\nfocus mostly on first-order stationarity, our analysis accommodates irreducible\nand heavy-tailed noise in the zeroth-order oracle and significantly extends the\nanalysis to second-order stationarity. We show that under weaker noise\nconditions, our method achieves the same high-probability first-order iteration\ncomplexity bounds, while also exhibiting promising second-order iteration\ncomplexity bounds. Specifically, the method identifies a first-order\n$\\epsilon$-stationary point in $\\mathcal{O}(\\epsilon^{-2})$ iterations and a\nsecond-order $\\epsilon$-stationary point in $\\mathcal{O}(\\epsilon^{-3})$\niterations with high probability, provided that $\\epsilon$ is lower bounded by\na constant determined by the irreducible noise level in estimation. We validate\nour theoretical findings and evaluate the practical performance of our method\non CUTEst benchmark test set.\n","date":"2025-03-24"}
{"id":"2503.19092","title":"Rankers, Judges, and Assistants: Towards Understanding the Interplay of\n  LLMs in Information Retrieval Evaluation","abstract":"  Large language models (LLMs) are increasingly integral to information\nretrieval (IR), powering ranking, evaluation, and AI-assisted content creation.\nThis widespread adoption necessitates a critical examination of potential\nbiases arising from the interplay between these LLM-based components. This\npaper synthesizes existing research and presents novel experiment designs that\nexplore how LLM-based rankers and assistants influence LLM-based judges. We\nprovide the first empirical evidence of LLM judges exhibiting significant bias\ntowards LLM-based rankers. Furthermore, we observe limitations in LLM judges'\nability to discern subtle system performance differences. Contrary to some\nprevious findings, our preliminary study does not find evidence of bias against\nAI-generated content. These results highlight the need for a more holistic view\nof the LLM-driven information ecosystem. To this end, we offer initial\nguidelines and a research agenda to ensure the reliable use of LLMs in IR\nevaluation.\n","date":"2025-03-24"}
{"id":"2503.19096","title":"Uncertainty-Aware Decomposed Hybrid Networks","abstract":"  The robustness of image recognition algorithms remains a critical challenge,\nas current models often depend on large quantities of labeled data. In this\npaper, we propose a hybrid approach that combines the adaptability of neural\nnetworks with the interpretability, transparency, and robustness of\ndomain-specific quasi-invariant operators. Our method decomposes the\nrecognition into multiple task-specific operators that focus on different\ncharacteristics, supported by a novel confidence measurement tailored to these\noperators. This measurement enables the network to prioritize reliable features\nand accounts for noise. We argue that our design enhances transparency and\nrobustness, leading to improved performance, particularly in low-data regimes.\nExperimental results in traffic sign detection highlight the effectiveness of\nthe proposed method, especially in semi-supervised and unsupervised scenarios,\nunderscoring its potential for data-constrained applications.\n","date":"2025-03-24"}
{"id":"2503.19099","title":"Masks and Mimicry: Strategic Obfuscation and Impersonation Attacks on\n  Authorship Verification","abstract":"  The increasing use of Artificial Intelligence (AI) technologies, such as\nLarge Language Models (LLMs) has led to nontrivial improvements in various\ntasks, including accurate authorship identification of documents. However,\nwhile LLMs improve such defense techniques, they also simultaneously provide a\nvehicle for malicious actors to launch new attack vectors. To combat this\nsecurity risk, we evaluate the adversarial robustness of authorship models\n(specifically an authorship verification model) to potent LLM-based attacks.\nThese attacks include untargeted methods - \\textit{authorship obfuscation} and\ntargeted methods - \\textit{authorship impersonation}. For both attacks, the\nobjective is to mask or mimic the writing style of an author while preserving\nthe original texts' semantics, respectively. Thus, we perturb an accurate\nauthorship verification model, and achieve maximum attack success rates of 92\\%\nand 78\\% for both obfuscation and impersonation attacks, respectively.\n","date":"2025-03-24"}
{"id":"2503.19100","title":"Anomaly Detection Using Computer Vision: A Comparative Analysis of Class\n  Distinction and Performance Metrics","abstract":"  This paper showcases an experimental study on anomaly detection using\ncomputer vision. The study focuses on class distinction and performance\nevaluation, combining OpenCV with deep learning techniques while employing a\nTensorFlow-based convolutional neural network for real-time face recognition\nand classification. The system effectively distinguishes among three classes:\nauthorized personnel (admin), intruders, and non-human entities. A\nMobileNetV2-based deep learning model is utilized to optimize real-time\nperformance, ensuring high computational efficiency without compromising\naccuracy. Extensive dataset preprocessing, including image augmentation and\nnormalization, enhances the models generalization capabilities. Our analysis\ndemonstrates classification accuracies of 90.20% for admin, 98.60% for\nintruders, and 75.80% for non-human detection, while maintaining an average\nprocessing rate of 30 frames per second. The study leverages transfer learning,\nbatch normalization, and Adam optimization to achieve stable and robust\nlearning, and a comparative analysis of class differentiation strategies\nhighlights the impact of feature extraction techniques and training\nmethodologies. The results indicate that advanced feature selection and data\naugmentation significantly enhance detection performance, particularly in\ndistinguishing human from non-human scenes. As an experimental study, this\nresearch provides critical insights into optimizing deep learning-based\nsurveillance systems for high-security environments and improving the accuracy\nand efficiency of real-time anomaly detection.\n","date":"2025-03-24"}
{"id":"2503.19107","title":"Information-Seeking Decision Strategies Mitigate Risk in Dynamic,\n  Uncertain Environments","abstract":"  To survive in dynamic and uncertain environments, individuals must develop\neffective decision strategies that balance information gathering and decision\ncommitment. Models of such strategies often prioritize either optimizing\ntangible payoffs, like reward rate, or gathering information to support a\ndiversity of (possibly unknown) objectives. However, our understanding of the\nrelative merits of these two approaches remains incomplete, in part because\ndirect comparisons have been limited to idealized, static environments that\nlack the dynamic complexity of the real world. Here we compared the performance\nof normative reward- and information-seeking strategies in a dynamic foraging\ntask. Both strategies show similar transitions between exploratory and\nexploitative behaviors as environmental uncertainty changes. However, we find\nsubtle disparities in the actions they take, resulting in meaningful\nperformance differences: whereas reward-seeking strategies generate slightly\nmore reward on average, information-seeking strategies provide more consistent\nand predictable outcomes. Our findings support the adaptive value of\ninformation-seeking behaviors that can mitigate risk with minimal reward loss.\n","date":"2025-03-24"}
{"id":"2503.19108","title":"Your ViT is Secretly an Image Segmentation Model","abstract":"  Vision Transformers (ViTs) have shown remarkable performance and scalability\nacross various computer vision tasks. To apply single-scale ViTs to image\nsegmentation, existing methods adopt a convolutional adapter to generate\nmulti-scale features, a pixel decoder to fuse these features, and a Transformer\ndecoder that uses the fused features to make predictions. In this paper, we\nshow that the inductive biases introduced by these task-specific components can\ninstead be learned by the ViT itself, given sufficiently large models and\nextensive pre-training. Based on these findings, we introduce the Encoder-only\nMask Transformer (EoMT), which repurposes the plain ViT architecture to conduct\nimage segmentation. With large-scale models and pre-training, EoMT obtains a\nsegmentation accuracy similar to state-of-the-art models that use task-specific\ncomponents. At the same time, EoMT is significantly faster than these methods\ndue to its architectural simplicity, e.g., up to 4x faster with ViT-L. Across a\nrange of model sizes, EoMT demonstrates an optimal balance between segmentation\naccuracy and prediction speed, suggesting that compute resources are better\nspent on scaling the ViT itself rather than adding architectural complexity.\nCode: https:\/\/www.tue-mps.org\/eomt\/.\n","date":"2025-03-24"}
{"id":"2503.19114","title":"Understanding and Improving Information Preservation in Prompt\n  Compression for LLMs","abstract":"  Recent advancements in large language models (LLMs) have enabled their\nsuccessful application to a broad range of tasks. However, in\ninformation-intensive tasks, the prompt length can grow fast, leading to\nincreased computational requirements, performance degradation, and induced\nbiases from irrelevant or redundant information. Recently, various prompt\ncompression techniques have been introduced to optimize the trade-off between\nreducing input length and retaining performance. We propose a holistic\nevaluation framework that allows for in-depth analysis of prompt compression\nmethods. We focus on three key aspects, besides compression ratio: (i)\ndownstream task performance, (ii) grounding in the input context, and (iii)\ninformation preservation. Through this framework, we investigate\nstate-of-the-art soft and hard compression methods, showing that they struggle\nto preserve key details from the original prompt, limiting their performance on\ncomplex tasks. We demonstrate that modifying soft prompting methods to control\nbetter the granularity of the compressed information can significantly improve\ntheir effectiveness -- up to +23\\% in downstream task performance, more than +8\nBERTScore points in grounding, and 2.7x more entities preserved in compression.\n","date":"2025-03-24"}
{"id":"2503.19115","title":"Implementation of Support Vector Machines using Reaction Networks","abstract":"  Can machine learning algorithms be implemented using chemical reaction\nnetworks? We demonstrate that this is possible in the case of support vector\nmachines (SVMs). SVMs are powerful tools for data classification, leveraging VC\ntheory to handle high-dimensional data and small datasets effectively. In this\nwork, we propose a reaction network scheme for implementing SVMs, utilizing the\nsteady-state behavior of reaction network dynamics to model key computational\naspects of SVMs. This approach introduces a novel biochemical framework for\nimplementing machine learning algorithms in non-traditional computational\nenvironments.\n","date":"2025-03-24"}
{"id":"2503.19117","title":"Bayesian Semi-Parametric Spatial Dispersed Count Model for Precipitation\n  Analysis","abstract":"  The appropriateness of the Poisson model is frequently challenged when\nexamining spatial count data marked by unbalanced distributions,\nover-dispersion, or under-dispersion. Moreover, traditional parametric models\nmay inadequately capture the relationships among variables when covariates\ndisplay ambiguous functional forms or when spatial patterns are intricate and\nindeterminate. To tackle these issues, we propose an innovative Bayesian\nhierarchical modeling system. This method combines non-parametric techniques\nwith an adapted dispersed count model based on renewal theory, facilitating the\neffective management of unequal dispersion, non-linear correlations, and\ncomplex geographic dependencies in count data. We illustrate the efficacy of\nour strategy by applying it to lung and bronchus cancer mortality data from\nIowa, emphasizing environmental and demographic factors like ozone\nconcentrations, PM2.5, green space, and asthma prevalence. Our analysis\ndemonstrates considerable regional heterogeneity and non-linear relationships,\nproviding important insights into the impact of environmental and\nhealth-related factors on cancer death rates. This application highlights the\nsignificance of our methodology in public health research, where precise\nmodeling and forecasting are essential for guiding policy and intervention\nefforts. Additionally, we performed a simulation study to assess the resilience\nand accuracy of the suggested method, validating its superiority in managing\ndispersion and capturing intricate spatial patterns relative to conventional\nmethods. The suggested framework presents a flexible and robust instrument for\ngeographical count analysis, offering innovative insights for academics and\npractitioners in disciplines such as epidemiology, environmental science, and\nspatial statistics.\n","date":"2025-03-24"}
{"id":"2503.19119","title":"TrackRAD2025 challenge dataset: Real-time tumor tracking for MRI-guided\n  radiotherapy","abstract":"  Purpose: Magnetic resonance imaging (MRI) to visualize anatomical motion is\nbecoming increasingly important when treating cancer patients with\nradiotherapy. Hybrid MRI-linear accelerator (MRI-linac) systems allow real-time\nmotion management during irradiation. This paper presents a multi-institutional\nreal-time MRI time series dataset from different MRI-linac vendors. The dataset\nis designed to support developing and evaluating real-time tumor localization\n(tracking) algorithms for MRI-guided radiotherapy within the TrackRAD2025\nchallenge (https:\/\/trackrad2025.grand-challenge.org\/).\n  Acquisition and validation methods: The dataset consists of sagittal 2D cine\nMRIs in 585 patients from six centers (3 Dutch, 1 German, 1 Australian, and 1\nChinese). Tumors in the thorax, abdomen, and pelvis acquired on two\ncommercially available MRI-linacs (0.35 T and 1.5 T) were included. For 108\ncases, irradiation targets or tracking surrogates were manually segmented on\neach temporal frame. The dataset was randomly split into a public training set\nof 527 cases (477 unlabeled and 50 labeled) and a private testing set of 58\ncases (all labeled).\n  Data Format and Usage Notes: The data is publicly available under the\nTrackRAD2025 collection: https:\/\/doi.org\/10.57967\/hf\/4539. Both the images and\nsegmentations for each patient are available in metadata format.\n  Potential Applications: This novel clinical dataset will enable the\ndevelopment and evaluation of real-time tumor localization algorithms for\nMRI-guided radiotherapy. By enabling more accurate motion management and\nadaptive treatment strategies, this dataset has the potential to advance the\nfield of radiotherapy significantly.\n","date":"2025-03-24"}
{"id":"2503.19120","title":"Where is this coming from? Making groundedness count in the evaluation\n  of Document VQA models","abstract":"  Document Visual Question Answering (VQA) models have evolved at an impressive\nrate over the past few years, coming close to or matching human performance on\nsome benchmarks. We argue that common evaluation metrics used by popular\nbenchmarks do not account for the semantic and multimodal groundedness of a\nmodel's outputs. As a result, hallucinations and major semantic errors are\ntreated the same way as well-grounded outputs, and the evaluation scores do not\nreflect the reasoning capabilities of the model. In response, we propose a new\nevaluation methodology that accounts for the groundedness of predictions with\nregard to the semantic characteristics of the output as well as the multimodal\nplacement of the output within the input document. Our proposed methodology is\nparameterized in such a way that users can configure the score according to\ntheir preferences. We validate our scoring methodology using human judgment and\nshow its potential impact on existing popular leaderboards. Through extensive\nanalyses, we demonstrate that our proposed method produces scores that are a\nbetter indicator of a model's robustness and tends to give higher rewards to\nbetter-calibrated answers.\n","date":"2025-03-24"}
{"id":"2503.19123","title":"Overcoming Vocabulary Mismatch: Vocabulary-agnostic Teacher Guided\n  Language Modeling","abstract":"  Using large teacher models to guide the training of smaller student models\nhas become the prevailing paradigm for efficient and effective learning.\nHowever, vocabulary mismatches between teacher and student language models pose\nsignificant challenges in language modeling, resulting in divergent token\nsequences and output distributions. To overcome these limitations, we propose\nVocabulary-agnostic Teacher Guided Language Modeling (VocAgnoLM), a novel\napproach that bridges the gap caused by vocabulary mismatch through two key\nmethods: (1) Token-level Lexical Alignment, which aligns token sequences across\nmismatched vocabularies, and (2) Teacher Guided Loss, which leverages the loss\nof teacher model to guide effective student training. We demonstrate its\neffectiveness in language modeling with 1B student model using various 7B\nteacher models with different vocabularies. Notably, with\nQwen2.5-Math-Instruct, a teacher model sharing only about 6% of its vocabulary\nwith TinyLlama, VocAgnoLM achieves a 46% performance improvement compared to\nnaive continual pretraining. Furthermore, we demonstrate that VocAgnoLM\nconsistently benefits from stronger teacher models, providing a robust solution\nto vocabulary mismatches in language modeling.\n","date":"2025-03-24"}
{"id":"2503.19126","title":"Tractable downfall of basis pursuit in structured sparse optimization","abstract":"  The problem of finding the sparsest solution to a linear underdetermined\nsystem of equations, as it often appears in data analysis, optimal control and\nsystem identification problems, is considered. This non-convex problem is\ncommonly solved by convexification via $\\ell_1$-norm minimization, also known\nas basis pursuit. In this work, a class of structured matrices, representing\nthe system of equations, is introduced for which the basis pursuit approach\ntractably fails to recover the sparsest solution. In particular, we are able to\nidentify matrix columns that correspond to unrecoverable non-zero entries of\nthe sparsest solution, as well as to conclude the uniqueness of the sparsest\nsolution in polynomial time. These deterministic guarantees contrast popular\nprobabilistic ones, and as such, provide valuable insights into the a priori\ndesign of sparse optimization problems. As our matrix structure appears\nnaturally in optimal control problems, we exemplify our findings by showing\nthat it is possible to verify a priori that basis pursuit may fail in finding\nfuel optimal regulators for a class of discrete-time linear time-invariant\nsystems.\n","date":"2025-03-24"}
{"id":"2503.19134","title":"MIRAGE: Multimodal Immersive Reasoning and Guided Exploration for\n  Red-Team Jailbreak Attacks","abstract":"  While safety mechanisms have significantly progressed in filtering harmful\ntext inputs, MLLMs remain vulnerable to multimodal jailbreaks that exploit\ntheir cross-modal reasoning capabilities. We present MIRAGE, a novel multimodal\njailbreak framework that exploits narrative-driven context and role immersion\nto circumvent safety mechanisms in Multimodal Large Language Models (MLLMs). By\nsystematically decomposing the toxic query into environment, role, and action\ntriplets, MIRAGE constructs a multi-turn visual storytelling sequence of images\nand text using Stable Diffusion, guiding the target model through an engaging\ndetective narrative. This process progressively lowers the model's defences and\nsubtly guides its reasoning through structured contextual cues, ultimately\neliciting harmful responses. In extensive experiments on the selected datasets\nwith six mainstream MLLMs, MIRAGE achieves state-of-the-art performance,\nimproving attack success rates by up to 17.5% over the best baselines.\nMoreover, we demonstrate that role immersion and structured semantic\nreconstruction can activate inherent model biases, facilitating the model's\nspontaneous violation of ethical safeguards. These results highlight critical\nweaknesses in current multimodal safety mechanisms and underscore the urgent\nneed for more robust defences against cross-modal threats.\n","date":"2025-03-24"}
{"id":"2503.19136","title":"Stochastic Poisson Surface Reconstruction with One Solve using Geometric\n  Gaussian Processes","abstract":"  Poisson Surface Reconstruction is a widely-used algorithm for reconstructing\na surface from an oriented point cloud. To facilitate applications where only\npartial surface information is available, or scanning is performed\nsequentially, a recent line of work proposes to incorporate uncertainty into\nthe reconstructed surface via Gaussian process models. The resulting algorithms\nfirst perform Gaussian process interpolation, then solve a set of volumetric\npartial differential equations globally in space, resulting in a\ncomputationally expensive two-stage procedure. In this work, we apply\nrecently-developed techniques from geometric Gaussian processes to combine\ninterpolation and surface reconstruction into a single stage, requiring only\none linear solve per sample. The resulting reconstructed surface samples can be\nqueried locally in space, without the use of problem-dependent volumetric\nmeshes or grids. These capabilities enable one to (a) perform probabilistic\ncollision detection locally around the region of interest, (b) perform ray\ncasting without evaluating points not on the ray's trajectory, and (c) perform\nnext-view planning on a per-slice basis. They also improve reconstruction\nquality, by not requiring one to approximate kernel matrix inverses with\ndiagonal matrices as part of intermediate computations. Results show that our\napproach provides a cleaner, more-principled, and more-flexible stochastic\nsurface reconstruction pipeline.\n","date":"2025-03-24"}
{"id":"2503.19142","title":"Activation Functions Considered Harmful: Recovering Neural Network\n  Weights through Controlled Channels","abstract":"  With high-stakes machine learning applications increasingly moving to\nuntrusted end-user or cloud environments, safeguarding pre-trained model\nparameters becomes essential for protecting intellectual property and user\nprivacy. Recent advancements in hardware-isolated enclaves, notably Intel SGX,\nhold the promise to secure the internal state of machine learning applications\neven against compromised operating systems. However, we show that privileged\nsoftware adversaries can exploit input-dependent memory access patterns in\ncommon neural network activation functions to extract secret weights and biases\nfrom an SGX enclave.\n  Our attack leverages the SGX-Step framework to obtain a noise-free,\ninstruction-granular page-access trace. In a case study of an 11-input\nregression network using the Tensorflow Microlite library, we demonstrate\ncomplete recovery of all first-layer weights and biases, as well as partial\nrecovery of parameters from deeper layers under specific conditions. Our novel\nattack technique requires only 20 queries per input per weight to obtain all\nfirst-layer weights and biases with an average absolute error of less than 1%,\nimproving over prior model stealing attacks.\n  Additionally, a broader ecosystem analysis reveals the widespread use of\nactivation functions with input-dependent memory access patterns in popular\nmachine learning frameworks (either directly or via underlying math libraries).\nOur findings highlight the limitations of deploying confidential models in SGX\nenclaves and emphasise the need for stricter side-channel validation of machine\nlearning implementations, akin to the vetting efforts applied to secure\ncryptographic libraries.\n","date":"2025-03-24"}
{"id":"2503.19145","title":"Compositional Caching for Training-free Open-vocabulary Attribute\n  Detection","abstract":"  Attribute detection is crucial for many computer vision tasks, as it enables\nsystems to describe properties such as color, texture, and material. Current\napproaches often rely on labor-intensive annotation processes which are\ninherently limited: objects can be described at an arbitrary level of detail\n(e.g., color vs. color shades), leading to ambiguities when the annotators are\nnot instructed carefully. Furthermore, they operate within a predefined set of\nattributes, reducing scalability and adaptability to unforeseen downstream\napplications. We present Compositional Caching (ComCa), a training-free method\nfor open-vocabulary attribute detection that overcomes these constraints. ComCa\nrequires only the list of target attributes and objects as input, using them to\npopulate an auxiliary cache of images by leveraging web-scale databases and\nLarge Language Models to determine attribute-object compatibility. To account\nfor the compositional nature of attributes, cache images receive soft attribute\nlabels. Those are aggregated at inference time based on the similarity between\nthe input and cache images, refining the predictions of underlying\nVision-Language Models (VLMs). Importantly, our approach is model-agnostic,\ncompatible with various VLMs. Experiments on public datasets demonstrate that\nComCa significantly outperforms zero-shot and cache-based baselines, competing\nwith recent training-based methods, proving that a carefully designed\ntraining-free approach can successfully address open-vocabulary attribute\ndetection.\n","date":"2025-03-24"}
{"id":"2503.19146","title":"Risk-Based Thresholding for Reliable Anomaly Detection in Concentrated\n  Solar Power Plants","abstract":"  Efficient and reliable operation of Concentrated Solar Power (CSP) plants is\nessential for meeting the growing demand for sustainable energy. However,\nhigh-temperature solar receivers face severe operational risks, such as\nfreezing, deformation, and corrosion, resulting in costly downtime and\nmaintenance. To monitor CSP plants, cameras mounted on solar receivers record\ninfrared images at irregular intervals ranging from one to five minutes\nthroughout the day. Anomalous images can be detected by thresholding an anomaly\nscore, where the threshold is chosen to optimize metrics such as the F1-score\non a validation set. This work proposes a framework for generating more\nreliable decision thresholds with finite-sample coverage guarantees on any\nchosen risk function. Our framework also incorporates an abstention mechanism,\nallowing high-risk predictions to be deferred to domain experts. Second, we\npropose a density forecasting method to estimate the likelihood of an observed\nimage given a sequence of previously observed images, using this likelihood as\nits anomaly score. Third, we analyze the deployment results of our framework\nacross multiple training scenarios over several months for two CSP plants. This\nanalysis provides valuable insights to our industry partner for optimizing\nmaintenance operations. Finally, given the confidential nature of our dataset,\nwe provide an extended simulated dataset, leveraging recent advancements in\ngenerative modeling to create diverse thermal images that simulate multiple CSP\nplants. Our code is publicly available.\n","date":"2025-03-24"}
{"id":"2503.19149","title":"Out-of-distribution evaluations of channel agnostic masked autoencoders\n  in fluorescence microscopy","abstract":"  Developing computer vision for high-content screening is challenging due to\nvarious sources of distribution-shift caused by changes in experimental\nconditions, perturbagens, and fluorescent markers. The impact of different\nsources of distribution-shift are confounded in typical evaluations of models\nbased on transfer learning, which limits interpretations of how changes to\nmodel design and training affect generalisation. We propose an evaluation\nscheme that isolates sources of distribution-shift using the JUMP-CP dataset,\nallowing researchers to evaluate generalisation with respect to specific\nsources of distribution-shift. We then present a channel-agnostic masked\nautoencoder $\\mathbf{Campfire}$ which, via a shared decoder for all channels,\nscales effectively to datasets containing many different fluorescent markers,\nand show that it generalises to out-of-distribution experimental batches,\nperturbagens, and fluorescent markers, and also demonstrates successful\ntransfer learning from one cell type to another.\n","date":"2025-03-24"}
{"id":"2503.19152","title":"PSO-UNet: Particle Swarm-Optimized U-Net Framework for Precise\n  Multimodal Brain Tumor Segmentation","abstract":"  Medical image segmentation, particularly for brain tumor analysis, demands\nprecise and computationally efficient models due to the complexity of\nmultimodal MRI datasets and diverse tumor morphologies. This study introduces\nPSO-UNet, which integrates Particle Swarm Optimization (PSO) with the U-Net\narchitecture for dynamic hyperparameter optimization. Unlike traditional manual\ntuning or alternative optimization approaches, PSO effectively navigates\ncomplex hyperparameter search spaces, explicitly optimizing the number of\nfilters, kernel size, and learning rate. PSO-UNet substantially enhances\nsegmentation performance, achieving Dice Similarity Coefficients (DSC) of\n0.9578 and 0.9523 and Intersection over Union (IoU) scores of 0.9194 and 0.9097\non the BraTS 2021 and Figshare datasets, respectively. Moreover, the method\nreduces computational complexity significantly, utilizing only 7.8 million\nparameters and executing in approximately 906 seconds, markedly faster than\ncomparable U-Net-based frameworks. These outcomes underscore PSO-UNet's robust\ngeneralization capabilities across diverse MRI modalities and tumor\nclassifications, emphasizing its clinical potential and clear advantages over\nconventional hyperparameter tuning methods. Future research will explore hybrid\noptimization strategies and validate the framework against other bio-inspired\nalgorithms to enhance its robustness and scalability.\n","date":"2025-03-24"}
{"id":"2503.19157","title":"HOIGPT: Learning Long Sequence Hand-Object Interaction with Language\n  Models","abstract":"  We introduce HOIGPT, a token-based generative method that unifies 3D\nhand-object interactions (HOI) perception and generation, offering the first\ncomprehensive solution for captioning and generating high-quality 3D HOI\nsequences from a diverse range of conditional signals (\\eg text, objects,\npartial sequences). At its core, HOIGPT utilizes a large language model to\npredict the bidrectional transformation between HOI sequences and natural\nlanguage descriptions. Given text inputs, HOIGPT generates a sequence of hand\nand object meshes; given (partial) HOI sequences, HOIGPT generates text\ndescriptions and completes the sequences. To facilitate HOI understanding with\na large language model, this paper introduces two key innovations: (1) a novel\nphysically grounded HOI tokenizer, the hand-object decomposed VQ-VAE, for\ndiscretizing HOI sequences, and (2) a motion-aware language model trained to\nprocess and generate both text and HOI tokens. Extensive experiments\ndemonstrate that HOIGPT sets new state-of-the-art performance on both text\ngeneration (+2.01% R Precision) and HOI generation (-2.56 FID) across multiple\ntasks and benchmarks.\n","date":"2025-03-24"}
{"id":"2503.19158","title":"Integrating Biological-Informed Recurrent Neural Networks for\n  Glucose-Insulin Dynamics Modeling","abstract":"  Type 1 Diabetes (T1D) management is a complex task due to many variability\nfactors. Artificial Pancreas (AP) systems have alleviated patient burden by\nautomating insulin delivery through advanced control algorithms. However, the\neffectiveness of these systems depends on accurate modeling of glucose-insulin\ndynamics, which traditional mathematical models often fail to capture due to\ntheir inability to adapt to patient-specific variations. This study introduces\na Biological-Informed Recurrent Neural Network (BIRNN) framework to address\nthese limitations. The BIRNN leverages a Gated Recurrent Units (GRU)\narchitecture augmented with physics-informed loss functions that embed\nphysiological constraints, ensuring a balance between predictive accuracy and\nconsistency with biological principles. The framework is validated using the\ncommercial UVA\/Padova simulator, outperforming traditional linear models in\nglucose prediction accuracy and reconstruction of unmeasured states, even under\ncircadian variations in insulin sensitivity. The results demonstrate the\npotential of BIRNN for personalized glucose regulation and future adaptive\ncontrol strategies in AP systems.\n","date":"2025-03-24"}
{"id":"2503.19166","title":"Multi-objective Pseudo Boolean Functions in Runtime Analysis: A Review","abstract":"  Recently, there has been growing interest within the theoretical community in\nanalytically studying multi-objective evolutionary algorithms. This runtime\nanalysis-focused research can help formally understand algorithm behaviour,\nexplain empirical observations, and provide theoretical insights to support\nalgorithm development and exploration. However, the test problems commonly used\nin the theoretical analysis are predominantly limited to problems with heavy\n``artificial'' characteristics (e.g., symmetric objectives and linear Pareto\nfronts), which may not be able to well represent realistic scenarios. In this\npaper, we survey commonly used multi-objective functions in the theory domain\nand systematically review their features, limitations and implications to\npractical use. Moreover, we present several new functions with more realistic\nfeatures, such as local optimality and nonlinearity of the Pareto front,\nthrough simply mixing and matching classical single-objective functions in the\narea (e.g., LeadingOnes, Jump and RoyalRoad). We hope these functions can\nenrich the existing test problem suites, and strengthen the connection between\ntheoretic and practical research.\n","date":"2025-03-24"}
{"id":"2503.19168","title":"Language Model Uncertainty Quantification with Attention Chain","abstract":"  Accurately quantifying a large language model's (LLM) predictive uncertainty\nis crucial for judging the reliability of its answers. While most existing\nresearch focuses on short, directly answerable questions with closed-form\noutputs (e.g., multiple-choice), involving intermediate reasoning steps in LLM\nresponses is increasingly important. This added complexity complicates\nuncertainty quantification (UQ) because the probabilities assigned to answer\ntokens are conditioned on a vast space of preceding reasoning tokens. Direct\nmarginalization is infeasible, and the dependency inflates probability\nestimates, causing overconfidence in UQ. To address this, we propose UQAC, an\nefficient method that narrows the reasoning space to a tractable size for\nmarginalization. UQAC iteratively constructs an \"attention chain\" of tokens\ndeemed \"semantically crucial\" to the final answer via a backtracking procedure.\nStarting from the answer tokens, it uses attention weights to identify the most\ninfluential predecessors, then iterates this process until reaching the input\ntokens. Similarity filtering and probability thresholding further refine the\nresulting chain, allowing us to approximate the marginal probabilities of the\nanswer tokens, which serve as the LLM's confidence. We validate UQAC on\nmultiple reasoning benchmarks with advanced open-source LLMs, demonstrating\nthat it consistently delivers reliable UQ estimates with high computational\nefficiency.\n","date":"2025-03-24"}
{"id":"2503.19173","title":"Graph neural networks extrapolate out-of-distribution for shortest paths","abstract":"  Neural networks (NNs), despite their success and wide adoption, still\nstruggle to extrapolate out-of-distribution (OOD), i.e., to inputs that are not\nwell-represented by their training dataset. Addressing the OOD generalization\ngap is crucial when models are deployed in environments significantly different\nfrom the training set, such as applying Graph Neural Networks (GNNs) trained on\nsmall graphs to large, real-world graphs. One promising approach for achieving\nrobust OOD generalization is the framework of neural algorithmic alignment,\nwhich incorporates ideas from classical algorithms by designing neural\narchitectures that resemble specific algorithmic paradigms (e.g. dynamic\nprogramming). The hope is that trained models of this form would have superior\nOOD capabilities, in much the same way that classical algorithms work for all\ninstances. We rigorously analyze the role of algorithmic alignment in achieving\nOOD generalization, focusing on graph neural networks (GNNs) applied to the\ncanonical shortest path problem. We prove that GNNs, trained to minimize a\nsparsity-regularized loss over a small set of shortest path instances, exactly\nimplement the Bellman-Ford (BF) algorithm for shortest paths. In fact, if a GNN\nminimizes this loss within an error of $\\epsilon$, it implements the BF\nalgorithm with an error of $O(\\epsilon)$. Consequently, despite limited\ntraining data, these GNNs are guaranteed to extrapolate to arbitrary\nshortest-path problems, including instances of any size. Our empirical results\nsupport our theory by showing that NNs trained by gradient descent are able to\nminimize this loss and extrapolate in practice.\n","date":"2025-03-24"}
{"id":"2503.19174","title":"AssertionForge: Enhancing Formal Verification Assertion Generation with\n  Structured Representation of Specifications and RTL","abstract":"  Generating SystemVerilog Assertions (SVAs) from natural language\nspecifications remains a major challenge in formal verification (FV) due to the\ninherent ambiguity and incompleteness of specifications. Existing LLM-based\napproaches, such as AssertLLM, focus on extracting information solely from\nspecification documents, often failing to capture essential internal signal\ninteractions and design details present in the RTL code, leading to incomplete\nor incorrect assertions. We propose a novel approach that constructs a\nKnowledge Graph (KG) from both specifications and RTL, using a\nhardware-specific schema with domain-specific entity and relation types. We\ncreate an initial KG from the specification and then systematically fuse it\nwith information extracted from the RTL code, resulting in a unified,\ncomprehensive KG. This combined representation enables a more thorough\nunderstanding of the design and allows for a multi-resolution context synthesis\nprocess which is designed to extract diverse verification contexts from the KG.\nExperiments on four designs demonstrate that our method significantly enhances\nSVA quality over prior methods. This structured representation not only\nimproves FV but also paves the way for future research in tasks like code\ngeneration and design understanding.\n","date":"2025-03-24"}
{"id":"2503.19176","title":"SoK: How Robust is Audio Watermarking in Generative AI models?","abstract":"  Audio watermarking is increasingly used to verify the provenance of\nAI-generated content, enabling applications such as detecting AI-generated\nspeech, protecting music IP, and defending against voice cloning. To be\neffective, audio watermarks must resist removal attacks that distort signals to\nevade detection. While many schemes claim robustness, these claims are\ntypically tested in isolation and against a limited set of attacks. A\nsystematic evaluation against diverse removal attacks is lacking, hindering\npractical deployment. In this paper, we investigate whether recent watermarking\nschemes that claim robustness can withstand a broad range of removal attacks.\nFirst, we introduce a taxonomy covering 22 audio watermarking schemes. Next, we\nsummarize their underlying technologies and potential vulnerabilities. We then\npresent a large-scale empirical study to assess their robustness. To support\nthis, we build an evaluation framework encompassing 22 types of removal attacks\n(109 configurations) including signal-level, physical-level, and AI-induced\ndistortions. We reproduce 9 watermarking schemes using open-source code,\nidentify 8 new highly effective attacks, and highlight 11 key findings that\nexpose the fundamental limitations of these methods across 3 public datasets.\nOur results reveal that none of the surveyed schemes can withstand all tested\ndistortions. This evaluation offers a comprehensive view of how current\nwatermarking methods perform under real-world threats. Our demo and code are\navailable at https:\/\/sokaudiowm.github.io\/.\n","date":"2025-03-24"}
{"id":"2503.19182","title":"Evaluating Bias in LLMs for Job-Resume Matching: Gender, Race, and\n  Education","abstract":"  Large Language Models (LLMs) offer the potential to automate hiring by\nmatching job descriptions with candidate resumes, streamlining recruitment\nprocesses, and reducing operational costs. However, biases inherent in these\nmodels may lead to unfair hiring practices, reinforcing societal prejudices and\nundermining workplace diversity. This study examines the performance and\nfairness of LLMs in job-resume matching tasks within the English language and\nU.S. context. It evaluates how factors such as gender, race, and educational\nbackground influence model decisions, providing critical insights into the\nfairness and reliability of LLMs in HR applications. Our findings indicate that\nwhile recent models have reduced biases related to explicit attributes like\ngender and race, implicit biases concerning educational background remain\nsignificant. These results highlight the need for ongoing evaluation and the\ndevelopment of advanced bias mitigation strategies to ensure equitable hiring\npractices when using LLMs in industry settings.\n","date":"2025-03-24"}
{"id":"2503.19186","title":"Protein Structure-Function Relationship: A Kernel-PCA Approach for\n  Reaction Coordinate Identification","abstract":"  In this study, we propose a Kernel-PCA model designed to capture\nstructure-function relationships in a protein. This model also enables ranking\nof reaction coordinates according to their impact on protein properties. By\nleveraging machine learning techniques, including Kernel and principal\ncomponent analysis (PCA), our model uncovers meaningful patterns in\nhigh-dimensional protein data obtained from molecular dynamics (MD)\nsimulations. The effectiveness of our model in accurately identifying reaction\ncoordinates has been demonstrated through its application to a G\nprotein-coupled receptor. Furthermore, this model utilizes a network-based\napproach to uncover correlations in the dynamic behavior of residues associated\nwith a specific protein property. These findings underscore the potential of\nour model as a powerful tool for protein structure-function analysis and\nvisualization.\n","date":"2025-03-24"}
{"id":"2503.19190","title":"Universal Architectures for the Learning of Polyhedral Norms and Convex\n  Regularization Functionals","abstract":"  This paper addresses the task of learning convex regularizers to guide the\nreconstruction of images from limited data. By imposing that the reconstruction\nbe amplitude-equivariant, we narrow down the class of admissible functionals to\nthose that can be expressed as a power of a seminorm. We then show that such\nfunctionals can be approximated to arbitrary precision with the help of\npolyhedral norms. In particular, we identify two dual parameterizations of such\nsystems: (i) a synthesis form with an $\\ell_1$-penalty that involves some\nlearnable dictionary; and (ii) an analysis form with an $\\ell_\\infty$-penalty\nthat involves a trainable regularization operator. After having provided\ngeometric insights and proved that the two forms are universal, we propose an\nimplementation that relies on a specific architecture (tight frame with a\nweighted $\\ell_1$ penalty) that is easy to train. We illustrate its use for\ndenoising and the reconstruction of biomedical images. We find that the\nproposed framework outperforms the sparsity-based methods of compressed\nsensing, while it offers essentially the same convergence and robustness\nguarantees.\n","date":"2025-03-24"}
{"id":"2503.19191","title":"FDS: Frequency-Aware Denoising Score for Text-Guided Latent Diffusion\n  Image Editing","abstract":"  Text-guided image editing using Text-to-Image (T2I) models often fails to\nyield satisfactory results, frequently introducing unintended modifications,\nsuch as the loss of local detail and color changes. In this paper, we analyze\nthese failure cases and attribute them to the indiscriminate optimization\nacross all frequency bands, even though only specific frequencies may require\nadjustment. To address this, we introduce a simple yet effective approach that\nenables the selective optimization of specific frequency bands within localized\nspatial regions for precise edits. Our method leverages wavelets to decompose\nimages into different spatial resolutions across multiple frequency bands,\nenabling precise modifications at various levels of detail. To extend the\napplicability of our approach, we provide a comparative analysis of different\nfrequency-domain techniques. Additionally, we extend our method to 3D texture\nediting by performing frequency decomposition on the triplane representation,\nenabling frequency-aware adjustments for 3D textures. Quantitative evaluations\nand user studies demonstrate the effectiveness of our method in producing\nhigh-quality and precise edits.\n","date":"2025-03-24"}
{"id":"2503.19193","title":"Browsing Lost Unformed Recollections: A Benchmark for Tip-of-the-Tongue\n  Search and Reasoning","abstract":"  We introduce Browsing Lost Unformed Recollections, a tip-of-the-tongue\nknown-item search and reasoning benchmark for general AI assistants. BLUR\nintroduces a set of 573 real-world validated questions that demand searching\nand reasoning across multi-modal and multilingual inputs, as well as proficient\ntool use, in order to excel on. Humans easily ace these questions (scoring on\naverage 98%), while the best-performing system scores around 56%. To facilitate\nprogress toward addressing this challenging and aspirational use case for\ngeneral AI assistants, we release 350 questions through a public leaderboard,\nretain the answers to 250 of them, and have the rest as a private test set.\n","date":"2025-03-24"}
{"id":"2503.19195","title":"Mining-Gym: A Configurable RL Benchmarking Environment for Truck\n  Dispatch Scheduling","abstract":"  Mining process optimization particularly truck dispatch scheduling is a\ncritical factor in enhancing the efficiency of open pit mining operations\nHowever the dynamic and stochastic nature of mining environments characterized\nby uncertainties such as equipment failures truck maintenance and variable haul\ncycle times poses significant challenges for traditional optimization methods\nWhile Reinforcement Learning RL has shown promise in adaptive decision making\nfor mining logistics its practical deployment requires rigorous evaluation in\nrealistic and customizable simulation environments The lack of standardized\nbenchmarking environments limits fair algorithm comparisons reproducibility and\nthe real world applicability of RL based approaches in open pit mining settings\nTo address this challenge we introduce Mining Gym a configurable open source\nbenchmarking environment designed for training testing and comparing RL\nalgorithms in mining process optimization Built on Discrete Event Simulation\nDES and seamlessly integrated with the OpenAI Gym interface Mining Gym provides\na structured testbed that enables the direct application of advanced RL\nalgorithms from Stable Baselines The framework models key mining specific\nuncertainties such as equipment failures queue congestion and the stochasticity\nof mining processes ensuring a realistic and adaptive learning environment\nAdditionally Mining Gym features a graphical user interface GUI for intuitive\nmine site configuration a comprehensive data logging system a built in KPI\ndashboard and real time visual representation of the mine site These\ncapabilities facilitate standardized reproducible evaluations across multiple\nRL strategies and baseline heuristics\n","date":"2025-03-24"}
{"id":"2503.19199","title":"Open-Vocabulary Functional 3D Scene Graphs for Real-World Indoor Spaces","abstract":"  We introduce the task of predicting functional 3D scene graphs for real-world\nindoor environments from posed RGB-D images. Unlike traditional 3D scene graphs\nthat focus on spatial relationships of objects, functional 3D scene graphs\ncapture objects, interactive elements, and their functional relationships. Due\nto the lack of training data, we leverage foundation models, including visual\nlanguage models (VLMs) and large language models (LLMs), to encode functional\nknowledge. We evaluate our approach on an extended SceneFun3D dataset and a\nnewly collected dataset, FunGraph3D, both annotated with functional 3D scene\ngraphs. Our method significantly outperforms adapted baselines, including\nOpen3DSG and ConceptGraph, demonstrating its effectiveness in modeling complex\nscene functionalities. We also demonstrate downstream applications such as 3D\nquestion answering and robotic manipulation using functional 3D scene graphs.\nSee our project page at https:\/\/openfungraph.github.io\n","date":"2025-03-24"}
{"id":"2503.19201","title":"A Shared Low-Rank Adaptation Approach to Personalized RLHF","abstract":"  Reinforcement Learning from Human Feedback (RLHF) has emerged as a pivotal\ntechnique for aligning artificial intelligence systems with human values,\nachieving remarkable success in fine-tuning large language models. However,\nexisting RLHF frameworks often assume that human preferences are relatively\nhomogeneous and can be captured by a single, unified reward model. This\nassumption overlooks the inherent diversity and heterogeneity across\nindividuals, limiting the adaptability of RLHF to personalized scenarios and\nrisking misalignments that can diminish user satisfaction and trust in AI\nsystems. In this paper, we address these challenges by introducing Low-Rank\nAdaptation (LoRA) into the personalized RLHF framework. We apply LoRA in the\nthe aggregated parameter space of all personalized reward functions, thereby\nenabling efficient learning of personalized reward models from potentially\nlimited local datasets. Our approach exploits potential shared structures among\nthe local ground-truth reward models while allowing for individual adaptation,\nwithout relying on restrictive assumptions about shared representations as in\nprior works. We further establish sample complexity guarantees for our method.\nTheoretical analysis demonstrates the effectiveness of the proposed approach in\ncapturing both shared and individual-specific structures within heterogeneous\nhuman preferences, addressing the dual challenge of personalization\nrequirements and practical data constraints. Experimental results on real-world\ndatasets corroborate the efficiency of our algorithm in the personalized RLHF\nsetting.\n","date":"2025-03-24"}
{"id":"2503.19202","title":"Benchmarking Object Detectors under Real-World Distribution Shifts in\n  Satellite Imagery","abstract":"  Object detectors have achieved remarkable performance in many applications;\nhowever, these deep learning models are typically designed under the i.i.d.\nassumption, meaning they are trained and evaluated on data sampled from the\nsame (source) distribution. In real-world deployment, however, target\ndistributions often differ from source data, leading to substantial performance\ndegradation. Domain Generalisation (DG) seeks to bridge this gap by enabling\nmodels to generalise to Out-Of-Distribution (OOD) data without access to target\ndistributions during training, enhancing robustness to unseen conditions. In\nthis work, we examine the generalisability and robustness of state-of-the-art\nobject detectors under real-world distribution shifts, focusing particularly on\nspatial domain shifts. Despite the need, a standardised benchmark dataset\nspecifically designed for assessing object detection under realistic DG\nscenarios is currently lacking. To address this, we introduce Real-World\nDistribution Shifts (RWDS), a suite of three novel DG benchmarking datasets\nthat focus on humanitarian and climate change applications. These datasets\nenable the investigation of domain shifts across (i) climate zones and (ii)\nvarious disasters and geographic regions. To our knowledge, these are the first\nDG benchmarking datasets tailored for object detection in real-world,\nhigh-impact contexts. We aim for these datasets to serve as valuable resources\nfor evaluating the robustness and generalisation of future object detection\nmodels. Our datasets and code are available at https:\/\/github.com\/RWGAI\/RWDS.\n","date":"2025-03-24"}
{"id":"2503.19206","title":"Overtrained Language Models Are Harder to Fine-Tune","abstract":"  Large language models are pre-trained on ever-growing token budgets under the\nassumption that better pre-training performance translates to improved\ndownstream models. In this work, we challenge this assumption and show that\nextended pre-training can make models harder to fine-tune, leading to degraded\nfinal performance. We term this phenomenon catastrophic overtraining. For\nexample, the instruction-tuned OLMo-1B model pre-trained on 3T tokens leads to\nover 2% worse performance on multiple standard LLM benchmarks than its 2.3T\ntoken counterpart. Through controlled experiments and theoretical analysis, we\nshow that catastrophic overtraining arises from a systematic increase in the\nbroad sensitivity of pre-trained parameters to modifications, including but not\nlimited to fine-tuning. Our findings call for a critical reassessment of\npre-training design that considers the downstream adaptability of the model.\n","date":"2025-03-24"}
{"id":"2503.19207","title":"FRESA:Feedforward Reconstruction of Personalized Skinned Avatars from\n  Few Images","abstract":"  We present a novel method for reconstructing personalized 3D human avatars\nwith realistic animation from only a few images. Due to the large variations in\nbody shapes, poses, and cloth types, existing methods mostly require hours of\nper-subject optimization during inference, which limits their practical\napplications. In contrast, we learn a universal prior from over a thousand\nclothed humans to achieve instant feedforward generation and zero-shot\ngeneralization. Specifically, instead of rigging the avatar with shared\nskinning weights, we jointly infer personalized avatar shape, skinning weights,\nand pose-dependent deformations, which effectively improves overall geometric\nfidelity and reduces deformation artifacts. Moreover, to normalize pose\nvariations and resolve coupled ambiguity between canonical shapes and skinning\nweights, we design a 3D canonicalization process to produce pixel-aligned\ninitial conditions, which helps to reconstruct fine-grained geometric details.\nWe then propose a multi-frame feature aggregation to robustly reduce artifacts\nintroduced in canonicalization and fuse a plausible avatar preserving\nperson-specific identities. Finally, we train the model in an end-to-end\nframework on a large-scale capture dataset, which contains diverse human\nsubjects paired with high-quality 3D scans. Extensive experiments show that our\nmethod generates more authentic reconstruction and animation than\nstate-of-the-arts, and can be directly generalized to inputs from casually\ntaken phone photos. Project page and code is available at\nhttps:\/\/github.com\/rongakowang\/FRESA.\n","date":"2025-03-24"}
{"id":"2503.19209","title":"Byzantine Resilient Federated Multi-Task Representation Learning","abstract":"  In this paper, we propose BR-MTRL, a Byzantine-resilient multi-task\nrepresentation learning framework that handles faulty or malicious agents. Our\napproach leverages representation learning through a shared neural network\nmodel, where all clients share fixed layers, except for a client-specific final\nlayer. This structure captures shared features among clients while enabling\nindividual adaptation, making it a promising approach for leveraging client\ndata and computational power in heterogeneous federated settings to learn\npersonalized models. To learn the model, we employ an alternating gradient\ndescent strategy: each client optimizes its local model, updates its final\nlayer, and sends estimates of the shared representation to a central server for\naggregation. To defend against Byzantine agents, we employ geometric median\naggregation for robust client-server communication. Our method enables\npersonalized learning while maintaining resilience in distributed settings. We\nimplemented the proposed alternating gradient descent algorithm in a federated\ntestbed built using Amazon Web Services (AWS) platform and compared its\nperformance with various benchmark algorithms and their variations. Through\nextensive experiments using real-world datasets, including CIFAR-10 and\nFEMINIST, we demonstrated the effectiveness and robustness of our approach and\nits transferability to new unseen clients with limited data, even in the\npresence of Byzantine adversaries.\n","date":"2025-03-24"}
{"id":"2503.19211","title":"Towards Terminology Management Automation for Arabic","abstract":"  This paper presents a method and supporting tools for automation of\nterminology management for Arabic. The tools extract lists of parallel\nterminology matching terms in foreign languages to their Arabic counterparts\nfrom field specific texts. This has significant implications as it can be used\nto improve consistent translation and use of terms in specialized Arabic\nacademic books, and provides automated aid for enhancing cross lingual text\nprocessing. This automation of terminology management aims to reduce processing\ntime, and ensure use of consistent and correct terminology. The extraction\ntakes advantage of naturally occurring term translations. It considers several\ncandidate phrases of varying lengths that co-occur next to the foreign terms.\nThen it computes several similarity metrics, including lexicographic, phonetic,\nmorphological, and semantic ones to decide the problem. We experiment with\nheuristic, machine learning, and ML with post processing approaches. This paper\nreports on a novel curated dataset for the task, an existing expert reviewed\nindustry parallel corpora, and on the performance of the three approaches. The\nbest approach achieved 94.9% precision and 92.4% recall.\n","date":"2025-03-24"}
{"id":"2503.19212","title":"Continual Reinforcement Learning for HVAC Systems Control: Integrating\n  Hypernetworks and Transfer Learning","abstract":"  Buildings with Heating, Ventilation, and Air Conditioning (HVAC) systems play\na crucial role in ensuring indoor comfort and efficiency. While traditionally\ngoverned by physics-based models, the emergence of big data has enabled\ndata-driven methods like Deep Reinforcement Learning (DRL). However,\nReinforcement Learning (RL)-based techniques often suffer from sample\ninefficiency and limited generalization, especially across varying HVAC\nsystems. We introduce a model-based reinforcement learning framework that uses\na Hypernetwork to continuously learn environment dynamics across tasks with\ndifferent action spaces. This enables efficient synthetic rollout generation\nand improved sample usage. Our approach demonstrates strong backward transfer\nin a continual learning setting after training on a second task, minimal\nfine-tuning on the first task allows rapid convergence within just 5 episodes\nand thus outperforming Model Free Reinforcement Learning (MFRL) and effectively\nmitigating catastrophic forgetting. These findings have significant\nimplications for reducing energy consumption and operational costs in building\nmanagement, thus supporting global sustainability goals.\n  Keywords: Deep Reinforcement Learning, HVAC Systems Control, Hypernetworks,\nTransfer and Continual Learning, Catastrophic Forgetting\n","date":"2025-03-24"}
{"id":"2503.19213","title":"A Survey of Large Language Model Agents for Question Answering","abstract":"  This paper surveys the development of large language model (LLM)-based agents\nfor question answering (QA). Traditional agents face significant limitations,\nincluding substantial data requirements and difficulty in generalizing to new\nenvironments. LLM-based agents address these challenges by leveraging LLMs as\ntheir core reasoning engine. These agents achieve superior QA results compared\nto traditional QA pipelines and naive LLM QA systems by enabling interaction\nwith external environments. We systematically review the design of LLM agents\nin the context of QA tasks, organizing our discussion across key stages:\nplanning, question understanding, information retrieval, and answer generation.\nAdditionally, this paper identifies ongoing challenges and explores future\nresearch directions to enhance the performance of LLM agent QA systems.\n","date":"2025-03-24"}
{"id":"2503.19215","title":"On Symmetries in Convolutional Weights","abstract":"  We explore the symmetry of the mean k x k weight kernel in each layer of\nvarious convolutional neural networks. Unlike individual neurons, the mean\nkernels in internal layers tend to be symmetric about their centers instead of\nfavoring specific directions. We investigate why this symmetry emerges in\nvarious datasets and models, and how it is impacted by certain architectural\nchoices. We show how symmetry correlates with desirable properties such as\nshift and flip consistency, and might constitute an inherent inductive bias in\nconvolutional neural networks.\n","date":"2025-03-24"}
{"id":"2503.19217","title":"LLM Benchmarking with LLaMA2: Evaluating Code Development Performance\n  Across Multiple Programming Languages","abstract":"  The rapid evolution of large language models (LLMs) has opened new\npossibilities for automating various tasks in software development. This paper\nevaluates the capabilities of the Llama 2-70B model in automating these tasks\nfor scientific applications written in commonly used programming languages.\nUsing representative test problems, we assess the model's capacity to generate\ncode, documentation, and unit tests, as well as its ability to translate\nexisting code between commonly used programming languages. Our comprehensive\nanalysis evaluates the compilation, runtime behavior, and correctness of the\ngenerated and translated code. Additionally, we assess the quality of\nautomatically generated code, documentation and unit tests. Our results\nindicate that while Llama 2-70B frequently generates syntactically correct and\nfunctional code for simpler numerical tasks, it encounters substantial\ndifficulties with more complex, parallelized, or distributed computations,\nrequiring considerable manual corrections. We identify key limitations and\nsuggest areas for future improvements to better leverage AI-driven automation\nin scientific computing workflows.\n","date":"2025-03-24"}
{"id":"2503.19218","title":"Analytic DAG Constraints for Differentiable DAG Learning","abstract":"  Recovering the underlying Directed Acyclic Graph (DAG) structures from\nobservational data presents a formidable challenge, partly due to the\ncombinatorial nature of the DAG-constrained optimization problem. Recently,\nresearchers have identified gradient vanishing as one of the primary obstacles\nin differentiable DAG learning and have proposed several DAG constraints to\nmitigate this issue. By developing the necessary theory to establish a\nconnection between analytic functions and DAG constraints, we demonstrate that\nanalytic functions from the set $\\{f(x) = c_0 + \\sum_{i=1}^{\\infty}c_ix^i |\n\\forall i > 0, c_i > 0; r = \\lim_{i\\rightarrow \\infty}c_{i}\/c_{i+1} > 0\\}$ can\nbe employed to formulate effective DAG constraints. Furthermore, we establish\nthat this set of functions is closed under several functional operators,\nincluding differentiation, summation, and multiplication. Consequently, these\noperators can be leveraged to create novel DAG constraints based on existing\nones. Using these properties, we design a series of DAG constraints and develop\nan efficient algorithm to evaluate them. Experiments in various settings\ndemonstrate that our DAG constraints outperform previous state-of-the-art\ncomparators. Our implementation is available at\nhttps:\/\/github.com\/zzhang1987\/AnalyticDAGLearning.\n","date":"2025-03-24"}
{"id":"2503.19223","title":"Face Spoofing Detection using Deep Learning","abstract":"  Digital image spoofing has emerged as a significant security threat in\nbiometric authentication systems, particularly those relying on facial\nrecognition. This study evaluates the performance of three vision based models,\nMobileNetV2, ResNET50, and Vision Transformer, ViT, for spoof detection in\nimage classification, utilizing a dataset of 150,986 images divided into\ntraining , 140,002, testing, 10,984, and validation ,39,574, sets. Spoof\ndetection is critical for enhancing the security of image recognition systems,\nand this research compares the models effectiveness through accuracy,\nprecision, recall, and F1 score metrics. Results reveal that MobileNetV2\noutperforms other architectures on the test dataset, achieving an accuracy of\n91.59%, precision of 91.72%, recall of 91.59%, and F1 score of 91.58%, compared\nto ViT 86.54%, 88.28%, 86.54%, and 86.39%, respectively. On the validation\ndataset, MobileNetV2, and ViT excel, with MobileNetV2 slightly ahead at 97.17%\naccuracy versus ViT 96.36%. MobileNetV2 demonstrates faster convergence during\ntraining and superior generalization to unseen data, despite both models\nshowing signs of overfitting. These findings highlight MobileNetV2 balanced\nperformance and robustness, making it the preferred choice for spoof detection\napplications where reliability on new data is essential. The study underscores\nthe importance of model selection in security sensitive contexts and suggests\nMobileNetV2 as a practical solution for real world deployment.\n","date":"2025-03-25"}
{"id":"2503.19232","title":"HoGS: Unified Near and Far Object Reconstruction via Homogeneous\n  Gaussian Splatting","abstract":"  Novel view synthesis has demonstrated impressive progress recently, with 3D\nGaussian splatting (3DGS) offering efficient training time and photorealistic\nreal-time rendering. However, reliance on Cartesian coordinates limits 3DGS's\nperformance on distant objects, which is important for reconstructing unbounded\noutdoor environments. We found that, despite its ultimate simplicity, using\nhomogeneous coordinates, a concept on the projective geometry, for the 3DGS\npipeline remarkably improves the rendering accuracies of distant objects. We\ntherefore propose Homogeneous Gaussian Splatting (HoGS) incorporating\nhomogeneous coordinates into the 3DGS framework, providing a unified\nrepresentation for enhancing near and distant objects. HoGS effectively manages\nboth expansive spatial positions and scales particularly in outdoor unbounded\nenvironments by adopting projective geometry principles. Experiments show that\nHoGS significantly enhances accuracy in reconstructing distant objects while\nmaintaining high-quality rendering of nearby objects, along with fast training\nspeed and real-time rendering capability. Our implementations are available on\nour project page https:\/\/kh129.github.io\/hogs\/.\n","date":"2025-03-25"}
{"id":"2503.19240","title":"Beyond Object Categories: Multi-Attribute Reference Understanding for\n  Visual Grounding","abstract":"  Referring expression comprehension (REC) aims at achieving object\nlocalization based on natural language descriptions. However, existing REC\napproaches are constrained by object category descriptions and single-attribute\nintention descriptions, hindering their application in real-world scenarios. In\nnatural human-robot interactions, users often express their desires through\nindividual states and intentions, accompanied by guiding gestures, rather than\ndetailed object descriptions. To address this challenge, we propose Multi-ref\nEC, a novel task framework that integrates state descriptions, derived\nintentions, and embodied gestures to locate target objects. We introduce the\nState-Intention-Gesture Attributes Reference (SIGAR) dataset, which combines\nstate and intention expressions with embodied references. Through extensive\nexperiments with various baseline models on SIGAR, we demonstrate that properly\nordered multi-attribute references contribute to improved localization\nperformance, revealing that single-attribute reference is insufficient for\nnatural human-robot interaction scenarios. Our findings underscore the\nimportance of multi-attribute reference expressions in advancing\nvisual-language understanding.\n","date":"2025-03-25"}
{"id":"2503.19248","title":"Limited-angle x-ray nano-tomography with machine-learning enabled\n  iterative reconstruction engine","abstract":"  A long-standing challenge in tomography is the 'missing wedge' problem, which\narises when the acquisition of projection images within a certain angular range\nis restricted due to geometrical constraints. This incomplete dataset results\nin significant artifacts and poor resolution in the reconstructed image. To\ntackle this challenge, we propose an approach dubbed Perception Fused Iterative\nTomography Reconstruction Engine, which integrates a convolutional neural\nnetwork (CNN) with perceptional knowledge as a smart regularizer into an\niterative solving engine. We employ the Alternating Direction Method of\nMultipliers to optimize the solution in both physics and image domains, thereby\nachieving a physically coherent and visually enhanced result. We demonstrate\nthe effectiveness of the proposed approach using various experimental datasets\nobtained with different x-ray microscopy techniques. All show significantly\nimproved reconstruction even with a missing wedge of over 100 degrees - a\nscenario where conventional methods fail. Notably, it also improves the\nreconstruction in case of sparse projections, despite the network not being\nspecifically trained for that. This demonstrates the robustness and generality\nof our method of addressing commonly occurring challenges in 3D x-ray imaging\napplications for real-world problems.\n","date":"2025-03-25"}
{"id":"2503.19253","title":"$L^2$FMamba: Lightweight Light Field Image Super-Resolution with State\n  Space Model","abstract":"  Transformers bring significantly improved performance to the light field\nimage super-resolution task due to their long-range dependency modeling\ncapability. However, the inherently high computational complexity of their core\nself-attention mechanism has increasingly hindered their advancement in this\ntask. To address this issue, we first introduce the LF-VSSM block, a novel\nmodule inspired by progressive feature extraction, to efficiently capture\ncritical long-range spatial-angular dependencies in light field images. LF-VSSM\nsuccessively extracts spatial features within sub-aperture images,\nspatial-angular features between sub-aperture images, and spatial-angular\nfeatures between light field image pixels. On this basis, we propose a\nlightweight network, $L^2$FMamba (Lightweight Light Field Mamba), which\nintegrates the LF-VSSM block to leverage light field features for\nsuper-resolution tasks while overcoming the computational challenges of\nTransformer-based approaches. Extensive experiments on multiple light field\ndatasets demonstrate that our method reduces the number of parameters and\ncomplexity while achieving superior super-resolution performance with faster\ninference speed.\n","date":"2025-03-25"}
{"id":"2503.19255","title":"Data-Driven, ML-assisted Approaches to Problem Well-Posedness","abstract":"  Classically, to solve differential equation problems, it is necessary to\nspecify sufficient initial and\/or boundary conditions so as to allow the\nexistence of a unique solution. Well-posedness of differential equation\nproblems thus involves studying the existence and uniqueness of solutions, and\ntheir dependence to such pre-specified conditions. However, in part due to\nmathematical necessity, these conditions are usually specified \"to arbitrary\nprecision\" only on (appropriate portions of) the boundary of the space-time\ndomain. This does not mirror how data acquisition is performed in realistic\nsituations, where one may observe entire \"patches\" of solution data at\narbitrary space-time locations; alternatively one might have access to more\nthan one solutions stemming from the same differential operator. In our short\nwork, we demonstrate how standard tools from machine and manifold learning can\nbe used to infer, in a data driven manner, certain well-posedness features of\ndifferential equation problems, for initial\/boundary condition combinations\nunder which rigorous existence\/uniqueness theorems are not known. Our study\nnaturally combines a data assimilation perspective with an operator-learning\none.\n","date":"2025-03-25"}
{"id":"2503.19257","title":"SCI-IDEA: Context-Aware Scientific Ideation Using Token and Sentence\n  Embeddings","abstract":"  Every scientific discovery starts with an idea inspired by prior work,\ninterdisciplinary concepts, and emerging challenges. Recent advancements in\nlarge language models (LLMs) trained on scientific corpora have driven interest\nin AI-supported idea generation. However, generating context-aware,\nhigh-quality, and innovative ideas remains challenging. We introduce SCI-IDEA,\na framework that uses LLM prompting strategies and Aha Moment detection for\niterative idea refinement. SCI-IDEA extracts essential facets from research\npublications, assessing generated ideas on novelty, excitement, feasibility,\nand effectiveness. Comprehensive experiments validate SCI-IDEA's effectiveness,\nachieving average scores of 6.84, 6.86, 6.89, and 6.84 (on a 1-10 scale) across\nnovelty, excitement, feasibility, and effectiveness, respectively. Evaluations\nemployed GPT-4o, GPT-4.5, DeepSeek-32B (each under 2-shot prompting), and\nDeepSeek-70B (3-shot prompting), with token-level embeddings used for Aha\nMoment detection. Similarly, it achieves scores of 6.87, 6.86, 6.83, and 6.87\nusing GPT-4o under 5-shot prompting, GPT-4.5 under 3-shot prompting,\nDeepSeek-32B under zero-shot chain-of-thought prompting, and DeepSeek-70B under\n5-shot prompting with sentence-level embeddings. We also address ethical\nconsiderations such as intellectual credit, potential misuse, and balancing\nhuman creativity with AI-driven ideation. Our results highlight SCI-IDEA's\npotential to facilitate the structured and flexible exploration of\ncontext-aware scientific ideas, supporting innovation while maintaining ethical\nstandards.\n","date":"2025-03-25"}
{"id":"2503.19258","title":"Adaptive Multi-Order Graph Regularized NMF with Dual Sparsity for\n  Hyperspectral Unmixing","abstract":"  Hyperspectral unmixing (HU) is a critical yet challenging task in remote\nsensing. However, existing nonnegative matrix factorization (NMF) methods with\ngraph learning mostly focus on first-order or second-order nearest neighbor\nrelationships and usually require manual parameter tuning, which fails to\ncharacterize intrinsic data structures. To address the above issues, we propose\na novel adaptive multi-order graph regularized NMF method (MOGNMF) with three\nkey features. First, multi-order graph regularization is introduced into the\nNMF framework to exploit global and local information comprehensively. Second,\nthese parameters associated with the multi-order graph are learned adaptively\nthrough a data-driven approach. Third, dual sparsity is embedded to obtain\nbetter robustness, i.e., $\\ell_{1\/2}$-norm on the abundance matrix and\n$\\ell_{2,1}$-norm on the noise matrix. To solve the proposed model, we develop\nan alternating minimization algorithm whose subproblems have explicit\nsolutions, thus ensuring effectiveness. Experiments on simulated and real\nhyperspectral data indicate that the proposed method delivers better unmixing\nresults.\n","date":"2025-03-25"}
{"id":"2503.19260","title":"Linguistic Blind Spots of Large Language Models","abstract":"  Large language models (LLMs) are the foundation of many AI applications\ntoday. However, despite their remarkable proficiency in generating coherent\ntext, questions linger regarding their ability to perform fine-grained\nlinguistic annotation tasks, such as detecting nouns or verbs, or identifying\nmore complex syntactic structures like clauses in input texts. These tasks\nrequire precise syntactic and semantic understanding of input text, and when\nLLMs underperform on specific linguistic structures, it raises concerns about\ntheir reliability for detailed linguistic analysis and whether their (even\ncorrect) outputs truly reflect an understanding of the inputs. In this paper,\nwe empirically study the performance of recent LLMs on fine-grained linguistic\nannotation tasks. Through a series of experiments, we find that recent LLMs\nshow limited efficacy in addressing linguistic queries and often struggle with\nlinguistically complex inputs. We show that the most capable LLM (Llama3-70b)\nmakes notable errors in detecting linguistic structures, such as misidentifying\nembedded clauses, failing to recognize verb phrases, and confusing complex\nnominals with clauses. Our results provide insights to inform future\nadvancements in LLM design and development.\n","date":"2025-03-25"}
{"id":"2503.19262","title":"Learning Hazing to Dehazing: Towards Realistic Haze Generation for\n  Real-World Image Dehazing","abstract":"  Existing real-world image dehazing methods primarily attempt to fine-tune\npre-trained models or adapt their inference procedures, thus heavily relying on\nthe pre-trained models and associated training data. Moreover, restoring\nheavily distorted information under dense haze requires generative diffusion\nmodels, whose potential in dehazing remains underutilized partly due to their\nlengthy sampling processes. To address these limitations, we introduce a novel\nhazing-dehazing pipeline consisting of a Realistic Hazy Image Generation\nframework (HazeGen) and a Diffusion-based Dehazing framework (DiffDehaze).\nSpecifically, HazeGen harnesses robust generative diffusion priors of\nreal-world hazy images embedded in a pre-trained text-to-image diffusion model.\nBy employing specialized hybrid training and blended sampling strategies,\nHazeGen produces realistic and diverse hazy images as high-quality training\ndata for DiffDehaze. To alleviate the inefficiency and fidelity concerns\nassociated with diffusion-based methods, DiffDehaze adopts an Accelerated\nFidelity-Preserving Sampling process (AccSamp). The core of AccSamp is the\nTiled Statistical Alignment Operation (AlignOp), which can provide a clean and\nfaithful dehazing estimate within a small fraction of sampling steps to reduce\ncomplexity and enable effective fidelity guidance. Extensive experiments\ndemonstrate the superior dehazing performance and visual quality of our\napproach over existing methods. The code is available at\nhttps:\/\/github.com\/ruiyi-w\/Learning-Hazing-to-Dehazing.\n","date":"2025-03-25"}
{"id":"2503.19263","title":"DWIM: Towards Tool-aware Visual Reasoning via Discrepancy-aware Workflow\n  Generation & Instruct-Masking Tuning","abstract":"  Visual reasoning (VR), which is crucial in many fields for enabling\nhuman-like visual understanding, remains highly challenging. Recently,\ncompositional visual reasoning approaches, which leverage the reasoning\nabilities of large language models (LLMs) with integrated tools to solve\nproblems, have shown promise as more effective strategies than end-to-end VR\nmethods. However, these approaches face limitations, as frozen LLMs lack tool\nawareness in VR, leading to performance bottlenecks. While leveraging LLMs for\nreasoning is widely used in other domains, they are not directly applicable to\nVR due to limited training data, imperfect tools that introduce errors and\nreduce data collection efficiency in VR, and challenging in fine-tuning on\nnoisy workflows. To address these challenges, we propose DWIM: i)\nDiscrepancy-aware training Workflow generation, which assesses tool usage and\nextracts more viable workflows for training; and ii) Instruct-Masking\nfine-tuning, which guides the model to only clone effective actions, enabling\nthe generation of more practical solutions. Our experiments demonstrate that\nDWIM achieves state-of-the-art performance across various VR tasks, exhibiting\nstrong generalization on multiple widely-used datasets.\n","date":"2025-03-25"}
{"id":"2503.19265","title":"PHEONA: An Evaluation Framework for Large Language Model-based\n  Approaches to Computational Phenotyping","abstract":"  Computational phenotyping is essential for biomedical research but often\nrequires significant time and resources, especially since traditional methods\ntypically involve extensive manual data review. While machine learning and\nnatural language processing advancements have helped, further improvements are\nneeded. Few studies have explored using Large Language Models (LLMs) for these\ntasks despite known advantages of LLMs for text-based tasks. To facilitate\nfurther research in this area, we developed an evaluation framework, Evaluation\nof PHEnotyping for Observational Health Data (PHEONA), that outlines\ncontext-specific considerations. We applied and demonstrated PHEONA on concept\nclassification, a specific task within a broader phenotyping process for Acute\nRespiratory Failure (ARF) respiratory support therapies. From the sample\nconcepts tested, we achieved high classification accuracy, suggesting the\npotential for LLM-based methods to improve computational phenotyping processes.\n","date":"2025-03-25"}
{"id":"2503.19267","title":"NeoRL-2: Near Real-World Benchmarks for Offline Reinforcement Learning\n  with Extended Realistic Scenarios","abstract":"  Offline reinforcement learning (RL) aims to learn from historical data\nwithout requiring (costly) access to the environment. To facilitate offline RL\nresearch, we previously introduced NeoRL, which highlighted that datasets from\nreal-world tasks are often conservative and limited. With years of experience\napplying offline RL to various domains, we have identified additional\nreal-world challenges. These include extremely conservative data distributions\nproduced by deployed control systems, delayed action effects caused by\nhigh-latency transitions, external factors arising from the uncontrollable\nvariance of transitions, and global safety constraints that are difficult to\nevaluate during the decision-making process. These challenges are\nunderrepresented in previous benchmarks but frequently occur in real-world\ntasks. To address this, we constructed the extended Near Real-World Offline RL\nBenchmark (NeoRL-2), which consists of 7 datasets from 7 simulated tasks along\nwith their corresponding evaluation simulators. Benchmarking results from\nstate-of-the-art offline RL approaches demonstrate that current methods often\nstruggle to outperform the data-collection behavior policy, highlighting the\nneed for more effective methods. We hope NeoRL-2 will accelerate the\ndevelopment of reinforcement learning algorithms for real-world applications.\nThe benchmark project page is available at https:\/\/github.com\/polixir\/NeoRL2.\n","date":"2025-03-25"}
{"id":"2503.19271","title":"MARS: Memory-Enhanced Agents with Reflective Self-improvement","abstract":"  Large language models (LLMs) have made significant advances in the field of\nnatural language processing, but they still face challenges such as continuous\ndecision-making, lack of long-term memory, and limited context windows in\ndynamic environments. To address these issues, this paper proposes an\ninnovative framework Memory-Enhanced Agents with Reflective Self-improvement.\nThe MARS framework comprises three agents: the User, the Assistant, and the\nChecker. By integrating iterative feedback, reflective mechanisms, and a memory\noptimization mechanism based on the Ebbinghaus forgetting curve, it\nsignificantly enhances the agents capabilities in handling multi-tasking and\nlong-span information.\n","date":"2025-03-25"}
{"id":"2503.19274","title":"CoMAC: Conversational Agent for Multi-Source Auxiliary Context with\n  Sparse and Symmetric Latent Interactions","abstract":"  Recent advancements in AI-driven conversational agents have exhibited immense\npotential of AI applications. Effective response generation is crucial to the\nsuccess of these agents. While extensive research has focused on leveraging\nmultiple auxiliary data sources (e.g., knowledge bases and personas) to enhance\nresponse generation, existing methods often struggle to efficiently extract\nrelevant information from these sources. There are still clear limitations in\nthe ability to combine versatile conversational capabilities with adherence to\nknown facts and adaptation to large variations in user preferences and belief\nsystems, which continues to hinder the wide adoption of conversational AI\ntools. This paper introduces a novel method, Conversational Agent for\nMulti-Source Auxiliary Context with Sparse and Symmetric Latent Interactions\n(CoMAC), for conversation generation, which employs specialized encoding\nstreams and post-fusion grounding networks for multiple data sources to\nidentify relevant persona and knowledge information for the conversation. CoMAC\nalso leverages a novel text similarity metric that allows bi-directional\ninformation sharing among multiple sources and focuses on a selective subset of\nmeaningful words. Our experiments show that CoMAC improves the relevant persona\nand knowledge prediction accuracies and response generation quality\nsignificantly over two state-of-the-art methods.\n","date":"2025-03-25"}
{"id":"2503.19276","title":"Context-Aware Semantic Segmentation: Enhancing Pixel-Level Understanding\n  with Large Language Models for Advanced Vision Applications","abstract":"  Semantic segmentation has made significant strides in pixel-level image\nunderstanding, yet it remains limited in capturing contextual and semantic\nrelationships between objects. Current models, such as CNN and\nTransformer-based architectures, excel at identifying pixel-level features but\nfail to distinguish semantically similar objects (e.g., \"doctor\" vs. \"nurse\" in\na hospital scene) or understand complex contextual scenarios (e.g.,\ndifferentiating a running child from a regular pedestrian in autonomous\ndriving). To address these limitations, we proposed a novel Context-Aware\nSemantic Segmentation framework that integrates Large Language Models (LLMs)\nwith state-of-the-art vision backbones. Our hybrid model leverages the Swin\nTransformer for robust visual feature extraction and GPT-4 for enriching\nsemantic understanding through text embeddings. A Cross-Attention Mechanism is\nintroduced to align vision and language features, enabling the model to reason\nabout context more effectively. Additionally, Graph Neural Networks (GNNs) are\nemployed to model object relationships within the scene, capturing dependencies\nthat are overlooked by traditional models. Experimental results on benchmark\ndatasets (e.g., COCO, Cityscapes) demonstrate that our approach outperforms the\nexisting methods in both pixel-level accuracy (mIoU) and contextual\nunderstanding (mAP). This work bridges the gap between vision and language,\npaving the path for more intelligent and context-aware vision systems in\napplications including autonomous driving, medical imaging, and robotics.\n","date":"2025-03-25"}
{"id":"2503.19278","title":"Multiscale Feature Importance-based Bit Allocation for End-to-End\n  Feature Coding for Machines","abstract":"  Feature Coding for Machines (FCM) aims to compress intermediate features\neffectively for remote intelligent analytics, which is crucial for future\nintelligent visual applications. In this paper, we propose a Multiscale Feature\nImportance-based Bit Allocation (MFIBA) for end-to-end FCM. First, we find that\nthe importance of features for machine vision tasks varies with the scales,\nobject size, and image instances. Based on this finding, we propose a\nMultiscale Feature Importance Prediction (MFIP) module to predict the\nimportance weight for each scale of features. Secondly, we propose a task\nloss-rate model to establish the relationship between the task accuracy losses\nof using compressed features and the bitrate of encoding these features.\nFinally, we develop a MFIBA for end-to-end FCM, which is able to assign coding\nbits of multiscale features more reasonably based on their importance.\nExperimental results demonstrate that when combined with a retained Efficient\nLearned Image Compression (ELIC), the proposed MFIBA achieves an average of\n38.202% bitrate savings in object detection compared to the anchor ELIC.\nMoreover, the proposed MFIBA achieves an average of 17.212% and 36.492% feature\nbitrate savings for instance segmentation and keypoint detection, respectively.\nWhen the proposed MFIBA is applied to the LIC-TCM, it achieves an average of\n18.103%, 19.866% and 19.597% bit rate savings on three machine vision tasks,\nrespectively, which validates the proposed MFIBA has good generalizability and\nadaptability to different machine vision tasks and FCM base codecs.\n","date":"2025-03-25"}
{"id":"2503.19279","title":"Machine-assisted writing evaluation: Exploring pre-trained language\n  models in analyzing argumentative moves","abstract":"  The study investigates the efficacy of pre-trained language models (PLMs) in\nanalyzing argumentative moves in a longitudinal learner corpus. Prior studies\non argumentative moves often rely on qualitative analysis and manual coding,\nlimiting their efficiency and generalizability. The study aims to: 1) to assess\nthe reliability of PLMs in analyzing argumentative moves; 2) to utilize\nPLM-generated annotations to illustrate developmental patterns and predict\nwriting quality. A longitudinal corpus of 1643 argumentative texts from 235\nEnglish learners in China is collected and annotated into six move types:\nclaim, data, counter-claim, counter-data, rebuttal, and non-argument. The\ncorpus is divided into training, validation, and application sets annotated by\nhuman experts and PLMs. We use BERT as one of the implementations of PLMs. The\nresults indicate a robust reliability of PLMs in analyzing argumentative moves,\nwith an overall F1 score of 0.743, surpassing existing models in the field.\nAdditionally, PLM-labeled argumentative moves effectively capture developmental\npatterns and predict writing quality. Over time, students exhibit an increase\nin the use of data and counter-claims and a decrease in non-argument moves.\nWhile low-quality texts are characterized by a predominant use of claims and\ndata supporting only oneside position, mid- and high-quality texts demonstrate\nan integrative perspective with a higher ratio of counter-claims, counter-data,\nand rebuttals. This study underscores the transformative potential of\nintegrating artificial intelligence into language education, enhancing the\nefficiency and accuracy of evaluating students' writing. The successful\napplication of PLMs can catalyze the development of educational technology,\npromoting a more data-driven and personalized learning environment that\nsupports diverse educational needs.\n","date":"2025-03-25"}
{"id":"2503.19280","title":"LogicLearner: A Tool for the Guided Practice of Propositional Logic\n  Proofs","abstract":"  The study of propositional logic -- fundamental to the theory of computing --\nis a cornerstone of the undergraduate computer science curriculum. Learning to\nsolve logical proofs requires repeated guided practice, but undergraduate\nstudents often lack access to on-demand tutoring in a judgment-free\nenvironment. In this work, we highlight the need for guided practice tools in\nundergraduate mathematics education and outline the desiderata of an effective\npractice tool. We accordingly develop LogicLearner, a web application for\nguided logic proof practice. LogicLearner consists of an interface to attempt\nlogic proofs step-by-step and an automated proof solver to generate solutions\non the fly, allowing users to request guidance as needed. We pilot LogicLearner\nas a practice tool in two semesters of an undergraduate discrete mathematics\ncourse and receive strongly positive feedback for usability and pedagogical\nvalue in student surveys. To the best of our knowledge, LogicLearner is the\nonly learning tool that provides an end-to-end practice environment for logic\nproofs with immediate, judgment-free feedback.\n","date":"2025-03-25"}
{"id":"2503.19281","title":"CubeRobot: Grounding Language in Rubik's Cube Manipulation via\n  Vision-Language Model","abstract":"  Proving Rubik's Cube theorems at the high level represents a notable\nmilestone in human-level spatial imagination and logic thinking and reasoning.\nTraditional Rubik's Cube robots, relying on complex vision systems and fixed\nalgorithms, often struggle to adapt to complex and dynamic scenarios. To\novercome this limitation, we introduce CubeRobot, a novel vision-language model\n(VLM) tailored for solving 3x3 Rubik's Cubes, empowering embodied agents with\nmultimodal understanding and execution capabilities. We used the CubeCoT image\ndataset, which contains multiple-level tasks (43 subtasks in total) that humans\nare unable to handle, encompassing various cube states. We incorporate a\ndual-loop VisionCoT architecture and Memory Stream, a paradigm for extracting\ntask-related features from VLM-generated planning queries, thus enabling\nCubeRobot to independent planning, decision-making, reflection and separate\nmanagement of high- and low-level Rubik's Cube tasks. Furthermore, in low-level\nRubik's Cube restoration tasks, CubeRobot achieved a high accuracy rate of\n100%, similar to 100% in medium-level tasks, and achieved an accuracy rate of\n80% in high-level tasks.\n","date":"2025-03-25"}
{"id":"2503.19283","title":"ISPDiffuser: Learning RAW-to-sRGB Mappings with Texture-Aware Diffusion\n  Models and Histogram-Guided Color Consistency","abstract":"  RAW-to-sRGB mapping, or the simulation of the traditional camera image signal\nprocessor (ISP), aims to generate DSLR-quality sRGB images from raw data\ncaptured by smartphone sensors. Despite achieving comparable results to\nsophisticated handcrafted camera ISP solutions, existing learning-based methods\nstill struggle with detail disparity and color distortion. In this paper, we\npresent ISPDiffuser, a diffusion-based decoupled framework that separates the\nRAW-to-sRGB mapping into detail reconstruction in grayscale space and color\nconsistency mapping from grayscale to sRGB. Specifically, we propose a\ntexture-aware diffusion model that leverages the generative ability of\ndiffusion models to focus on local detail recovery, in which a texture\nenrichment loss is further proposed to prompt the diffusion model to generate\nmore intricate texture details. Subsequently, we introduce a histogram-guided\ncolor consistency module that utilizes color histogram as guidance to learn\nprecise color information for grayscale to sRGB color consistency mapping, with\na color consistency loss designed to constrain the learned color information.\nExtensive experimental results show that the proposed ISPDiffuser outperforms\nstate-of-the-art competitors both quantitatively and visually. The code is\navailable at https:\/\/github.com\/RenYangSCU\/ISPDiffuser.\n","date":"2025-03-25"}
{"id":"2503.19285","title":"No Black Box Anymore: Demystifying Clinical Predictive Modeling with\n  Temporal-Feature Cross Attention Mechanism","abstract":"  Despite the outstanding performance of deep learning models in clinical\nprediction tasks, explainability remains a significant challenge. Inspired by\ntransformer architectures, we introduce the Temporal-Feature Cross Attention\nMechanism (TFCAM), a novel deep learning framework designed to capture dynamic\ninteractions among clinical features across time, enhancing both predictive\naccuracy and interpretability. In an experiment with 1,422 patients with\nChronic Kidney Disease, predicting progression to End-Stage Renal Disease,\nTFCAM outperformed LSTM and RETAIN baselines, achieving an AUROC of 0.95 and an\nF1-score of 0.69. Beyond performance gains, TFCAM provides multi-level\nexplainability by identifying critical temporal periods, ranking feature\nimportance, and quantifying how features influence each other across time\nbefore affecting predictions. Our approach addresses the \"black box\"\nlimitations of deep learning in healthcare, offering clinicians transparent\ninsights into disease progression mechanisms while maintaining state-of-the-art\npredictive performance.\n","date":"2025-03-25"}
{"id":"2503.19292","title":"Adaptive Wavelet Filters as Practical Texture Feature Amplifiers for\n  Parkinson's Disease Screening in OCT","abstract":"  Parkinson's disease (PD) is a prevalent neurodegenerative disorder globally.\nThe eye's retina is an extension of the brain and has great potential in PD\nscreening. Recent studies have suggested that texture features extracted from\nretinal layers can be adopted as biomarkers for PD diagnosis under optical\ncoherence tomography (OCT) images. Frequency domain learning techniques can\nenhance the feature representations of deep neural networks (DNNs) by\ndecomposing frequency components involving rich texture features. Additionally,\nprevious works have not exploited texture features for automated PD screening\nin OCT. Motivated by the above analysis, we propose a novel Adaptive Wavelet\nFilter (AWF) that serves as the Practical Texture Feature Amplifier to fully\nleverage the merits of texture features to boost the PD screening performance\nof DNNs with the aid of frequency domain learning. Specifically, AWF first\nenhances texture feature representation diversities via channel mixer, then\nemphasizes informative texture feature representations with the well-designed\nadaptive wavelet filtering token mixer. By combining the AWFs with the DNN\nstem, AWFNet is constructed for automated PD screening. Additionally, we\nintroduce a novel Balanced Confidence (BC) Loss by mining the potential of\nsample-wise predicted probabilities of all classes and class frequency prior,\nto further boost the PD screening performance and trustworthiness of AWFNet.\nThe extensive experiments manifest the superiority of our AWFNet and BC over\nstate-of-the-art methods in terms of PD screening performance and\ntrustworthiness.\n","date":"2025-03-25"}
{"id":"2503.19295","title":"Exploring Semantic Feature Discrimination for Perceptual Image\n  Super-Resolution and Opinion-Unaware No-Reference Image Quality Assessment","abstract":"  Generative Adversarial Networks (GANs) have been widely applied to image\nsuper-resolution (SR) to enhance the perceptual quality. However, most existing\nGAN-based SR methods typically perform coarse-grained discrimination directly\non images and ignore the semantic information of images, making it challenging\nfor the super resolution networks (SRN) to learn fine-grained and\nsemantic-related texture details. To alleviate this issue, we propose a\nsemantic feature discrimination method, SFD, for perceptual SR. Specifically,\nwe first design a feature discriminator (Feat-D), to discriminate the\npixel-wise middle semantic features from CLIP, aligning the feature\ndistributions of SR images with that of high-quality images. Additionally, we\npropose a text-guided discrimination method (TG-D) by introducing learnable\nprompt pairs (LPP) in an adversarial manner to perform discrimination on the\nmore abstract output feature of CLIP, further enhancing the discriminative\nability of our method. With both Feat-D and TG-D, our SFD can effectively\ndistinguish between the semantic feature distributions of low-quality and\nhigh-quality images, encouraging SRN to generate more realistic and\nsemantic-relevant textures. Furthermore, based on the trained Feat-D and LPP,\nwe propose a novel opinion-unaware no-reference image quality assessment (OU\nNR-IQA) method, SFD-IQA, greatly improving OU NR-IQA performance without any\nadditional targeted training. Extensive experiments on classical SISR,\nreal-world SISR, and OU NR-IQA tasks demonstrate the effectiveness of our\nproposed methods.\n","date":"2025-03-25"}
{"id":"2503.19296","title":"Fine-grained Textual Inversion Network for Zero-Shot Composed Image\n  Retrieval","abstract":"  Composed Image Retrieval (CIR) allows users to search target images with a\nmultimodal query, comprising a reference image and a modification text that\ndescribes the user's modification demand over the reference image.\nNevertheless, due to the expensive labor cost of training data annotation,\nrecent researchers have shifted to the challenging task of zero-shot CIR\n(ZS-CIR), which targets fulfilling CIR without annotated triplets. The pioneer\nZS-CIR studies focus on converting the CIR task into a standard text-to-image\nretrieval task by pre-training a textual inversion network that can map a given\nimage into a single pseudo-word token. Despite their significant progress,\ntheir coarse-grained textual inversion may be insufficient to capture the full\ncontent of the image accurately. To overcome this issue, in this work, we\npropose a novel Fine-grained Textual Inversion Network for ZS-CIR, named\nFTI4CIR. In particular, FTI4CIR comprises two main components: fine-grained\npseudo-word token mapping and tri-wise caption-based semantic regularization.\nThe former maps the image into a subject-oriented pseudo-word token and several\nattribute-oriented pseudo-word tokens to comprehensively express the image in\nthe textual form, while the latter works on jointly aligning the fine-grained\npseudo-word tokens to the real-word token embedding space based on a\nBLIP-generated image caption template. Extensive experiments conducted on three\nbenchmark datasets demonstrate the superiority of our proposed method.\n","date":"2025-03-25"}
{"id":"2503.19300","title":"UniMoMo: Unified Generative Modeling of 3D Molecules for De Novo Binder\n  Design","abstract":"  The design of target-specific molecules such as small molecules, peptides,\nand antibodies is vital for biological research and drug discovery. Existing\ngenerative methods are restricted to single-domain molecules, failing to\naddress versatile therapeutic needs or utilize cross-domain transferability to\nenhance model performance. In this paper, we introduce Unified generative\nModeling of 3D Molecules (UniMoMo), the first framework capable of designing\nbinders of multiple molecular domains using a single model. In particular,\nUniMoMo unifies the representations of different molecules as graphs of blocks,\nwhere each block corresponds to either a standard amino acid or a molecular\nfragment. Based on these unified representations, UniMoMo utilizes a geometric\nlatent diffusion model for 3D molecular generation, featuring an iterative\nfull-atom autoencoder to compress blocks into latent space points, followed by\nan E(3)-equivariant diffusion process. Extensive benchmarks across peptides,\nantibodies, and small molecules demonstrate the superiority of our unified\nframework over existing domain-specific models, highlighting the benefits of\nmulti-domain training.\n","date":"2025-03-25"}
{"id":"2503.19302","title":"Observation Adaptation via Annealed Importance Resampling for Partially\n  Observable Markov Decision Processes","abstract":"  Partially observable Markov decision processes (POMDPs) are a general\nmathematical model for sequential decision-making in stochastic environments\nunder state uncertainty. POMDPs are often solved \\textit{online}, which enables\nthe algorithm to adapt to new information in real time. Online solvers\ntypically use bootstrap particle filters based on importance resampling for\nupdating the belief distribution. Since directly sampling from the ideal state\ndistribution given the latest observation and previous state is infeasible,\nparticle filters approximate the posterior belief distribution by propagating\nstates and adjusting weights through prediction and resampling steps. However,\nin practice, the importance resampling technique often leads to particle\ndegeneracy and sample impoverishment when the state transition model poorly\naligns with the posterior belief distribution, especially when the received\nobservation is highly informative. We propose an approach that constructs a\nsequence of bridge distributions between the state-transition and optimal\ndistributions through iterative Monte Carlo steps, better accommodating noisy\nobservations in online POMDP solvers. Our algorithm demonstrates significantly\nsuperior performance compared to state-of-the-art methods when evaluated across\nmultiple challenging POMDP domains.\n","date":"2025-03-25"}
{"id":"2503.19303","title":"BIMII-Net: Brain-Inspired Multi-Iterative Interactive Network for RGB-T\n  Road Scene Semantic Segmentation","abstract":"  RGB-T road scene semantic segmentation enhances visual scene understanding in\ncomplex environments characterized by inadequate illumination or occlusion by\nfusing information from RGB and thermal images. Nevertheless, existing RGB-T\nsemantic segmentation models typically depend on simple addition or\nconcatenation strategies or ignore the differences between information at\ndifferent levels. To address these issues, we proposed a novel RGB-T road scene\nsemantic segmentation network called Brain-Inspired Multi-Iteration Interaction\nNetwork (BIMII-Net). First, to meet the requirements of accurate texture and\nlocal information extraction in road scenarios like autonomous driving, we\nproposed a deep continuous-coupled neural network (DCCNN) architecture based on\na brain-inspired model. Second, to enhance the interaction and expression\ncapabilities among multi-modal information, we designed a cross explicit\nattention-enhanced fusion module (CEAEF-Module) in the feature fusion stage of\nBIMII-Net to effectively integrate features at different levels. Finally, we\nconstructed a complementary interactive multi-layer decoder structure,\nincorporating the shallow-level feature iteration module (SFI-Module), the\ndeep-level feature iteration module (DFI-Module), and the multi-feature\nenhancement module (MFE-Module) to collaboratively extract texture details and\nglobal skeleton information, with multi-module joint supervision further\noptimizing the segmentation results. Experimental results demonstrate that\nBIMII-Net achieves state-of-the-art (SOTA) performance in the brain-inspired\ncomputing domain and outperforms most existing RGB-T semantic segmentation\nmethods. It also exhibits strong generalization capabilities on multiple RGB-T\ndatasets, proving the effectiveness of brain-inspired computer models in\nmulti-modal image segmentation tasks.\n","date":"2025-03-25"}
{"id":"2503.19306","title":"Centroid Decision Forest","abstract":"  This paper introduces the centroid decision forest (CDF), a novel ensemble\nlearning framework that redefines the splitting strategy and tree building in\nthe ordinary decision trees for high-dimensional classification. The splitting\napproach in CDF differs from the traditional decision trees in theat the class\nseparability score (CSS) determines the selection of the most discriminative\nfeatures at each node to construct centroids of the partitions (daughter\nnodes). The splitting criterion uses the Euclidean distance measurements from\neach class centroid to achieve a splitting mechanism that is more flexible and\nrobust. Centroids are constructed by computing the mean feature values of the\nselected features for each class, ensuring a class-representative division of\nthe feature space. This centroid-driven approach enables CDF to capture complex\nclass structures while maintaining interpretability and scalability. To\nevaluate CDF, 23 high-dimensional datasets are used to assess its performance\nagainst different state-of-the-art classifiers through classification accuracy\nand Cohen's kappa statistic. The experimental results show that CDF outperforms\nthe conventional methods establishing its effectiveness and flexibility for\nhigh-dimensional classification problems.\n","date":"2025-03-25"}
{"id":"2503.19307","title":"Analyzing the Synthetic-to-Real Domain Gap in 3D Hand Pose Estimation","abstract":"  Recent synthetic 3D human datasets for the face, body, and hands have pushed\nthe limits on photorealism. Face recognition and body pose estimation have\nachieved state-of-the-art performance using synthetic training data alone, but\nfor the hand, there is still a large synthetic-to-real gap. This paper presents\nthe first systematic study of the synthetic-to-real gap of 3D hand pose\nestimation. We analyze the gap and identify key components such as the forearm,\nimage frequency statistics, hand pose, and object occlusions. To facilitate our\nanalysis, we propose a data synthesis pipeline to synthesize high-quality data.\nWe demonstrate that synthetic hand data can achieve the same level of accuracy\nas real data when integrating our identified components, paving the path to use\nsynthetic data alone for hand pose estimation. Code and data are available at:\nhttps:\/\/github.com\/delaprada\/HandSynthesis.git.\n","date":"2025-03-25"}
{"id":"2503.19308","title":"A Comprehensive Analysis of Mamba for 3D Volumetric Medical Image\n  Segmentation","abstract":"  Mamba, with its selective State Space Models (SSMs), offers a more\ncomputationally efficient solution than Transformers for long-range dependency\nmodeling. However, there is still a debate about its effectiveness in\nhigh-resolution 3D medical image segmentation. In this study, we present a\ncomprehensive investigation into Mamba's capabilities in 3D medical image\nsegmentation by tackling three pivotal questions: Can Mamba replace\nTransformers? Can it elevate multi-scale representation learning? Is complex\nscanning necessary to unlock its full potential? We evaluate Mamba's\nperformance across three large public benchmarks-AMOS, TotalSegmentator, and\nBraTS. Our findings reveal that UlikeMamba, a U-shape Mamba-based network,\nconsistently surpasses UlikeTrans, a U-shape Transformer-based network,\nparticularly when enhanced with custom-designed 3D depthwise convolutions,\nboosting accuracy and computational efficiency. Further, our proposed\nmulti-scale Mamba block demonstrates superior performance in capturing both\nfine-grained details and global context, especially in complex segmentation\ntasks, surpassing Transformer-based counterparts. We also critically assess\ncomplex scanning strategies, finding that simpler methods often suffice, while\nour Tri-scan approach delivers notable advantages in the most challenging\nscenarios. By integrating these advancements, we introduce a new network for 3D\nmedical image segmentation, positioning Mamba as a transformative force that\noutperforms leading models such as nnUNet, CoTr, and U-Mamba, offering\ncompetitive accuracy with superior computational efficiency. This study\nprovides key insights into Mamba's unique advantages, paving the way for more\nefficient and accurate approaches to 3D medical imaging.\n","date":"2025-03-25"}
{"id":"2503.19309","title":"Iterative Hypothesis Generation for Scientific Discovery with Monte\n  Carlo Nash Equilibrium Self-Refining Trees","abstract":"  Scientific hypothesis generation is a fundamentally challenging task in\nresearch, requiring the synthesis of novel and empirically grounded insights.\nTraditional approaches rely on human intuition and domain expertise, while\npurely large language model (LLM) based methods often struggle to produce\nhypotheses that are both innovative and reliable. To address these limitations,\nwe propose the Monte Carlo Nash Equilibrium Self-Refine Tree (MC-NEST), a novel\nframework that integrates Monte Carlo Tree Search with Nash Equilibrium\nstrategies to iteratively refine and validate hypotheses. MC-NEST dynamically\nbalances exploration and exploitation through adaptive sampling strategies,\nwhich prioritize high-potential hypotheses while maintaining diversity in the\nsearch space. We demonstrate the effectiveness of MC-NEST through comprehensive\nexperiments across multiple domains, including biomedicine, social science, and\ncomputer science. MC-NEST achieves average scores of 2.65, 2.74, and 2.80 (on a\n1-3 scale) for novelty, clarity, significance, and verifiability metrics on the\nsocial science, computer science, and biomedicine datasets, respectively,\noutperforming state-of-the-art prompt-based methods, which achieve 2.36, 2.51,\nand 2.52 on the same datasets. These results underscore MC-NEST's ability to\ngenerate high-quality, empirically grounded hypotheses across diverse domains.\nFurthermore, MC-NEST facilitates structured human-AI collaboration, ensuring\nthat LLMs augment human creativity rather than replace it. By addressing key\nchallenges such as iterative refinement and the exploration-exploitation\nbalance, MC-NEST sets a new benchmark in automated hypothesis generation.\nAdditionally, MC-NEST's ethical design enables responsible AI use, emphasizing\ntransparency and human supervision in hypothesis generation.\n","date":"2025-03-25"}
{"id":"2503.19311","title":"LRSCLIP: A Vision-Language Foundation Model for Aligning Remote Sensing\n  Image with Longer Text","abstract":"  This study addresses the technical bottlenecks in handling long text and the\n\"hallucination\" issue caused by insufficient short text information in remote\nsensing vision-language foundation models (VLFM). We propose a novel\nvision-language foundation model, LRSCLIP, and a multimodal dataset, LRS2M. The\nmain contributions are as follows: (1) By integrating multi-source remote\nsensing data and adopting a large language model labeling strategy, we\nconstruct the LRS2M dataset, which contains 2 million image-text pairs,\nproviding both short and long texts for the first time, thus solving the\nproblem of semantic granularity limitations in existing datasets; (2) The\ndesign of the LRSCLIP architecture based on Long-CLIP's KPS module, which\nextends CLIP's text processing capacity and achieves fine-grained cross-modal\nfeature alignment through a dual-text loss weighting mechanism. Experimental\nresults show that LRSCLIP improves retrieval accuracy by 10\\%-20\\% over the\nLong-CLIP baseline in the zero-shot long-text cross-modal retrieval task. For\nthe zero-shot short-text cross-modal retrieval task, LRSCLIP achieves\nimprovements over the current best model, GeoRSCLIP, with increases of 0.17\\%,\n0.67\\%, and 0.92\\% in Text to Image R@1, Image to Text R@1, and mR on RSITMD,\nrespectively, and 0.04\\%, 2.93\\%, and 1.28\\% on RSICD. In the zero-shot image\nclassification task (average accuracy=75.75\\%) and semantic localization task\n(Rmi=0.7653), LRSCLIP achieves state-of-the-art performance. These results\nvalidate the dual advantages of fine-grained semantic understanding and global\nfeature matching in LRSCLIP. This work provides a new benchmark model and data\nsupport for remote sensing multimodal learning. The related code has been open\nsource and is available at https:\/\/github.com\/MitsuiChen14\/LRSCLIP.\n","date":"2025-03-25"}
{"id":"2503.19312","title":"ImageGen-CoT: Enhancing Text-to-Image In-context Learning with\n  Chain-of-Thought Reasoning","abstract":"  In this work, we study the problem of Text-to-Image In-Context Learning\n(T2I-ICL). While Unified Multimodal LLMs (MLLMs) have advanced rapidly in\nrecent years, they struggle with contextual reasoning in T2I-ICL scenarios. To\naddress this limitation, we propose a novel framework that incorporates a\nthought process called ImageGen-CoT prior to image generation. To avoid\ngenerating unstructured ineffective reasoning steps, we develop an automatic\npipeline to curate a high-quality ImageGen-CoT dataset. We then fine-tune MLLMs\nusing this dataset to enhance their contextual reasoning capabilities. To\nfurther enhance performance, we explore test-time scale-up strategies and\npropose a novel hybrid scaling approach. This approach first generates multiple\nImageGen-CoT chains and then produces multiple images for each chain via\nsampling. Extensive experiments demonstrate the effectiveness of our proposed\nmethod. Notably, fine-tuning with the ImageGen-CoT dataset leads to a\nsubstantial 80\\% performance gain for SEED-X on T2I-ICL tasks. See our project\npage at https:\/\/ImageGen-CoT.github.io\/. Code and model weights will be\nopen-sourced.\n","date":"2025-03-25"}
{"id":"2503.19314","title":"RGL: A Graph-Centric, Modular Framework for Efficient\n  Retrieval-Augmented Generation on Graphs","abstract":"  Recent advances in graph learning have paved the way for innovative\nretrieval-augmented generation (RAG) systems that leverage the inherent\nrelational structures in graph data. However, many existing approaches suffer\nfrom rigid, fixed settings and significant engineering overhead, limiting their\nadaptability and scalability. Additionally, the RAG community has largely\noverlooked the decades of research in the graph database community regarding\nthe efficient retrieval of interesting substructures on large-scale graphs. In\nthis work, we introduce the RAG-on-Graphs Library (RGL), a modular framework\nthat seamlessly integrates the complete RAG pipeline-from efficient graph\nindexing and dynamic node retrieval to subgraph construction, tokenization, and\nfinal generation-into a unified system. RGL addresses key challenges by\nsupporting a variety of graph formats and integrating optimized implementations\nfor essential components, achieving speedups of up to 143x compared to\nconventional methods. Moreover, its flexible utilities, such as dynamic node\nfiltering, allow for rapid extraction of pertinent subgraphs while reducing\ntoken consumption. Our extensive evaluations demonstrate that RGL not only\naccelerates the prototyping process but also enhances the performance and\napplicability of graph-based RAG systems across a range of tasks.\n","date":"2025-03-25"}
{"id":"2503.19324","title":"How to optimize K-means?","abstract":"  Center-based clustering algorithms (e.g., K-means) are popular for clustering\ntasks, but they usually struggle to achieve high accuracy on complex datasets.\nWe believe the main reason is that traditional center-based clustering\nalgorithms identify only one clustering center in each cluster. Once the\ndistribution of the dataset is complex, a single clustering center cannot\nstrongly represent distant objects within the cluster. How to optimize the\nexisting center-based clustering algorithms will be valuable research. In this\npaper, we propose a general optimization method called ECAC, and it can\noptimize different center-based clustering algorithms. ECAC is independent of\nthe clustering principle and is embedded as a component between the center\nprocess and the category assignment process of center-based clustering\nalgorithms. Specifically, ECAC identifies several extended-centers for each\nclustering center. The extended-centers will act as relays to expand the\nrepresentative capability of the clustering center in the complex cluster, thus\nimproving the accuracy of center-based clustering algorithms. We conducted\nnumerous experiments to verify the robustness and effectiveness of ECAC. ECAC\nis robust to diverse datasets and diverse clustering centers. After ECAC\noptimization, the accuracy (NMI as well as RI) of center-based clustering\nalgorithms improves by an average of 33.4% and 64.1%, respectively, and even\nK-means accurately identifies complex-shaped clusters.\n","date":"2025-03-25"}
{"id":"2503.19325","title":"Long-Context Autoregressive Video Modeling with Next-Frame Prediction","abstract":"  Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context vision modeling\nfaces challenges due to visual redundancy. Existing RoPE lacks effective\ntemporal decay for remote context and fails to extrapolate well to long video\nsequences. Additionally, training on long videos is computationally expensive,\nas vision tokens grow much faster than language tokens. To tackle these issues,\nwe propose balancing locality and long-range dependency. We introduce FlexRoPE,\nan test-time technique that adds flexible temporal decay to RoPE, enabling\nextrapolation to 16x longer vision contexts. Furthermore, we propose long\nshort-term context modeling, where a high-resolution short-term context window\nensures fine-grained temporal consistency, while an unlimited long-term context\nwindow encodes long-range information using fewer tokens. With this approach,\nwe can train on long video sequences with a manageable token context length. We\ndemonstrate that FAR achieves state-of-the-art performance in both short- and\nlong-video generation, providing a simple yet effective baseline for video\nautoregressive modeling.\n","date":"2025-03-25"}
{"id":"2503.19326","title":"Process or Result? Manipulated Ending Tokens Can Mislead Reasoning LLMs\n  to Ignore the Correct Reasoning Steps","abstract":"  Recent reasoning large language models (LLMs) have demonstrated remarkable\nimprovements in mathematical reasoning capabilities through long\nChain-of-Thought. The reasoning tokens of these models enable self-correction\nwithin reasoning chains, enhancing robustness. This motivates our exploration:\nhow vulnerable are reasoning LLMs to subtle errors in their input reasoning\nchains? We introduce \"Compromising Thought\" (CPT), a vulnerability where models\npresented with reasoning tokens containing manipulated calculation results tend\nto ignore correct reasoning steps and adopt incorrect results instead. Through\nsystematic evaluation across multiple reasoning LLMs, we design three\nincreasingly explicit prompting methods to measure CPT resistance, revealing\nthat models struggle significantly to identify and correct these manipulations.\nNotably, contrary to existing research suggesting structural alterations affect\nmodel performance more than content modifications, we find that local ending\ntoken manipulations have greater impact on reasoning outcomes than structural\nchanges. Moreover, we discover a security vulnerability in DeepSeek-R1 where\ntampered reasoning tokens can trigger complete reasoning cessation. Our work\nenhances understanding of reasoning robustness and highlights security\nconsiderations for reasoning-intensive applications.\n","date":"2025-03-25"}
{"id":"2503.19328","title":"Substance over Style: Evaluating Proactive Conversational Coaching\n  Agents","abstract":"  While NLP research has made strides in conversational tasks, many approaches\nfocus on single-turn responses with well-defined objectives or evaluation\ncriteria. In contrast, coaching presents unique challenges with initially\nundefined goals that evolve through multi-turn interactions, subjective\nevaluation criteria, mixed-initiative dialogue. In this work, we describe and\nimplement five multi-turn coaching agents that exhibit distinct conversational\nstyles, and evaluate them through a user study, collecting first-person\nfeedback on 155 conversations. We find that users highly value core\nfunctionality, and that stylistic components in absence of core components are\nviewed negatively. By comparing user feedback with third-person evaluations\nfrom health experts and an LM, we reveal significant misalignment across\nevaluation approaches. Our findings provide insights into design and evaluation\nof conversational coaching agents and contribute toward improving\nhuman-centered NLP applications.\n","date":"2025-03-25"}
{"id":"2503.19329","title":"Wavelet-based Global-Local Interaction Network with Cross-Attention for\n  Multi-View Diabetic Retinopathy Detection","abstract":"  Multi-view diabetic retinopathy (DR) detection has recently emerged as a\npromising method to address the issue of incomplete lesions faced by\nsingle-view DR. However, it is still challenging due to the variable sizes and\nscattered locations of lesions. Furthermore, existing multi-view DR methods\ntypically merge multiple views without considering the correlations and\nredundancies of lesion information across them. Therefore, we propose a novel\nmethod to overcome the challenges of difficult lesion information learning and\ninadequate multi-view fusion. Specifically, we introduce a two-branch network\nto obtain both local lesion features and their global dependencies. The\nhigh-frequency component of the wavelet transform is used to exploit lesion\nedge information, which is then enhanced by global semantic to facilitate\ndifficult lesion learning. Additionally, we present a cross-view fusion module\nto improve multi-view fusion and reduce redundancy. Experimental results on\nlarge public datasets demonstrate the effectiveness of our method. The code is\nopen sourced on https:\/\/github.com\/HuYongting\/WGLIN.\n","date":"2025-03-25"}
{"id":"2503.19330","title":"MATT-GS: Masked Attention-based 3DGS for Robot Perception and Object\n  Detection","abstract":"  This paper presents a novel masked attention-based 3D Gaussian Splatting\n(3DGS) approach to enhance robotic perception and object detection in\nindustrial and smart factory environments. U2-Net is employed for background\nremoval to isolate target objects from raw images, thereby minimizing clutter\nand ensuring that the model processes only relevant data. Additionally, a Sobel\nfilter-based attention mechanism is integrated into the 3DGS framework to\nenhance fine details - capturing critical features such as screws, wires, and\nintricate textures essential for high-precision tasks. We validate our approach\nusing quantitative metrics, including L1 loss, SSIM, PSNR, comparing the\nperformance of the background-removed and attention-incorporated 3DGS model\nagainst the ground truth images and the original 3DGS training baseline. The\nresults demonstrate significant improves in visual fidelity and detail\npreservation, highlighting the effectiveness of our method in enhancing robotic\nvision for object recognition and manipulation in complex industrial settings.\n","date":"2025-03-25"}
{"id":"2503.19331","title":"ChA-MAEViT: Unifying Channel-Aware Masked Autoencoders and Multi-Channel\n  Vision Transformers for Improved Cross-Channel Learning","abstract":"  Prior work using Masked Autoencoders (MAEs) typically relies on random patch\nmasking based on the assumption that images have significant redundancies\nacross different channels, allowing for the reconstruction of masked content\nusing cross-channel correlations. However, this assumption does not hold in\nMulti-Channel Imaging (MCI), where channels may provide complementary\ninformation with minimal feature overlap. Thus, these MAEs primarily learn\nlocal structures within individual channels from patch reconstruction, failing\nto fully leverage cross-channel interactions and limiting their MCI\neffectiveness. In this paper, we present ChA-MAEViT, an MAE-based method that\nenhances feature learning across MCI channels via four key strategies: (1)\ndynamic channel-patch masking, which compels the model to reconstruct missing\nchannels in addition to masked patches, thereby enhancing cross-channel\ndependencies and improving robustness to varying channel configurations; (2)\nmemory tokens, which serve as long-term memory aids to promote information\nsharing across channels, addressing the challenges of reconstructing\nstructurally diverse channels; (3) hybrid token fusion module, which merges\nfine-grained patch tokens with a global class token to capture richer\nrepresentations; and (4) Channel-Aware Decoder, a lightweight decoder utilizes\nchannel tokens to effectively reconstruct image patches. Experiments on\nsatellite and microscopy datasets, CHAMMI, JUMP-CP, and So2Sat, show that\nChA-MAEViT significantly outperforms state-of-the-art MCI-ViTs by 3.0-21.5%,\nhighlighting the importance of cross-channel interactions in MCI.\n","date":"2025-03-25"}
{"id":"2503.19332","title":"Divide-and-Conquer: Dual-Hierarchical Optimization for Semantic 4D\n  Gaussian Spatting","abstract":"  Semantic 4D Gaussians can be used for reconstructing and understanding\ndynamic scenes, with temporal variations than static scenes. Directly applying\nstatic methods to understand dynamic scenes will fail to capture the temporal\nfeatures. Few works focus on dynamic scene understanding based on Gaussian\nSplatting, since once the same update strategy is employed for both dynamic and\nstatic parts, regardless of the distinction and interaction between Gaussians,\nsignificant artifacts and noise appear. We propose Dual-Hierarchical\nOptimization (DHO), which consists of Hierarchical Gaussian Flow and\nHierarchical Gaussian Guidance in a divide-and-conquer manner. The former\nimplements effective division of static and dynamic rendering and features. The\nlatter helps to mitigate the issue of dynamic foreground rendering distortion\nin textured complex scenes. Extensive experiments show that our method\nconsistently outperforms the baselines on both synthetic and real-world\ndatasets, and supports various downstream tasks. Project Page:\nhttps:\/\/sweety-yan.github.io\/DHO.\n","date":"2025-03-25"}
{"id":"2503.19333","title":"E-PINNs: Epistemic Physics-Informed Neural Networks","abstract":"  Physics-informed neural networks (PINNs) have demonstrated promise as a\nframework for solving forward and inverse problems involving partial\ndifferential equations. Despite recent progress in the field, it remains\nchallenging to quantify uncertainty in these networks. While approaches such as\nBayesian PINNs (B-PINNs) provide a principled approach to capturing uncertainty\nthrough Bayesian inference, they can be computationally expensive for\nlarge-scale applications. In this work, we propose Epistemic Physics-Informed\nNeural Networks (E-PINNs), a framework that leverages a small network, the\n\\emph{epinet}, to efficiently quantify uncertainty in PINNs. The proposed\napproach works as an add-on to existing, pre-trained PINNs with a small\ncomputational overhead. We demonstrate the applicability of the proposed\nframework in various test cases and compare the results with B-PINNs using\nHamiltonian Monte Carlo (HMC) posterior estimation and dropout-equipped PINNs\n(Dropout-PINNs). Our experiments show that E-PINNs provide similar coverage to\nB-PINNs, with often comparable sharpness, while being computationally more\nefficient. This observation, combined with E-PINNs' more consistent uncertainty\nestimates and better calibration compared to Dropout-PINNs for the examples\npresented, indicates that E-PINNs offer a promising approach in terms of\naccuracy-efficiency trade-off.\n","date":"2025-03-25"}
{"id":"2503.19338","title":"Membership Inference Attacks on Large-Scale Models: A Survey","abstract":"  The adoption of the Large Language Model (LLM) has accelerated dramatically\nsince the ChatGPT from OpenAI went online in November 2022. Recent advances in\nLarge Multimodal Models (LMMs), which process diverse data types and enable\ninteraction through various channels, have expanded beyond the text-to-text\nlimitations of early LLMs, attracting significant and concurrent attention from\nboth researchers and industry. While LLMs and LMMs are starting to spread\nwidely, concerns about their privacy risks are increasing as well. Membership\nInference Attacks (MIAs), techniques used to determine whether a particular\ndata point was part of a model's training set, serve as a key metric for\nassessing the privacy vulnerabilities of machine learning models. Hu et al.\nshow that various machine learning algorithms are vulnerable to MIA. Despite\nextensive studies on MIAs in traditional models, there remains a lack of\nsystematic surveys addressing their effectiveness and implications in modern\nlarge-scale models like LLMs and LMMs. In this paper, we systematically\nreviewed recent studies of MIA against LLMs and LMMs. We analyzed and\ncategorized each attack based on their methodology and scenario and discussed\nthe limitations in existing research. Additionally, we examine privacy concerns\nassociated with the fine-tuning process. Finally, we provided some suggestions\nfor future research in this direction.\n","date":"2025-03-25"}
{"id":"2503.19339","title":"Efficient IoT Intrusion Detection with an Improved Attention-Based\n  CNN-BiLSTM Architecture","abstract":"  The ever-increasing security vulnerabilities in the Internet-of-Things (IoT)\nsystems require improved threat detection approaches. This paper presents a\ncompact and efficient approach to detect botnet attacks by employing an\nintegrated approach that consists of traffic pattern analysis, temporal support\nlearning, and focused feature extraction. The proposed attention-based model\nbenefits from a hybrid CNN-BiLSTM architecture and achieves 99% classification\naccuracy in detecting botnet attacks utilizing the N-BaIoT dataset, while\nmaintaining high precision and recall across various scenarios. The proposed\nmodel's performance is further validated by key parameters, such as Mathews\nCorrelation Coefficient and Cohen's kappa Correlation Coefficient. The\nclose-to-ideal results for these parameters demonstrate the proposed model's\nability to detect botnet attacks accurately and efficiently in practical\nsettings and on unseen data. The proposed model proved to be a powerful defense\nmechanism for IoT networks to face emerging security challenges.\n","date":"2025-03-25"}
{"id":"2503.19340","title":"BADGR: Bundle Adjustment Diffusion Conditioned by GRadients for\n  Wide-Baseline Floor Plan Reconstruction","abstract":"  Reconstructing precise camera poses and floor plan layouts from wide-baseline\nRGB panoramas is a difficult and unsolved problem. We introduce BADGR, a novel\ndiffusion model that jointly performs reconstruction and bundle adjustment (BA)\nto refine poses and layouts from a coarse state, using 1D floor boundary\npredictions from dozens of images of varying input densities. Unlike a guided\ndiffusion model, BADGR is conditioned on dense per-entity outputs from a\nsingle-step Levenberg Marquardt (LM) optimizer and is trained to predict camera\nand wall positions while minimizing reprojection errors for view-consistency.\nThe objective of layout generation from denoising diffusion process complements\nBA optimization by providing additional learned layout-structural constraints\non top of the co-visible features across images. These constraints help BADGR\nto make plausible guesses on spatial relations which help constrain pose graph,\nsuch as wall adjacency, collinearity, and learn to mitigate errors from dense\nboundary observations with global contexts. BADGR trains exclusively on 2D\nfloor plans, simplifying data acquisition, enabling robust augmentation, and\nsupporting variety of input densities. Our experiments and analysis validate\nour method, which significantly outperforms the state-of-the-art pose and floor\nplan layout reconstruction with different input densities.\n","date":"2025-03-25"}
{"id":"2503.19347","title":"Stop Walking in Circles! Bailing Out Early in Projected Gradient Descent","abstract":"  Projected Gradient Descent (PGD) under the $L_\\infty$ ball has become one of\nthe defacto methods used in adversarial robustness evaluation for computer\nvision (CV) due to its reliability and efficacy, making a strong and\neasy-to-implement iterative baseline. However, PGD is computationally demanding\nto apply, especially when using thousands of iterations is the current\nbest-practice recommendation to generate an adversarial example for a single\nimage. In this work, we introduce a simple novel method for early termination\nof PGD based on cycle detection by exploiting the geometry of how PGD is\nimplemented in practice and show that it can produce large speedup factors\nwhile providing the \\emph{exact} same estimate of model robustness as standard\nPGD. This method substantially speeds up PGD without sacrificing any attack\nstrength, enabling evaluations of robustness that were previously\ncomputationally intractable.\n","date":"2025-03-25"}
{"id":"2503.19349","title":"Optimal Parameter Adaptation for Safety-Critical Control via Safe\n  Barrier Bayesian Optimization","abstract":"  Safety is of paramount importance in control systems to avoid costly risks\nand catastrophic damages. The control barrier function (CBF) method, a\npromising solution for safety-critical control, poses a new challenge of\nenhancing control performance due to its direct modification of original\ncontrol design and the introduction of uncalibrated parameters. In this work,\nwe shed light on the crucial role of configurable parameters in the CBF method\nfor performance enhancement with a systematical categorization. Based on that,\nwe propose a novel framework combining the CBF method with Bayesian\noptimization (BO) to optimize the safe control performance. Considering\nfeasibility\/safety-critical constraints, we develop a safe version of BO using\nthe barrier-based interior method to efficiently search for promising feasible\nconfigurable parameters. Furthermore, we provide theoretical criteria of our\nframework regarding safety and optimality. An essential advantage of our\nframework lies in that it can work in model-agnostic environments, leaving\nsufficient flexibility in designing objective and constraint functions.\nFinally, simulation experiments on swing-up control and high-fidelity adaptive\ncruise control are conducted to demonstrate the effectiveness of our framework.\n","date":"2025-03-25"}
{"id":"2503.19351","title":"Multi-Object Sketch Animation by Scene Decomposition and Motion Planning","abstract":"  Sketch animation, which brings static sketches to life by generating dynamic\nvideo sequences, has found widespread applications in GIF design, cartoon\nproduction, and daily entertainment. While current sketch animation methods\nperform well in single-object sketch animation, they struggle in multi-object\nscenarios. By analyzing their failures, we summarize two challenges of\ntransitioning from single-object to multi-object sketch animation: object-aware\nmotion modeling and complex motion optimization. For multi-object sketch\nanimation, we propose MoSketch based on iterative optimization through Score\nDistillation Sampling (SDS), without any other data for training. We propose\nfour modules: LLM-based scene decomposition, LLM-based motion planning, motion\nrefinement network and compositional SDS, to tackle the two challenges in a\ndivide-and-conquer strategy. Extensive qualitative and quantitative experiments\ndemonstrate the superiority of our method over existing sketch animation\napproaches. MoSketch takes a pioneering step towards multi-object sketch\nanimation, opening new avenues for future research and applications. The code\nwill be released.\n","date":"2025-03-25"}
{"id":"2503.19353","title":"QUAD: Quantization and Parameter-Efficient Tuning of LLM with Activation\n  Decomposition","abstract":"  Large Language Models (LLMs) excel in diverse applications but suffer\ninefficiency due to massive scale. While quantization reduces computational\ncosts, existing methods degrade accuracy in medium-sized LLMs (e.g.,\nLlama-3-8B) due to activation outliers. To address this, we propose QUAD\n(Quantization with Activation Decomposition), a framework leveraging Singular\nValue Decomposition (SVD) to suppress activation outliers for effective 4-bit\nquantization. QUAD estimates activation singular vectors offline using\ncalibration data to construct an orthogonal transformation matrix P, shifting\noutliers to additional dimensions in full precision while quantizing rest\ncomponents to 4-bit. Additionally, QUAD enables parameter-efficient fine-tuning\nvia adaptable full-precision outlier weights, narrowing the accuracy gap\nbetween quantized and full-precision models. Experiments demonstrate that QUAD\nachieves 94% ~ 96% accuracy under W4A4 quantization and 98% accuracy with\nW4A4\/A8 and parameter-efficient fine-tuning for Llama-3 and Qwen-2.5 models.\nOur code is available at \\href{https:\/\/github.com\/hyx1999\/Quad}{repository}.\n","date":"2025-03-25"}
{"id":"2503.19354","title":"Data-driven Mesoscale Weather Forecasting Combining Swin-Unet and\n  Diffusion Models","abstract":"  Data-driven weather prediction models exhibit promising performance and\nadvance continuously. In particular, diffusion models represent fine-scale\ndetails without spatial smoothing, which is crucial for mesoscale predictions,\nsuch as heavy rainfall forecasting. However, the applications of diffusion\nmodels to mesoscale prediction remain limited. To address this gap, this study\nproposes an architecture that combines a diffusion model with Swin-Unet as a\ndeterministic model, achieving mesoscale predictions while maintaining\nflexibility. The proposed architecture trains the two models independently,\nallowing the diffusion model to remain unchanged when the deterministic model\nis updated. Comparisons using the Fractions Skill Score and power spectral\nanalysis demonstrate that incorporating the diffusion model leads to improved\naccuracy compared to predictions without it. These findings underscore the\npotential of the proposed architecture to enhance mesoscale predictions,\nparticularly for strong rainfall events, while maintaining flexibility.\n","date":"2025-03-25"}
{"id":"2503.19355","title":"ST-VLM: Kinematic Instruction Tuning for Spatio-Temporal Reasoning in\n  Vision-Language Models","abstract":"  Spatio-temporal reasoning is essential in understanding real-world\nenvironments in various fields, eg, autonomous driving and sports analytics.\nRecent advances have improved the spatial reasoning ability of Vision-Language\nModels (VLMs) by introducing large-scale data, but these models still struggle\nto analyze kinematic elements like traveled distance and speed of moving\nobjects. To bridge this gap, we construct a spatio-temporal reasoning dataset\nand benchmark involving kinematic instruction tuning, referred to as STKit and\nSTKit-Bench. They consist of real-world videos with 3D annotations, detailing\nobject motion dynamics: traveled distance, speed, movement direction,\ninter-object distance comparisons, and relative movement direction. To further\nscale such data construction to videos without 3D labels, we propose an\nautomatic pipeline to generate pseudo-labels using 4D reconstruction in\nreal-world scale. With our kinematic instruction tuning data for\nspatio-temporal reasoning, we present ST-VLM, a VLM enhanced for\nspatio-temporal reasoning, which exhibits outstanding performance on\nSTKit-Bench. Furthermore, we show that ST-VLM generalizes robustly across\ndiverse domains and tasks, outperforming baselines on other spatio-temporal\nbenchmarks (eg, ActivityNet, TVQA+). Finally, by integrating learned\nspatio-temporal reasoning with existing abilities, ST-VLM enables complex\nmulti-step reasoning. Project page: https:\/\/ikodoh.github.io\/ST-VLM.\n","date":"2025-03-25"}
{"id":"2503.19356","title":"Can Vision-Language Models Answer Face to Face Questions in the\n  Real-World?","abstract":"  AI models have made significant strides in recent years in their ability to\ndescribe and answer questions about real-world images. They have also made\nprogress in the ability to converse with users in real-time using audio input.\nThis raises the question: have we reached the point where AI models, connected\nto a camera and microphone, can converse with users in real-time about scenes\nand events that are unfolding live in front of the camera? This has been a\nlong-standing goal in AI and is a prerequisite for real-world AI assistants and\nhumanoid robots to interact with humans in everyday situations. In this work,\nwe introduce a new dataset and benchmark, the Qualcomm Interactive Video\nDataset (IVD), which allows us to assess the extent to which existing models\ncan support these abilities, and to what degree these capabilities can be\ninstilled through fine-tuning. The dataset is based on a simple\nquestion-answering setup, where users ask questions that the system has to\nanswer, in real-time, based on the camera and audio input. We show that\nexisting models fall far behind human performance on this task, and we identify\nthe main sources for the performance gap. However, we also show that for many\nof the required perceptual skills, fine-tuning on this form of data can\nsignificantly reduce this gap.\n","date":"2025-03-25"}
{"id":"2503.19357","title":"Correcting Deviations from Normality: A Reformulated Diffusion Model for\n  Multi-Class Unsupervised Anomaly Detection","abstract":"  Recent advances in diffusion models have spurred research into their\napplication for Reconstruction-based unsupervised anomaly detection. However,\nthese methods may struggle with maintaining structural integrity and recovering\nthe anomaly-free content of abnormal regions, especially in multi-class\nscenarios. Furthermore, diffusion models are inherently designed to generate\nimages from pure noise and struggle to selectively alter anomalous regions of\nan image while preserving normal ones. This leads to potential degradation of\nnormal regions during reconstruction, hampering the effectiveness of anomaly\ndetection. This paper introduces a reformulation of the standard diffusion\nmodel geared toward selective region alteration, allowing the accurate\nidentification of anomalies. By modeling anomalies as noise in the latent\nspace, our proposed Deviation correction diffusion (DeCo-Diff) model preserves\nthe normal regions and encourages transformations exclusively on anomalous\nareas. This selective approach enhances the reconstruction quality,\nfacilitating effective unsupervised detection and localization of anomaly\nregions. Comprehensive evaluations demonstrate the superiority of our method in\naccurately identifying and localizing anomalies in complex images, with\npixel-level AUPRC improvements of 11-14% over state-of-the-art models on well\nknown anomaly detection datasets. The code is available at\nhttps:\/\/github.com\/farzad-bz\/DeCo-Diff\n","date":"2025-03-25"}
{"id":"2503.19358","title":"From Sparse to Dense: Camera Relocalization with Scene-Specific Detector\n  from Feature Gaussian Splatting","abstract":"  This paper presents a novel camera relocalization method, STDLoc, which\nleverages Feature Gaussian as scene representation. STDLoc is a full\nrelocalization pipeline that can achieve accurate relocalization without\nrelying on any pose prior. Unlike previous coarse-to-fine localization methods\nthat require image retrieval first and then feature matching, we propose a\nnovel sparse-to-dense localization paradigm. Based on this scene\nrepresentation, we introduce a novel matching-oriented Gaussian sampling\nstrategy and a scene-specific detector to achieve efficient and robust initial\npose estimation. Furthermore, based on the initial localization results, we\nalign the query feature map to the Gaussian feature field by dense feature\nmatching to enable accurate localization. The experiments on indoor and outdoor\ndatasets show that STDLoc outperforms current state-of-the-art localization\nmethods in terms of localization accuracy and recall.\n","date":"2025-03-25"}
{"id":"2503.19359","title":"Show and Segment: Universal Medical Image Segmentation via In-Context\n  Learning","abstract":"  Medical image segmentation remains challenging due to the vast diversity of\nanatomical structures, imaging modalities, and segmentation tasks. While deep\nlearning has made significant advances, current approaches struggle to\ngeneralize as they require task-specific training or fine-tuning on unseen\nclasses. We present Iris, a novel In-context Reference Image guided\nSegmentation framework that enables flexible adaptation to novel tasks through\nthe use of reference examples without fine-tuning. At its core, Iris features a\nlightweight context task encoding module that distills task-specific\ninformation from reference context image-label pairs. This rich context\nembedding information is used to guide the segmentation of target objects. By\ndecoupling task encoding from inference, Iris supports diverse strategies from\none-shot inference and context example ensemble to object-level context example\nretrieval and in-context tuning. Through comprehensive evaluation across twelve\ndatasets, we demonstrate that Iris performs strongly compared to task-specific\nmodels on in-distribution tasks. On seven held-out datasets, Iris shows\nsuperior generalization to out-of-distribution data and unseen classes.\nFurther, Iris's task encoding module can automatically discover anatomical\nrelationships across datasets and modalities, offering insights into medical\nobjects without explicit anatomical supervision.\n","date":"2025-03-25"}
{"id":"2503.19361","title":"ImageSet2Text: Describing Sets of Images through Text","abstract":"  We introduce ImageSet2Text, a novel approach that leverages vision-language\nfoundation models to automatically create natural language descriptions of\nimage sets. Inspired by concept bottleneck models (CBMs) and based on\nvisual-question answering (VQA) chains, ImageSet2Text iteratively extracts key\nconcepts from image subsets, encodes them into a structured graph, and refines\ninsights using an external knowledge graph and CLIP-based validation. This\niterative process enhances interpretability and enables accurate and detailed\nset-level summarization. Through extensive experiments, we evaluate\nImageSet2Text's descriptions on accuracy, completeness, readability and overall\nquality, benchmarking it against existing vision-language models and\nintroducing new datasets for large-scale group image captioning.\n","date":"2025-03-25"}
{"id":"2503.19367","title":"VGAT: A Cancer Survival Analysis Framework Transitioning from Generative\n  Visual Question Answering to Genomic Reconstruction","abstract":"  Multimodal learning combining pathology images and genomic sequences enhances\ncancer survival analysis but faces clinical implementation barriers due to\nlimited access to genomic sequencing in under-resourced regions. To enable\nsurvival prediction using only whole-slide images (WSI), we propose the\nVisual-Genomic Answering-Guided Transformer (VGAT), a framework integrating\nVisual Question Answering (VQA) techniques for genomic modality reconstruction.\nBy adapting VQA's text feature extraction approach, we derive stable genomic\nrepresentations that circumvent dimensionality challenges in raw genomic data.\nSimultaneously, a cluster-based visual prompt module selectively enhances\ndiscriminative WSI patches, addressing noise from unfiltered image regions.\nEvaluated across five TCGA datasets, VGAT outperforms existing WSI-only\nmethods, demonstrating the viability of genomic-informed inference without\nsequencing. This approach bridges multimodal research and clinical feasibility\nin resource-constrained settings. The code link is\nhttps:\/\/github.com\/CZZZZZZZZZZZZZZZZZ\/VGAT.\n","date":"2025-03-25"}
{"id":"2503.19369","title":"EfficientMT: Efficient Temporal Adaptation for Motion Transfer in\n  Text-to-Video Diffusion Models","abstract":"  The progress on generative models has led to significant advances on\ntext-to-video (T2V) generation, yet the motion controllability of generated\nvideos remains limited. Existing motion transfer methods explored the motion\nrepresentations of reference videos to guide generation. Nevertheless, these\nmethods typically rely on sample-specific optimization strategy, resulting in\nhigh computational burdens. In this paper, we propose EfficientMT, a novel and\nefficient end-to-end framework for video motion transfer. By leveraging a small\nset of synthetic paired motion transfer samples, EfficientMT effectively adapts\na pretrained T2V model into a general motion transfer framework that can\naccurately capture and reproduce diverse motion patterns. Specifically, we\nrepurpose the backbone of the T2V model to extract temporal information from\nreference videos, and further propose a scaler module to distill motion-related\ninformation. Subsequently, we introduce a temporal integration mechanism that\nseamlessly incorporates reference motion features into the video generation\nprocess. After training on our self-collected synthetic paired samples,\nEfficientMT enables general video motion transfer without requiring test-time\noptimization. Extensive experiments demonstrate that our EfficientMT\noutperforms existing methods in efficiency while maintaining flexible motion\ncontrollability. Our code will be available\nhttps:\/\/github.com\/PrototypeNx\/EfficientMT.\n","date":"2025-03-25"}
{"id":"2503.19371","title":"Flow to Learn: Flow Matching on Neural Network Parameters","abstract":"  Foundational language models show a remarkable ability to learn new concepts\nduring inference via context data. However, similar work for images lag behind.\nTo address this challenge, we introduce FLoWN, a flow matching model that\nlearns to generate neural network parameters for different tasks. Our approach\nmodels the flow on latent space, while conditioning the process on context\ndata. Experiments verify that FLoWN attains various desiderata for a\nmeta-learning model. In addition, it matches or exceeds baselines on\nin-distribution tasks, provides better initializations for classifier training,\nand is performant on out-of-distribution few-shot tasks while having a\nfine-tuning mechanism to improve performance.\n","date":"2025-03-25"}
{"id":"2503.19373","title":"DeClotH: Decomposable 3D Cloth and Human Body Reconstruction from a\n  Single Image","abstract":"  Most existing methods of 3D clothed human reconstruction from a single image\ntreat the clothed human as a single object without distinguishing between cloth\nand human body. In this regard, we present DeClotH, which separately\nreconstructs 3D cloth and human body from a single image. This task remains\nlargely unexplored due to the extreme occlusion between cloth and the human\nbody, making it challenging to infer accurate geometries and textures.\nMoreover, while recent 3D human reconstruction methods have achieved impressive\nresults using text-to-image diffusion models, directly applying such an\napproach to this problem often leads to incorrect guidance, particularly in\nreconstructing 3D cloth. To address these challenges, we propose two core\ndesigns in our framework. First, to alleviate the occlusion issue, we leverage\n3D template models of cloth and human body as regularizations, which provide\nstrong geometric priors to prevent erroneous reconstruction by the occlusion.\nSecond, we introduce a cloth diffusion model specifically designed to provide\ncontextual information about cloth appearance, thereby enhancing the\nreconstruction of 3D cloth. Qualitative and quantitative experiments\ndemonstrate that our proposed approach is highly effective in reconstructing\nboth 3D cloth and the human body. More qualitative results are provided at\nhttps:\/\/hygenie1228.github.io\/DeClotH\/.\n","date":"2025-03-25"}
{"id":"2503.19377","title":"Interpretable Generative Models through Post-hoc Concept Bottlenecks","abstract":"  Concept bottleneck models (CBM) aim to produce inherently interpretable\nmodels that rely on human-understandable concepts for their predictions.\nHowever, existing approaches to design interpretable generative models based on\nCBMs are not yet efficient and scalable, as they require expensive generative\nmodel training from scratch as well as real images with labor-intensive concept\nsupervision. To address these challenges, we present two novel and low-cost\nmethods to build interpretable generative models through post-hoc techniques\nand we name our approaches: concept-bottleneck autoencoder (CB-AE) and concept\ncontroller (CC). Our proposed approaches enable efficient and scalable training\nwithout the need of real data and require only minimal to no concept\nsupervision. Additionally, our methods generalize across modern generative\nmodel families including generative adversarial networks and diffusion models.\nWe demonstrate the superior interpretability and steerability of our methods on\nnumerous standard datasets like CelebA, CelebA-HQ, and CUB with large\nimprovements (average ~25%) over the prior work, while being 4-15x faster to\ntrain. Finally, a large-scale user study is performed to validate the\ninterpretability and steerability of our methods.\n","date":"2025-03-25"}
{"id":"2503.19380","title":"Social Network User Profiling for Anomaly Detection Based on Graph\n  Neural Networks","abstract":"  This study proposes a risk pricing anomaly detection method for social\nnetwork user portraits based on graph neural networks (GNNs), aiming to improve\nthe ability to identify abnormal users in social network environments. In view\nof the limitations of traditional methods in social network data modeling, this\npaper combines graph autoencoders (GAEs) and graph attention networks (GATs) to\nachieve accurate detection of abnormal users through dynamic aggregation of\nneighbor features and reconstruction error evaluation. The Facebook Page-Page\nNetwork dataset is used in the experiment and compared with VAE, GNN,\nTransformer and GAE. The results show that the proposed method achieves the\nbest performance in AUC, F1-score, Precision and Recall, verifying its\neffectiveness. In addition, this paper explores the computational efficiency of\nthe model in large-scale data and looks forward to combining self-supervised\nlearning, federated learning, and other technologies in the future to improve\nthe robustness and privacy protection of risk assessment. The research results\ncan provide efficient anomaly detection solutions for financial risk control,\nsocial security management, and other fields.\n","date":"2025-03-25"}
{"id":"2503.19381","title":"Towards Build Optimization Using Digital Twins","abstract":"  Despite the indisputable benefits of Continuous Integration (CI) pipelines\n(or builds), CI still presents significant challenges regarding long durations,\nfailures, and flakiness. Prior studies addressed CI challenges in isolation,\nyet these issues are interrelated and require a holistic approach for effective\noptimization. To bridge this gap, this paper proposes a novel idea of\ndeveloping Digital Twins (DTs) of build processes to enable global and\ncontinuous improvement. To support such an idea, we introduce the CI Build\nprocess Digital Twin (CBDT) framework as a minimum viable product. This\nframework offers digital shadowing functionalities, including real-time build\ndata acquisition and continuous monitoring of build process performance\nmetrics. Furthermore, we discuss guidelines and challenges in the practical\nimplementation of CBDTs, including (1) modeling different aspects of the build\nprocess using Machine Learning, (2) exploring what-if scenarios based on\nhistorical patterns, and (3) implementing prescriptive services such as\nautomated failure and performance repair to continuously improve build\nprocesses.\n","date":"2025-03-25"}
{"id":"2503.19382","title":"Causal invariant geographic network representations with feature and\n  structural distribution shifts","abstract":"  The existing methods learn geographic network representations through deep\ngraph neural networks (GNNs) based on the i.i.d. assumption. However, the\nspatial heterogeneity and temporal dynamics of geographic data make the\nout-of-distribution (OOD) generalisation problem particularly salient. The\nlatter are particularly sensitive to distribution shifts (feature and\nstructural shifts) between testing and training data and are the main causes of\nthe OOD generalisation problem. Spurious correlations are present between\ninvariant and background representations due to selection biases and\nenvironmental effects, resulting in the model extremes being more likely to\nlearn background representations. The existing approaches focus on background\nrepresentation changes that are determined by shifts in the feature\ndistributions of nodes in the training and test data while ignoring changes in\nthe proportional distributions of heterogeneous and homogeneous neighbour\nnodes, which we refer to as structural distribution shifts. We propose a\nfeature-structure mixed invariant representation learning (FSM-IRL) model that\naccounts for both feature distribution shifts and structural distribution\nshifts. To address structural distribution shifts, we introduce a sampling\nmethod based on causal attention, encouraging the model to identify nodes\npossessing strong causal relationships with labels or nodes that are more\nsimilar to the target node. Inspired by the Hilbert-Schmidt independence\ncriterion, we implement a reweighting strategy to maximise the orthogonality of\nthe node representations, thereby mitigating the spurious correlations among\nthe node representations and suppressing the learning of background\nrepresentations. Our experiments demonstrate that FSM-IRL exhibits strong\nlearning capabilities on both geographic and social network datasets in OOD\nscenarios.\n","date":"2025-03-25"}
{"id":"2503.19383","title":"MVPortrait: Text-Guided Motion and Emotion Control for Multi-view Vivid\n  Portrait Animation","abstract":"  Recent portrait animation methods have made significant strides in generating\nrealistic lip synchronization. However, they often lack explicit control over\nhead movements and facial expressions, and cannot produce videos from multiple\nviewpoints, resulting in less controllable and expressive animations. Moreover,\ntext-guided portrait animation remains underexplored, despite its user-friendly\nnature. We present a novel two-stage text-guided framework, MVPortrait\n(Multi-view Vivid Portrait), to generate expressive multi-view portrait\nanimations that faithfully capture the described motion and emotion. MVPortrait\nis the first to introduce FLAME as an intermediate representation, effectively\nembedding facial movements, expressions, and view transformations within its\nparameter space. In the first stage, we separately train the FLAME motion and\nemotion diffusion models based on text input. In the second stage, we train a\nmulti-view video generation model conditioned on a reference portrait image and\nmulti-view FLAME rendering sequences from the first stage. Experimental results\nexhibit that MVPortrait outperforms existing methods in terms of motion and\nemotion control, as well as view consistency. Furthermore, by leveraging FLAME\nas a bridge, MVPortrait becomes the first controllable portrait animation\nframework that is compatible with text, speech, and video as driving signals.\n","date":"2025-03-25"}
{"id":"2503.19385","title":"Inference-Time Scaling for Flow Models via Stochastic Generation and\n  Rollover Budget Forcing","abstract":"  We propose an inference-time scaling approach for pretrained flow models.\nRecently, inference-time scaling has gained significant attention in LLMs and\ndiffusion models, improving sample quality or better aligning outputs with user\npreferences by leveraging additional computation. For diffusion models,\nparticle sampling has allowed more efficient scaling due to the stochasticity\nat intermediate denoising steps. On the contrary, while flow models have gained\npopularity as an alternative to diffusion models--offering faster generation\nand high-quality outputs in state-of-the-art image and video generative\nmodels--efficient inference-time scaling methods used for diffusion models\ncannot be directly applied due to their deterministic generative process. To\nenable efficient inference-time scaling for flow models, we propose three key\nideas: 1) SDE-based generation, enabling particle sampling in flow models, 2)\nInterpolant conversion, broadening the search space and enhancing sample\ndiversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of\ncomputational resources across timesteps to maximize budget utilization. Our\nexperiments show that SDE-based generation, particularly variance-preserving\n(VP) interpolant-based generation, improves the performance of particle\nsampling methods for inference-time scaling in flow models. Additionally, we\ndemonstrate that RBF with VP-SDE achieves the best performance, outperforming\nall previous inference-time scaling approaches.\n","date":"2025-03-25"}
{"id":"2503.19386","title":"Exploring Textual Semantics Diversity for Image Transmission in Semantic\n  Communication Systems using Visual Language Model","abstract":"  In recent years, the rapid development of machine learning has brought\nreforms and challenges to traditional communication systems. Semantic\ncommunication has appeared as an effective strategy to effectively extract\nrelevant semantic signals semantic segmentation labels and image features for\nimage transmission. However, the insufficient number of extracted semantic\nfeatures of images will potentially result in a low reconstruction accuracy,\nwhich hinders the practical applications and still remains challenging for\nsolving. In order to fill this gap, this letter proposes a multi-text\ntransmission semantic communication (Multi-SC) system, which uses the visual\nlanguage model (VLM) to assist in the transmission of image semantic signals.\nUnlike previous image transmission semantic communication systems, the proposed\nsystem divides the image into multiple blocks and extracts multiple text\ninformation from the image using a modified large language and visual assistant\n(LLaVA), and combines semantic segmentation tags with semantic text for image\nrecovery. Simulation results show that the proposed text semantics diversity\nscheme can significantly improve the reconstruction accuracy compared with\nrelated works.\n","date":"2025-03-25"}
{"id":"2503.19391","title":"TraF-Align: Trajectory-aware Feature Alignment for Asynchronous\n  Multi-agent Perception","abstract":"  Cooperative perception presents significant potential for enhancing the\nsensing capabilities of individual vehicles, however, inter-agent latency\nremains a critical challenge. Latencies cause misalignments in both spatial and\nsemantic features, complicating the fusion of real-time observations from the\nego vehicle with delayed data from others. To address these issues, we propose\nTraF-Align, a novel framework that learns the flow path of features by\npredicting the feature-level trajectory of objects from past observations up to\nthe ego vehicle's current time. By generating temporally ordered sampling\npoints along these paths, TraF-Align directs attention from the current-time\nquery to relevant historical features along each trajectory, supporting the\nreconstruction of current-time features and promoting semantic interaction\nacross multiple frames. This approach corrects spatial misalignment and ensures\nsemantic consistency across agents, effectively compensating for motion and\nachieving coherent feature fusion. Experiments on two real-world datasets,\nV2V4Real and DAIR-V2X-Seq, show that TraF-Align sets a new benchmark for\nasynchronous cooperative perception.\n","date":"2025-03-25"}
{"id":"2503.19394","title":"Quantifying Symptom Causality in Clinical Decision Making: An\n  Exploration Using CausaLM","abstract":"  Current machine learning approaches to medical diagnosis often rely on\ncorrelational patterns between symptoms and diseases, risking misdiagnoses when\nsymptoms are ambiguous or common across multiple conditions. In this work, we\nmove beyond correlation to investigate the causal influence of key\nsymptoms-specifically \"chest pain\" on diagnostic predictions. Leveraging the\nCausaLM framework, we generate counterfactual text representations in which\ntarget concepts are effectively \"forgotten\" enabling a principled estimation of\nthe causal effect of that concept on a model's predicted disease distribution.\nBy employing Textual Representation-based Average Treatment Effect (TReATE), we\nquantify how the presence or absence of a symptom shapes the model's diagnostic\noutcomes, and contrast these findings against correlation-based baselines such\nas CONEXP. Our results offer deeper insight into the decision-making behavior\nof clinical NLP models and have the potential to inform more trustworthy,\ninterpretable, and causally-grounded decision support tools in medical\npractice.\n","date":"2025-03-25"}
{"id":"2503.19404","title":"LangBridge: Interpreting Image as a Combination of Language Embeddings","abstract":"  Recent years have witnessed remarkable advances in Large Vision-Language\nModels (LVLMs), which have achieved human-level performance across various\ncomplex vision-language tasks. Following LLaVA's paradigm, mainstream LVLMs\ntypically employ a shallow MLP for visual-language alignment through a\ntwo-stage training process: pretraining for cross-modal alignment followed by\ninstruction tuning. While this approach has proven effective, the underlying\nmechanisms of how MLPs bridge the modality gap remain poorly understood.\nAlthough some research has explored how LLMs process transformed visual tokens,\nfew studies have investigated the fundamental alignment mechanism. Furthermore,\nthe MLP adapter requires retraining whenever switching LLM backbones. To\naddress these limitations, we first investigate the working principles of MLP\nadapters and discover that they learn to project visual embeddings into\nsubspaces spanned by corresponding text embeddings progressively. Based on this\ninsight, we propose LangBridge, a novel adapter that explicitly maps visual\ntokens to linear combinations of LLM vocabulary embeddings. This innovative\ndesign enables pretraining-free adapter transfer across different LLMs while\nmaintaining performance. Our experimental results demonstrate that a LangBridge\nadapter pre-trained on Qwen2-0.5B can be directly applied to larger models such\nas LLaMA3-8B or Qwen2.5-14B while maintaining competitive performance. Overall,\nLangBridge enables interpretable vision-language alignment by grounding visual\nrepresentations in LLM vocab embedding, while its plug-and-play design ensures\nefficient reuse across multiple LLMs with nearly no performance degradation.\nSee our project page at https:\/\/jiaqiliao77.github.io\/LangBridge.github.io\/\n","date":"2025-03-25"}
{"id":"2503.19405","title":"Multi-modal 3D Pose and Shape Estimation with Computed Tomography","abstract":"  In perioperative care, precise in-bed 3D patient pose and shape estimation\n(PSE) can be vital in optimizing patient positioning in preoperative planning,\nenabling accurate overlay of medical images for augmented reality-based\nsurgical navigation, and mitigating risks of prolonged immobility during\nrecovery. Conventional PSE methods relying on modalities such as RGB-D,\ninfrared, or pressure maps often struggle with occlusions caused by bedding and\ncomplex patient positioning, leading to inaccurate estimation that can affect\nclinical outcomes. To address these challenges, we present the first\nmulti-modal in-bed patient 3D PSE network that fuses detailed geometric\nfeatures extracted from routinely acquired computed tomography (CT) scans with\ndepth maps (mPSE-CT). mPSE-CT incorporates a shape estimation module that\nutilizes probabilistic correspondence alignment, a pose estimation module with\na refined neural network, and a final parameters mixing module. This\nmulti-modal network robustly reconstructs occluded body regions and enhances\nthe accuracy of the estimated 3D human mesh model. We validated mPSE-CT using\nproprietary whole-body rigid phantom and volunteer datasets in clinical\nscenarios. mPSE-CT outperformed the best-performing prior method by 23% and\n49.16% in pose and shape estimation respectively, demonstrating its potential\nfor improving clinical outcomes in challenging perioperative environments.\n","date":"2025-03-25"}
{"id":"2503.19406","title":"M$^2$CD: A Unified MultiModal Framework for Optical-SAR Change Detection\n  with Mixture of Experts and Self-Distillation","abstract":"  Most existing change detection (CD) methods focus on optical images captured\nat different times, and deep learning (DL) has achieved remarkable success in\nthis domain. However, in extreme scenarios such as disaster response, synthetic\naperture radar (SAR), with its active imaging capability, is more suitable for\nproviding post-event data. This introduces new challenges for CD methods, as\nexisting weight-sharing Siamese networks struggle to effectively learn the\ncross-modal data distribution between optical and SAR images. To address this\nchallenge, we propose a unified MultiModal CD framework, M$^2$CD. We integrate\nMixture of Experts (MoE) modules into the backbone to explicitly handle diverse\nmodalities, thereby enhancing the model's ability to learn multimodal data\ndistributions. Additionally, we innovatively propose an Optical-to-SAR guided\npath (O2SP) and implement self-distillation during training to reduce the\nfeature space discrepancy between different modalities, further alleviating the\nmodel's learning burden. We design multiple variants of M$^2$CD based on both\nCNN and Transformer backbones. Extensive experiments validate the effectiveness\nof the proposed framework, with the MiT-b1 version of M$^2$CD outperforming all\nstate-of-the-art (SOTA) methods in optical-SAR CD tasks.\n","date":"2025-03-25"}
{"id":"2503.19407","title":"A Prototype-Guided Coarse Annotations Refining Approach for Whole Slide\n  Images","abstract":"  The fine-grained annotations in whole slide images (WSIs) show the boundaries\nof various pathological regions. However, generating such detailed annotation\nis often costly, whereas the coarse annotations are relatively simpler to\nproduce. Existing methods for refining coarse annotations often rely on\nextensive training samples or clean datasets, and fail to capture both\nintra-slide and inter-slide latent sematic patterns, limiting their precision.\nIn this paper, we propose a prototype-guided approach. Specifically, we\nintroduce a local-to-global approach to construct non-redundant representative\nprototypes by jointly modeling intra-slide local semantics and inter-slide\ncontextual relationships. Then a prototype-guided pseudo-labeling module is\nproposed for refining coarse annotations. Finally, we employ dynamic data\nsampling and re-finetuning strategy to train a patch classifier. Extensive\nexperiments on three publicly available WSI datasets, covering lymph, liver,\nand colorectal cancers, demonstrate that our method significantly outperforms\nexisting state-of-the-art (SOTA) methods. The code will be available.\n","date":"2025-03-25"}
{"id":"2503.19412","title":"Estimation of the Acoustic Field in a Uniform Duct with Mean Flow using\n  Neural Networks","abstract":"  The study of sound propagation in a uniform duct having a mean flow has many\napplications, such as in the design of gas turbines, heating, ventilation and\nair conditioning ducts, automotive intake and exhaust systems, and in the\nmodeling of speech. In this paper, the convective effects of the mean flow on\nthe plane wave acoustic field inside a uniform duct were studied using\nartificial neural networks. The governing differential equation and the\nassociated boundary conditions form a constrained optimization problem. It is\nconverted to an unconstrained optimization problem and solved by approximating\nthe acoustic field variable to a neural network. The complex-valued acoustic\npressure and particle velocity were predicted at different frequencies, and\nvalidated against the analytical solution and the finite element models. The\neffect of the mean flow is studied in terms of the acoustic impedance. A\nclosed-form expression that describes the influence of various factors on the\nacoustic field is derived.\n","date":"2025-03-25"}
{"id":"2503.19416","title":"EmoHead: Emotional Talking Head via Manipulating Semantic Expression\n  Parameters","abstract":"  Generating emotion-specific talking head videos from audio input is an\nimportant and complex challenge for human-machine interaction. However, emotion\nis highly abstract concept with ambiguous boundaries, and it necessitates\ndisentangled expression parameters to generate emotionally expressive talking\nhead videos. In this work, we present EmoHead to synthesize talking head videos\nvia semantic expression parameters. To predict expression parameter for\narbitrary audio input, we apply an audio-expression module that can be\nspecified by an emotion tag. This module aims to enhance correlation from audio\ninput across various emotions. Furthermore, we leverage pre-trained hyperplane\nto refine facial movements by probing along the vertical direction. Finally,\nthe refined expression parameters regularize neural radiance fields and\nfacilitate the emotion-consistent generation of talking head videos.\nExperimental results demonstrate that semantic expression parameters lead to\nbetter reconstruction quality and controllability.\n","date":"2025-03-25"}
{"id":"2503.19418","title":"Multi-Agent Deep Reinforcement Learning for Safe Autonomous Driving with\n  RICS-Assisted MEC","abstract":"  Environment sensing and fusion via onboard sensors are envisioned to be\nwidely applied in future autonomous driving networks. This paper considers a\nvehicular system with multiple self-driving vehicles that is assisted by\nmulti-access edge computing (MEC), where image data collected by the sensors is\noffloaded from cellular vehicles to the MEC server using\nvehicle-to-infrastructure (V2I) links. Sensory data can also be shared among\nsurrounding vehicles via vehicle-to-vehicle (V2V) communication links. To\nimprove spectrum utilization, the V2V links may reuse the same frequency\nspectrum with V2I links, which may cause severe interference. To tackle this\nissue, we leverage reconfigurable intelligent computational surfaces (RICSs) to\njointly enable V2I reflective links and mitigate interference appearing at the\nV2V links. Considering the limitations of traditional algorithms in addressing\nthis problem, such as the assumption for quasi-static channel state\ninformation, which restricts their ability to adapt to dynamic environmental\nchanges and leads to poor performance under frequently varying channel\nconditions, in this paper, we formulate the problem at hand as a Markov game.\nOur novel formulation is applied to time-varying channels subject to multi-user\ninterference and introduces a collaborative learning mechanism among users. The\nconsidered optimization problem is solved via a driving safety-enabled\nmulti-agent deep reinforcement learning (DS-MADRL) approach that capitalizes on\nthe RICS presence. Our extensive numerical investigations showcase that the\nproposed reinforcement learning approach achieves faster convergence and\nsignificant enhancements in both data rate and driving safety, as compared to\nvarious state-of-the-art benchmarks.\n","date":"2025-03-25"}
{"id":"2503.19423","title":"A novel forecasting framework combining virtual samples and enhanced\n  Transformer models for tourism demand forecasting","abstract":"  Accurate tourism demand forecasting is hindered by limited historical data\nand complex spatiotemporal dependencies among tourist origins. A novel\nforecasting framework integrating virtual sample generation and a novel\nTransformer predictor addresses constraints arising from restricted data\navailability. A spatiotemporal GAN produces realistic virtual samples by\ndynamically modeling spatial correlations through a graph convolutional\nnetwork, and an enhanced Transformer captures local patterns with causal\nconvolutions and long-term dependencies with self-attention,eliminating\nautoregressive decoding. A joint training strategy refines virtual sample\ngeneration based on predictor feedback to maintain robust performance under\ndata-scarce conditions. Experimental evaluations on real-world daily and\nmonthly tourism demand datasets indicate a reduction in average MASE by 18.37%\ncompared to conventional Transformer-based models, demonstrating improved\nforecasting accuracy. The integration of adaptive spatiotemporal sample\naugmentation with a specialized Transformer can effectively address\nlimited-data forecasting scenarios in tourism management.\n","date":"2025-03-25"}
{"id":"2503.19426","title":"DeCAP: Context-Adaptive Prompt Generation for Debiasing Zero-shot\n  Question Answering in Large Language Models","abstract":"  While Large Language Models (LLMs) excel in zero-shot Question Answering\n(QA), they tend to expose biases in their internal knowledge when faced with\nsocially sensitive questions, leading to a degradation in performance. Existing\nzero-shot methods are efficient but fail to consider context and prevent bias\npropagation in the answers. To address this, we propose DeCAP, a method for\ndebiasing LLMs using Context-Adaptive Prompt Generation. DeCAP leverages a\nQuestion Ambiguity Detection to take appropriate debiasing actions based on the\ncontext and a Neutral Answer Guidance Generation to suppress the LLMs make\nobjective judgments about the context, minimizing the propagation of bias from\ntheir internal knowledge. Our various experiments across eight LLMs show that\nDeCAP achieves state-of-the-art zero-shot debiased QA performance. This\ndemonstrates DeCAP's efficacy in enhancing the fairness and accuracy of LLMs in\ndiverse QA settings.\n","date":"2025-03-25"}
{"id":"2503.19427","title":"ASP-VMUNet: Atrous Shifted Parallel Vision Mamba U-Net for Skin Lesion\n  Segmentation","abstract":"  Skin lesion segmentation is a critical challenge in computer vision, and it\nis essential to separate pathological features from healthy skin for\ndiagnostics accurately. Traditional Convolutional Neural Networks (CNNs) are\nlimited by narrow receptive fields, and Transformers face significant\ncomputational burdens. This paper presents a novel skin lesion segmentation\nframework, the Atrous Shifted Parallel Vision Mamba UNet (ASP-VMUNet), which\nintegrates the efficient and scalable Mamba architecture to overcome\nlimitations in traditional CNNs and computationally demanding Transformers. The\nframework introduces an atrous scan technique that minimizes background\ninterference and expands the receptive field, enhancing Mamba's scanning\ncapabilities. Additionally, the inclusion of a Parallel Vision Mamba (PVM)\nlayer and a shift round operation optimizes feature segmentation and fosters\nrich inter-segment information exchange. A supplementary CNN branch with a\nSelective-Kernel (SK) Block further refines the segmentation by blending local\nand global contextual information. Tested on four benchmark datasets\n(ISIC16\/17\/18 and PH2), ASP-VMUNet demonstrates superior performance in skin\nlesion segmentation, validated by comprehensive ablation studies. This approach\nnot only advances medical image segmentation but also highlights the benefits\nof hybrid architectures in medical imaging technology. Our code is available at\nhttps:\/\/github.com\/BaoBao0926\/ASP-VMUNet\/tree\/main.\n","date":"2025-03-25"}
{"id":"2503.19429","title":"Quantifying the Ease of Reproducing Training Data in Unconditional\n  Diffusion Models","abstract":"  Diffusion models, which have been advancing rapidly in recent years, may\ngenerate samples that closely resemble the training data. This phenomenon,\nknown as memorization, may lead to copyright issues. In this study, we propose\na method to quantify the ease of reproducing training data in unconditional\ndiffusion models. The average of a sample population following the Langevin\nequation in the reverse diffusion process moves according to a first-order\nordinary differential equation (ODE). This ODE establishes a 1-to-1\ncorrespondence between images and their noisy counterparts in the latent space.\nSince the ODE is reversible and the initial noisy images are sampled randomly,\nthe volume of an image's projected area represents the probability of\ngenerating those images. We examined the ODE, which projects images to latent\nspace, and succeeded in quantifying the ease of reproducing training data by\nmeasuring the volume growth rate in this process. Given the relatively low\ncomputational complexity of this method, it allows us to enhance the quality of\ntraining data by detecting and modifying the easily memorized training samples.\n","date":"2025-03-25"}
{"id":"2503.19443","title":"COB-GS: Clear Object Boundaries in 3DGS Segmentation Based on\n  Boundary-Adaptive Gaussian Splitting","abstract":"  Accurate object segmentation is crucial for high-quality scene understanding\nin the 3D vision domain. However, 3D segmentation based on 3D Gaussian\nSplatting (3DGS) struggles with accurately delineating object boundaries, as\nGaussian primitives often span across object edges due to their inherent volume\nand the lack of semantic guidance during training. In order to tackle these\nchallenges, we introduce Clear Object Boundaries for 3DGS Segmentation\n(COB-GS), which aims to improve segmentation accuracy by clearly delineating\nblurry boundaries of interwoven Gaussian primitives within the scene. Unlike\nexisting approaches that remove ambiguous Gaussians and sacrifice visual\nquality, COB-GS, as a 3DGS refinement method, jointly optimizes semantic and\nvisual information, allowing the two different levels to cooperate with each\nother effectively. Specifically, for the semantic guidance, we introduce a\nboundary-adaptive Gaussian splitting technique that leverages semantic gradient\nstatistics to identify and split ambiguous Gaussians, aligning them closely\nwith object boundaries. For the visual optimization, we rectify the degraded\nsuboptimal texture of the 3DGS scene, particularly along the refined boundary\nstructures. Experimental results show that COB-GS substantially improves\nsegmentation accuracy and robustness against inaccurate masks from pre-trained\nmodel, yielding clear boundaries while preserving high visual quality. Code is\navailable at https:\/\/github.com\/ZestfulJX\/COB-GS.\n","date":"2025-03-25"}
{"id":"2503.19448","title":"Towards Robust Time-of-Flight Depth Denoising with Confidence-Aware\n  Diffusion Model","abstract":"  Time-of-Flight (ToF) sensors efficiently capture scene depth, but the\nnonlinear depth construction procedure often results in extremely large noise\nvariance or even invalid areas. Recent methods based on deep neural networks\n(DNNs) achieve enhanced ToF denoising accuracy but tend to struggle when\npresented with severe noise corruption due to limited prior knowledge of ToF\ndata distribution. In this paper, we propose DepthCAD, a novel ToF denoising\napproach that ensures global structural smoothness by leveraging the rich prior\nknowledge in Stable Diffusion and maintains local metric accuracy by steering\nthe diffusion process with confidence guidance. To adopt the pretrained image\ndiffusion model to ToF depth denoising, we apply the diffusion on raw ToF\ncorrelation measurements with dynamic range normalization before converting to\ndepth maps. Experimental results validate the state-of-the-art performance of\nthe proposed scheme, and the evaluation on real data further verifies its\nrobustness against real-world ToF noise.\n","date":"2025-03-25"}
{"id":"2503.19449","title":"VecTrans: LLM Transformation Framework for Better Auto-vectorization on\n  High-performance CPU","abstract":"  Large language models (LLMs) have demonstrated great capabilities in code\ngeneration, yet their effective application in compiler optimizations remains\nan open challenge due to issues such as hallucinations and a lack of\ndomain-specific reasoning. Vectorization, a crucial optimization for enhancing\ncode performance, often fails because of the compiler's inability to recognize\ncomplex code patterns, which commonly require extensive empirical expertise.\nLLMs, with their ability to capture intricate patterns, thus providing a\npromising solution to this challenge. This paper presents VecTrans, a novel\nframework that leverages LLMs to enhance compiler-based code vectorization.\nVecTrans first employs compiler analysis to identify potentially vectorizable\ncode regions. It then utilizes an LLM to refactor these regions into patterns\nthat are more amenable to the compiler's auto-vectorization. To ensure semantic\ncorrectness, VecTrans further integrates a hybrid validation mechanism at the\nintermediate representation (IR) level. With the above efforts, VecTrans\ncombines the adaptability of LLMs with the precision of compiler vectorization,\nthereby effectively opening up the vectorization opportunities. Experimental\nresults show that among all 50 TSVC functions unvectorizable by Clang, GCC, and\nBiShengCompiler, VecTrans successfully vectorizes 23 cases (46%) and achieves\nan average speedup of 2.02x, greatly surpassing state-of-the-art performance.\n","date":"2025-03-25"}
{"id":"2503.19452","title":"SparseGS-W: Sparse-View 3D Gaussian Splatting in the Wild with\n  Generative Priors","abstract":"  Synthesizing novel views of large-scale scenes from unconstrained in-the-wild\nimages is an important but challenging task in computer vision. Existing\nmethods, which optimize per-image appearance and transient occlusion through\nimplicit neural networks from dense training views (approximately 1000 images),\nstruggle to perform effectively under sparse input conditions, resulting in\nnoticeable artifacts. To this end, we propose SparseGS-W, a novel framework\nbased on 3D Gaussian Splatting that enables the reconstruction of complex\noutdoor scenes and handles occlusions and appearance changes with as few as\nfive training images. We leverage geometric priors and constrained diffusion\npriors to compensate for the lack of multi-view information from extremely\nsparse input. Specifically, we propose a plug-and-play Constrained Novel-View\nEnhancement module to iteratively improve the quality of rendered novel views\nduring the Gaussian optimization process. Furthermore, we propose an Occlusion\nHandling module, which flexibly removes occlusions utilizing the inherent\nhigh-quality inpainting capability of constrained diffusion priors. Both\nmodules are capable of extracting appearance features from any user-provided\nreference image, enabling flexible modeling of illumination-consistent scenes.\nExtensive experiments on the PhotoTourism and Tanks and Temples datasets\ndemonstrate that SparseGS-W achieves state-of-the-art performance not only in\nfull-reference metrics, but also in commonly used non-reference metrics such as\nFID, ClipIQA, and MUSIQ.\n","date":"2025-03-25"}
{"id":"2503.19455","title":"Data-centric Federated Graph Learning with Large Language Models","abstract":"  In federated graph learning (FGL), a complete graph is divided into multiple\nsubgraphs stored in each client due to privacy concerns, and all clients\njointly train a global graph model by only transmitting model parameters. A\npain point of FGL is the heterogeneity problem, where nodes or structures\npresent non-IID properties among clients (e.g., different node label\ndistributions), dramatically undermining the convergence and performance of\nFGL. To address this, existing efforts focus on design strategies at the model\nlevel, i.e., they design models to extract common knowledge to mitigate\nheterogeneity. However, these model-level strategies fail to fundamentally\naddress the heterogeneity problem as the model needs to be designed from\nscratch when transferring to other tasks. Motivated by large language models\n(LLMs) having achieved remarkable success, we aim to utilize LLMs to fully\nunderstand and augment local text-attributed graphs, to address data\nheterogeneity at the data level. In this paper, we propose a general framework\nLLM4FGL that innovatively decomposes the task of LLM for FGL into two sub-tasks\ntheoretically. Specifically, for each client, it first utilizes the LLM to\ngenerate missing neighbors and then infers connections between generated nodes\nand raw nodes. To improve the quality of generated nodes, we design a novel\nfederated generation-and-reflection mechanism for LLMs, without the need to\nmodify the parameters of the LLM but relying solely on the collective feedback\nfrom all clients. After neighbor generation, all the clients utilize a\npre-trained edge predictor to infer the missing edges. Furthermore, our\nframework can seamlessly integrate as a plug-in with existing FGL methods.\nExperiments on three real-world datasets demonstrate the superiority of our\nmethod compared to advanced baselines.\n","date":"2025-03-25"}
{"id":"2503.19457","title":"G-DexGrasp: Generalizable Dexterous Grasping Synthesis Via Part-Aware\n  Prior Retrieval and Prior-Assisted Generation","abstract":"  Recent advances in dexterous grasping synthesis have demonstrated significant\nprogress in producing reasonable and plausible grasps for many task purposes.\nBut it remains challenging to generalize to unseen object categories and\ndiverse task instructions. In this paper, we propose G-DexGrasp, a\nretrieval-augmented generation approach that can produce high-quality dexterous\nhand configurations for unseen object categories and language-based task\ninstructions. The key is to retrieve generalizable grasping priors, including\nthe fine-grained contact part and the affordance-related distribution of\nrelevant grasping instances, for the following synthesis pipeline.\nSpecifically, the fine-grained contact part and affordance act as generalizable\nguidance to infer reasonable grasping configurations for unseen objects with a\ngenerative model, while the relevant grasping distribution plays as\nregularization to guarantee the plausibility of synthesized grasps during the\nsubsequent refinement optimization. Our comparison experiments validate the\neffectiveness of our key designs for generalization and demonstrate the\nremarkable performance against the existing approaches. Project page:\nhttps:\/\/g-dexgrasp.github.io\/\n","date":"2025-03-25"}
{"id":"2503.19458","title":"GaussianUDF: Inferring Unsigned Distance Functions through 3D Gaussian\n  Splatting","abstract":"  Reconstructing open surfaces from multi-view images is vital in digitalizing\ncomplex objects in daily life. A widely used strategy is to learn unsigned\ndistance functions (UDFs) by checking if their appearance conforms to the image\nobservations through neural rendering. However, it is still hard to learn\ncontinuous and implicit UDF representations through 3D Gaussians splatting\n(3DGS) due to the discrete and explicit scene representation, i.e., 3D\nGaussians. To resolve this issue, we propose a novel approach to bridge the gap\nbetween 3D Gaussians and UDFs. Our key idea is to overfit thin and flat 2D\nGaussian planes on surfaces, and then, leverage the self-supervision and\ngradient-based inference to supervise unsigned distances in both near and far\narea to surfaces. To this end, we introduce novel constraints and strategies to\nconstrain the learning of 2D Gaussians to pursue more stable optimization and\nmore reliable self-supervision, addressing the challenges brought by\ncomplicated gradient field on or near the zero level set of UDFs. We report\nnumerical and visual comparisons with the state-of-the-art on widely used\nbenchmarks and real data to show our advantages in terms of accuracy,\nefficiency, completeness, and sharpness of reconstructed open surfaces with\nboundaries.\n","date":"2025-03-25"}
{"id":"2503.19462","title":"AccVideo: Accelerating Video Diffusion Model with Synthetic Dataset","abstract":"  Diffusion models have achieved remarkable progress in the field of video\ngeneration. However, their iterative denoising nature requires a large number\nof inference steps to generate a video, which is slow and computationally\nexpensive. In this paper, we begin with a detailed analysis of the challenges\npresent in existing diffusion distillation methods and propose a novel\nefficient method, namely AccVideo, to reduce the inference steps for\naccelerating video diffusion models with synthetic dataset. We leverage the\npretrained video diffusion model to generate multiple valid denoising\ntrajectories as our synthetic dataset, which eliminates the use of useless data\npoints during distillation. Based on the synthetic dataset, we design a\ntrajectory-based few-step guidance that utilizes key data points from the\ndenoising trajectories to learn the noise-to-video mapping, enabling video\ngeneration in fewer steps. Furthermore, since the synthetic dataset captures\nthe data distribution at each diffusion timestep, we introduce an adversarial\ntraining strategy to align the output distribution of the student model with\nthat of our synthetic dataset, thereby enhancing the video quality. Extensive\nexperiments demonstrate that our model achieves 8.5x improvements in generation\nspeed compared to the teacher model while maintaining comparable performance.\nCompared to previous accelerating methods, our approach is capable of\ngenerating videos with higher quality and resolution, i.e., 5-seconds,\n720x1280, 24fps.\n","date":"2025-03-25"}
{"id":"2503.19466","title":"A Probabilistic Neuro-symbolic Layer for Algebraic Constraint\n  Satisfaction","abstract":"  In safety-critical applications, guaranteeing the satisfaction of constraints\nover continuous environments is crucial, e.g., an autonomous agent should never\ncrash into obstacles or go off-road. Neural models struggle in the presence of\nthese constraints, especially when they involve intricate algebraic\nrelationships. To address this, we introduce a differentiable probabilistic\nlayer that guarantees the satisfaction of non-convex algebraic constraints over\ncontinuous variables. This probabilistic algebraic layer (PAL) can be\nseamlessly plugged into any neural architecture and trained via maximum\nlikelihood without requiring approximations. PAL defines a distribution over\nconjunctions and disjunctions of linear inequalities, parameterized by\npolynomials. This formulation enables efficient and exact renormalization via\nsymbolic integration, which can be amortized across different data points and\neasily parallelized on a GPU. We showcase PAL and our integration scheme on a\nnumber of benchmarks for algebraic constraint integration and on real-world\ntrajectory data.\n","date":"2025-03-25"}
{"id":"2503.19468","title":"Noisier2Inverse: Self-Supervised Learning for Image Reconstruction with\n  Correlated Noise","abstract":"  We propose Noisier2Inverse, a correction-free self-supervised deep learning\napproach for general inverse problems. The proposed method learns a\nreconstruction function without the need for ground truth samples and is\napplicable in cases where measurement noise is statistically correlated. This\nincludes computed tomography, where detector imperfections or photon scattering\ncreate correlated noise patterns, as well as microscopy and seismic imaging,\nwhere physical interactions during measurement introduce dependencies in the\nnoise structure. Similar to Noisier2Noise, a key step in our approach is the\ngeneration of noisier data from which the reconstruction network learns.\nHowever, unlike Noisier2Noise, the proposed loss function operates in\nmeasurement space and is trained to recover an extrapolated image instead of\nthe original noisy one. This eliminates the need for an extrapolation step\nduring inference, which would otherwise suffer from ill-posedness. We\nnumerically demonstrate that our method clearly outperforms previous\nself-supervised approaches that account for correlated noise.\n","date":"2025-03-25"}
{"id":"2503.19469","title":"Enhancing Small Language Models for Cross-Lingual Generalized Zero-Shot\n  Classification with Soft Prompt Tuning","abstract":"  In NLP, Zero-Shot Classification (ZSC) has become essential for enabling\nmodels to classify text into categories unseen during training, particularly in\nlow-resource languages and domains where labeled data is scarce. While\npretrained language models (PLMs) have shown promise in ZSC, they often rely on\nlarge training datasets or external knowledge, limiting their applicability in\nmultilingual and low-resource scenarios. Recent approaches leveraging natural\nlanguage prompts reduce the dependence on large training datasets but struggle\nto effectively incorporate available labeled data from related classification\ntasks, especially when these datasets originate from different languages or\ndistributions. Moreover, existing prompt-based methods typically rely on\nmanually crafted prompts in a specific language, limiting their adaptability\nand effectiveness in cross-lingual settings. To address these challenges, we\nintroduce RoSPrompt, a lightweight and data-efficient approach for training\nsoft prompts that enhance cross-lingual ZSC while ensuring robust\ngeneralization across data distribution shifts. RoSPrompt is designed for small\nmultilingual PLMs, enabling them to leverage high-resource languages to improve\nperformance in low-resource settings without requiring extensive fine-tuning or\nhigh computational costs. We evaluate our approach on multiple multilingual\nPLMs across datasets covering 106 languages, demonstrating strong cross-lingual\ntransfer performance and robust generalization capabilities over unseen\nclasses.\n","date":"2025-03-25"}
{"id":"2503.19470","title":"ReSearch: Learning to Reason with Search for LLMs via Reinforcement\n  Learning","abstract":"  Large Language Models (LLMs) have shown remarkable capabilities in reasoning,\nexemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating\nreasoning with external search processes remains challenging, especially for\ncomplex multi-hop questions requiring multiple retrieval steps. We propose\nReSearch, a novel framework that trains LLMs to Reason with Search via\nreinforcement learning without using any supervised data on reasoning steps.\nOur approach treats search operations as integral components of the reasoning\nchain, where when and how to perform searches is guided by text-based thinking,\nand search results subsequently influence further reasoning. We train ReSearch\non Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct\nextensive experiments. Despite being trained on only one dataset, our models\ndemonstrate strong generalizability across various benchmarks. Analysis reveals\nthat ReSearch naturally elicits advanced reasoning capabilities such as\nreflection and self-correction during the reinforcement learning process.\n","date":"2025-03-25"}
{"id":"2503.19474","title":"A-MESS: Anchor based Multimodal Embedding with Semantic Synchronization\n  for Multimodal Intent Recognition","abstract":"  In the domain of multimodal intent recognition (MIR), the objective is to\nrecognize human intent by integrating a variety of modalities, such as language\ntext, body gestures, and tones. However, existing approaches face difficulties\nadequately capturing the intrinsic connections between the modalities and\noverlooking the corresponding semantic representations of intent. To address\nthese limitations, we present the Anchor-based Multimodal Embedding with\nSemantic Synchronization (A-MESS) framework. We first design an Anchor-based\nMultimodal Embedding (A-ME) module that employs an anchor-based embedding\nfusion mechanism to integrate multimodal inputs. Furthermore, we develop a\nSemantic Synchronization (SS) strategy with the Triplet Contrastive Learning\npipeline, which optimizes the process by synchronizing multimodal\nrepresentation with label descriptions produced by the large language model.\nComprehensive experiments indicate that our A-MESS achieves state-of-the-art\nand provides substantial insight into multimodal representation and downstream\ntasks.\n","date":"2025-03-25"}
{"id":"2503.19476","title":"Extracting Interpretable Logic Rules from Graph Neural Networks","abstract":"  Graph neural networks (GNNs) operate over both input feature spaces and\ncombinatorial graph structures, making it challenging to understand the\nrationale behind their predictions. As GNNs gain widespread popularity and\ndemonstrate success across various domains, such as drug discovery, studying\ntheir interpretability has become a critical task. To address this, many\nexplainability methods have been proposed, with recent efforts shifting from\ninstance-specific explanations to global concept-based explainability. However,\nthese approaches face several limitations, such as relying on predefined\nconcepts and explaining only a limited set of patterns. To address this, we\npropose a novel framework, LOGICXGNN, for extracting interpretable logic rules\nfrom GNNs. LOGICXGNN is model-agnostic, efficient, and data-driven, eliminating\nthe need for predefined concepts. More importantly, it can serve as a\nrule-based classifier and even outperform the original neural models. Its\ninterpretability facilitates knowledge discovery, as demonstrated by its\nability to extract detailed and accurate chemistry knowledge that is often\noverlooked by existing methods. Another key advantage of LOGICXGNN is its\nability to generate new graph instances in a controlled and transparent manner,\noffering significant potential for applications such as drug design. We\nempirically demonstrate these merits through experiments on real-world datasets\nsuch as MUTAG and BBBP.\n","date":"2025-03-25"}
{"id":"2503.19478","title":"TeLL Me what you cant see","abstract":"  During criminal investigations, images of persons of interest directly\ninfluence the success of identification procedures. However, law enforcement\nagencies often face challenges related to the scarcity of high-quality images\nor their obsolescence, which can affect the accuracy and success of people\nsearching processes. This paper introduces a novel forensic mugshot\naugmentation framework aimed at addressing these limitations. Our approach\nenhances the identification probability of individuals by generating\nadditional, high-quality images through customizable data augmentation\ntechniques, while maintaining the biometric integrity and consistency of the\noriginal data. Several experimental results show that our method significantly\nimproves identification accuracy and robustness across various forensic\nscenarios, demonstrating its effectiveness as a trustworthy tool law\nenforcement applications. Index Terms: Digital Forensics, Person\nre-identification, Feature extraction, Data augmentation, Visual-Language\nmodels.\n","date":"2025-03-25"}
{"id":"2503.19479","title":"Bayesian Optimization of a Lightweight and Accurate Neural Network for\n  Aerodynamic Performance Prediction","abstract":"  Ensuring high accuracy and efficiency of predictive models is paramount in\nthe aerospace industry, particularly in the context of multidisciplinary design\nand optimization processes. These processes often require numerous evaluations\nof complex objective functions, which can be computationally expensive and\ntime-consuming. To build efficient and accurate predictive models, we propose a\nnew approach that leverages Bayesian Optimization (BO) to optimize the\nhyper-parameters of a lightweight and accurate Neural Network (NN) for\naerodynamic performance prediction. To clearly describe the interplay between\ndesign variables, hierarchical and categorical kernels are used in the BO\nformulation. We demonstrate the efficiency of our approach through two\ncomprehensive case studies, where the optimized NN significantly outperforms\nbaseline models and other publicly available NNs in terms of accuracy and\nparameter efficiency. For the drag coefficient prediction task, the Mean\nAbsolute Percentage Error (MAPE) of our optimized model drops from 0.1433\\% to\n0.0163\\%, which is nearly an order of magnitude improvement over the baseline\nmodel. Additionally, our model achieves a MAPE of 0.82\\% on a benchmark\naircraft self-noise prediction problem, significantly outperforming existing\nmodels (where their MAPE values are around 2 to 3\\%) while requiring less\ncomputational resources. The results highlight the potential of our framework\nto enhance the scalability and performance of NNs in large-scale MDO problems,\noffering a promising solution for the aerospace industry.\n","date":"2025-03-25"}
{"id":"2503.19480","title":"GenHancer: Imperfect Generative Models are Secretly Strong\n  Vision-Centric Enhancers","abstract":"  The synergy between generative and discriminative models receives growing\nattention. While discriminative Contrastive Language-Image Pre-Training (CLIP)\nexcels in high-level semantics, it struggles with perceiving fine-grained\nvisual details. Generally, to enhance representations, generative models take\nCLIP's visual features as conditions for reconstruction. However, the\nunderlying principle remains underexplored. In this work, we empirically found\nthat visually perfect generations are not always optimal for representation\nenhancement. The essence lies in effectively extracting fine-grained knowledge\nfrom generative models while mitigating irrelevant information. To explore\ncritical factors, we delve into three aspects: (1) Conditioning mechanisms: We\nfound that even a small number of local tokens can drastically reduce the\ndifficulty of reconstruction, leading to collapsed training. We thus conclude\nthat utilizing only global visual tokens as conditions is the most effective\nstrategy. (2) Denoising configurations: We observed that end-to-end training\nintroduces extraneous information. To address this, we propose a two-stage\ntraining strategy to prioritize learning useful visual knowledge. Additionally,\nwe demonstrate that lightweight denoisers can yield remarkable improvements.\n(3) Generation paradigms: We explore both continuous and discrete denoisers\nwith desirable outcomes, validating the versatility of our method. Through our\nin-depth explorations, we have finally arrived at an effective method, namely\nGenHancer, which consistently outperforms prior arts on the MMVP-VLM benchmark,\ne.g., 6.0% on OpenAICLIP. The enhanced CLIP can be further plugged into\nmultimodal large language models for better vision-centric performance. All the\nmodels and codes are made publicly available.\n","date":"2025-03-25"}
{"id":"2503.19482","title":"KSHSeek: Data-Driven Approaches to Mitigating and Detecting\n  Knowledge-Shortcut Hallucinations in Generative Models","abstract":"  The emergence of large language models (LLMs) has significantly advanced the\ndevelopment of natural language processing (NLP), especially in text generation\ntasks like question answering. However, model hallucinations remain a major\nchallenge in natural language generation (NLG) tasks due to their complex\ncauses. We systematically expand on the causes of factual hallucinations from\nthe perspective of knowledge shortcuts, analyzing hallucinations arising from\ncorrect and defect-free data and demonstrating that knowledge-shortcut\nhallucinations are prevalent in generative models. To mitigate this issue, we\npropose a high similarity pruning algorithm at the data preprocessing level to\nreduce spurious correlations in the data. Additionally, we design a specific\ndetection method for knowledge-shortcut hallucinations to evaluate the\neffectiveness of our mitigation strategy. Experimental results show that our\napproach effectively reduces knowledge-shortcut hallucinations, particularly in\nfine-tuning tasks, without negatively impacting model performance in question\nanswering. This work introduces a new paradigm for mitigating specific\nhallucination issues in generative models, enhancing their robustness and\nreliability in real-world applications.\n","date":"2025-03-25"}
{"id":"2503.19486","title":"Exploring Disentangled and Controllable Human Image Synthesis: From\n  End-to-End to Stage-by-Stage","abstract":"  Achieving fine-grained controllability in human image synthesis is a\nlong-standing challenge in computer vision. Existing methods primarily focus on\neither facial synthesis or near-frontal body generation, with limited ability\nto simultaneously control key factors such as viewpoint, pose, clothing, and\nidentity in a disentangled manner. In this paper, we introduce a new\ndisentangled and controllable human synthesis task, which explicitly separates\nand manipulates these four factors within a unified framework. We first develop\nan end-to-end generative model trained on MVHumanNet for factor\ndisentanglement. However, the domain gap between MVHumanNet and in-the-wild\ndata produce unsatisfacotry results, motivating the exploration of virtual\ntry-on (VTON) dataset as a potential solution. Through experiments, we observe\nthat simply incorporating the VTON dataset as additional data to train the\nend-to-end model degrades performance, primarily due to the inconsistency in\ndata forms between the two datasets, which disrupts the disentanglement\nprocess. To better leverage both datasets, we propose a stage-by-stage\nframework that decomposes human image generation into three sequential steps:\nclothed A-pose generation, back-view synthesis, and pose and view control. This\nstructured pipeline enables better dataset utilization at different stages,\nsignificantly improving controllability and generalization, especially for\nin-the-wild scenarios. Extensive experiments demonstrate that our\nstage-by-stage approach outperforms end-to-end models in both visual fidelity\nand disentanglement quality, offering a scalable solution for real-world tasks.\nAdditional demos are available on the project page:\nhttps:\/\/taited.github.io\/discohuman-project\/.\n","date":"2025-03-25"}
{"id":"2503.19495","title":"TFIC: End-to-End Text-Focused Image Compression for Coding for Machines","abstract":"  Traditional image compression methods aim to faithfully reconstruct images\nfor human perception. In contrast, Coding for Machines focuses on compressing\nimages to preserve information relevant to a specific machine task. In this\npaper, we present an image compression system designed to retain text-specific\nfeatures for subsequent Optical Character Recognition (OCR). Our encoding\nprocess requires half the time needed by the OCR module, making it especially\nsuitable for devices with limited computational capacity. In scenarios where\non-device OCR is computationally prohibitive, images are compressed and later\nprocessed to recover the text content. Experimental results demonstrate that\nour method achieves significant improvements in text extraction accuracy at low\nbitrates, even improving over the accuracy of OCR performed on uncompressed\nimages, thus acting as a local pre-processing step.\n","date":"2025-03-25"}
{"id":"2503.19496","title":"SMT-EX: An Explainable Surrogate Modeling Toolbox for Mixed-Variables\n  Design Exploration","abstract":"  Surrogate models are of high interest for many engineering applications,\nserving as cheap-to-evaluate time-efficient approximations of black-box\nfunctions to help engineers and practitioners make decisions and understand\ncomplex systems. As such, the need for explainability methods is rising and\nmany studies have been performed to facilitate knowledge discovery from\nsurrogate models. To respond to these enquiries, this paper introduces SMT-EX,\nan enhancement of the open-source Python Surrogate Modeling Toolbox (SMT) that\nintegrates explainability techniques into a state-of-the-art surrogate\nmodelling framework. More precisely, SMT-EX includes three key explainability\nmethods: Shapley Additive Explanations, Partial Dependence Plot, and Individual\nConditional Expectations. A peculiar explainability dependency of SMT has been\ndeveloped for such purpose that can be easily activated once the surrogate\nmodel is built, offering a user-friendly and efficient tool for swift insight\nextraction. The effectiveness of SMT-EX is showcased through two test cases.\nThe first case is a 10-variable wing weight problem with purely continuous\nvariables and the second one is a 3-variable mixed-categorical cantilever beam\nbending problem. Relying on SMT-EX analyses for these problems, we demonstrate\nits versatility in addressing a diverse range of problem characteristics.\nSMT-Explainability is freely available on Github:\nhttps:\/\/github.com\/SMTorg\/smt-explainability .\n","date":"2025-03-25"}
{"id":"2503.19498","title":"DomainCQA: Crafting Expert-Level QA from Domain-Specific Charts","abstract":"  Chart Question Answering (CQA) benchmarks are essential for evaluating the\ncapability of Multimodal Large Language Models (MLLMs) to interpret visual\ndata. However, current benchmarks focus primarily on the evaluation of\ngeneral-purpose CQA but fail to adequately capture domain-specific challenges.\nWe introduce DomainCQA, a systematic methodology for constructing\ndomain-specific CQA benchmarks, and demonstrate its effectiveness by developing\nAstroChart, a CQA benchmark in the field of astronomy. Our evaluation shows\nthat chart reasoning and combining chart information with domain knowledge for\ndeeper analysis and summarization, rather than domain-specific knowledge, pose\nthe primary challenge for existing MLLMs, highlighting a critical gap in\ncurrent benchmarks. By providing a scalable and rigorous framework, DomainCQA\nenables more precise assessment and improvement of MLLMs for domain-specific\napplications.\n","date":"2025-03-25"}
{"id":"2503.19501","title":"Pose-Based Fall Detection System: Efficient Monitoring on Standard CPUs","abstract":"  Falls among elderly residents in assisted living homes pose significant\nhealth risks, often leading to injuries and a decreased quality of life.\nCurrent fall detection solutions typically rely on sensor-based systems that\nrequire dedicated hardware, or on video-based models that demand high\ncomputational resources and GPUs for real-time processing. In contrast, this\npaper presents a robust fall detection system that does not require any\nadditional sensors or high-powered hardware. The system uses pose estimation\ntechniques, combined with threshold-based analysis and a voting mechanism, to\neffectively distinguish between fall and non-fall activities. For pose\ndetection, we leverage MediaPipe, a lightweight and efficient framework that\nenables real-time processing on standard CPUs with minimal computational\noverhead. By analyzing motion, body position, and key pose points, the system\nprocesses pose features with a 20-frame buffer, minimizing false positives and\nmaintaining high accuracy even in real-world settings. This unobtrusive,\nresource-efficient approach provides a practical solution for enhancing\nresident safety in old age homes, without the need for expensive sensors or\nhigh-end computational resources.\n","date":"2025-03-25"}
{"id":"2503.19502","title":"Towards Long-Range ENSO Prediction with an Explainable Deep Learning\n  Model","abstract":"  El Ni\\~no-Southern Oscillation (ENSO) is a prominent mode of interannual\nclimate variability with far-reaching global impacts. Its evolution is governed\nby intricate air-sea interactions, posing significant challenges for long-term\nprediction. In this study, we introduce CTEFNet, a multivariate deep learning\nmodel that synergizes convolutional neural networks and transformers to enhance\nENSO forecasting. By integrating multiple oceanic and atmospheric predictors,\nCTEFNet extends the effective forecast lead time to 20 months while mitigating\nthe impact of the spring predictability barrier, outperforming both dynamical\nmodels and state-of-the-art deep learning approaches. Furthermore, CTEFNet\noffers physically meaningful and statistically significant insights through\ngradient-based sensitivity analysis, revealing the key precursor signals that\ngovern ENSO dynamics, which align with well-established theories and reveal new\ninsights about inter-basin interactions among the Pacific, Atlantic, and Indian\nOceans. The CTEFNet's superior predictive skill and interpretable sensitivity\nassessments underscore its potential for advancing climate prediction. Our\nfindings highlight the importance of multivariate coupling in ENSO evolution\nand demonstrate the promise of deep learning in capturing complex climate\ndynamics with enhanced interpretability.\n","date":"2025-03-25"}
{"id":"2503.19503","title":"Adaptive Weighted Parameter Fusion with CLIP for Class-Incremental\n  Learning","abstract":"  Class-incremental Learning (CIL) enables the model to incrementally absorb\nknowledge from new classes and build a generic classifier across all previously\nencountered classes. When the model optimizes with new classes, the knowledge\nof previous classes is inevitably erased, leading to catastrophic forgetting.\nAddressing this challenge requires making a trade-off between retaining old\nknowledge and accommodating new information. However, this balancing process\noften requires sacrificing some information, which can lead to a partial loss\nin the model's ability to discriminate between classes. To tackle this issue,\nwe design the adaptive weighted parameter fusion with Contrastive\nLanguage-Image Pre-training (CLIP), which not only takes into account the\nvariability of the data distribution of different tasks, but also retains all\nthe effective information of the parameter matrix to the greatest extent. In\naddition, we introduce a balance factor that can balance the data distribution\nalignment and distinguishability of adjacent tasks. Experimental results on\nseveral traditional benchmarks validate the superiority of the proposed method.\n","date":"2025-03-25"}
{"id":"2503.19505","title":"Single-Step Latent Consistency Model for Remote Sensing Image\n  Super-Resolution","abstract":"  Recent advancements in diffusion models (DMs) have greatly advanced remote\nsensing image super-resolution (RSISR). However, their iterative sampling\nprocesses often result in slow inference speeds, limiting their application in\nreal-time tasks. To address this challenge, we propose the latent consistency\nmodel for super-resolution (LCMSR), a novel single-step diffusion approach\ndesigned to enhance both efficiency and visual quality in RSISR tasks. Our\nproposal is structured into two distinct stages. In the first stage, we\npretrain a residual autoencoder to encode the differential information between\nhigh-resolution (HR) and low-resolution (LR) images, transitioning the\ndiffusion process into a latent space to reduce computational costs. The second\nstage focuses on consistency diffusion learning, which aims to learn the\ndistribution of residual encodings in the latent space, conditioned on LR\nimages. The consistency constraint enforces that predictions at any two\ntimesteps along the reverse diffusion trajectory remain consistent, enabling\ndirect mapping from noise to data. As a result, the proposed LCMSR reduces the\niterative steps of traditional diffusion models from 50-1000 or more to just a\nsingle step, significantly improving efficiency. Experimental results\ndemonstrate that LCMSR effectively balances efficiency and performance,\nachieving inference times comparable to non-diffusion models while maintaining\nhigh-quality output.\n","date":"2025-03-25"}
{"id":"2503.19508","title":"Improved Alignment of Modalities in Large Vision Language Models","abstract":"  Recent advancements in vision-language models have achieved remarkable\nresults in making language models understand vision inputs. However, a unified\napproach to align these models across diverse tasks such as image captioning\nand visual question answering remains a challenge. Existing methods either\nrequire very big language models or very big datasets which is not efficient in\nutilizing existing models. This paper addresses this gap and devises a training\nstrategy of auto-regressive vision-language models, to unify vision-language\ntasks like image-captioning and visual question answering. We propose four\ntraining stages for aligning the vision model with the language model, in other\nwords, the language model is given an ability to process visual inputs. We also\ndevise different attention masks for training transformer-based language models\nthat improve the quality of visual features. Further, we introduce some\nfindings, 1) the attention mask should not be applied on visual inputs, 2) the\nLanguage model converges faster on AI- generated data, 3) More work should be\ndone in the alignment stage during the pre-training of the model, 4) the model\ncan easily adapt to any downstream tasks like visual question answering on\nhealthcare datasets like PathVQA. After training the model for one epoch for\nall the stages, it outperforms large models like VILA-13 billion models on\ncommon benchmarks like CIDEr scores on COCO and Flickr30k datasets and achieves\nvery close scores to GIT-2 on the same dataset despite being a much smaller\nmodel trained on a much smaller dataset. All of the training is done using best\npractices available like multi- GPU parallel training, lower-precision training\nwith 16-bit float numbers, faster attention (SDPA), and gradient accumulation,\nand completed the training within 12 hours.\n","date":"2025-03-25"}
{"id":"2503.19510","title":"RoboFlamingo-Plus: Fusion of Depth and RGB Perception with\n  Vision-Language Models for Enhanced Robotic Manipulation","abstract":"  As robotic technologies advancing towards more complex multimodal\ninteractions and manipulation tasks, the integration of advanced\nVision-Language Models (VLMs) has become a key driver in the field. Despite\nprogress with current methods, challenges persist in fusing depth and RGB\ninformation within 3D environments and executing tasks guided by linguistic\ninstructions. In response to these challenges, we have enhanced the existing\nRoboFlamingo framework by introducing RoboFlamingo-Plus, which incorporates\ndepth data into VLMs to significantly improve robotic manipulation performance.\nOur research achieves a nuanced fusion of RGB and depth information by\nintegrating a pre-trained Vision Transformer (ViT) with a resampling technique,\nclosely aligning this combined data with linguistic cues for superior\nmultimodal understanding. The novelty of RoboFlamingo-Plus lies in its\nadaptation of inputs for depth data processing, leveraging a pre-trained\nresampler for depth feature extraction, and employing cross-attention\nmechanisms for optimal feature integration. These improvements allow\nRoboFlamingo-Plus to not only deeply understand 3D environments but also easily\nperform complex, language-guided tasks in challenging settings. Experimental\nresults show that RoboFlamingo-Plus boosts robotic manipulation by 10-20% over\ncurrent methods, marking a significant advancement. Codes and model weights are\npublic at RoboFlamingo-Plus.\n","date":"2025-03-25"}
{"id":"2503.19516","title":"DataPlatter: Boosting Robotic Manipulation Generalization with Minimal\n  Costly Data","abstract":"  The growing adoption of Vision-Language-Action (VLA) models in embodied AI\nintensifies the demand for diverse manipulation demonstrations. However, high\ncosts associated with data collection often result in insufficient data\ncoverage across all scenarios, which limits the performance of the models. It\nis observed that the spatial reasoning phase (SRP) in large workspace dominates\nthe failure cases. Fortunately, this data can be collected with low cost,\nunderscoring the potential of leveraging inexpensive data to improve model\nperformance. In this paper, we introduce the DataPlatter method, a framework\nthat decouples training trajectories into distinct task stages and leverages\nabundant easily collectible SRP data to enhance VLA model's generalization.\nThrough analysis we demonstrate that sub-task-specific training with additional\nSRP data with proper proportion can act as a performance catalyst for robot\nmanipulation, maximizing the utilization of costly physical interaction phase\n(PIP) data. Experiments show that through introducing large proportion of\ncost-effective SRP trajectories into a limited set of PIP data, we can achieve\na maximum improvement of 41\\% on success rate in zero-shot scenes, while with\nthe ability to transfer manipulation skill to novel targets.\n","date":"2025-03-25"}
{"id":"2503.19523","title":"One Framework to Rule Them All: Unifying RL-Based and RL-Free Methods in\n  RLHF","abstract":"  In this article, we primarily examine a variety of RL-based and RL-free\nmethods designed to address Reinforcement Learning from Human Feedback (RLHF)\nand Large Reasoning Models (LRMs). We begin with a concise overview of the\ntypical steps involved in RLHF and LRMs. Next, we reinterpret several RL-based\nand RL-free algorithms through the perspective of neural structured bandit\nprediction, providing a clear conceptual framework that uncovers a deeper\nconnection between these seemingly distinct approaches. Following this, we\nbriefly review some core principles of reinforcement learning, drawing\nattention to an often-overlooked aspect in existing RLHF studies. This leads to\na detailed derivation of the standard RLHF objective within a full RL context,\ndemonstrating its equivalence to neural structured bandit prediction. Finally,\nby reinvestigating the principles behind Proximal Policy Optimization (PPO), we\npinpoint areas needing adjustment, which culminates in the introduction of the\nGeneralized Reinforce Optimization (GRO) framework, seamlessly integrating\nRL-based and RL-free methods in RLHF. We look forward to the community's\nefforts to empirically validate GRO and invite constructive feedback.\n","date":"2025-03-25"}
{"id":"2503.19530","title":"VectorFit : Adaptive Singular & Bias Vector Fine-Tuning of Pre-trained\n  Foundation Models","abstract":"  Popular PEFT methods achieve parameter efficiency by assuming that\nincremental weight updates are inherently low-rank, which often leads to a\nperformance gap compared to full fine-tuning. While recent methods have\nattempted to address this limitation, they typically lack sufficient parameter\nand memory efficiency. We propose VectorFit, an effective and easily deployable\napproach that adaptively trains the singular vectors and biases of pre-trained\nweight matrices. We demonstrate that the utilization of structural and\ntransformational characteristics of pre-trained weights enables high-rank\nupdates comparable to those of full fine-tuning. As a result, VectorFit\nachieves superior performance with 9X less trainable parameters compared to\nstate-of-the-art PEFT methods. Through extensive experiments over 17 datasets\nspanning diverse language and vision tasks such as natural language\nunderstanding and generation, question answering, image classification, and\nimage generation, we exhibit that VectorFit consistently outperforms baselines,\neven in extremely low-budget scenarios.\n","date":"2025-03-25"}
{"id":"2503.19540","title":"FLEX: A Benchmark for Evaluating Robustness of Fairness in Large\n  Language Models","abstract":"  Recent advancements in Large Language Models (LLMs) have significantly\nenhanced interactions between users and models. These advancements concurrently\nunderscore the need for rigorous safety evaluations due to the manifestation of\nsocial biases, which can lead to harmful societal impacts. Despite these\nconcerns, existing benchmarks may overlook the intrinsic weaknesses of LLMs,\nwhich can generate biased responses even with simple adversarial instructions.\nTo address this critical gap, we introduce a new benchmark, Fairness Benchmark\nin LLM under Extreme Scenarios (FLEX), designed to test whether LLMs can\nsustain fairness even when exposed to prompts constructed to induce bias. To\nthoroughly evaluate the robustness of LLMs, we integrate prompts that amplify\npotential biases into the fairness assessment. Comparative experiments between\nFLEX and existing benchmarks demonstrate that traditional evaluations may\nunderestimate the inherent risks in models. This highlights the need for more\nstringent LLM evaluation benchmarks to guarantee safety and fairness.\n","date":"2025-03-25"}
{"id":"2503.19543","title":"Scene-agnostic Pose Regression for Visual Localization","abstract":"  Absolute Pose Regression (APR) predicts 6D camera poses but lacks the\nadaptability to unknown environments without retraining, while Relative Pose\nRegression (RPR) generalizes better yet requires a large image retrieval\ndatabase. Visual Odometry (VO) generalizes well in unseen environments but\nsuffers from accumulated error in open trajectories. To address this dilemma,\nwe introduce a new task, Scene-agnostic Pose Regression (SPR), which can\nachieve accurate pose regression in a flexible way while eliminating the need\nfor retraining or databases. To benchmark SPR, we created a large-scale\ndataset, 360SPR, with over 200K photorealistic panoramas, 3.6M pinhole images\nand camera poses in 270 scenes at three different sensor heights. Furthermore,\na SPR-Mamba model is initially proposed to address SPR in a dual-branch manner.\nExtensive experiments and studies demonstrate the effectiveness of our SPR\nparadigm, dataset, and model. In the unknown scenes of both 360SPR and 360Loc\ndatasets, our method consistently outperforms APR, RPR and VO. The dataset and\ncode are available at\nhttps:\/\/junweizheng93.github.io\/publications\/SPR\/SPR.html.\n","date":"2025-03-25"}
{"id":"2503.19545","title":"Tiling artifacts and trade-offs of feature normalization in the\n  segmentation of large biological images","abstract":"  Segmentation of very large images is a common problem in microscopy, medical\nimaging or remote sensing. The problem is usually addressed by sliding window\ninference, which can theoretically lead to seamlessly stitched predictions.\nHowever, in practice many of the popular pipelines still suffer from tiling\nartifacts. We investigate the root cause of these issues and show that they\nstem from the normalization layers within the neural networks. We propose\nindicators to detect normalization issues and further explore the trade-offs\nbetween artifact-free and high-quality predictions, using three diverse\nmicroscopy datasets as examples. Finally, we propose to use BatchRenorm as the\nmost suitable normalization strategy, which effectively removes tiling\nartifacts and enhances transfer performance, thereby improving the reusability\nof trained networks for new datasets.\n","date":"2025-03-25"}
{"id":"2503.19546","title":"Practical Fine-Tuning of Autoregressive Models on Limited Handwritten\n  Texts","abstract":"  A common use case for OCR applications involves users uploading documents and\nprogressively correcting automatic recognition to obtain the final transcript.\nThis correction phase presents an opportunity for progressive adaptation of the\nOCR model, making it crucial to adapt early, while ensuring stability and\nreliability. We demonstrate that state-of-the-art transformer-based models can\neffectively support this adaptation, gradually reducing the annotator's\nworkload. Our results show that fine-tuning can reliably start with just 16\nlines, yielding a 10% relative improvement in CER, and scale up to 40% with 256\nlines. We further investigate the impact of model components, clarifying the\nroles of the encoder and decoder in the fine-tuning process. To guide\nadaptation, we propose reliable stopping criteria, considering both direct\napproaches and global trend analysis. Additionally, we show that OCR models can\nbe leveraged to cut annotation costs by half through confidence-based selection\nof informative lines, achieving the same performance with fewer annotations.\n","date":"2025-03-25"}
{"id":"2503.19549","title":"Noise Resilient Over-The-Air Federated Learning In Heterogeneous\n  Wireless Networks","abstract":"  In 6G wireless networks, Artificial Intelligence (AI)-driven applications\ndemand the adoption of Federated Learning (FL) to enable efficient and\nprivacy-preserving model training across distributed devices. Over-The-Air\nFederated Learning (OTA-FL) exploits the superposition property of multiple\naccess channels, allowing edge users in 6G networks to efficiently share\nspectral resources and perform low-latency global model aggregation. However,\nthese advantages come with challenges, as traditional OTA-FL techniques suffer\ndue to the joint effects of Additive White Gaussian Noise (AWGN) at the server,\nfading, and both data and system heterogeneity at the participating edge\ndevices. In this work, we propose the novel Noise Resilient Over-the-Air\nFederated Learning (NoROTA-FL) framework to jointly tackle these challenges in\nfederated wireless networks. In NoROTA-FL, the local optimization problems find\ncontrolled inexact solutions, which manifests as an additional proximal\nconstraint at the clients. This approach provides robustness against\nstraggler-induced partial work, heterogeneity, noise, and fading. From a\ntheoretical perspective, we leverage the zeroth- and first-order inexactness\nand establish convergence guarantees for non-convex optimization problems in\nthe presence of heterogeneous data and varying system capabilities.\nExperimentally, we validate NoROTA-FL on real-world datasets, including\nFEMNIST, CIFAR10, and CIFAR100, demonstrating its robustness in noisy and\nheterogeneous environments. Compared to state-of-the-art baselines such as\nCOTAF and FedProx, NoROTA-FL achieves significantly more stable convergence and\nhigher accuracy, particularly in the presence of stragglers.\n","date":"2025-03-25"}
{"id":"2503.19551","title":"Scaling Laws of Synthetic Data for Language Models","abstract":"  Large language models (LLMs) achieve strong performance across diverse tasks,\nlargely driven by high-quality web data used in pre-training. However, recent\nstudies indicate this data source is rapidly depleting. Synthetic data emerges\nas a promising alternative, but it remains unclear whether synthetic datasets\nexhibit predictable scalability comparable to raw pre-training data. In this\nwork, we systematically investigate the scaling laws of synthetic data by\nintroducing SynthLLM, a scalable framework that transforms pre-training corpora\ninto diverse, high-quality synthetic datasets. Our approach achieves this by\nautomatically extracting and recombining high-level concepts across multiple\ndocuments using a graph algorithm. Key findings from our extensive mathematical\nexperiments on SynthLLM include: (1) SynthLLM generates synthetic data that\nreliably adheres to the rectified scaling law across various model sizes; (2)\nPerformance improvements plateau near 300B tokens; and (3) Larger models\napproach optimal performance with fewer training tokens. For instance, an 8B\nmodel peaks at 1T tokens, while a 3B model requires 4T. Moreover, comparisons\nwith existing synthetic data generation and augmentation methods demonstrate\nthat SynthLLM achieves superior performance and scalability. Our findings\nhighlight synthetic data as a scalable and reliable alternative to organic\npre-training corpora, offering a viable path toward continued improvement in\nmodel performance.\n","date":"2025-03-25"}
{"id":"2503.19554","title":"Causal Bayesian Optimization with Unknown Graphs","abstract":"  Causal Bayesian Optimization (CBO) is a methodology designed to optimize an\noutcome variable by leveraging known causal relationships through targeted\ninterventions. Traditional CBO methods require a fully and accurately specified\ncausal graph, which is a limitation in many real-world scenarios where such\ngraphs are unknown. To address this, we propose a new method for the CBO\nframework that operates without prior knowledge of the causal graph. Consistent\nwith causal bandit theory, we demonstrate through theoretical analysis and that\nfocusing on the direct causal parents of the target variable is sufficient for\noptimization, and provide empirical validation in the context of CBO.\nFurthermore we introduce a new method that learns a Bayesian posterior over the\ndirect parents of the target variable. This allows us to optimize the outcome\nvariable while simultaneously learning the causal structure. Our contributions\ninclude a derivation of the closed-form posterior distribution for the linear\ncase. In the nonlinear case where the posterior is not tractable, we present a\nGaussian Process (GP) approximation that still enables CBO by inferring the\nparents of the outcome variable. The proposed method performs competitively\nwith existing benchmarks and scales well to larger graphs, making it a\npractical tool for real-world applications where causal information is\nincomplete.\n","date":"2025-03-25"}
{"id":"2503.19557","title":"Dance Like a Chicken: Low-Rank Stylization for Human Motion Diffusion","abstract":"  Text-to-motion generative models span a wide range of 3D human actions but\nstruggle with nuanced stylistic attributes such as a \"Chicken\" style. Due to\nthe scarcity of style-specific data, existing approaches pull the generative\nprior towards a reference style, which often results in out-of-distribution low\nquality generations. In this work, we introduce LoRA-MDM, a lightweight\nframework for motion stylization that generalizes to complex actions while\nmaintaining editability. Our key insight is that adapting the generative prior\nto include the style, while preserving its overall distribution, is more\neffective than modifying each individual motion during generation. Building on\nthis idea, LoRA-MDM learns to adapt the prior to include the reference style\nusing only a few samples. The style can then be used in the context of\ndifferent textual prompts for generation. The low-rank adaptation shifts the\nmotion manifold in a semantically meaningful way, enabling realistic style\ninfusion even for actions not present in the reference samples. Moreover,\npreserving the distribution structure enables advanced operations such as style\nblending and motion editing. We compare LoRA-MDM to state-of-the-art stylized\nmotion generation methods and demonstrate a favorable balance between text\nfidelity and style consistency.\n","date":"2025-03-25"}
{"id":"2503.19564","title":"FedMM-X: A Trustworthy and Interpretable Framework for Federated\n  Multi-Modal Learning in Dynamic Environments","abstract":"  As artificial intelligence systems increasingly operate in Real-world\nenvironments, the integration of multi-modal data sources such as vision,\nlanguage, and audio presents both unprecedented opportunities and critical\nchallenges for achieving trustworthy intelligence. In this paper, we propose a\nnovel framework that unifies federated learning with explainable multi-modal\nreasoning to ensure trustworthiness in decentralized, dynamic settings. Our\napproach, called FedMM-X (Federated Multi-Modal Explainable Intelligence),\nleverages cross-modal consistency checks, client-level interpretability\nmechanisms, and dynamic trust calibration to address challenges posed by data\nheterogeneity, modality imbalance, and out-of-distribution generalization.\nThrough rigorous evaluation across federated multi-modal benchmarks involving\nvision-language tasks, we demonstrate improved performance in both accuracy and\ninterpretability while reducing vulnerabilities to adversarial and spurious\ncorrelations. Further, we introduce a novel trust score aggregation method to\nquantify global model reliability under dynamic client participation. Our\nfindings pave the way toward developing robust, interpretable, and socially\nresponsible AI systems in Real-world environments.\n","date":"2025-03-25"}
{"id":"2503.19570","title":"Improved tissue sodium concentration quantification in breast cancer by\n  reducing partial volume effects: a preliminary study","abstract":"  Introduction: In sodium (23Na) MRI, partial volume effects (PVE) are one of\nthe most common causes of errors in the quantification of tissue sodium\nconcentration (TSC) in vivo. Advanced image reconstruction algorithms, such as\ncompressed sensing (CS), have been shown to potentially reduce PVE. Therefore,\nwe investigated the feasibility of CS-based methods for image quality and TSC\nquantification accuracy improvement in patients with breast cancer (BC).\nSubjects and Methods: Three healthy participants and 12 female participants\nwith BC were examined on a 7T MRI scanner in this study. We reconstructed\n23Na-MRI images using the weighted total variation (wTV) and directional total\nvariation (dTV), anatomically guided total variation (AG-TV), and adaptive\ncombine (ADC) reconstruction and performed image quality assessment. We\nevaluated agreement in tumor volumes delineated on sodium data using the Dice\nscore and performed TSC quantification for different image reconstruction\napproaches. Results: All methods provided sodium images of the breast with good\nquality. The mean Dice scores for wTV, dTV, and AG-TV were 65%, 72%, and 75%,\nrespectively. In the breast tumors, average TSC values were 83.0, 72.0, 80.0,\nand 84.0 mmol\/L, respectively. There was a significant difference between dTV\nand wTV (p<0.001), as well as between dTV and AG-TV (p<0.001) and dTV and ADC\nalgorithm (p<0.001). Conclusion: The results of this study showed that there\nare differences in tumor appearance and TSC estimations that might be depending\non the type of image reconstruction and parameters used, most likely due to\ndifferences in their robustness in reducing PVE.\n","date":"2025-03-25"}
{"id":"2503.19574","title":"Context-Efficient Retrieval with Factual Decomposition","abstract":"  There has recently been considerable interest in incorporating information\nretrieval into large language models (LLMs). Retrieval from a dynamically\nexpanding external corpus of text allows a model to incorporate current events\nand can be viewed as a form of episodic memory. Here we demonstrate that\npre-processing the external corpus into semi-structured ''atomic facts'' makes\nretrieval more efficient. More specifically, we demonstrate that our particular\nform of atomic facts improves performance on various question answering tasks\nwhen the amount of retrieved text is limited. Limiting the amount of retrieval\nreduces the size of the context and improves inference efficiency.\n","date":"2025-03-25"}
{"id":"2503.19576","title":"SINR: Sparsity Driven Compressed Implicit Neural Representations","abstract":"  Implicit Neural Representations (INRs) are increasingly recognized as a\nversatile data modality for representing discretized signals, offering benefits\nsuch as infinite query resolution and reduced storage requirements. Existing\nsignal compression approaches for INRs typically employ one of two strategies:\n1. direct quantization with entropy coding of the trained INR; 2. deriving a\nlatent code on top of the INR through a learnable transformation. Thus, their\nperformance is heavily dependent on the quantization and entropy coding schemes\nemployed. In this paper, we introduce SINR, an innovative compression algorithm\nthat leverages the patterns in the vector spaces formed by weights of INRs. We\ncompress these vector spaces using a high-dimensional sparse code within a\ndictionary. Further analysis reveals that the atoms of the dictionary used to\ngenerate the sparse code do not need to be learned or transmitted to\nsuccessfully recover the INR weights. We demonstrate that the proposed approach\ncan be integrated with any existing INR-based signal compression technique. Our\nresults indicate that SINR achieves substantial reductions in storage\nrequirements for INRs across various configurations, outperforming conventional\nINR-based compression baselines. Furthermore, SINR maintains high-quality\ndecoding across diverse data modalities, including images, occupancy fields,\nand Neural Radiance Fields.\n","date":"2025-03-25"}
{"id":"2503.19577","title":"Post-Hoc Calibrated Anomaly Detection","abstract":"  Deep unsupervised anomaly detection has seen improvements in a supervised\nbinary classification paradigm in which auxiliary external data is included in\nthe training set as anomalous data in a process referred to as outlier\nexposure, which opens the possibility of exploring the efficacy of post-hoc\ncalibration for anomaly detection and localization. Post-hoc Platt scaling and\nBeta calibration are found to improve results with gradient-based input\nperturbation, as well as post-hoc training with a strictly proper loss of a\nbase model initially trained on an unsupervised loss. Post-hoc calibration is\nalso found at times to be more effective using random synthesized spectral data\nas labeled anomalous data in the calibration set, suggesting that outlier\nexposure is superior only for initial training.\n","date":"2025-03-25"}
{"id":"2503.19584","title":"Multi-agent Application System in Office Collaboration Scenarios","abstract":"  This paper introduces a multi-agent application system designed to enhance\noffice collaboration efficiency and work quality. The system integrates\nartificial intelligence, machine learning, and natural language processing\ntechnologies, achieving functionalities such as task allocation, progress\nmonitoring, and information sharing. The agents within the system are capable\nof providing personalized collaboration support based on team members' needs\nand incorporate data analysis tools to improve decision-making quality. The\npaper also proposes an intelligent agent architecture that separates Plan and\nSolver, and through techniques such as multi-turn query rewriting and business\ntool retrieval, it enhances the agent's multi-intent and multi-turn dialogue\ncapabilities. Furthermore, the paper details the design of tools and multi-turn\ndialogue in the context of office collaboration scenarios, and validates the\nsystem's effectiveness through experiments and evaluations. Ultimately, the\nsystem has demonstrated outstanding performance in real business applications,\nparticularly in query understanding, task planning, and tool calling. Looking\nforward, the system is expected to play a more significant role in addressing\ncomplex interaction issues within dynamic environments and large-scale\nmulti-agent systems.\n","date":"2025-03-25"}
{"id":"2503.19586","title":"Distinct social-linguistic processing between humans and large\n  audio-language models: Evidence from model-brain alignment","abstract":"  Voice-based AI development faces unique challenges in processing both\nlinguistic and paralinguistic information. This study compares how large\naudio-language models (LALMs) and humans integrate speaker characteristics\nduring speech comprehension, asking whether LALMs process\nspeaker-contextualized language in ways that parallel human cognitive\nmechanisms. We compared two LALMs' (Qwen2-Audio and Ultravox 0.5) processing\npatterns with human EEG responses. Using surprisal and entropy metrics from the\nmodels, we analyzed their sensitivity to speaker-content incongruency across\nsocial stereotype violations (e.g., a man claiming to regularly get manicures)\nand biological knowledge violations (e.g., a man claiming to be pregnant).\nResults revealed that Qwen2-Audio exhibited increased surprisal for\nspeaker-incongruent content and its surprisal values significantly predicted\nhuman N400 responses, while Ultravox 0.5 showed limited sensitivity to speaker\ncharacteristics. Importantly, neither model replicated the human-like\nprocessing distinction between social violations (eliciting N400 effects) and\nbiological violations (eliciting P600 effects). These findings reveal both the\npotential and limitations of current LALMs in processing speaker-contextualized\nlanguage, and suggest differences in social-linguistic processing mechanisms\nbetween humans and LALMs.\n","date":"2025-03-25"}
{"id":"2503.19588","title":"Video Anomaly Detection with Contours -- A Study","abstract":"  In Pose-based Video Anomaly Detection prior art is rooted on the assumption\nthat abnormal events can be mostly regarded as a result of uncommon human\nbehavior. Opposed to utilizing skeleton representations of humans, however, we\ninvestigate the potential of learning recurrent motion patterns of normal human\nbehavior using 2D contours. Keeping all advantages of pose-based methods, such\nas increased object anonymization, the shift from human skeletons to contours\nis hypothesized to leave the opportunity to cover more object categories open\nfor future research. We propose formulating the problem as a regression and a\nclassification task, and additionally explore two distinct data representation\ntechniques for contours. To further reduce the computational complexity of\nPose-based Video Anomaly Detection solutions, all methods in this study are\nbased on shallow Neural Networks from the field of Deep Learning, and evaluated\non the three most prominent benchmark datasets within Video Anomaly Detection\nand their human-related counterparts, totaling six datasets. Our results\nindicate that this novel perspective on Pose-based Video Anomaly Detection\nmarks a promising direction for future research.\n","date":"2025-03-25"}
{"id":"2503.19589","title":"Prompt-Guided Dual-Path UNet with Mamba for Medical Image Segmentation","abstract":"  Convolutional neural networks (CNNs) and transformers are widely employed in\nconstructing UNet architectures for medical image segmentation tasks. However,\nCNNs struggle to model long-range dependencies, while transformers suffer from\nquadratic computational complexity. Recently, Mamba, a type of State Space\nModels, has gained attention for its exceptional ability to model long-range\ninteractions while maintaining linear computational complexity. Despite the\nemergence of several Mamba-based methods, they still present the following\nlimitations: first, their network designs generally lack perceptual\ncapabilities for the original input data; second, they primarily focus on\ncapturing global information, while often neglecting local details. To address\nthese challenges, we propose a prompt-guided CNN-Mamba dual-path UNet, termed\nPGM-UNet, for medical image segmentation. Specifically, we introduce a\nprompt-guided residual Mamba module that adaptively extracts dynamic visual\nprompts from the original input data, effectively guiding Mamba in capturing\nglobal information. Additionally, we design a local-global information fusion\nnetwork, comprising a local information extraction module, a prompt-guided\nresidual Mamba module, and a multi-focus attention fusion module, which\neffectively integrates local and global information. Furthermore, inspired by\nKolmogorov-Arnold Networks (KANs), we develop a multi-scale information\nextraction module to capture richer contextual information without altering the\nresolution. We conduct extensive experiments on the ISIC-2017, ISIC-2018, DIAS,\nand DRIVE. The results demonstrate that the proposed method significantly\noutperforms state-of-the-art approaches in multiple medical image segmentation\ntasks.\n","date":"2025-03-25"}
{"id":"2503.19591","title":"Boosting the Transferability of Audio Adversarial Examples with Acoustic\n  Representation Optimization","abstract":"  With the widespread application of automatic speech recognition (ASR)\nsystems, their vulnerability to adversarial attacks has been extensively\nstudied. However, most existing adversarial examples are generated on specific\nindividual models, resulting in a lack of transferability. In real-world\nscenarios, attackers often cannot access detailed information about the target\nmodel, making query-based attacks unfeasible. To address this challenge, we\npropose a technique called Acoustic Representation Optimization that aligns\nadversarial perturbations with low-level acoustic characteristics derived from\nspeech representation models. Rather than relying on model-specific,\nhigher-layer abstractions, our approach leverages fundamental acoustic\nrepresentations that remain consistent across diverse ASR architectures. By\nenforcing an acoustic representation loss to guide perturbations toward these\nrobust, lower-level representations, we enhance the cross-model transferability\nof adversarial examples without degrading audio quality. Our method is\nplug-and-play and can be integrated with any existing attack methods. We\nevaluate our approach on three modern ASR models, and the experimental results\ndemonstrate that our method significantly improves the transferability of\nadversarial examples generated by previous methods while preserving the audio\nquality.\n","date":"2025-03-25"}
{"id":"2503.19592","title":"SACB-Net: Spatial-awareness Convolutions for Medical Image Registration","abstract":"  Deep learning-based image registration methods have shown state-of-the-art\nperformance and rapid inference speeds. Despite these advances, many existing\napproaches fall short in capturing spatially varying information in non-local\nregions of feature maps due to the reliance on spatially-shared convolution\nkernels. This limitation leads to suboptimal estimation of deformation fields.\nIn this paper, we propose a 3D Spatial-Awareness Convolution Block (SACB) to\nenhance the spatial information within feature representations. Our SACB\nestimates the spatial clusters within feature maps by leveraging feature\nsimilarity and subsequently parameterizes the adaptive convolution kernels\nacross diverse regions. This adaptive mechanism generates the convolution\nkernels (weights and biases) tailored to spatial variations, thereby enabling\nthe network to effectively capture spatially varying information. Building on\nSACB, we introduce a pyramid flow estimator (named SACB-Net) that integrates\nSACBs to facilitate multi-scale flow composition, particularly addressing large\ndeformations. Experimental results on the brain IXI and LPBA datasets as well\nas Abdomen CT datasets demonstrate the effectiveness of SACB and the\nsuperiority of SACB-Net over the state-of-the-art learning-based registration\nmethods. The code is available at https:\/\/github.com\/x-xc\/SACB_Net .\n","date":"2025-03-25"}
{"id":"2503.19595","title":"Optimizing Language Models for Inference Time Objectives using\n  Reinforcement Learning","abstract":"  In this work, we investigate the merits of explicitly optimizing for\ninference time algorithmic performance during model training. We show how\noptimizing for inference time performance can improve overall model efficacy.\nWe consider generic inference time objectives with $k$ samples, with a focus on\npass@$k$ and majority voting as two main applications. With language model\ntraining on reasoning datasets, we showcase the performance trade-off enabled\nby training with such objectives. When training on code generation tasks, we\nshow that the approach significantly improves pass@$k$ objectives compared to\nthe baseline method.\n","date":"2025-03-25"}
{"id":"2503.19598","title":"The Greatest Good Benchmark: Measuring LLMs' Alignment with Utilitarian\n  Moral Dilemmas","abstract":"  The question of how to make decisions that maximise the well-being of all\npersons is very relevant to design language models that are beneficial to\nhumanity and free from harm. We introduce the Greatest Good Benchmark to\nevaluate the moral judgments of LLMs using utilitarian dilemmas. Our analysis\nacross 15 diverse LLMs reveals consistently encoded moral preferences that\ndiverge from established moral theories and lay population moral standards.\nMost LLMs have a marked preference for impartial beneficence and rejection of\ninstrumental harm. These findings showcase the 'artificial moral compass' of\nLLMs, offering insights into their moral alignment.\n","date":"2025-03-25"}
{"id":"2503.19599","title":"HoarePrompt: Structural Reasoning About Program Correctness in Natural\n  Language","abstract":"  While software requirements are often expressed in natural language,\nverifying the correctness of a program against natural language requirements is\na hard and underexplored problem. Large language models (LLMs) are promising\ncandidates for addressing this challenge, however our experience shows that\nthey are ineffective in this task, often failing to detect even straightforward\nbugs. To address this gap, we introduce HoarePrompt, a novel approach that\nadapts fundamental ideas from program analysis and verification to natural\nlanguage artifacts. Drawing inspiration from the strongest postcondition\ncalculus, HoarePrompt employs a systematic, step-by-step process in which an\nLLM generates natural language descriptions of reachable program states at\nvarious points in the code. To manage loops, we propose few-shot-driven\nk-induction, an adaptation of the k-induction method widely used in model\nchecking. Once program states are described, HoarePrompt leverages the LLM to\nassess whether the program, annotated with these state descriptions, conforms\nto the natural language requirements. For evaluating the quality of classifiers\nof program correctness with respect to natural language requirements, we\nconstructed CoCoClaNeL, a challenging dataset of solutions to programming\ncompetition problems. Our experiments show that HoarePrompt improves the MCC by\n62% compared to directly using Zero-shot-CoT prompts for correctness\nclassification. Furthermore, HoarePrompt outperforms a classifier that assesses\ncorrectness via LLM-based test generation by increasing the MCC by 93%. The\ninductive reasoning mechanism contributes a 28% boost to MCC, underscoring its\neffectiveness in managing loops.\n","date":"2025-03-25"}
{"id":"2503.19602","title":"Innate Reasoning is Not Enough: In-Context Learning Enhances Reasoning\n  Large Language Models with Less Overthinking","abstract":"  Recent advances in Large Language Models (LLMs) have introduced Reasoning\nLarge Language Models (RLLMs), which employ extended thinking processes with\nreflection and self-correction capabilities, demonstrating the effectiveness of\ntest-time scaling. RLLMs exhibit innate Chain-of-Thought (CoT) reasoning\ncapability obtained from training, leading to a natural question: \"Is CoT\nprompting, a popular In-Context Learning (ICL) method for chat LLMs, necessary\nto enhance the reasoning capability of RLLMs?\" In this work, we present the\nfirst comprehensive analysis of the impacts of Zero-shot CoT and Few-shot CoT\non RLLMs across mathematical reasoning tasks. We examine models ranging from\n1.5B to 32B parameters, finding that contrary to concerns, CoT prompting\nsignificantly enhances RLLMs' performance in most scenarios. Our results reveal\ndistinct patterns: large-capacity models show minimal improvement on simple\ntasks but substantial gains on complex problems, while smaller models exhibit\nthe opposite behavior. Further analysis demonstrates that CoT prompting\neffectively controls the distribution of the numbers of thinking tokens and\nreasoning steps, reducing excessive reflections by approximately 90% in some\ncases. Moreover, attention logits analysis reveals the RLLMs' overfitting to\nreflection-related words, which is mitigated by external CoT guidance. Notably,\nour experiments indicate that for RLLMs, one-shot CoT consistently yields\nsuperior performance compared to Few-shot CoT approaches. Our findings provide\nimportant insights for optimizing RLLMs' performance through appropriate\nprompting strategies.\n","date":"2025-03-25"}
{"id":"2503.19604","title":"GIViC: Generative Implicit Video Compression","abstract":"  While video compression based on implicit neural representations (INRs) has\nrecently demonstrated great potential, existing INR-based video codecs still\ncannot achieve state-of-the-art (SOTA) performance compared to their\nconventional or autoencoder-based counterparts given the same coding\nconfiguration. In this context, we propose a Generative Implicit Video\nCompression framework, GIViC, aiming at advancing the performance limits of\nthis type of coding methods. GIViC is inspired by the characteristics that INRs\nshare with large language and diffusion models in exploiting long-term\ndependencies. Through the newly designed implicit diffusion process, GIViC\nperforms diffusive sampling across coarse-to-fine spatiotemporal\ndecompositions, gradually progressing from coarser-grained full-sequence\ndiffusion to finer-grained per-token diffusion. A novel Hierarchical Gated\nLinear Attention-based transformer (HGLA), is also integrated into the\nframework, which dual-factorizes global dependency modeling along scale and\nsequential axes. The proposed GIViC model has been benchmarked against SOTA\nconventional and neural codecs using a Random Access (RA) configuration (YUV\n4:2:0, GOPSize=32), and yields BD-rate savings of 15.94%, 22.46% and 8.52% over\nVVC VTM, DCVC-FM and NVRC, respectively. As far as we are aware, GIViC is the\nfirst INR-based video codec that outperforms VTM based on the RA coding\nconfiguration. The source code will be made available.\n","date":"2025-03-25"}
{"id":"2503.19605","title":"Lean Formalization of Generalization Error Bound by Rademacher\n  Complexity","abstract":"  We formalize the generalization error bound using Rademacher complexity in\nthe Lean 4 theorem prover. Generalization error quantifies the gap between a\nlearning machine's performance on given training data versus unseen test data,\nand Rademacher complexity serves as an estimate of this error based on the\ncomplexity of learning machines, or hypothesis class. Unlike traditional\nmethods such as PAC learning and VC dimension, Rademacher complexity is\napplicable across diverse machine learning scenarios including deep learning\nand kernel methods. We formalize key concepts and theorems, including the\nempirical and population Rademacher complexities, and establish generalization\nerror bounds through formal proofs of McDiarmid's inequality, Hoeffding's\nlemma, and symmetrization arguments.\n","date":"2025-03-25"}
{"id":"2503.19606","title":"Single Shot AI-assisted quantification of KI-67 proliferation index in\n  breast cancer","abstract":"  Reliable quantification of Ki-67, a key proliferation marker in breast\ncancer, is essential for molecular subtyping and informed treatment planning.\nConventional approaches, including visual estimation and manual counting,\nsuffer from interobserver variability and limited reproducibility. This study\nintroduces an AI-assisted method using the YOLOv8 object detection framework\nfor automated Ki-67 scoring. High-resolution digital images (40x magnification)\nof immunohistochemically stained tumor sections were captured from Ki-67\nhotspot regions and manually annotated by a domain expert to distinguish\nKi-67-positive and negative tumor cells. The dataset was augmented and divided\ninto training (80%), validation (10%), and testing (10%) subsets. Among the\nYOLOv8 variants tested, the Medium model achieved the highest performance, with\na mean Average Precision at 50% Intersection over Union (mAP50) exceeding 85%\nfor Ki-67-positive cells. The proposed approach offers an efficient, scalable,\nand objective alternative to conventional scoring methods, supporting greater\nconsistency in Ki-67 evaluation. Future directions include developing\nuser-friendly clinical interfaces and expanding to multi-institutional datasets\nto enhance generalizability and facilitate broader adoption in diagnostic\npractice.\n","date":"2025-03-25"}
{"id":"2503.19607","title":"Enabling Rapid Shared Human-AI Mental Model Alignment via the\n  After-Action Review","abstract":"  In this work, we present two novel contributions toward improving research in\nhuman-machine teaming (HMT): 1) a Minecraft testbed to accelerate testing and\ndeployment of collaborative AI agents and 2) a tool to allow users to revisit\nand analyze behaviors within an HMT episode to facilitate shared mental model\ndevelopment. Our browser-based Minecraft testbed allows for rapid testing of\ncollaborative agents in a continuous-space, real-time, partially-observable\nenvironment with real humans without cumbersome setup typical to human-AI\ninteraction user studies. As Minecraft has an extensive player base and a rich\necosystem of pre-built AI agents, we hope this contribution can help to\nfacilitate research quickly in the design of new collaborative agents and in\nunderstanding different human factors within HMT. Our mental model alignment\ntool facilitates user-led post-mission analysis by including video displays of\nfirst-person perspectives of the team members (i.e., the human and AI) that can\nbe replayed, and a chat interface that leverages GPT-4 to provide answers to\nvarious queries regarding the AI's experiences and model details.\n","date":"2025-03-25"}
{"id":"2503.19611","title":"Analyzable Chain-of-Musical-Thought Prompting for High-Fidelity Music\n  Generation","abstract":"  Autoregressive (AR) models have demonstrated impressive capabilities in\ngenerating high-fidelity music. However, the conventional next-token prediction\nparadigm in AR models does not align with the human creative process in music\ncomposition, potentially compromising the musicality of generated samples. To\novercome this limitation, we introduce MusiCoT, a novel chain-of-thought (CoT)\nprompting technique tailored for music generation. MusiCoT empowers the AR\nmodel to first outline an overall music structure before generating audio\ntokens, thereby enhancing the coherence and creativity of the resulting\ncompositions. By leveraging the contrastive language-audio pretraining (CLAP)\nmodel, we establish a chain of \"musical thoughts\", making MusiCoT scalable and\nindependent of human-labeled data, in contrast to conventional CoT methods.\nMoreover, MusiCoT allows for in-depth analysis of music structure, such as\ninstrumental arrangements, and supports music referencing -- accepting\nvariable-length audio inputs as optional style references. This innovative\napproach effectively addresses copying issues, positioning MusiCoT as a vital\npractical method for music prompting. Our experimental results indicate that\nMusiCoT consistently achieves superior performance across both objective and\nsubjective metrics, producing music quality that rivals state-of-the-art\ngeneration models.\n  Our samples are available at https:\/\/MusiCoT.github.io\/.\n","date":"2025-03-25"}
{"id":"2503.19612","title":"RL-finetuning LLMs from on- and off-policy data with a single algorithm","abstract":"  We introduce a novel reinforcement learning algorithm (AGRO, for\nAny-Generation Reward Optimization) for fine-tuning large-language models. AGRO\nleverages the concept of generation consistency, which states that the optimal\npolicy satisfies the notion of consistency across any possible generation of\nthe model. We derive algorithms that find optimal solutions via the\nsample-based policy gradient and provide theoretical guarantees on their\nconvergence. Our experiments demonstrate the effectiveness of AGRO in both\non-policy and off-policy settings, showing improved performance on the\nmathematical reasoning dataset over baseline algorithms.\n","date":"2025-03-25"}
{"id":"2503.19618","title":"Learning to chain-of-thought with Jensen's evidence lower bound","abstract":"  We propose a way to optimize chain-of-thought with reinforcement learning,\nbut without external reward function. Our algorithm relies on viewing\nchain-of-thought as latent variable as part of a probabilistic inference\nproblem. Contrary to the full evidence lower bound, we propose to apply a much\nsimpler Jensen's lower bound, which derives tractable objectives with simple\nalgorithmic components (e.g., without the need for parametric approximate\nposterior), making it more conducive to modern large-scale training. The lower\nbound approach naturally interpolates other methods such as supervised\nfine-tuning and online reinforcement learning, whose practical trade-offs we\nwill illustrate. Finally, we show that on mathematical reasoning problems,\noptimizing with Jensen's lower bound is as effective as policy gradient with\nexternal reward. Taken together, our results showcase as a proof of concept to\nthis new algorithmic paradigm's potential to more generic applications.\n","date":"2025-03-25"}
{"id":"2503.19620","title":"Optimization through In-Context Learning and Iterative LLM Prompting for\n  Nuclear Engineering Design Problems","abstract":"  The optimization of nuclear engineering designs, such as nuclear fuel\nassembly configurations, involves managing competing objectives like reactivity\ncontrol and power distribution. This study explores the use of Optimization by\nPrompting, an iterative approach utilizing large language models (LLMs), to\naddress these challenges. The method is straightforward to implement, requiring\nno hyperparameter tuning or complex mathematical formulations. Optimization\nproblems can be described in plain English, with only an evaluator and a\nparsing script needed for execution. The in-context learning capabilities of\nLLMs enable them to understand problem nuances, therefore, they have the\npotential to surpass traditional metaheuristic optimization methods. This study\ndemonstrates the application of LLMs as optimizers to Boiling Water Reactor\n(BWR) fuel lattice design, showing the capability of commercial LLMs to achieve\nsuperior optimization results compared to traditional methods.\n","date":"2025-03-25"}
{"id":"2503.19622","title":"Exploring Hallucination of Large Multimodal Models in Video\n  Understanding: Benchmark, Analysis and Mitigation","abstract":"  The hallucination of large multimodal models (LMMs), providing responses that\nappear correct but are actually incorrect, limits their reliability and\napplicability. This paper aims to study the hallucination problem of LMMs in\nvideo modality, which is dynamic and more challenging compared to static\nmodalities like images and text. From this motivation, we first present a\ncomprehensive benchmark termed HAVEN for evaluating hallucinations of LMMs in\nvideo understanding tasks. It is built upon three dimensions, i.e.,\nhallucination causes, hallucination aspects, and question formats, resulting in\n6K questions. Then, we quantitatively study 7 influential factors on\nhallucinations, e.g., duration time of videos, model sizes, and model\nreasoning, via experiments of 16 LMMs on the presented benchmark. In addition,\ninspired by recent thinking models like OpenAI o1, we propose a video-thinking\nmodel to mitigate the hallucinations of LMMs via supervised reasoning\nfine-tuning (SRFT) and direct preference optimization (TDPO)-- where SRFT\nenhances reasoning capabilities while TDPO reduces hallucinations in the\nthinking process. Extensive experiments and analyses demonstrate the\neffectiveness. Remarkably, it improves the baseline by 7.65% in accuracy on\nhallucination evaluation and reduces the bias score by 4.5%. The code and data\nare public at https:\/\/github.com\/Hongcheng-Gao\/HAVEN.\n","date":"2025-03-25"}
{"id":"2503.19625","title":"DynOPETs: A Versatile Benchmark for Dynamic Object Pose Estimation and\n  Tracking in Moving Camera Scenarios","abstract":"  In the realm of object pose estimation, scenarios involving both dynamic\nobjects and moving cameras are prevalent. However, the scarcity of\ncorresponding real-world datasets significantly hinders the development and\nevaluation of robust pose estimation models. This is largely attributed to the\ninherent challenges in accurately annotating object poses in dynamic scenes\ncaptured by moving cameras. To bridge this gap, this paper presents a novel\ndataset DynOPETs and a dedicated data acquisition and annotation pipeline\ntailored for object pose estimation and tracking in such unconstrained\nenvironments. Our efficient annotation method innovatively integrates pose\nestimation and pose tracking techniques to generate pseudo-labels, which are\nsubsequently refined through pose graph optimization. The resulting dataset\noffers accurate pose annotations for dynamic objects observed from moving\ncameras. To validate the effectiveness and value of our dataset, we perform\ncomprehensive evaluations using 18 state-of-the-art methods, demonstrating its\npotential to accelerate research in this challenging domain. The dataset will\nbe made publicly available to facilitate further exploration and advancement in\nthe field.\n","date":"2025-03-25"}
{"id":"2503.19626","title":"Red Teaming with Artificial Intelligence-Driven Cyberattacks: A Scoping\n  Review","abstract":"  The progress of artificial intelligence (AI) has made sophisticated methods\navailable for cyberattacks and red team activities. These AI attacks can\nautomate the process of penetrating a target or collecting sensitive data. The\nnew methods can also accelerate the execution of the attacks. This review\narticle examines the use of AI technologies in cybersecurity attacks. It also\ntries to describe typical targets for such attacks. We employed a scoping\nreview methodology to analyze articles and identify AI methods, targets, and\nmodels that red teams can utilize to simulate cybercrime. From the 470 records\nscreened, 11 were included in the review. Various cyberattack methods were\nidentified, targeting sensitive data, systems, social media profiles,\npasswords, and URLs. The application of AI in cybercrime to develop versatile\nattack models presents an increasing threat. Furthermore, AI-based techniques\nin red team use can provide new ways to address these issues.\n","date":"2025-03-25"}
{"id":"2503.19633","title":"1.4 Million Open-Source Distilled Reasoning Dataset to Empower Large\n  Language Model Training","abstract":"  The AM-DeepSeek-R1-Distilled is a large-scale dataset with thinking traces\nfor general reasoning tasks, composed of high-quality and challenging reasoning\nproblems. These problems are collected from a multitude of open-source\ndatasets, subjected to semantic deduplication and meticulous cleaning to\neliminate test set contamination. All responses within the dataset are\ndistilled from reasoning models (predominantly DeepSeek-R1) and have undergone\nrigorous verification procedures. Mathematical problems are validated by\nchecking against reference answers, code problems are verified using test\ncases, and other tasks are evaluated with the aid of a reward model. The\nAM-Distill-Qwen-32B model, which was trained through only simple Supervised\nFine-Tuning (SFT) using this batch of data, outperformed the\nDeepSeek-R1-Distill-Qwen-32B model on four benchmarks: AIME2024, MATH-500,\nGPQA-Diamond, and LiveCodeBench. Additionally, the AM-Distill-Qwen-72B model\nsurpassed the DeepSeek-R1-Distill-Llama-70B model on all benchmarks as well. We\nare releasing these 1.4 million problems and their corresponding responses to\nthe research community with the objective of fostering the development of\npowerful reasoning-oriented Large Language Models (LLMs). The dataset was\npublished in\n\\href{https:\/\/huggingface.co\/datasets\/a-m-team\/AM-DeepSeek-R1-Distilled-1.4M}{https:\/\/huggingface.co\/datasets\/a-m-team\/AM-DeepSeek-R1-Distilled-1.4M}.\n","date":"2025-03-25"}
{"id":"2503.19634","title":"Burst Image Super-Resolution with Mamba","abstract":"  Burst image super-resolution (BISR) aims to enhance the resolution of a\nkeyframe by leveraging information from multiple low-resolution images captured\nin quick succession. In the deep learning era, BISR methods have evolved from\nfully convolutional networks to transformer-based architectures, which, despite\ntheir effectiveness, suffer from the quadratic complexity of self-attention. We\nsee Mamba as the next natural step in the evolution of this field, offering a\ncomparable global receptive field and selective information routing with only\nlinear time complexity. In this work, we introduce BurstMamba, a Mamba-based\narchitecture for BISR. Our approach decouples the task into two specialized\nbranches: a spatial module for keyframe super-resolution and a temporal module\nfor subpixel prior extraction, striking a balance between computational\nefficiency and burst information integration. To further enhance burst\nprocessing with Mamba, we propose two novel strategies: (i) optical flow-based\nserialization, which aligns burst sequences only during state updates to\npreserve subpixel details, and (ii) a wavelet-based reparameterization of the\nstate-space update rules, prioritizing high-frequency features for improved\nburst-to-keyframe information passing. Our framework achieves SOTA performance\non public benchmarks of SyntheticSR, RealBSR-RGB, and RealBSR-RAW.\n","date":"2025-03-25"}
{"id":"2503.19637","title":"Kernel Learning Assisted Synthesis Condition Exploration for Ternary\n  Spinel","abstract":"  Machine learning and high-throughput experimentation have greatly accelerated\nthe discovery of mixed metal oxide catalysts by leveraging their compositional\nflexibility. However, the lack of established synthesis routes for solid-state\nmaterials remains a significant challenge in inorganic chemistry. An\ninterpretable machine learning model is therefore essential, as it provides\ninsights into the key factors governing phase formation. Here, we focus on the\nformation of single-phase Fe$_2$(ZnCo)O$_4$, synthesized via a high-throughput\nco-precipitation method. We combined a kernel classification model with a novel\napplication of global SHAP analysis to pinpoint the experimental features most\ncritical to single phase synthesizability by interpreting the contributions of\neach feature. Global SHAP analysis reveals that precursor and precipitating\nagent contributions to single-phase spinel formation align closely with\nestablished crystal growth theories. These results not only underscore the\nimportance of interpretable machine learning in refining synthesis protocols\nbut also establish a framework for data-informed experimental design in\ninorganic synthesis.\n","date":"2025-03-25"}
{"id":"2503.19640","title":"An Efficient Data Reuse with Tile-Based Adaptive Stationary for\n  Transformer Accelerators","abstract":"  Transformer-based models have become the \\textit{de facto} backbone across\nmany fields, such as computer vision and natural language processing. However,\nas these models scale in size, external memory access (EMA) for weight and\nactivations becomes a critical bottleneck due to its significantly higher\nenergy consumption compared to internal computations. While most prior work has\nfocused on optimizing the self-attention mechanism, little attention has been\ngiven to optimizing data transfer during linear projections, where EMA costs\nare equally important. In this paper, we propose the Tile-based Adaptive\nStationary (TAS) scheme that selects the input or weight stationary in a tile\ngranularity, based on the input sequence length. Our experimental results\ndemonstrate that TAS can significantly reduce EMA by more than 97\\% compared to\ntraditional stationary schemes, while being compatible with various attention\noptimization techniques and hardware accelerators.\n","date":"2025-03-25"}
{"id":"2503.19642","title":"Exploring Cultural Nuances in Emotion Perception Across 15 African\n  Languages","abstract":"  Understanding how emotions are expressed across languages is vital for\nbuilding culturally-aware and inclusive NLP systems. However, emotion\nexpression in African languages is understudied, limiting the development of\neffective emotion detection tools in these languages. In this work, we present\na cross-linguistic analysis of emotion expression in 15 African languages. We\nexamine four key dimensions of emotion representation: text length, sentiment\npolarity, emotion co-occurrence, and intensity variations. Our findings reveal\ndiverse language-specific patterns in emotional expression -- with Somali texts\ntypically longer, while others like IsiZulu and Algerian Arabic show more\nconcise emotional expression. We observe a higher prevalence of negative\nsentiment in several Nigerian languages compared to lower negativity in\nlanguages like IsiXhosa. Further, emotion co-occurrence analysis demonstrates\nstrong cross-linguistic associations between specific emotion pairs\n(anger-disgust, sadness-fear), suggesting universal psychological connections.\nIntensity distributions show multimodal patterns with significant variations\nbetween language families; Bantu languages display similar yet distinct\nprofiles, while Afroasiatic languages and Nigerian Pidgin demonstrate wider\nintensity ranges. These findings highlight the need for language-specific\napproaches to emotion detection while identifying opportunities for transfer\nlearning across related languages.\n","date":"2025-03-25"}
{"id":"2503.19647","title":"Show or Tell? Effectively prompting Vision-Language Models for semantic\n  segmentation","abstract":"  Large Vision-Language Models (VLMs) are increasingly being regarded as\nfoundation models that can be instructed to solve diverse tasks by prompting,\nwithout task-specific training. We examine the seemingly obvious question: how\nto effectively prompt VLMs for semantic segmentation. To that end, we\nsystematically evaluate the segmentation performance of several recent models\nguided by either text or visual prompts on the out-of-distribution MESS dataset\ncollection. We introduce a scalable prompting scheme, few-shot prompted\nsemantic segmentation, inspired by open-vocabulary segmentation and few-shot\nlearning. It turns out that VLMs lag far behind specialist models trained for a\nspecific segmentation task, by about 30% on average on the\nIntersection-over-Union metric. Moreover, we find that text prompts and visual\nprompts are complementary: each one of the two modes fails on many examples\nthat the other one can solve. Our analysis suggests that being able to\nanticipate the most effective prompt modality can lead to a 11% improvement in\nperformance. Motivated by our findings, we propose PromptMatcher, a remarkably\nsimple training-free baseline that combines both text and visual prompts,\nachieving state-of-the-art results outperforming the best text-prompted VLM by\n2.5%, and the top visual-prompted VLM by 3.5% on few-shot prompted semantic\nsegmentation.\n","date":"2025-03-25"}
{"id":"2503.19649","title":"Recover from Horcrux: A Spectrogram Augmentation Method for Cardiac\n  Feature Monitoring from Radar Signal Components","abstract":"  Radar-based wellness monitoring is becoming an effective measurement to\nprovide accurate vital signs in a contactless manner, but data scarcity retards\nthe related research on deep-learning-based methods. Data augmentation is\ncommonly used to enrich the dataset by modifying the existing data, but most\naugmentation techniques can only couple with classification tasks. To enable\nthe augmentation for regression tasks, this research proposes a spectrogram\naugmentation method, Horcrux, for radar-based cardiac feature monitoring (e.g.,\nheartbeat detection, electrocardiogram reconstruction) with both classification\nand regression tasks involved. The proposed method is designed to increase the\ndiversity of input samples while the augmented spectrogram is still faithful to\nthe original ground truth vital sign. In addition, Horcrux proposes to inject\nzero values in specific areas to enhance the awareness of the deep learning\nmodel on subtle cardiac features, improving the performance for the limited\ndataset. Experimental result shows that Horcrux achieves an overall improvement\nof 16.20% in cardiac monitoring and has the potential to be extended to other\nspectrogram-based tasks. The code will be released upon publication.\n","date":"2025-03-25"}
{"id":"2503.19650","title":"HausaNLP at SemEval-2025 Task 3: Towards a Fine-Grained Model-Aware\n  Hallucination Detection","abstract":"  This paper presents our findings of the Multilingual Shared Task on\nHallucinations and Related Observable Overgeneration Mistakes, MU-SHROOM, which\nfocuses on identifying hallucinations and related overgeneration errors in\nlarge language models (LLMs). The shared task involves detecting specific text\nspans that constitute hallucinations in the outputs generated by LLMs in 14\nlanguages. To address this task, we aim to provide a nuanced, model-aware\nunderstanding of hallucination occurrences and severity in English. We used\nnatural language inference and fine-tuned a ModernBERT model using a synthetic\ndataset of 400 samples, achieving an Intersection over Union (IoU) score of\n0.032 and a correlation score of 0.422. These results indicate a moderately\npositive correlation between the model's confidence scores and the actual\npresence of hallucinations. The IoU score indicates that our model has a\nrelatively low overlap between the predicted hallucination span and the truth\nannotation. The performance is unsurprising, given the intricate nature of\nhallucination detection. Hallucinations often manifest subtly, relying on\ncontext, making pinpointing their exact boundaries formidable.\n","date":"2025-03-25"}
{"id":"2503.19651","title":"Enhancing Graphical Lasso: A Robust Scheme for Non-Stationary Mean Data","abstract":"  This work addresses the problem of graph learning from data following a\nGaussian Graphical Model (GGM) with a time-varying mean. Graphical Lasso (GL),\nthe standard method for estimating sparse precision matrices, assumes that the\nobserved data follows a zero-mean Gaussian distribution. However, this\nassumption is often violated in real-world scenarios where the mean evolves\nover time due to external influences, trends, or regime shifts. When the mean\nis not properly accounted for, applying GL directly can lead to estimating a\nbiased precision matrix, hence hindering the graph learning task. To overcome\nthis limitation, we propose Graphical Lasso with Adaptive Targeted Adaptive\nImportance Sampling (GL-ATAIS), an iterative method that jointly estimates the\ntime-varying mean and the precision matrix. Our approach integrates Bayesian\ninference with frequentist estimation, leveraging importance sampling to obtain\nan estimate of the mean while using a regularized maximum likelihood estimator\nto infer the precision matrix. By iteratively refining both estimates, GL-ATAIS\nmitigates the bias introduced by time-varying means, leading to more accurate\ngraph recovery. Our numerical evaluation demonstrates the impact of properly\naccounting for time-dependent means and highlights the advantages of GL-ATAIS\nover standard GL in recovering the true graph structure.\n","date":"2025-03-25"}
{"id":"2503.19653","title":"OpenSDI: Spotting Diffusion-Generated Images in the Open World","abstract":"  This paper identifies OpenSDI, a challenge for spotting diffusion-generated\nimages in open-world settings. In response to this challenge, we define a new\nbenchmark, the OpenSDI dataset (OpenSDID), which stands out from existing\ndatasets due to its diverse use of large vision-language models that simulate\nopen-world diffusion-based manipulations. Another outstanding feature of\nOpenSDID is its inclusion of both detection and localization tasks for images\nmanipulated globally and locally by diffusion models. To address the OpenSDI\nchallenge, we propose a Synergizing Pretrained Models (SPM) scheme to build up\na mixture of foundation models. This approach exploits a collaboration\nmechanism with multiple pretrained foundation models to enhance generalization\nin the OpenSDI context, moving beyond traditional training by synergizing\nmultiple pretrained models through prompting and attending strategies. Building\non this scheme, we introduce MaskCLIP, an SPM-based model that aligns\nContrastive Language-Image Pre-Training (CLIP) with Masked Autoencoder (MAE).\nExtensive evaluations on OpenSDID show that MaskCLIP significantly outperforms\ncurrent state-of-the-art methods for the OpenSDI challenge, achieving\nremarkable relative improvements of 14.23% in IoU (14.11% in F1) and 2.05% in\naccuracy (2.38% in F1) compared to the second-best model in localization and\ndetection tasks, respectively. Our dataset and code are available at\nhttps:\/\/github.com\/iamwangyabin\/OpenSDI.\n","date":"2025-03-25"}
{"id":"2503.19654","title":"RGB-Th-Bench: A Dense benchmark for Visual-Thermal Understanding of\n  Vision Language Models","abstract":"  We introduce RGB-Th-Bench, the first benchmark designed to evaluate the\nability of Vision-Language Models (VLMs) to comprehend RGB-Thermal image pairs.\nWhile VLMs have demonstrated remarkable progress in visual reasoning and\nmultimodal understanding, their evaluation has been predominantly limited to\nRGB-based benchmarks, leaving a critical gap in assessing their capabilities in\ninfrared vision tasks. Existing visible-infrared datasets are either\ntask-specific or lack high-quality annotations necessary for rigorous model\nevaluation. To address these limitations, RGB-Th-Bench provides a comprehensive\nevaluation framework covering 14 distinct skill dimensions, with a total of\n1,600+ expert-annotated Yes\/No questions. The benchmark employs two accuracy\nmetrics: a standard question-level accuracy and a stricter skill-level\naccuracy, which evaluates model robustness across multiple questions within\neach skill dimension. This design ensures a thorough assessment of model\nperformance, including resilience to adversarial and hallucinated responses. We\nconduct extensive evaluations on 19 state-of-the-art VLMs, revealing\nsignificant performance gaps in RGB-Thermal understanding. Our results show\nthat even the strongest models struggle with thermal image comprehension, with\nperformance heavily constrained by their RGB-based capabilities. Additionally,\nthe lack of large-scale application-specific and expert-annotated\nthermal-caption-pair datasets in pre-training is an important reason of the\nobserved performance gap. RGB-Th-Bench highlights the urgent need for further\nadvancements in multimodal learning to bridge the gap between visible and\nthermal image understanding. The dataset is available through this link, and\nthe evaluation code will also be made publicly available.\n","date":"2025-03-25"}
{"id":"2503.19656","title":"Towards Reliable Time Series Forecasting under Future Uncertainty:\n  Ambiguity and Novelty Rejection Mechanisms","abstract":"  In real-world time series forecasting, uncertainty and lack of reliable\nevaluation pose significant challenges. Notably, forecasting errors often arise\nfrom underfitting in-distribution data and failing to handle\nout-of-distribution inputs. To enhance model reliability, we introduce a dual\nrejection mechanism combining ambiguity and novelty rejection. Ambiguity\nrejection, using prediction error variance, allows the model to abstain under\nlow confidence, assessed through historical error variance analysis without\nfuture ground truth. Novelty rejection, employing Variational Autoencoders and\nMahalanobis distance, detects deviations from training data. This dual approach\nimproves forecasting reliability in dynamic environments by reducing errors and\nadapting to data changes, advancing reliability in complex scenarios.\n","date":"2025-03-25"}
{"id":"2503.19658","title":"BiblioPage: A Dataset of Scanned Title Pages for Bibliographic Metadata\n  Extraction","abstract":"  Manual digitization of bibliographic metadata is time consuming and labor\nintensive, especially for historical and real-world archives with highly\nvariable formatting across documents. Despite advances in machine learning, the\nabsence of dedicated datasets for metadata extraction hinders automation. To\naddress this gap, we introduce BiblioPage, a dataset of scanned title pages\nannotated with structured bibliographic metadata. The dataset consists of\napproximately 2,000 monograph title pages collected from 14 Czech libraries,\nspanning a wide range of publication periods, typographic styles, and layout\nstructures. Each title page is annotated with 16 bibliographic attributes,\nincluding title, contributors, and publication metadata, along with precise\npositional information in the form of bounding boxes. To extract structured\ninformation from this dataset, we valuated object detection models such as YOLO\nand DETR combined with transformer-based OCR, achieving a maximum mAP of 52 and\nan F1 score of 59. Additionally, we assess the performance of various visual\nlarge language models, including LlamA 3.2-Vision and GPT-4o, with the best\nmodel reaching an F1 score of 67. BiblioPage serves as a real-world benchmark\nfor bibliographic metadata extraction, contributing to document understanding,\ndocument question answering, and document information extraction. Dataset and\nevaluation scripts are availible at: https:\/\/github.com\/DCGM\/biblio-dataset\n","date":"2025-03-25"}
{"id":"2503.19661","title":"CoSimGen: Controllable Diffusion Model for Simultaneous Image and Mask\n  Generation","abstract":"  The acquisition of annotated datasets with paired images and segmentation\nmasks is a critical challenge in domains such as medical imaging, remote\nsensing, and computer vision. Manual annotation demands significant resources,\nfaces ethical constraints, and depends heavily on domain expertise. Existing\ngenerative models often target single-modality outputs, either images or\nsegmentation masks, failing to address the need for high-quality, simultaneous\nimage-mask generation. Additionally, these models frequently lack adaptable\nconditioning mechanisms, restricting control over the generated outputs and\nlimiting their applicability for dataset augmentation and rare scenario\nsimulation. We propose CoSimGen, a diffusion-based framework for controllable\nsimultaneous image and mask generation. Conditioning is intuitively achieved\nthrough (1) text prompts grounded in class semantics, (2) spatial embedding of\ncontext prompts to provide spatial coherence, and (3) spectral embedding of\ntimestep information to model noise levels during diffusion. To enhance\ncontrollability and training efficiency, the framework incorporates contrastive\ntriplet loss between text and class embeddings, alongside diffusion and\nadversarial losses. Initial low-resolution outputs 128 x 128 are super-resolved\nto 512 x 512, producing high-fidelity images and masks with strict adherence to\nconditions. We evaluate CoSimGen on metrics such as FID, KID, LPIPS, Class FID,\nPositive predicted value for image fidelity and semantic alignment of generated\nsamples over 4 diverse datasets. CoSimGen achieves state-of-the-art performance\nacross all datasets, achieving the lowest KID of 0.11 and LPIPS of 0.53 across\ndatasets.\n","date":"2025-03-25"}
{"id":"2503.19666","title":"Towards Efficient Training of Graph Neural Networks: A Multiscale\n  Approach","abstract":"  Graph Neural Networks (GNNs) have emerged as a powerful tool for learning and\ninferring from graph-structured data, and are widely used in a variety of\napplications, often considering large amounts of data and large graphs.\nHowever, training on such data requires large memory and extensive\ncomputations. In this paper, we introduce a novel framework for efficient\nmultiscale training of GNNs, designed to integrate information across\nmultiscale representations of a graph. Our approach leverages a hierarchical\ngraph representation, taking advantage of coarse graph scales in the training\nprocess, where each coarse scale graph has fewer nodes and edges. Based on this\napproach, we propose a suite of GNN training methods: such as coarse-to-fine,\nsub-to-full, and multiscale gradient computation. We demonstrate the\neffectiveness of our methods on various datasets and learning tasks.\n","date":"2025-03-25"}
{"id":"2503.19668","title":"A multitask transformer to sign language translation using motion\n  gesture primitives","abstract":"  The absence of effective communication the deaf population represents the\nmain social gap in this community. Furthermore, the sign language, main deaf\ncommunication tool, is unlettered, i.e., there is no formal written\nrepresentation. In consequence, main challenge today is the automatic\ntranslation among spatiotemporal sign representation and natural text language.\nRecent approaches are based on encoder-decoder architectures, where the most\nrelevant strategies integrate attention modules to enhance non-linear\ncorrespondences, besides, many of these approximations require complex training\nand architectural schemes to achieve reasonable predictions, because of the\nabsence of intermediate text projections. However, they are still limited by\nthe redundant background information of the video sequences. This work\nintroduces a multitask transformer architecture that includes a gloss learning\nrepresentation to achieve a more suitable translation. The proposed approach\nalso includes a dense motion representation that enhances gestures and includes\nkinematic information, a key component in sign language. From this\nrepresentation it is possible to avoid background information and exploit the\ngeometry of the signs, in addition, it includes spatiotemporal representations\nthat facilitate the alignment between gestures and glosses as an intermediate\ntextual representation. The proposed approach outperforms the state-of-the-art\nevaluated on the CoL-SLTD dataset, achieving a BLEU-4 of 72,64% in split 1, and\na BLEU-4 of 14,64% in split 2. Additionally, the strategy was validated on the\nRWTH-PHOENIX-Weather 2014 T dataset, achieving a competitive BLEU-4 of 11,58%.\n","date":"2025-03-25"}
{"id":"2503.19670","title":"fine-CLIP: Enhancing Zero-Shot Fine-Grained Surgical Action Recognition\n  with Vision-Language Models","abstract":"  While vision-language models like CLIP have advanced zero-shot surgical phase\nrecognition, they struggle with fine-grained surgical activities, especially\naction triplets. This limitation arises because current CLIP formulations rely\non global image features, which overlook the fine-grained semantics and\ncontextual details crucial for complex tasks like zero-shot triplet\nrecognition. Furthermore, these models do not explore the hierarchical\nstructure inherent in triplets, reducing their ability to generalize to novel\ntriplets. To address these challenges, we propose fine-CLIP, which learns\nobject-centric features and leverages the hierarchy in triplet formulation. Our\napproach integrates three components: hierarchical prompt modeling to capture\nshared semantics, LoRA-based vision backbone adaptation for enhanced feature\nextraction, and a graph-based condensation strategy that groups similar patch\nfeatures into meaningful object clusters. Since triplet classification is a\nchallenging task, we introduce an alternative yet meaningful base-to-novel\ngeneralization benchmark with two settings on the CholecT50 dataset:\nUnseen-Target, assessing adaptability to triplets with novel anatomical\nstructures, and Unseen-Instrument-Verb, where models need to generalize to\nnovel instrument-verb interactions. fine-CLIP shows significant improvements in\nF1 and mAP, enhancing zero-shot recognition of novel surgical triplets.\n","date":"2025-03-25"}
{"id":"2503.19673","title":"MultimodalStudio: A Heterogeneous Sensor Dataset and Framework for\n  Neural Rendering across Multiple Imaging Modalities","abstract":"  Neural Radiance Fields (NeRF) have shown impressive performances in the\nrendering of 3D scenes from arbitrary viewpoints. While RGB images are widely\npreferred for training volume rendering models, the interest in other radiance\nmodalities is also growing. However, the capability of the underlying implicit\nneural models to learn and transfer information across heterogeneous imaging\nmodalities has seldom been explored, mostly due to the limited training data\navailability. For this purpose, we present MultimodalStudio (MMS): it\nencompasses MMS-DATA and MMS-FW. MMS-DATA is a multimodal multi-view dataset\ncontaining 32 scenes acquired with 5 different imaging modalities: RGB,\nmonochrome, near-infrared, polarization and multispectral. MMS-FW is a novel\nmodular multimodal NeRF framework designed to handle multimodal raw data and\nable to support an arbitrary number of multi-channel devices. Through extensive\nexperiments, we demonstrate that MMS-FW trained on MMS-DATA can transfer\ninformation between different imaging modalities and produce higher quality\nrenderings than using single modalities alone. We publicly release the dataset\nand the framework, to promote the research on multimodal volume rendering and\nbeyond.\n","date":"2025-03-25"}
{"id":"2503.19677","title":"Deep Learning for Speech Emotion Recognition: A CNN Approach Utilizing\n  Mel Spectrograms","abstract":"  This paper explores the application of Convolutional Neural Networks CNNs for\nclassifying emotions in speech through Mel Spectrogram representations of audio\nfiles. Traditional methods such as Gaussian Mixture Models and Hidden Markov\nModels have proven insufficient for practical deployment, prompting a shift\ntowards deep learning techniques. By transforming audio data into a visual\nformat, the CNN model autonomously learns to identify intricate patterns,\nenhancing classification accuracy. The developed model is integrated into a\nuser-friendly graphical interface, facilitating realtime predictions and\npotential applications in educational environments. The study aims to advance\nthe understanding of deep learning in speech emotion recognition, assess the\nmodels feasibility, and contribute to the integration of technology in learning\ncontexts\n","date":"2025-03-25"}
{"id":"2503.19683","title":"Unlocking the Hidden Potential of CLIP in Generalizable Deepfake\n  Detection","abstract":"  This paper tackles the challenge of detecting partially manipulated facial\ndeepfakes, which involve subtle alterations to specific facial features while\nretaining the overall context, posing a greater detection difficulty than fully\nsynthetic faces. We leverage the Contrastive Language-Image Pre-training (CLIP)\nmodel, specifically its ViT-L\/14 visual encoder, to develop a generalizable\ndetection method that performs robustly across diverse datasets and unknown\nforgery techniques with minimal modifications to the original model. The\nproposed approach utilizes parameter-efficient fine-tuning (PEFT) techniques,\nsuch as LN-tuning, to adjust a small subset of the model's parameters,\npreserving CLIP's pre-trained knowledge and reducing overfitting. A tailored\npreprocessing pipeline optimizes the method for facial images, while\nregularization strategies, including L2 normalization and metric learning on a\nhyperspherical manifold, enhance generalization. Trained on the FaceForensics++\ndataset and evaluated in a cross-dataset fashion on Celeb-DF-v2, DFDC, FFIW,\nand others, the proposed method achieves competitive detection accuracy\ncomparable to or outperforming much more complex state-of-the-art techniques.\nThis work highlights the efficacy of CLIP's visual encoder in facial deepfake\ndetection and establishes a simple, powerful baseline for future research,\nadvancing the field of generalizable deepfake detection. The code is available\nat: https:\/\/github.com\/yermandy\/deepfake-detection\n","date":"2025-03-25"}
{"id":"2503.19693","title":"AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through\n  Lightweight Vocabulary Adaptation","abstract":"  Large Language Models (LLMs) have shown impressive versatility as general\npurpose models. However, their broad applicability comes at a high-cost\ncomputational overhead, particularly in auto-regressive decoding where each\nstep requires a forward pass. In domain-specific settings, general-purpose\ncapabilities are unnecessary and can be exchanged for efficiency. In this work,\nwe take a novel perspective on domain adaptation, reducing latency and\ncomputational costs by adapting the vocabulary to focused domains of interest.\nWe introduce AdaptiVocab, an end-to-end approach for vocabulary adaptation,\ndesigned to enhance LLM efficiency in low-resource domains. AdaptiVocab can be\napplied to any tokenizer and architecture, modifying the vocabulary by\nreplacing tokens with domain-specific n-gram-based tokens, thereby reducing the\nnumber of tokens required for both input processing and output generation.\nAdaptiVocab initializes new n-token embeddings using an exponentially weighted\ncombination of existing embeddings and employs a lightweight fine-tuning phase\nthat can be efficiently performed on a single GPU. We evaluate two 7B LLMs\nacross three niche domains, assessing efficiency, generation quality, and\nend-task performance. Our results show that AdaptiVocab reduces token usage by\nover 25% without compromising performance\n","date":"2025-03-25"}
{"id":"2503.19699","title":"Optimal Path Planning and Cost Minimization for a Drone Delivery System\n  Via Model Predictive Control","abstract":"  In this study, we formulate the drone delivery problem as a control problem\nand solve it using Model Predictive Control. Two experiments are performed: The\nfirst is on a less challenging grid world environment with lower\ndimensionality, and the second is with a higher dimensionality and added\ncomplexity. The MPC method was benchmarked against three popular Multi-Agent\nReinforcement Learning (MARL): Independent $Q$-Learning (IQL), Joint Action\nLearners (JAL), and Value-Decomposition Networks (VDN). It was shown that the\nMPC method solved the problem quicker and required fewer optimal numbers of\ndrones to achieve a minimized cost and navigate the optimal path.\n","date":"2025-03-25"}
{"id":"2503.19700","title":"Optimization of MedSAM model based on bounding box adaptive perturbation\n  algorithm","abstract":"  The MedSAM model, built upon the SAM framework, enhances medical image\nsegmentation through generalizable training but still exhibits notable\nlimitations. First, constraints in the perturbation window settings during\ntraining can cause MedSAM to incorrectly segment small tissues or organs\ntogether with adjacent structures, leading to segmentation errors. Second, when\ndealing with medical image targets characterized by irregular shapes and\ncomplex structures, segmentation often relies on narrowing the bounding box to\nrefine segmentation intent. However, MedSAM's performance under reduced\nbounding box prompts remains suboptimal. To address these challenges, this\nstudy proposes a bounding box adaptive perturbation algorithm to optimize the\ntraining process. The proposed approach aims to reduce segmentation errors for\nsmall targets and enhance the model's accuracy when processing reduced bounding\nbox prompts, ultimately improving the robustness and reliability of the MedSAM\nmodel for complex medical imaging tasks.\n","date":"2025-03-25"}
{"id":"2503.19702","title":"HausaNLP at SemEval-2025 Task 2: Entity-Aware Fine-tuning vs. Prompt\n  Engineering in Entity-Aware Machine Translation","abstract":"  This paper presents our findings for SemEval 2025 Task 2, a shared task on\nentity-aware machine translation (EA-MT). The goal of this task is to develop\ntranslation models that can accurately translate English sentences into target\nlanguages, with a particular focus on handling named entities, which often pose\nchallenges for MT systems. The task covers 10 target languages with English as\nthe source. In this paper, we describe the different systems we employed,\ndetail our results, and discuss insights gained from our experiments.\n","date":"2025-03-25"}
{"id":"2503.19703","title":"High-Quality Spatial Reconstruction and Orthoimage Generation Using\n  Efficient 2D Gaussian Splatting","abstract":"  Highly accurate geometric precision and dense image features characterize\nTrue Digital Orthophoto Maps (TDOMs), which are in great demand for\napplications such as urban planning, infrastructure management, and\nenvironmental monitoring. Traditional TDOM generation methods need\nsophisticated processes, such as Digital Surface Models (DSM) and occlusion\ndetection, which are computationally expensive and prone to errors. This work\npresents an alternative technique rooted in 2D Gaussian Splatting (2DGS), free\nof explicit DSM and occlusion detection. With depth map generation, spatial\ninformation for every pixel within the TDOM is retrieved and can reconstruct\nthe scene with high precision. Divide-and-conquer strategy achieves excellent\nGS training and rendering with high-resolution TDOMs at a lower resource cost,\nwhich preserves higher quality of rendering on complex terrain and thin\nstructure without a decrease in efficiency. Experimental results demonstrate\nthe efficiency of large-scale scene reconstruction and high-precision terrain\nmodeling. This approach provides accurate spatial data, which assists users in\nbetter planning and decision-making based on maps.\n","date":"2025-03-25"}
{"id":"2503.19706","title":"Bootstrap Your Own Views: Masked Ego-Exo Modeling for Fine-grained\n  View-invariant Video Representations","abstract":"  View-invariant representation learning from egocentric (first-person, ego)\nand exocentric (third-person, exo) videos is a promising approach toward\ngeneralizing video understanding systems across multiple viewpoints. However,\nthis area has been underexplored due to the substantial differences in\nperspective, motion patterns, and context between ego and exo views. In this\npaper, we propose a novel masked ego-exo modeling that promotes both causal\ntemporal dynamics and cross-view alignment, called Bootstrap Your Own Views\n(BYOV), for fine-grained view-invariant video representation learning from\nunpaired ego-exo videos. We highlight the importance of capturing the\ncompositional nature of human actions as a basis for robust cross-view\nunderstanding. Specifically, self-view masking and cross-view masking\npredictions are designed to learn view-invariant and powerful representations\nconcurrently. Experimental results demonstrate that our BYOV significantly\nsurpasses existing approaches with notable gains across all metrics in four\ndownstream ego-exo video tasks. The code is available at\nhttps:\/\/github.com\/park-jungin\/byov.\n","date":"2025-03-25"}
{"id":"2503.19707","title":"Mind the Gap: Benchmarking Spatial Reasoning in Vision-Language Models","abstract":"  Vision-Language Models (VLMs) have recently emerged as powerful tools,\nexcelling in tasks that integrate visual and textual comprehension, such as\nimage captioning, visual question answering, and image-text retrieval. However,\nexisting benchmarks for VLMs include spatial components, which often fail to\nisolate spatial reasoning from related tasks such as object detection or\nsemantic comprehension. In this paper, we address these deficiencies with a\nmulti-faceted approach towards understanding spatial reasoning. Informed by the\ndiverse and multi-dimensional nature of human spatial reasoning abilities, we\npresent a detailed analysis that first delineates the core elements of spatial\nreasoning: spatial relations, orientation and navigation, mental rotation, and\nspatial visualization, and then assesses the performance of these models in\nboth synthetic and real-world images, bridging controlled and naturalistic\ncontexts. We analyze 13 state-of-the-art Vision-Language Models, uncovering\npivotal insights into their spatial reasoning performance. Our results reveal\nprofound shortcomings in current VLMs, with average accuracy across the 13\nmodels approximating random chance, highlighting spatial reasoning as a\npersistent obstacle. This work not only exposes the pressing need to advance\nspatial reasoning within VLMs but also establishes a solid platform for future\nexploration. Code available on GitHub (https:\/\/github.com\/stogiannidis\/srbench)\nand dataset available on HuggingFace\n(https:\/\/huggingface.co\/datasets\/stogiannidis\/srbench).\n","date":"2025-03-25"}
{"id":"2503.19708","title":"Data-efficient rapid prediction of urban airflow and temperature fields\n  for complex building geometries","abstract":"  Accurately predicting urban microclimate, including wind speed and\ntemperature, based solely on building geometry requires capturing complex\ninteractions between buildings and airflow, particularly long-range wake\neffects influenced by directional geometry. Traditional methods relying on\ncomputational fluid dynamics (CFD) are prohibitively expensive for large-scale\nsimulations, while data-driven approaches struggle with limited training data\nand the need to model both local and far-field dependencies. In response, we\npropose a novel framework that leverages a multi-directional distance feature\n(MDDF) combined with localized training to achieve effective wind field\npredictions with minimal CFD data. By reducing the problem's dimensionality,\nlocalized training effectively increases the number of training samples, while\nMDDF encodes the surrounding geometric information to accurately model wake\ndynamics and flow redirection. Trained on only 24 CFD simulations, our\nlocalized Fourier neural operator (Local-FNO) model generates full 3D wind\nvelocity and temperature predictions in under one minute, yielding a 500-fold\nspeedup over conventional CFD methods. With mean absolute errors of 0.3 m\/s for\nwind speed and 0.3 $^{\\circ}$C for temperature on unseen urban configurations,\nour method demonstrates strong generalization capabilities and significant\npotential for practical urban applications.\n","date":"2025-03-25"}
{"id":"2503.19711","title":"Writing as a testbed for open ended agents","abstract":"  Open-ended tasks are particularly challenging for LLMs due to the vast\nsolution space, demanding both expansive exploration and adaptable strategies,\nespecially when success lacks a clear, objective definition. Writing, with its\nvast solution space and subjective evaluation criteria, provides a compelling\ntestbed for studying such problems. In this paper, we investigate the potential\nof LLMs to act as collaborative co-writers, capable of suggesting and\nimplementing text improvements autonomously. We analyse three prominent LLMs -\nGemini 1.5 Pro, Claude 3.5 Sonnet, and GPT-4o - focusing on how their action\ndiversity, human alignment, and iterative improvement capabilities impact\noverall performance. This work establishes a framework for benchmarking\nautonomous writing agents and, more broadly, highlights fundamental challenges\nand potential solutions for building systems capable of excelling in diverse\nopen-ended domains.\n","date":"2025-03-25"}
{"id":"2503.19712","title":"Decoupled Dynamics Framework with Neural Fields for 3D Spatio-temporal\n  Prediction of Vehicle Collisions","abstract":"  This study proposes a neural framework that predicts 3D vehicle collision\ndynamics by independently modeling global rigid-body motion and local\nstructural deformation. Unlike approaches directly predicting absolute\ndisplacement, this method explicitly separates the vehicle's overall\ntranslation and rotation from its structural deformation. Two specialized\nnetworks form the core of the framework: a quaternion-based Rigid Net for rigid\nmotion and a coordinate-based Deformation Net for local deformation. By\nindependently handling fundamentally distinct physical phenomena, the proposed\narchitecture achieves accurate predictions without requiring separate\nsupervision for each component. The model, trained on only 10% of available\nsimulation data, significantly outperforms baseline models, including single\nmulti-layer perceptron (MLP) and deep operator networks (DeepONet), with\nprediction errors reduced by up to 83%. Extensive validation demonstrates\nstrong generalization to collision conditions outside the training range,\naccurately predicting responses even under severe impacts involving extreme\nvelocities and large impact angles. Furthermore, the framework successfully\nreconstructs high-resolution deformation details from low-resolution inputs\nwithout increased computational effort. Consequently, the proposed approach\nprovides an effective, computationally efficient method for rapid and reliable\nassessment of vehicle safety across complex collision scenarios, substantially\nreducing the required simulation data and time while preserving prediction\nfidelity.\n","date":"2025-03-25"}
{"id":"2503.19713","title":"Semi-SD: Semi-Supervised Metric Depth Estimation via Surrounding Cameras\n  for Autonomous Driving","abstract":"  In this paper, we introduce Semi-SD, a novel metric depth estimation\nframework tailored for surrounding cameras equipment in autonomous driving. In\nthis work, the input data consists of adjacent surrounding frames and camera\nparameters. We propose a unified spatial-temporal-semantic fusion module to\nconstruct the visual fused features. Cross-attention components for surrounding\ncameras and adjacent frames are utilized to focus on metric scale information\nrefinement and temporal feature matching. Building on this, we propose a pose\nestimation framework using surrounding cameras, their corresponding estimated\ndepths, and extrinsic parameters, which effectively address the scale ambiguity\nin multi-camera setups. Moreover, semantic world model and monocular depth\nestimation world model are integrated to supervised the depth estimation, which\nimprove the quality of depth estimation. We evaluate our algorithm on DDAD and\nnuScenes datasets, and the results demonstrate that our method achieves\nstate-of-the-art performance in terms of surrounding camera based depth\nestimation quality. The source code will be available on\nhttps:\/\/github.com\/xieyuser\/Semi-SD.\n","date":"2025-03-25"}
{"id":"2503.19717","title":"Invertible Koopman neural operator for data-driven modeling of partial\n  differential equations","abstract":"  Koopman operator theory is a popular candidate for data-driven modeling\nbecause it provides a global linearization representation for nonlinear\ndynamical systems. However, existing Koopman operator-based methods suffer from\nshortcomings in constructing the well-behaved observable function and its\ninverse and are inefficient enough when dealing with partial differential\nequations (PDEs). To address these issues, this paper proposes the Invertible\nKoopman Neural Operator (IKNO), a novel data-driven modeling approach inspired\nby the Koopman operator theory and neural operator. IKNO leverages an\nInvertible Neural Network to parameterize observable function and its inverse\nsimultaneously under the same learnable parameters, explicitly guaranteeing the\nreconstruction relation, thus eliminating the dependency on the reconstruction\nloss, which is an essential improvement over the original Koopman Neural\nOperator (KNO). The structured linear matrix inspired by the Koopman operator\ntheory is parameterized to learn the evolution of observables' low-frequency\nmodes in the frequency space rather than directly in the observable space,\nsustaining IKNO is resolution-invariant like other neural operators. Moreover,\nwith preprocessing such as interpolation and dimension expansion, IKNO can be\nextended to operator learning tasks defined on non-Cartesian domains. We fully\nsupport the above claims based on rich numerical and real-world examples and\ndemonstrate the effectiveness of IKNO and superiority over other neural\noperators.\n","date":"2025-03-25"}
{"id":"2503.19719","title":"On What Depends the Robustness of Multi-source Models to Missing Data in\n  Earth Observation?","abstract":"  In recent years, the development of robust multi-source models has emerged in\nthe Earth Observation (EO) field. These are models that leverage data from\ndiverse sources to improve predictive accuracy when there is missing data.\nDespite these advancements, the factors influencing the varying effectiveness\nof such models remain poorly understood. In this study, we evaluate the\npredictive performance of six state-of-the-art multi-source models in\npredicting scenarios where either a single data source is missing or only a\nsingle source is available. Our analysis reveals that the efficacy of these\nmodels is intricately tied to the nature of the task, the complementarity among\ndata sources, and the model design. Surprisingly, we observe instances where\nthe removal of certain data sources leads to improved predictive performance,\nchallenging the assumption that incorporating all available data is always\nbeneficial. These findings prompt critical reflections on model complexity and\nthe necessity of all collected data sources, potentially shaping the way for\nmore streamlined approaches in EO applications.\n","date":"2025-03-25"}
{"id":"2503.19721","title":"EventMamba: Enhancing Spatio-Temporal Locality with State Space Models\n  for Event-Based Video Reconstruction","abstract":"  Leveraging its robust linear global modeling capability, Mamba has notably\nexcelled in computer vision. Despite its success, existing Mamba-based vision\nmodels have overlooked the nuances of event-driven tasks, especially in video\nreconstruction. Event-based video reconstruction (EBVR) demands spatial\ntranslation invariance and close attention to local event relationships in the\nspatio-temporal domain. Unfortunately, conventional Mamba algorithms apply\nstatic window partitions and standard reshape scanning methods, leading to\nsignificant losses in local connectivity. To overcome these limitations, we\nintroduce EventMamba--a specialized model designed for EBVR tasks. EventMamba\ninnovates by incorporating random window offset (RWO) in the spatial domain,\nmoving away from the restrictive fixed partitioning. Additionally, it features\na new consistent traversal serialization approach in the spatio-temporal\ndomain, which maintains the proximity of adjacent events both spatially and\ntemporally. These enhancements enable EventMamba to retain Mamba's robust\nmodeling capabilities while significantly preserving the spatio-temporal\nlocality of event data. Comprehensive testing on multiple datasets shows that\nEventMamba markedly enhances video reconstruction, drastically improving\ncomputation speed while delivering superior visual quality compared to\nTransformer-based methods.\n","date":"2025-03-25"}
{"id":"2503.19730","title":"CamSAM2: Segment Anything Accurately in Camouflaged Videos","abstract":"  Video camouflaged object segmentation (VCOS), aiming at segmenting\ncamouflaged objects that seamlessly blend into their environment, is a\nfundamental vision task with various real-world applications. With the release\nof SAM2, video segmentation has witnessed significant progress. However, SAM2's\ncapability of segmenting camouflaged videos is suboptimal, especially when\ngiven simple prompts such as point and box. To address the problem, we propose\nCamouflaged SAM2 (CamSAM2), which enhances SAM2's ability to handle camouflaged\nscenes without modifying SAM2's parameters. Specifically, we introduce a\ndecamouflaged token to provide the flexibility of feature adjustment for VCOS.\nTo make full use of fine-grained and high-resolution features from the current\nframe and previous frames, we propose implicit object-aware fusion (IOF) and\nexplicit object-aware fusion (EOF) modules, respectively. Object prototype\ngeneration (OPG) is introduced to abstract and memorize object prototypes with\ninformative details using high-quality features from previous frames. Extensive\nexperiments are conducted to validate the effectiveness of our approach. While\nCamSAM2 only adds negligible learnable parameters to SAM2, it substantially\noutperforms SAM2 on three VCOS datasets, especially achieving 12.2 mDice gains\nwith click prompt on MoCA-Mask and 19.6 mDice gains with mask prompt on\nSUN-SEG-Hard, with Hiera-T as the backbone. The code will be available at\nhttps:\/\/github.com\/zhoustan\/CamSAM2.\n","date":"2025-03-25"}
{"id":"2503.19731","title":"PCM : Picard Consistency Model for Fast Parallel Sampling of Diffusion\n  Models","abstract":"  Recently, diffusion models have achieved significant advances in vision,\ntext, and robotics. However, they still face slow generation speeds due to\nsequential denoising processes. To address this, a parallel sampling method\nbased on Picard iteration was introduced, effectively reducing sequential steps\nwhile ensuring exact convergence to the original output. Nonetheless, Picard\niteration does not guarantee faster convergence, which can still result in slow\ngeneration in practice. In this work, we propose a new parallelization scheme,\nthe Picard Consistency Model (PCM), which significantly reduces the number of\ngeneration steps in Picard iteration. Inspired by the consistency model, PCM is\ndirectly trained to predict the fixed-point solution, or the final output, at\nany stage of the convergence trajectory. Additionally, we introduce a new\nconcept called model switching, which addresses PCM's limitations and ensures\nexact convergence. Extensive experiments demonstrate that PCM achieves up to a\n2.71x speedup over sequential sampling and a 1.77x speedup over Picard\niteration across various tasks, including image generation and robotic control.\n","date":"2025-03-25"}
{"id":"2503.19733","title":"How to RETIRE Tabular Data in Favor of Discrete Digital Signal\n  Representation","abstract":"  The successes achieved by deep neural networks in computer vision tasks have\nled in recent years to the emergence of a new research area dubbed\nMulti-Dimensional Encoding (MDE). Methods belonging to this family aim to\ntransform tabular data into a homogeneous form of discrete digital signals\n(images) to apply convolutional networks to initially unsuitable problems.\nDespite the successive emerging works, the pool of multi-dimensional encoding\nmethods is still low, and the scope of research on existing modality encoding\ntechniques is quite limited. To contribute to this area of research, we propose\nthe Radar-based Encoding from Tabular to Image REpresentation (RETIRE), which\nallows tabular data to be represented as radar graphs, capturing the feature\ncharacteristics of each problem instance. RETIRE was compared with a pool of\nstate-of-the-art MDE algorithms as well as with XGBoost in terms of\nclassification accuracy and computational complexity. In addition, an analysis\nwas carried out regarding transferability and explainability to provide more\ninsight into both RETIRE and existing MDE techniques. The results obtained,\nsupported by statistical analysis, confirm the superiority of RETIRE over other\nestablished MDE methods.\n","date":"2025-03-25"}
{"id":"2503.19735","title":"InterSliceBoost: Identifying Tissue Layers in Three-dimensional\n  Ultrasound Images for Chronic Lower Back Pain (cLBP) Assessment","abstract":"  Available studies on chronic lower back pain (cLBP) typically focus on one or\na few specific tissues rather than conducting a comprehensive layer-by-layer\nanalysis. Since three-dimensional (3-D) images often contain hundreds of\nslices, manual annotation of these anatomical structures is both time-consuming\nand error-prone. We aim to develop and validate a novel approach called\nInterSliceBoost to enable the training of a segmentation model on a partially\nannotated dataset without compromising segmentation performance. The\narchitecture of InterSliceBoost includes two components: an inter-slice\ngenerator and a segmentation model. The generator utilizes residual block-based\nencoders to extract features from adjacent image-mask pairs (IMPs).\nDifferential features are calculated and input into a decoder to generate\ninter-slice IMPs. The segmentation model is trained on partially annotated\ndatasets (e.g., skipping 1, 2, 3, or 7 images) and the generated inter-slice\nIMPs. To validate the performance of InterSliceBoost, we utilized a dataset of\n76 B-mode ultrasound scans acquired on 29 subjects enrolled in an ongoing cLBP\nstudy. InterSliceBoost, trained on only 33% of the image slices, achieved a\nmean Dice coefficient of 80.84% across all six layers on the independent test\nset, with Dice coefficients of 73.48%, 61.11%, 81.87%, 95.74%, 83.52% and\n88.74% for segmenting dermis, superficial fat, superficial fascial membrane,\ndeep fat, deep fascial membrane, and muscle. This performance is significantly\nhigher than the conventional model trained on fully annotated images (p<0.05).\nInterSliceBoost can effectively segment the six tissue layers depicted on 3-D\nB-model ultrasound images in settings with partial annotations.\n","date":"2025-03-25"}
{"id":"2503.19736","title":"GRN+: A Simplified Generative Reinforcement Network for Tissue Layer\n  Analysis in 3D Ultrasound Images for Chronic Low-back Pain","abstract":"  3D ultrasound delivers high-resolution, real-time images of soft tissues,\nwhich is essential for pain research. However, manually distinguishing various\ntissues for quantitative analysis is labor-intensive. To streamline this\nprocess, we developed and validated GRN+, a novel multi-model framework that\nautomates layer segmentation with minimal annotated data. GRN+ combines a\nResNet-based generator and a U-Net segmentation model. Through a method called\nSegmentation-guided Enhancement (SGE), the generator produces new images and\nmatching masks under the guidance of the segmentation model, with its weights\nadjusted according to the segmentation loss gradient. To prevent gradient\nexplosion and secure stable training, a two-stage backpropagation strategy was\nimplemented: the first stage propagates the segmentation loss through both the\ngenerator and segmentation model, while the second stage concentrates on\noptimizing the segmentation model alone, thereby refining mask prediction using\nthe generated images. Tested on 69 fully annotated 3D ultrasound scans from 29\nsubjects with six manually labeled tissue layers, GRN+ outperformed all other\nsemi-supervised methods in terms of the Dice coefficient using only 5% labeled\ndata, despite not using unlabeled data for unsupervised training. Additionally,\nwhen applied to fully annotated datasets, GRN+ with SGE achieved a 2.16% higher\nDice coefficient while incurring lower computational costs compared to other\nmodels. Overall, GRN+ provides accurate tissue segmentation while reducing both\ncomputational expenses and the dependency on extensive annotations, making it\nan effective tool for 3D ultrasound analysis in cLBP patients.\n","date":"2025-03-25"}
{"id":"2503.19739","title":"FUSE: Label-Free Image-Event Joint Monocular Depth Estimation via\n  Frequency-Decoupled Alignment and Degradation-Robust Fusion","abstract":"  Image-event joint depth estimation methods leverage complementary modalities\nfor robust perception, yet face challenges in generalizability stemming from\ntwo factors: 1) limited annotated image-event-depth datasets causing\ninsufficient cross-modal supervision, and 2) inherent frequency mismatches\nbetween static images and dynamic event streams with distinct spatiotemporal\npatterns, leading to ineffective feature fusion. To address this dual\nchallenge, we propose Frequency-decoupled Unified Self-supervised Encoder\n(FUSE) with two synergistic components: The Parameter-efficient Self-supervised\nTransfer (PST) establishes cross-modal knowledge transfer through latent space\nalignment with image foundation models, effectively mitigating data scarcity by\nenabling joint encoding without depth ground truth. Complementing this, we\npropose the Frequency-Decoupled Fusion module (FreDFuse) to explicitly decouple\nhigh-frequency edge features from low-frequency structural components,\nresolving modality-specific frequency mismatches through physics-aware fusion.\nThis combined approach enables FUSE to construct a universal image-event\nencoder that only requires lightweight decoder adaptation for target datasets.\nExtensive experiments demonstrate state-of-the-art performance with 14% and\n24.9% improvements in Abs.Rel on MVSEC and DENSE datasets. The framework\nexhibits remarkable zero-shot adaptability to challenging scenarios including\nextreme lighting and motion blur, significantly advancing real-world deployment\ncapabilities. The source code for our method is publicly available at:\nhttps:\/\/github.com\/sunpihai-up\/FUSE\n","date":"2025-03-25"}
{"id":"2503.19740","title":"Surg-3M: A Dataset and Foundation Model for Perception in Surgical\n  Settings","abstract":"  Advancements in computer-assisted surgical procedures heavily rely on\naccurate visual data interpretation from camera systems used during surgeries.\nTraditional open-access datasets focusing on surgical procedures are often\nlimited by their small size, typically consisting of fewer than 100 videos with\nless than 100K images. To address these constraints, a new dataset called\nSurg-3M has been compiled using a novel aggregation pipeline that collects\nhigh-resolution videos from online sources. Featuring an extensive collection\nof over 4K surgical videos and more than 3 million high-quality images from\nmultiple procedure types, Surg-3M offers a comprehensive resource surpassing\nexisting alternatives in size and scope, including two novel tasks. To\ndemonstrate the effectiveness of this dataset, we present SurgFM, a\nself-supervised foundation model pretrained on Surg-3M that achieves impressive\nresults in downstream tasks such as surgical phase recognition, action\nrecognition, and tool presence detection. Combining key components from\nConvNeXt, DINO, and an innovative augmented distillation method, SurgFM\nexhibits exceptional performance compared to specialist architectures across\nvarious benchmarks. Our experimental results show that SurgFM outperforms\nstate-of-the-art models in multiple downstream tasks, including significant\ngains in surgical phase recognition (+8.9pp, +4.7pp, and +3.9pp of Jaccard in\nAutoLaparo, M2CAI16, and Cholec80), action recognition (+3.1pp of mAP in\nCholecT50) and tool presence detection (+4.6pp of mAP in Cholec80). Moreover,\neven when using only half of the data, SurgFM outperforms state-of-the-art\nmodels in AutoLaparo and achieves state-of-the-art performance in Cholec80.\nBoth Surg-3M and SurgFM have significant potential to accelerate progress\ntowards developing autonomous robotic surgery systems.\n","date":"2025-03-25"}
{"id":"2503.19742","title":"Optimizing Photonic Structures with Large Language Model Driven\n  Algorithm Discovery","abstract":"  We study how large language models can be used in combination with\nevolutionary computation techniques to automatically discover optimization\nalgorithms for the design of photonic structures. Building on the Large\nLanguage Model Evolutionary Algorithm (LLaMEA) framework, we introduce\nstructured prompt engineering tailored to multilayer photonic problems such as\nBragg mirror, ellipsometry inverse analysis, and solar cell antireflection\ncoatings. We systematically explore multiple evolutionary strategies, including\n(1+1), (1+5), (2+10), and others, to balance exploration and exploitation. Our\nexperiments show that LLM-generated algorithms, generated using small-scale\nproblem instances, can match or surpass established methods like\nquasi-oppositional differential evolution on large-scale realistic real-world\nproblem instances. Notably, LLaMEA's self-debugging mutation loop, augmented by\nautomatically extracted problem-specific insights, achieves strong anytime\nperformance and reliable convergence across diverse problem scales. This work\ndemonstrates the feasibility of domain-focused LLM prompts and evolutionary\napproaches in solving optical design tasks, paving the way for rapid, automated\nphotonic inverse design.\n","date":"2025-03-25"}
{"id":"2503.19752","title":"Inducing Personality in LLM-Based Honeypot Agents: Measuring the Effect\n  on Human-Like Agenda Generation","abstract":"  This paper presents SANDMAN, an architecture for cyber deception that\nleverages Language Agents to emulate convincing human simulacra. Our 'Deceptive\nAgents' serve as advanced cyber decoys, designed for high-fidelity engagement\nwith attackers by extending the observation period of attack behaviours.\nThrough experimentation, measurement, and analysis, we demonstrate how a prompt\nschema based on the five-factor model of personality systematically induces\ndistinct 'personalities' in Large Language Models. Our results highlight the\nfeasibility of persona-driven Language Agents for generating diverse, realistic\nbehaviours, ultimately improving cyber deception strategies.\n","date":"2025-03-25"}
{"id":"2503.19753","title":"A Survey on Event-driven 3D Reconstruction: Development under Different\n  Categories","abstract":"  Event cameras have gained increasing attention for 3D reconstruction due to\ntheir high temporal resolution, low latency, and high dynamic range. They\ncapture per-pixel brightness changes asynchronously, allowing accurate\nreconstruction under fast motion and challenging lighting conditions. In this\nsurvey, we provide a comprehensive review of event-driven 3D reconstruction\nmethods, including stereo, monocular, and multimodal systems. We further\ncategorize recent developments based on geometric, learning-based, and hybrid\napproaches. Emerging trends, such as neural radiance fields and 3D Gaussian\nsplatting with event data, are also covered. The related works are structured\nchronologically to illustrate the innovations and progression within the field.\nTo support future research, we also highlight key research gaps and future\nresearch directions in dataset, experiment, evaluation, event representation,\netc.\n","date":"2025-03-25"}
{"id":"2503.19755","title":"ORION: A Holistic End-to-End Autonomous Driving Framework by\n  Vision-Language Instructed Action Generation","abstract":"  End-to-end (E2E) autonomous driving methods still struggle to make correct\ndecisions in interactive closed-loop evaluation due to limited causal reasoning\ncapability. Current methods attempt to leverage the powerful understanding and\nreasoning abilities of Vision-Language Models (VLMs) to resolve this dilemma.\nHowever, the problem is still open that few VLMs for E2E methods perform well\nin the closed-loop evaluation due to the gap between the semantic reasoning\nspace and the purely numerical trajectory output in the action space. To tackle\nthis issue, we propose ORION, a holistic E2E autonomous driving framework by\nvision-language instructed action generation. ORION uniquely combines a\nQT-Former to aggregate long-term history context, a Large Language Model (LLM)\nfor driving scenario reasoning, and a generative planner for precision\ntrajectory prediction. ORION further aligns the reasoning space and the action\nspace to implement a unified E2E optimization for both visual\nquestion-answering (VQA) and planning tasks. Our method achieves an impressive\nclosed-loop performance of 77.74 Driving Score (DS) and 54.62% Success Rate\n(SR) on the challenge Bench2Drive datasets, which outperforms state-of-the-art\n(SOTA) methods by a large margin of 14.28 DS and 19.61% SR.\n","date":"2025-03-25"}
{"id":"2503.19757","title":"Dita: Scaling Diffusion Transformer for Generalist\n  Vision-Language-Action Policy","abstract":"  While recent vision-language-action models trained on diverse robot datasets\nexhibit promising generalization capabilities with limited in-domain data,\ntheir reliance on compact action heads to predict discretized or continuous\nactions constrains adaptability to heterogeneous action spaces. We present\nDita, a scalable framework that leverages Transformer architectures to directly\ndenoise continuous action sequences through a unified multimodal diffusion\nprocess. Departing from prior methods that condition denoising on fused\nembeddings via shallow networks, Dita employs in-context conditioning --\nenabling fine-grained alignment between denoised actions and raw visual tokens\nfrom historical observations. This design explicitly models action deltas and\nenvironmental nuances. By scaling the diffusion action denoiser alongside the\nTransformer's scalability, Dita effectively integrates cross-embodiment\ndatasets across diverse camera perspectives, observation scenes, tasks, and\naction spaces. Such synergy enhances robustness against various variances and\nfacilitates the successful execution of long-horizon tasks. Evaluations across\nextensive benchmarks demonstrate state-of-the-art or comparative performance in\nsimulation. Notably, Dita achieves robust real-world adaptation to\nenvironmental variances and complex long-horizon tasks through 10-shot\nfinetuning, using only third-person camera inputs. The architecture establishes\na versatile, lightweight and open-source baseline for generalist robot policy\nlearning. Project Page: https:\/\/robodita.github.io.\n","date":"2025-03-25"}
{"id":"2503.19762","title":"Splitting Answer Set Programs with respect to Intensionality Statements\n  (Extended Version)","abstract":"  Splitting a logic program allows us to reduce the task of computing its\nstable models to similar tasks for its subprograms. This can be used to\nincrease solving performance and prove program correctness. We generalize the\nconditions under which this technique is applicable, by considering not only\ndependencies between predicates but also their arguments and context. This\nallows splitting programs commonly used in practice to which previous results\nwere not applicable.\n","date":"2025-03-25"}
{"id":"2503.19763","title":"Interpretable Deep Regression Models with Interval-Censored Failure Time\n  Data","abstract":"  Deep neural networks (DNNs) have become powerful tools for modeling complex\ndata structures through sequentially integrating simple functions in each\nhidden layer. In survival analysis, recent advances of DNNs primarily focus on\nenhancing model capabilities, especially in exploring nonlinear covariate\neffects under right censoring. However, deep learning methods for\ninterval-censored data, where the unobservable failure time is only known to\nlie in an interval, remain underexplored and limited to specific data type or\nmodel. This work proposes a general regression framework for interval-censored\ndata with a broad class of partially linear transformation models, where key\ncovariate effects are modeled parametrically while nonlinear effects of\nnuisance multi-modal covariates are approximated via DNNs, balancing\ninterpretability and flexibility. We employ sieve maximum likelihood estimation\nby leveraging monotone splines to approximate the cumulative baseline hazard\nfunction. To ensure reliable and tractable estimation, we develop an EM\nalgorithm incorporating stochastic gradient descent. We establish the\nasymptotic properties of parameter estimators and show that the DNN estimator\nachieves minimax-optimal convergence. Extensive simulations demonstrate\nsuperior estimation and prediction accuracy over state-of-the-art methods.\nApplying our method to the Alzheimer's Disease Neuroimaging Initiative dataset\nyields novel insights and improved predictive performance compared to\ntraditional approaches.\n","date":"2025-03-25"}
{"id":"2503.19764","title":"OpenLex3D: A New Evaluation Benchmark for Open-Vocabulary 3D Scene\n  Representations","abstract":"  3D scene understanding has been transformed by open-vocabulary language\nmodels that enable interaction via natural language. However, the evaluation of\nthese representations is limited to closed-set semantics that do not capture\nthe richness of language. This work presents OpenLex3D, a dedicated benchmark\nto evaluate 3D open-vocabulary scene representations. OpenLex3D provides\nentirely new label annotations for 23 scenes from Replica, ScanNet++, and HM3D,\nwhich capture real-world linguistic variability by introducing synonymical\nobject categories and additional nuanced descriptions. By introducing an\nopen-set 3D semantic segmentation task and an object retrieval task, we provide\ninsights on feature precision, segmentation, and downstream capabilities. We\nevaluate various existing 3D open-vocabulary methods on OpenLex3D, showcasing\nfailure cases, and avenues for improvement. The benchmark is publicly available\nat: https:\/\/openlex3d.github.io\/.\n","date":"2025-03-25"}
{"id":"2503.19769","title":"BiPrompt-SAM: Enhancing Image Segmentation via Explicit Selection\n  between Point and Text Prompts","abstract":"  Segmentation is a fundamental task in computer vision, with prompt-driven\nmethods gaining prominence due to their flexibility. The recent Segment\nAnything Model (SAM) has demonstrated powerful point-prompt segmentation\ncapabilities, while text-based segmentation models offer rich semantic\nunderstanding. However, existing approaches rarely explore how to effectively\ncombine these complementary modalities for optimal segmentation performance.\nThis paper presents BiPrompt-SAM, a novel dual-modal prompt segmentation\nframework that fuses the advantages of point and text prompts through an\nexplicit selection mechanism. Specifically, we leverage SAM's inherent ability\nto generate multiple mask candidates, combined with a semantic guidance mask\nfrom text prompts, and explicitly select the most suitable candidate based on\nsimilarity metrics. This approach can be viewed as a simplified Mixture of\nExperts (MoE) system, where the point and text modules act as distinct\n\"experts,\" and the similarity scoring serves as a rudimentary \"gating network.\"\nWe conducted extensive evaluations on both the Endovis17 medical dataset and\nRefCOCO series natural image datasets. On Endovis17, BiPrompt-SAM achieved\n89.55\\% mDice and 81.46\\% mIoU, comparable to state-of-the-art specialized\nmedical segmentation models. On the RefCOCO series datasets, our method\nattained 87.1\\%, 86.5\\%, and 85.8\\% IoU, significantly outperforming existing\napproaches. Experiments demonstrate that our explicit dual-selection method\neffectively combines the spatial precision of point prompts with the semantic\nrichness of text prompts, particularly excelling in scenarios involving\nsemantically complex objects, multiple similar objects, and partial occlusions.\nBiPrompt-SAM not only provides a simple yet effective implementation but also\noffers a new perspective on multi-modal prompt fusion.\n","date":"2025-03-25"}
{"id":"2503.19776","title":"Resilient Sensor Fusion under Adverse Sensor Failures via Multi-Modal\n  Expert Fusion","abstract":"  Modern autonomous driving perception systems utilize complementary\nmulti-modal sensors, such as LiDAR and cameras. Although sensor fusion\narchitectures enhance performance in challenging environments, they still\nsuffer significant performance drops under severe sensor failures, such as\nLiDAR beam reduction, LiDAR drop, limited field of view, camera drop, and\nocclusion. This limitation stems from inter-modality dependencies in current\nsensor fusion frameworks. In this study, we introduce an efficient and robust\nLiDAR-camera 3D object detector, referred to as MoME, which can achieve robust\nperformance through a mixture of experts approach. Our MoME fully decouples\nmodality dependencies using three parallel expert decoders, which use camera\nfeatures, LiDAR features, or a combination of both to decode object queries,\nrespectively. We propose Multi-Expert Decoding (MED) framework, where each\nquery is decoded selectively using one of three expert decoders. MoME utilizes\nan Adaptive Query Router (AQR) to select the most appropriate expert decoder\nfor each query based on the quality of camera and LiDAR features. This ensures\nthat each query is processed by the best-suited expert, resulting in robust\nperformance across diverse sensor failure scenarios. We evaluated the\nperformance of MoME on the nuScenes-R benchmark. Our MoME achieved\nstate-of-the-art performance in extreme weather and sensor failure conditions,\nsignificantly outperforming the existing models across various sensor failure\nscenarios.\n","date":"2025-03-25"}
{"id":"2503.19777","title":"LPOSS: Label Propagation Over Patches and Pixels for Open-vocabulary\n  Semantic Segmentation","abstract":"  We propose a training-free method for open-vocabulary semantic segmentation\nusing Vision-and-Language Models (VLMs). Our approach enhances the initial\nper-patch predictions of VLMs through label propagation, which jointly\noptimizes predictions by incorporating patch-to-patch relationships. Since VLMs\nare primarily optimized for cross-modal alignment and not for intra-modal\nsimilarity, we use a Vision Model (VM) that is observed to better capture these\nrelationships. We address resolution limitations inherent to patch-based\nencoders by applying label propagation at the pixel level as a refinement step,\nsignificantly improving segmentation accuracy near class boundaries. Our\nmethod, called LPOSS+, performs inference over the entire image, avoiding\nwindow-based processing and thereby capturing contextual interactions across\nthe full image. LPOSS+ achieves state-of-the-art performance among\ntraining-free methods, across a diverse set of datasets. Code:\nhttps:\/\/github.com\/vladan-stojnic\/LPOSS\n","date":"2025-03-25"}
{"id":"2503.19779","title":"PyGraph: Robust Compiler Support for CUDA Graphs in PyTorch","abstract":"  CUDA Graphs -- a recent hardware feature introduced for NVIDIA GPUs -- aim to\nreduce CPU launch overhead by capturing and launching a series of GPU tasks\n(kernels) as a DAG. However, deploying CUDA Graphs faces several challenges\ntoday due to the static structure of a graph. It also incurs performance\noverhead due to data copy. In fact, we show a counter-intuitive result --\ndeploying CUDA Graphs hurts performance in many cases.\n  We introduce PyGraph, a novel approach to automatically harness the power of\nCUDA Graphs within PyTorch2. Driven by three key observations, PyGraph embodies\nthree novel optimizations: it enables wider deployment of CUDA Graphs, reduces\nGPU kernel parameter copy overheads, and selectively deploys CUDA Graphs based\non a cost-benefit analysis. PyGraph seamlessly integrates with PyTorch2's\ncompilation toolchain, enabling efficient use of CUDA Graphs without manual\nmodifications to the code. We evaluate PyGraph across various machine learning\nbenchmarks, demonstrating substantial performance improvements over PyTorch2.\n","date":"2025-03-25"}
{"id":"2503.19783","title":"Fine-Grained Erasure in Text-to-Image Diffusion-based Foundation Models","abstract":"  Existing unlearning algorithms in text-to-image generative models often fail\nto preserve the knowledge of semantically related concepts when removing\nspecific target concepts: a challenge known as adjacency. To address this, we\npropose FADE (Fine grained Attenuation for Diffusion Erasure), introducing\nadjacency aware unlearning in diffusion models. FADE comprises two components:\n(1) the Concept Neighborhood, which identifies an adjacency set of related\nconcepts, and (2) Mesh Modules, employing a structured combination of\nExpungement, Adjacency, and Guidance loss components. These enable precise\nerasure of target concepts while preserving fidelity across related and\nunrelated concepts. Evaluated on datasets like Stanford Dogs, Oxford Flowers,\nCUB, I2P, Imagenette, and ImageNet1k, FADE effectively removes target concepts\nwith minimal impact on correlated concepts, achieving atleast a 12% improvement\nin retention performance over state-of-the-art methods.\n","date":"2025-03-25"}
{"id":"2503.19786","title":"Gemma 3 Technical Report","abstract":"  We introduce Gemma 3, a multimodal addition to the Gemma family of\nlightweight open models, ranging in scale from 1 to 27 billion parameters. This\nversion introduces vision understanding abilities, a wider coverage of\nlanguages and longer context - at least 128K tokens. We also change the\narchitecture of the model to reduce the KV-cache memory that tends to explode\nwith long context. This is achieved by increasing the ratio of local to global\nattention layers, and keeping the span on local attention short. The Gemma 3\nmodels are trained with distillation and achieve superior performance to Gemma\n2 for both pre-trained and instruction finetuned versions. In particular, our\nnovel post-training recipe significantly improves the math, chat,\ninstruction-following and multilingual abilities, making Gemma3-4B-IT\ncompetitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro\nacross benchmarks. We release all our models to the community.\n","date":"2025-03-25"}
{"id":"2503.19791","title":"SITA: Structurally Imperceptible and Transferable Adversarial Attacks\n  for Stylized Image Generation","abstract":"  Image generation technology has brought significant advancements across\nvarious fields but has also raised concerns about data misuse and potential\nrights infringements, particularly with respect to creating visual artworks.\nCurrent methods aimed at safeguarding artworks often employ adversarial\nattacks. However, these methods face challenges such as poor transferability,\nhigh computational costs, and the introduction of noticeable noise, which\ncompromises the aesthetic quality of the original artwork. To address these\nlimitations, we propose a Structurally Imperceptible and Transferable\nAdversarial (SITA) attacks. SITA leverages a CLIP-based destylization loss,\nwhich decouples and disrupts the robust style representation of the image. This\ndisruption hinders style extraction during stylized image generation, thereby\nimpairing the overall stylization process. Importantly, SITA eliminates the\nneed for a surrogate diffusion model, leading to significantly reduced\ncomputational overhead. The method's robust style feature disruption ensures\nhigh transferability across diverse models. Moreover, SITA introduces\nperturbations by embedding noise within the imperceptible structural details of\nthe image. This approach effectively protects against style extraction without\ncompromising the visual quality of the artwork. Extensive experiments\ndemonstrate that SITA offers superior protection for artworks against\nunauthorized use in stylized generation. It significantly outperforms existing\nmethods in terms of transferability, computational efficiency, and noise\nimperceptibility. Code is available at https:\/\/github.com\/A-raniy-day\/SITA.\n","date":"2025-03-25"}
{"id":"2503.19793","title":"In the Blink of an Eye: Instant Game Map Editing using a Generative-AI\n  Smart Brush","abstract":"  With video games steadily increasing in complexity, automated generation of\ngame content has found widespread interest. However, the task of 3D gaming map\nart creation remains underexplored to date due to its unique complexity and\ndomain-specific challenges. While recent works have addressed related topics\nsuch as retro-style level generation and procedural terrain creation, these\nworks primarily focus on simpler data distributions. To the best of our\nknowledge, we are the first to demonstrate the application of modern AI\ntechniques for high-resolution texture manipulation in complex, highly detailed\nAAA 3D game environments. We introduce a novel Smart Brush for map editing,\ndesigned to assist artists in seamlessly modifying selected areas of a game map\nwith minimal effort. By leveraging generative adversarial networks and\ndiffusion models we propose two variants of the brush that enable efficient and\ncontext-aware generation. Our hybrid workflow aims to enhance both artistic\nflexibility and production efficiency, enabling the refinement of environments\nwithout manually reworking every detail, thus helping to bridge the gap between\nautomation and creative control in game development. A comparative evaluation\nof our two methods with adapted versions of several state-of-the art models\nshows that our GAN-based brush produces the sharpest and most detailed outputs\nwhile preserving image context while the evaluated state-of-the-art models tend\ntowards blurrier results and exhibit difficulties in maintaining contextual\nconsistency.\n","date":"2025-03-25"}
{"id":"2503.19794","title":"PAVE: Patching and Adapting Video Large Language Models","abstract":"  Pre-trained video large language models (Video LLMs) exhibit remarkable\nreasoning capabilities, yet adapting these models to new tasks involving\nadditional modalities or data types (e.g., audio or 3D information) remains\nchallenging. In this paper, we present PAVE, a flexible framework for adapting\npre-trained Video LLMs to downstream tasks with side-channel signals, such as\naudio, 3D cues, or multi-view videos. PAVE introduces lightweight adapters,\nreferred to as \"patches,\" which add a small number of parameters and operations\nto a base model without modifying its architecture or pre-trained weights. In\ndoing so, PAVE can effectively adapt the pre-trained base model to support\ndiverse downstream tasks, including audio-visual question answering, 3D\nreasoning, multi-view video recognition, and high frame rate video\nunderstanding. Across these tasks, PAVE significantly enhances the performance\nof the base model, surpassing state-of-the-art task-specific models while\nincurring a minor cost of ~0.1% additional FLOPs and parameters. Further, PAVE\nsupports multi-task learning and generalizes well across different Video LLMs.\nOur code is available at https:\/\/github.com\/dragonlzm\/PAVE.\n","date":"2025-03-25"}
{"id":"2503.19798","title":"Unpaired Object-Level SAR-to-Optical Image Translation for Aircraft with\n  Keypoints-Guided Diffusion Models","abstract":"  Synthetic Aperture Radar (SAR) imagery provides all-weather, all-day, and\nhigh-resolution imaging capabilities but its unique imaging mechanism makes\ninterpretation heavily reliant on expert knowledge, limiting interpretability,\nespecially in complex target tasks. Translating SAR images into optical images\nis a promising solution to enhance interpretation and support downstream tasks.\nMost existing research focuses on scene-level translation, with limited work on\nobject-level translation due to the scarcity of paired data and the challenge\nof accurately preserving contour and texture details. To address these issues,\nthis study proposes a keypoint-guided diffusion model (KeypointDiff) for\nSAR-to-optical image translation of unpaired aircraft targets. This framework\nintroduces supervision on target class and azimuth angle via keypoints, along\nwith a training strategy for unpaired data. Based on the classifier-free\nguidance diffusion architecture, a class-angle guidance module (CAGM) is\ndesigned to integrate class and angle information into the diffusion generation\nprocess. Furthermore, adversarial loss and consistency loss are employed to\nimprove image fidelity and detail quality, tailored for aircraft targets.\nDuring sampling, aided by a pre-trained keypoint detector, the model eliminates\nthe requirement for manually labeled class and azimuth information, enabling\nautomated SAR-to-optical translation. Experimental results demonstrate that the\nproposed method outperforms existing approaches across multiple metrics,\nproviding an efficient and effective solution for object-level SAR-to-optical\ntranslation and downstream tasks. Moreover, the method exhibits strong\nzero-shot generalization to untrained aircraft types with the assistance of the\nkeypoint detector.\n","date":"2025-03-25"}
{"id":"2503.19800","title":"SemEval-2025 Task 9: The Food Hazard Detection Challenge","abstract":"  In this challenge, we explored text-based food hazard prediction with long\ntail distributed classes. The task was divided into two subtasks: (1)\npredicting whether a web text implies one of ten food-hazard categories and\nidentifying the associated food category, and (2) providing a more fine-grained\nclassification by assigning a specific label to both the hazard and the\nproduct. Our findings highlight that large language model-generated synthetic\ndata can be highly effective for oversampling long-tail distributions.\nFurthermore, we find that fine-tuned encoder-only, encoder-decoder, and\ndecoder-only systems achieve comparable maximum performance across both\nsubtasks. During this challenge, we gradually released (under CC BY-NC-SA 4.0)\na novel set of 6,644 manually labeled food-incident reports.\n","date":"2025-03-25"}
{"id":"2503.19801","title":"SeLIP: Similarity Enhanced Contrastive Language Image Pretraining for\n  Multi-modal Head MRI","abstract":"  Despite that deep learning (DL) methods have presented tremendous potential\nin many medical image analysis tasks, the practical applications of medical DL\nmodels are limited due to the lack of enough data samples with manual\nannotations. By noting that the clinical radiology examinations are associated\nwith radiology reports that describe the images, we propose to develop a\nfoundation model for multi-model head MRI by using contrastive learning on the\nimages and the corresponding radiology findings. In particular, a contrastive\nlearning framework is proposed, where a mixed syntax and semantic similarity\nmatching metric is integrated to reduce the thirst of extreme large dataset in\nconventional contrastive learning framework. Our proposed similarity enhanced\ncontrastive language image pretraining (SeLIP) is able to effectively extract\nmore useful features. Experiments revealed that our proposed SeLIP performs\nwell in many downstream tasks including image-text retrieval task,\nclassification task, and image segmentation, which highlights the importance of\nconsidering the similarities among texts describing different images in\ndeveloping medical image foundation models.\n","date":"2025-03-25"}
{"id":"2503.19804","title":"LENVIZ: A High-Resolution Low-Exposure Night Vision Benchmark Dataset","abstract":"  Low-light image enhancement is crucial for a myriad of applications, from\nnight vision and surveillance, to autonomous driving. However, due to the\ninherent limitations that come in hand with capturing images in\nlow-illumination environments, the task of enhancing such scenes still presents\na formidable challenge. To advance research in this field, we introduce our Low\nExposure Night Vision (LENVIZ) Dataset, a comprehensive multi-exposure\nbenchmark dataset for low-light image enhancement comprising of over 230K\nframes showcasing 24K real-world indoor and outdoor, with-and without human,\nscenes. Captured using 3 different camera sensors, LENVIZ offers a wide range\nof lighting conditions, noise levels, and scene complexities, making it the\nlargest publicly available up-to 4K resolution benchmark in the field. LENVIZ\nincludes high quality human-generated ground truth, for which each\nmulti-exposure low-light scene has been meticulously curated and edited by\nexpert photographers to ensure optimal image quality. Furthermore, we also\nconduct a comprehensive analysis of current state-of-the-art low-light image\nenhancement techniques on our dataset and highlight potential areas of\nimprovement.\n","date":"2025-03-25"}
{"id":"2503.19809","title":"Simulating Tracking Data to Advance Sports Analytics Research","abstract":"  Advanced analytics have transformed how sports teams operate, particularly in\nepisodic sports like baseball. Their impact on continuous invasion sports, such\nas soccer and ice hockey, has been limited due to increased game complexity and\nrestricted access to high-resolution game tracking data. In this demo, we\npresent a method to collect and utilize simulated soccer tracking data from the\nGoogle Research Football environment to support the development of models\ndesigned for continuous tracking data. The data is stored in a schema that is\nrepresentative of real tracking data and we provide processes that extract\nhigh-level features and events. We include examples of established tracking\ndata models to showcase the efficacy of the simulated data. We address the\nscarcity of publicly available tracking data, providing support for research at\nthe intersection of artificial intelligence and sports analytics.\n","date":"2025-03-25"}
{"id":"2503.19813","title":"Guidelines For The Choice Of The Baseline in XAI Attribution Methods","abstract":"  Given the broad adoption of artificial intelligence, it is essential to\nprovide evidence that AI models are reliable, trustable, and fair. To this end,\nthe emerging field of eXplainable AI develops techniques to probe such\nrequirements, counterbalancing the hype pushing the pervasiveness of this\ntechnology. Among the many facets of this issue, this paper focuses on baseline\nattribution methods, aiming at deriving a feature attribution map at the\nnetwork input relying on a \"neutral\" stimulus usually called \"baseline\". The\nchoice of the baseline is crucial as it determines the explanation of the\nnetwork behavior. In this framework, this paper has the twofold goal of\nshedding light on the implications of the choice of the baseline and providing\na simple yet effective method for identifying the best baseline for the task.\nTo achieve this, we propose a decision boundary sampling method, since the\nbaseline, by definition, lies on the decision boundary, which naturally becomes\nthe search domain. Experiments are performed on synthetic examples and\nvalidated relying on state-of-the-art methods. Despite being limited to the\nexperimental scope, this contribution is relevant as it offers clear guidelines\nand a simple proxy for baseline selection, reducing ambiguity and enhancing\ndeep models' reliability and trust.\n","date":"2025-03-25"}
{"id":"2503.19815","title":"Thinking agents for zero-shot generalization to qualitatively novel\n  tasks","abstract":"  Intelligent organisms can solve truly novel problems which they have never\nencountered before, either in their lifetime or their evolution. An important\ncomponent of this capacity is the ability to ``think'', that is, to mentally\nmanipulate objects, concepts and behaviors in order to plan and evaluate\npossible solutions to novel problems, even without environment interaction. To\ngenerate problems that are truly qualitatively novel, while still solvable\nzero-shot (by mental simulation), we use the combinatorial nature of\nenvironments: we train the agent while withholding a specific combination of\nthe environment's elements. The novel test task, based on this combination, is\nthus guaranteed to be truly novel, while still mentally simulable since the\nagent has been exposed to each individual element (and their pairwise\ninteractions) during training. We propose a method to train agents endowed with\nworld models to make use their mental simulation abilities, by selecting tasks\nbased on the difference between the agent's pre-thinking and post-thinking\nperformance. When tested on the novel, withheld problem, the resulting agent\nsuccessfully simulated alternative scenarios and used the resulting information\nto guide its behavior in the actual environment, solving the novel task in a\nsingle real-environment trial (zero-shot).\n","date":"2025-03-25"}
{"id":"2503.19817","title":"Bitstream Collisions in Neural Image Compression via Adversarial\n  Perturbations","abstract":"  Neural image compression (NIC) has emerged as a promising alternative to\nclassical compression techniques, offering improved compression ratios. Despite\nits progress towards standardization and practical deployment, there has been\nminimal exploration into it's robustness and security. This study reveals an\nunexpected vulnerability in NIC - bitstream collisions - where semantically\ndifferent images produce identical compressed bitstreams. Utilizing a novel\nwhitebox adversarial attack algorithm, this paper demonstrates that adding\ncarefully crafted perturbations to semantically different images can cause\ntheir compressed bitstreams to collide exactly. The collision vulnerability\nposes a threat to the practical usability of NIC, particularly in\nsecurity-critical applications. The cause of the collision is analyzed, and a\nsimple yet effective mitigation method is presented.\n","date":"2025-03-25"}
{"id":"2503.19819","title":"Domain-incremental White Blood Cell Classification with Privacy-aware\n  Continual Learning","abstract":"  White blood cell (WBC) classification plays a vital role in hematology for\ndiagnosing various medical conditions. However, it faces significant challenges\ndue to domain shifts caused by variations in sample sources (e.g., blood or\nbone marrow) and differing imaging conditions across hospitals. Traditional\ndeep learning models often suffer from catastrophic forgetting in such dynamic\nenvironments, while foundation models, though generally robust, experience\nperformance degradation when the distribution of inference data differs from\nthat of the training data. To address these challenges, we propose a generative\nreplay-based Continual Learning (CL) strategy designed to prevent forgetting in\nfoundation models for WBC classification. Our method employs lightweight\ngenerators to mimic past data with a synthetic latent representation to enable\nprivacy-preserving replay. To showcase the effectiveness, we carry out\nextensive experiments with a total of four datasets with different task\nordering and four backbone models including ResNet50, RetCCL, CTransPath, and\nUNI. Experimental results demonstrate that conventional fine-tuning methods\ndegrade performance on previously learned tasks and struggle with domain\nshifts. In contrast, our continual learning strategy effectively mitigates\ncatastrophic forgetting, preserving model performance across varying domains.\nThis work presents a practical solution for maintaining reliable WBC\nclassification in real-world clinical settings, where data distributions\nfrequently evolve.\n","date":"2025-03-25"}
{"id":"2503.19820","title":"A Systematic Review of EEG-based Machine Intelligence Algorithms for\n  Depression Diagnosis, and Monitoring","abstract":"  Depression disorder is a serious health condition that has affected the lives\nof millions of people around the world. Diagnosis of depression is a\nchallenging practice that relies heavily on subjective studies and, in most\ncases, suffers from late findings. Electroencephalography (EEG) biomarkers have\nbeen suggested and investigated in recent years as a potential transformative\nobjective practice. In this article, for the first time, a detailed systematic\nreview of EEG-based depression diagnosis approaches is conducted using advanced\nmachine learning techniques and statistical analyses. For this, 938 potentially\nrelevant articles (since 1985) were initially detected and filtered into 139\nrelevant articles based on the review scheme 'preferred reporting items for\nsystematic reviews and meta-analyses (PRISMA).' This article compares and\ndiscusses the selected articles and categorizes them according to the type of\nmachine learning techniques and statistical analyses. Algorithms, preprocessing\ntechniques, extracted features, and data acquisition systems are discussed and\nsummarized. This review paper explains the existing challenges of the current\nalgorithms and sheds light on the future direction of the field. This\nsystematic review outlines the issues and challenges in machine intelligence\nfor the diagnosis of EEG depression that can be addressed in future studies and\npossibly in future wearable technologies.\n","date":"2025-03-25"}
{"id":"2503.19821","title":"IgCraft: A versatile sequence generation framework for antibody\n  discovery and engineering","abstract":"  Designing antibody sequences to better resemble those observed in natural\nhuman repertoires is a key challenge in biologics development. We introduce\nIgCraft: a multi-purpose model for paired human antibody sequence generation,\nbuilt on Bayesian Flow Networks. IgCraft presents one of the first unified\ngenerative modeling frameworks capable of addressing multiple antibody sequence\ndesign tasks with a single model, including unconditional sampling, sequence\ninpainting, inverse folding, and CDR motif scaffolding. Our approach achieves\ncompetitive results across the full spectrum of these tasks while constraining\ngeneration to the space of human antibody sequences, exhibiting particular\nstrengths in CDR motif scaffolding (grafting) where we achieve state-of-the-art\nperformance in terms of humanness and preservation of structural properties. By\nintegrating previously separate tasks into a single scalable generative model,\nIgCraft provides a versatile platform for sampling human antibody sequences\nunder a variety of contexts relevant to antibody discovery and engineering.\nModel code and weights are publicly available at github.com\/mgreenig\/IgCraft.\n","date":"2025-03-25"}
{"id":"2503.19823","title":"GyralNet Subnetwork Partitioning via Differentiable Spectral Modularity\n  Optimization","abstract":"  Understanding the structural and functional organization of the human brain\nrequires a detailed examination of cortical folding patterns, among which the\nthree-hinge gyrus (3HG) has been identified as a key structural landmark.\nGyralNet, a network representation of cortical folding, models 3HGs as nodes\nand gyral crests as edges, highlighting their role as critical hubs in\ncortico-cortical connectivity. However, existing methods for analyzing 3HGs\nface significant challenges, including the sub-voxel scale of 3HGs at typical\nneuroimaging resolutions, the computational complexity of establishing\ncross-subject correspondences, and the oversimplification of treating 3HGs as\nindependent nodes without considering their community-level relationships. To\naddress these limitations, we propose a fully differentiable subnetwork\npartitioning framework that employs a spectral modularity maximization\noptimization strategy to modularize the organization of 3HGs within GyralNet.\nBy incorporating topological structural similarity and DTI-derived connectivity\npatterns as attribute features, our approach provides a biologically meaningful\nrepresentation of cortical organization. Extensive experiments on the Human\nConnectome Project (HCP) dataset demonstrate that our method effectively\npartitions GyralNet at the individual level while preserving the\ncommunity-level consistency of 3HGs across subjects, offering a robust\nfoundation for understanding brain connectivity.\n","date":"2025-03-25"}
{"id":"2503.19824","title":"AudCast: Audio-Driven Human Video Generation by Cascaded Diffusion\n  Transformers","abstract":"  Despite the recent progress of audio-driven video generation, existing\nmethods mostly focus on driving facial movements, leading to non-coherent head\nand body dynamics. Moving forward, it is desirable yet challenging to generate\nholistic human videos with both accurate lip-sync and delicate co-speech\ngestures w.r.t. given audio. In this work, we propose AudCast, a generalized\naudio-driven human video generation framework adopting a cascade\nDiffusion-Transformers (DiTs) paradigm, which synthesizes holistic human videos\nbased on a reference image and a given audio. 1) Firstly, an audio-conditioned\nHolistic Human DiT architecture is proposed to directly drive the movements of\nany human body with vivid gesture dynamics. 2) Then to enhance hand and face\ndetails that are well-knownly difficult to handle, a Regional Refinement DiT\nleverages regional 3D fitting as the bridge to reform the signals, producing\nthe final results. Extensive experiments demonstrate that our framework\ngenerates high-fidelity audio-driven holistic human videos with temporal\ncoherence and fine facial and hand details. Resources can be found at\nhttps:\/\/guanjz20.github.io\/projects\/AudCast.\n","date":"2025-03-25"}
{"id":"2503.19828","title":"Contextual Metric Meta-Evaluation by Measuring Local Metric Accuracy","abstract":"  Meta-evaluation of automatic evaluation metrics -- assessing evaluation\nmetrics themselves -- is crucial for accurately benchmarking natural language\nprocessing systems and has implications for scientific inquiry, production\nmodel development, and policy enforcement. While existing approaches to metric\nmeta-evaluation focus on general statements about the absolute and relative\nquality of metrics across arbitrary system outputs, in practice, metrics are\napplied in highly contextual settings, often measuring the performance for a\nhighly constrained set of system outputs. For example, we may only be\ninterested in evaluating a specific model or class of models. We introduce a\nmethod for contextual metric meta-evaluation by comparing the local metric\naccuracy of evaluation metrics. Across translation, speech recognition, and\nranking tasks, we demonstrate that the local metric accuracies vary both in\nabsolute value and relative effectiveness as we shift across evaluation\ncontexts. This observed variation highlights the importance of adopting\ncontext-specific metric evaluations over global ones.\n","date":"2025-03-25"}
{"id":"2503.19839","title":"FireEdit: Fine-grained Instruction-based Image Editing via Region-aware\n  Vision Language Model","abstract":"  Currently, instruction-based image editing methods have made significant\nprogress by leveraging the powerful cross-modal understanding capabilities of\nvision language models (VLMs). However, they still face challenges in three key\nareas: 1) complex scenarios; 2) semantic consistency; and 3) fine-grained\nediting. To address these issues, we propose FireEdit, an innovative\nFine-grained Instruction-based image editing framework that exploits a\nREgion-aware VLM. FireEdit is designed to accurately comprehend user\ninstructions and ensure effective control over the editing process.\nSpecifically, we enhance the fine-grained visual perception capabilities of the\nVLM by introducing additional region tokens. Relying solely on the output of\nthe LLM to guide the diffusion model may lead to suboptimal editing results.\nTherefore, we propose a Time-Aware Target Injection module and a Hybrid Visual\nCross Attention module. The former dynamically adjusts the guidance strength at\nvarious denoising stages by integrating timestep embeddings with the text\nembeddings. The latter enhances visual details for image editing, thereby\npreserving semantic consistency between the edited result and the source image.\nBy combining the VLM enhanced with fine-grained region tokens and the\ntime-dependent diffusion model, FireEdit demonstrates significant advantages in\ncomprehending editing instructions and maintaining high semantic consistency.\nExtensive experiments indicate that our approach surpasses the state-of-the-art\ninstruction-based image editing methods. Our project is available at\nhttps:\/\/zjgans.github.io\/fireedit.github.io.\n","date":"2025-03-25"}
{"id":"2503.19844","title":"A Comparative Analysis of Word Segmentation, Part-of-Speech Tagging, and\n  Named Entity Recognition for Historical Chinese Sources, 1900-1950","abstract":"  This paper compares large language models (LLMs) and traditional natural\nlanguage processing (NLP) tools for performing word segmentation,\npart-of-speech (POS) tagging, and named entity recognition (NER) on Chinese\ntexts from 1900 to 1950. Historical Chinese documents pose challenges for text\nanalysis due to their logographic script, the absence of natural word\nboundaries, and significant linguistic changes. Using a sample dataset from the\nShanghai Library Republican Journal corpus, traditional tools such as Jieba and\nspaCy are compared to LLMs, including GPT-4o, Claude 3.5, and the GLM series.\nThe results show that LLMs outperform traditional methods in all metrics,\nalbeit at considerably higher computational costs, highlighting a trade-off\nbetween accuracy and efficiency. Additionally, LLMs better handle\ngenre-specific challenges such as poetry and temporal variations (i.e.,\npre-1920 versus post-1920 texts), demonstrating that their contextual learning\ncapabilities can advance NLP approaches to historical texts by reducing the\nneed for domain-specific training data.\n","date":"2025-03-25"}
{"id":"2503.19846","title":"Attention IoU: Examining Biases in CelebA using Attention Maps","abstract":"  Computer vision models have been shown to exhibit and amplify biases across a\nwide array of datasets and tasks. Existing methods for quantifying bias in\nclassification models primarily focus on dataset distribution and model\nperformance on subgroups, overlooking the internal workings of a model. We\nintroduce the Attention-IoU (Attention Intersection over Union) metric and\nrelated scores, which use attention maps to reveal biases within a model's\ninternal representations and identify image features potentially causing the\nbiases. First, we validate Attention-IoU on the synthetic Waterbirds dataset,\nshowing that the metric accurately measures model bias. We then analyze the\nCelebA dataset, finding that Attention-IoU uncovers correlations beyond\naccuracy disparities. Through an investigation of individual attributes through\nthe protected attribute of Male, we examine the distinct ways biases are\nrepresented in CelebA. Lastly, by subsampling the training set to change\nattribute correlations, we demonstrate that Attention-IoU reveals potential\nconfounding variables not present in dataset labels.\n","date":"2025-03-25"}
{"id":"2503.19847","title":"Ab-initio simulation of excited-state potential energy surfaces with\n  transferable deep quantum Monte Carlo","abstract":"  The accurate quantum chemical calculation of excited states is a challenging\ntask, often requiring computationally demanding methods. When entire ground and\nexcited potential energy surfaces (PESs) are desired, e.g., to predict the\ninteraction of light excitation and structural changes, one is often forced to\nuse cheaper computational methods at the cost of reduced accuracy. Here we\nintroduce a novel method for the geometrically transferable optimization of\nneural network wave functions that leverages weight sharing and dynamical\nordering of electronic states. Our method enables the efficient prediction of\nground and excited-state PESs and their intersections at the highest accuracy,\ndemonstrating up to two orders of magnitude cost reduction compared to\nsingle-point calculations. We validate our approach on three challenging\nexcited-state PESs, including ethylene, the carbon dimer, and the\nmethylenimmonium cation, indicating that transferable deep-learning QMC can\npave the way towards highly accurate simulation of excited-state dynamics.\n","date":"2025-03-25"}
{"id":"2503.19848","title":"Guarding against artificial intelligence--hallucinated citations: the\n  case for full-text reference deposit","abstract":"  The tendency of generative artificial intelligence (AI) systems to\n\"hallucinate\" false information is well-known; AI-generated citations to\nnon-existent sources have made their way into the reference lists of\npeer-reviewed publications. Here, I propose a solution to this problem, taking\ninspiration from the Transparency and Openness Promotion (TOP) data sharing\nguidelines, the clash of generative AI with the American judiciary, and the\nprecedent set by submissions of prior art to the United States Patent and\nTrademark Office. Journals should require authors to submit the full text of\neach cited source along with their manuscripts, thereby preventing authors from\nciting any material whose full text they cannot produce. This solution requires\nlimited additional work on the part of authors or editors while effectively\nimmunizing journals against hallucinated references.\n","date":"2025-03-25"}
{"id":"2503.19850","title":"FALCONEye: Finding Answers and Localizing Content in ONE-hour-long\n  videos with multi-modal LLMs","abstract":"  Information retrieval in hour-long videos presents a significant challenge,\neven for state-of-the-art Vision-Language Models (VLMs), particularly when the\ndesired information is localized within a small subset of frames. Long video\ndata presents challenges for VLMs due to context window limitations and the\ndifficulty of pinpointing frames containing the answer. Our novel video agent,\nFALCONEye, combines a VLM and a Large Language Model (LLM) to search relevant\ninformation along the video, and locate the frames with the answer. FALCONEye\nnovelty relies on 1) the proposed meta-architecture, which is better suited to\ntackle hour-long videos compared to short video approaches in the\nstate-of-the-art; 2) a new efficient exploration algorithm to locate the\ninformation using short clips, captions and answer confidence; and 3) our\nstate-of-the-art VLMs calibration analysis for the answer confidence. Our agent\nis built over a small-size VLM and a medium-size LLM being accessible to run on\nstandard computational resources. We also release FALCON-Bench, a benchmark to\nevaluate long (average > 1 hour) Video Answer Search challenges, highlighting\nthe need for open-ended question evaluation. Our experiments show FALCONEye's\nsuperior performance than the state-of-the-art in FALCON-Bench, and similar or\nbetter performance in related benchmarks.\n","date":"2025-03-25"}
{"id":"2503.19851","title":"Towards Online Multi-Modal Social Interaction Understanding","abstract":"  Multimodal social interaction understanding (MMSI) is critical in human-robot\ninteraction systems. In real-world scenarios, AI agents are required to provide\nreal-time feedback. However, existing models often depend on both past and\nfuture contexts, which hinders them from applying to real-world problems. To\nbridge this gap, we propose an online MMSI setting, where the model must\nresolve MMSI tasks using only historical information, such as recorded\ndialogues and video streams. To address the challenges of missing the useful\nfuture context, we develop a novel framework, named Online-MMSI-VLM, that\nleverages two complementary strategies: multi-party conversation forecasting\nand social-aware visual prompting with multi-modal large language models.\nFirst, to enrich linguistic context, the multi-party conversation forecasting\nsimulates potential future utterances in a coarse-to-fine manner, anticipating\nupcoming speaker turns and then generating fine-grained conversational details.\nSecond, to effectively incorporate visual social cues like gaze and gesture,\nsocial-aware visual prompting highlights the social dynamics in video with\nbounding boxes and body keypoints for each person and frame. Extensive\nexperiments on three tasks and two datasets demonstrate that our method\nachieves state-of-the-art performance and significantly outperforms baseline\nmodels, indicating its effectiveness on Online-MMSI. The code and pre-trained\nmodels will be publicly released at: https:\/\/github.com\/Sampson-Lee\/OnlineMMSI.\n","date":"2025-03-25"}
{"id":"2503.19855","title":"Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time\n  Thinking","abstract":"  Recent advances in large language models (LLMs), such as OpenAI-o1 and\nDeepSeek-R1, have demonstrated the effectiveness of test-time scaling, where\nextended reasoning processes substantially enhance model performance. Despite\nthis, current models are constrained by limitations in handling long texts and\nreinforcement learning (RL) training efficiency. To address these issues, we\npropose a simple yet effective test-time scaling approach Multi-round Thinking.\nThis method iteratively refines model reasoning by leveraging previous answers\nas prompts for subsequent rounds. Extensive experiments across multiple models,\nincluding QwQ-32B and DeepSeek-R1, consistently show performance improvements\non various benchmarks such as AIME 2024, MATH-500, GPQA-diamond, and\nLiveCodeBench. For instance, the accuracy of QwQ-32B improved from 80.3% (Round\n1) to 82.1% (Round 2) on the AIME 2024 dataset, while DeepSeek-R1 showed a\nsimilar increase from 79.7% to 82.0%. These results confirm that Multi-round\nThinking is a broadly applicable, straightforward approach to achieving stable\nenhancements in model performance, underscoring its potential for future\ndevelopments in test-time scaling techniques. The key prompt: {Original\nquestion prompt} The assistant's previous answer is: <answer> {last round\nanswer} <\/answer>, and please re-answer.\n","date":"2025-03-25"}
{"id":"2503.19856","title":"Capacity-Constrained Online Learning with Delays: Scheduling Frameworks\n  and Regret Trade-offs","abstract":"  We study online learning with oblivious losses and delays under a novel\n``capacity constraint'' that limits how many past rounds can be tracked\nsimultaneously for delayed feedback. Under ``clairvoyance'' (i.e., delay\ndurations are revealed upfront each round) and\/or ``preemptibility'' (i.e., we\nhave ability to stop tracking previously chosen round feedback), we establish\nmatching upper and lower bounds (up to logarithmic terms) on achievable regret,\ncharacterizing the ``optimal capacity'' needed to match the minimax rates of\nclassical delayed online learning, which implicitly assume unlimited capacity.\nOur algorithms achieve minimax-optimal regret across all capacity levels, with\nperformance gracefully degrading under suboptimal capacity. For $K$ actions and\ntotal delay $D$ over $T$ rounds, under clairvoyance and assuming capacity $C =\n\\Omega(\\log(T))$, we achieve regret $\\widetilde{\\Theta}(\\sqrt{TK + DK\/C +\nD\\log(K)})$ for bandits and $\\widetilde{\\Theta}(\\sqrt{(D+T)\\log(K)})$ for\nfull-information feedback. When replacing clairvoyance with preemptibility, we\nrequire a known maximum delay bound $d_{\\max}$, adding\n$\\smash{\\widetilde{O}(d_{\\max})}$ to the regret. For fixed delays $d$ (i.e.,\n$D=Td$), the minimax regret is $\\Theta\\bigl(\\sqrt{TK(1+d\/C)+Td\\log(K)}\\bigr)$\nand the optimal capacity is $\\Theta(\\min\\{K\/\\log(K),d\\}\\bigr)$ in the bandit\nsetting, while in the full-information setting, the minimax regret is\n$\\Theta\\bigl(\\sqrt{T(d+1)\\log(K)}\\bigr)$ and the optimal capacity is\n$\\Theta(1)$. For round-dependent and fixed delays, our upper bounds are\nachieved using novel scheduling policies, based on Pareto-distributed proxy\ndelays and batching techniques. Crucially, our work unifies delayed bandits,\nlabel-efficient learning, and online scheduling frameworks, demonstrating that\nrobust online learning under delayed feedback is possible with surprisingly\nmodest tracking capacity.\n","date":"2025-03-25"}
{"id":"2503.19859","title":"An Overview of Low-Rank Structures in the Training and Adaptation of\n  Large Models","abstract":"  The rise of deep learning has revolutionized data processing and prediction\nin signal processing and machine learning, yet the substantial computational\ndemands of training and deploying modern large-scale deep models present\nsignificant challenges, including high computational costs and energy\nconsumption. Recent research has uncovered a widespread phenomenon in deep\nnetworks: the emergence of low-rank structures in weight matrices and learned\nrepresentations during training. These implicit low-dimensional patterns\nprovide valuable insights for improving the efficiency of training and\nfine-tuning large-scale models. Practical techniques inspired by this\nphenomenon, such as low-rank adaptation (LoRA) and training, enable significant\nreductions in computational cost while preserving model performance. In this\npaper, we present a comprehensive review of recent advances in exploiting\nlow-rank structures for deep learning and shed light on their mathematical\nfoundations. Mathematically, we present two complementary perspectives on\nunderstanding the low-rankness in deep networks: (i) the emergence of low-rank\nstructures throughout the whole optimization dynamics of gradient and (ii) the\nimplicit regularization effects that induce such low-rank structures at\nconvergence. From a practical standpoint, studying the low-rank learning\ndynamics of gradient descent offers a mathematical foundation for understanding\nthe effectiveness of LoRA in fine-tuning large-scale models and inspires\nparameter-efficient low-rank training strategies. Furthermore, the implicit\nlow-rank regularization effect helps explain the success of various masked\ntraining approaches in deep neural networks, ranging from dropout to masked\nself-supervised learning.\n","date":"2025-03-25"}
{"id":"2503.19860","title":"Unpaired Translation of Chest X-ray Images for Lung Opacity Diagnosis\n  via Adaptive Activation Masks and Cross-Domain Alignment","abstract":"  Chest X-ray radiographs (CXRs) play a pivotal role in diagnosing and\nmonitoring cardiopulmonary diseases. However, lung opacities in CXRs frequently\nobscure anatomical structures, impeding clear identification of lung borders\nand complicating the localization of pathology. This challenge significantly\nhampers segmentation accuracy and precise lesion identification, which are\ncrucial for diagnosis. To tackle these issues, our study proposes an unpaired\nCXR translation framework that converts CXRs with lung opacities into\ncounterparts without lung opacities while preserving semantic features. Central\nto our approach is the use of adaptive activation masks to selectively modify\nopacity regions in lung CXRs. Cross-domain alignment ensures translated CXRs\nwithout opacity issues align with feature maps and prediction labels from a\npre-trained CXR lesion classifier, facilitating the interpretability of the\ntranslation process. We validate our method using RSNA, MIMIC-CXR-JPG and JSRT\ndatasets, demonstrating superior translation quality through lower Frechet\nInception Distance (FID) and Kernel Inception Distance (KID) scores compared to\nexisting methods (FID: 67.18 vs. 210.4, KID: 0.01604 vs. 0.225). Evaluation on\nRSNA opacity, MIMIC acute respiratory distress syndrome (ARDS) patient CXRs and\nJSRT CXRs show our method enhances segmentation accuracy of lung borders and\nimproves lesion classification, further underscoring its potential in clinical\nsettings (RSNA: mIoU: 76.58% vs. 62.58%, Sensitivity: 85.58% vs. 77.03%; MIMIC\nARDS: mIoU: 86.20% vs. 72.07%, Sensitivity: 92.68% vs. 86.85%; JSRT: mIoU:\n91.08% vs. 85.6%, Sensitivity: 97.62% vs. 95.04%). Our approach advances CXR\nimaging analysis, especially in investigating segmentation impacts through\nimage translation techniques.\n","date":"2025-03-25"}
{"id":"2503.19867","title":"Geometric Meta-Learning via Coupled Ricci Flow: Unifying Knowledge\n  Representation and Quantum Entanglement","abstract":"  This paper establishes a unified framework integrating geometric flows with\ndeep learning through three fundamental innovations. First, we propose a\nthermodynamically coupled Ricci flow that dynamically adapts parameter space\ngeometry to loss landscape topology, formally proved to preserve isometric\nknowledge embedding (Theorem~\\ref{thm:isometric}). Second, we derive explicit\nphase transition thresholds and critical learning rates\n(Theorem~\\ref{thm:critical}) through curvature blowup analysis, enabling\nautomated singularity resolution via geometric surgery\n(Lemma~\\ref{lem:surgery}). Third, we establish an AdS\/CFT-type holographic\nduality (Theorem~\\ref{thm:ads}) between neural networks and conformal field\ntheories, providing entanglement entropy bounds for regularization design.\nExperiments demonstrate 2.1$\\times$ convergence acceleration and 63\\%\ntopological simplification while maintaining $\\mathcal{O}(N\\log N)$ complexity,\noutperforming Riemannian baselines by 15.2\\% in few-shot accuracy.\nTheoretically, we prove exponential stability (Theorem~\\ref{thm:converge})\nthrough a new Lyapunov function combining Perelman entropy with Wasserstein\ngradient flows, fundamentally advancing geometric deep learning.\n","date":"2025-03-25"}
{"id":"2503.19868","title":"GENIUS: A Generative Framework for Universal Multimodal Search","abstract":"  Generative retrieval is an emerging approach in information retrieval that\ngenerates identifiers (IDs) of target data based on a query, providing an\nefficient alternative to traditional embedding-based retrieval methods.\nHowever, existing models are task-specific and fall short of embedding-based\nretrieval in performance. This paper proposes GENIUS, a universal generative\nretrieval framework supporting diverse tasks across multiple modalities and\ndomains. At its core, GENIUS introduces modality-decoupled semantic\nquantization, transforming multimodal data into discrete IDs encoding both\nmodality and semantics. Moreover, to enhance generalization, we propose a query\naugmentation that interpolates between a query and its target, allowing GENIUS\nto adapt to varied query forms. Evaluated on the M-BEIR benchmark, it surpasses\nprior generative methods by a clear margin. Unlike embedding-based retrieval,\nGENIUS consistently maintains high retrieval speed across database size, with\ncompetitive performance across multiple benchmarks. With additional re-ranking,\nGENIUS often achieves results close to those of embedding-based methods while\npreserving efficiency.\n","date":"2025-03-25"}
{"id":"2503.19873","title":"Identification of Average Treatment Effects in Nonparametric Panel\n  Models","abstract":"  This paper studies identification of average treatment effects in a panel\ndata setting. It introduces a novel nonparametric factor model and proves\nidentification of average treatment effects. The identification proof is based\non the introduction of a consistent estimator. Underlying the proof is a result\nthat there is a consistent estimator for the expected outcome in the absence of\nthe treatment for each unit and time period; this result can be applied more\nbroadly, for example in problems of decompositions of group-level differences\nin outcomes, such as the much-studied gender wage gap.\n","date":"2025-03-25"}
{"id":"2503.19874","title":"Extensions of regret-minimization algorithm for optimal design","abstract":"  We explore extensions and applications of the regret minimization framework\nintroduced by~\\cite{design} for solving optimal experimental design problems.\nSpecifically, we incorporate the entropy regularizer into this framework,\nleading to a novel sample selection objective and a provable sample complexity\nbound that guarantees a $(1+\\epsilon)$-near optimal solution. We further extend\nthe method to handle regularized optimal design settings. As an application, we\nuse our algorithm to select a small set of representative samples from image\nclassification datasets without relying on label information. To evaluate the\nquality of the selected samples, we train a logistic regression model and\ncompare performance against several baseline sampling strategies. Experimental\nresults on MNIST, CIFAR-10, and a 50-class subset of ImageNet show that our\napproach consistently outperforms competing methods in most cases.\n","date":"2025-03-25"}
{"id":"2503.19877","title":"Scaling Evaluation-time Compute with Reasoning Models as Process\n  Evaluators","abstract":"  As language model (LM) outputs get more and more natural, it is becoming more\ndifficult than ever to evaluate their quality. Simultaneously, increasing LMs'\n\"thinking\" time through scaling test-time compute has proven an effective\ntechnique to solve challenging problems in domains such as math and code. This\nraises a natural question: can an LM's evaluation capability also be improved\nby spending more test-time compute? To answer this, we investigate employing\nreasoning models-LMs that natively generate long chain-of-thought reasoning-as\nevaluators. Specifically, we examine methods to leverage more test-time compute\nby (1) using reasoning models, and (2) prompting these models to evaluate not\nonly the response as a whole (i.e., outcome evaluation) but also assess each\nstep in the response separately (i.e., process evaluation). In experiments, we\nobserve that the evaluator's performance improves monotonically when generating\nmore reasoning tokens, similar to the trends observed in LM-based generation.\nFurthermore, we use these more accurate evaluators to rerank multiple\ngenerations, and demonstrate that spending more compute at evaluation time can\nbe as effective as using more compute at generation time in improving an LM's\nproblem-solving capability.\n","date":"2025-03-25"}
{"id":"2503.19878","title":"CausalRAG: Integrating Causal Graphs into Retrieval-Augmented Generation","abstract":"  Large language models (LLMs) have revolutionized natural language processing\n(NLP), particularly through Retrieval-Augmented Generation (RAG), which\nenhances LLM capabilities by integrating external knowledge. However,\ntraditional RAG systems face critical limitations, including disrupted\ncontextual integrity due to text chunking, and over-reliance on semantic\nsimilarity for retrieval. To address these issues, we propose CausalRAG, a\nnovel framework that incorporates causal graphs into the retrieval process. By\nconstructing and tracing causal relationships, CausalRAG preserves contextual\ncontinuity and improves retrieval precision, leading to more accurate and\ninterpretable responses. We evaluate CausalRAG against regular RAG and\ngraph-based RAG approaches, demonstrating its superiority across several\nmetrics. Our findings suggest that grounding retrieval in causal reasoning\nprovides a promising approach to knowledge-intensive tasks.\n","date":"2025-03-25"}
{"id":"2503.19881","title":"Mask$^2$DiT: Dual Mask-based Diffusion Transformer for Multi-Scene Long\n  Video Generation","abstract":"  Sora has unveiled the immense potential of the Diffusion Transformer (DiT)\narchitecture in single-scene video generation. However, the more challenging\ntask of multi-scene video generation, which offers broader applications,\nremains relatively underexplored. To bridge this gap, we propose Mask$^2$DiT, a\nnovel approach that establishes fine-grained, one-to-one alignment between\nvideo segments and their corresponding text annotations. Specifically, we\nintroduce a symmetric binary mask at each attention layer within the DiT\narchitecture, ensuring that each text annotation applies exclusively to its\nrespective video segment while preserving temporal coherence across visual\ntokens. This attention mechanism enables precise segment-level\ntextual-to-visual alignment, allowing the DiT architecture to effectively\nhandle video generation tasks with a fixed number of scenes. To further equip\nthe DiT architecture with the ability to generate additional scenes based on\nexisting ones, we incorporate a segment-level conditional mask, which\nconditions each newly generated segment on the preceding video segments,\nthereby enabling auto-regressive scene extension. Both qualitative and\nquantitative experiments confirm that Mask$^2$DiT excels in maintaining visual\nconsistency across segments while ensuring semantic alignment between each\nsegment and its corresponding text description. Our project page is\nhttps:\/\/tianhao-qi.github.io\/Mask2DiTProject.\n","date":"2025-03-25"}
{"id":"2503.19885","title":"Dynamics of Structured Complex-Valued Hopfield Neural Networks","abstract":"  In this paper, we explore the dynamics of structured complex-valued Hopfield\nneural networks (CvHNNs), which arise when the synaptic weight matrix possesses\nspecific structural properties. We begin by analyzing CvHNNs with a Hermitian\nsynaptic weight matrix and establish the existence of four-cycle dynamics in\nCvHNNs with skew-Hermitian weight matrices operating synchronously.\nFurthermore, we introduce two new classes of complex-valued matrices: braided\nHermitian and braided skew-Hermitian matrices. We demonstrate that CvHNNs\nutilizing these matrix types exhibit cycles of length eight when operating in\nfull parallel update mode. Finally, we conduct extensive computational\nexperiments on synchronous CvHNNs, exploring other synaptic weight matrix\nstructures. The findings provide a comprehensive overview of the dynamics of\nstructured CvHNNs, offering insights that may contribute to developing improved\nassociative memory models when integrated with suitable learning rules.\n","date":"2025-03-25"}
{"id":"2503.19886","title":"RCC-PFL: Robust Client Clustering under Noisy Labels in Personalized\n  Federated Learning","abstract":"  We address the problem of cluster identity estimation in a personalized\nfederated learning (PFL) setting in which users aim to learn different personal\nmodels. The backbone of effective learning in such a setting is to cluster\nusers into groups whose objectives are similar. A typical approach in the\nliterature is to achieve this by training users' data on different proposed\npersonal models and assign them to groups based on which model achieves the\nlowest value of the users' loss functions. This process is to be done\niteratively until group identities converge. A key challenge in such a setting\narises when users have noisy labeled data, which may produce misleading values\nof their loss functions, and hence lead to ineffective clustering. To overcome\nthis challenge, we propose a label-agnostic data similarity-based clustering\nalgorithm, coined RCC-PFL, with three main advantages: the cluster identity\nestimation procedure is independent from the training labels; it is a one-shot\nclustering algorithm performed prior to the training; and it requires fewer\ncommunication rounds and less computation compared to iterative-based\nclustering methods. We validate our proposed algorithm using various models and\ndatasets and show that it outperforms multiple baselines in terms of average\naccuracy and variance reduction.\n","date":"2025-03-25"}
{"id":"2503.19887","title":"A proposal for an incident regime that tracks and counters threats to\n  national security posed by AI systems","abstract":"  Recent progress in AI capabilities has heightened concerns that AI systems\ncould pose a threat to national security, for example, by making it easier for\nmalicious actors to perform cyberattacks on critical national infrastructure,\nor through loss of control of autonomous AI systems. In parallel, federal\nlegislators in the US have proposed nascent 'AI incident regimes' to identify\nand counter similar threats. In this paper, we consolidate these two trends and\npresent a proposal for a legally mandated post-deployment AI incident regie\nthat aims to counter potential national security threats from AI systems. We\nstart the paper by introducing the concept of 'security-critical' to describe\ndoctors that pose extreme risks to national security, before arguing that\n'security-critical' describes civilian nuclear power, aviation, life science\ndual-use research of concern, and frontier AI development. We then present in\ndetail our AI incident regime proposal,, justifying each component of the\nproposal by demonstrating its similarity to US domestic incident regimes in\nother 'security-critical' sectors. Finally, we sketch a hypothetical scenario\nwhere our proposed AI incident regime deals with an AI cyber incident. Our\nproposed AI incident regime is split into three phases. The first phase\nrevolves around a novel operationalization of what counts as an 'AI incident'\nand we suggest that AI providers must create a 'national security case' before\ndeploying a frontier AI system. The second and third phases spell out that AI\nproviders should notify a government agency about incidents, and that the\ngovernment agency should be involved in amending AI providers' security and\nsafety procedures, in order to counter future threats to national security. Our\nproposal is timely, given ongoing policy interest in the potential national\nsecurity threats posed by AI systems.\n","date":"2025-03-25"}
{"id":"2503.19893","title":"Visuo-Tactile Object Pose Estimation for a Multi-Finger Robot Hand with\n  Low-Resolution In-Hand Tactile Sensing","abstract":"  Accurate 3D pose estimation of grasped objects is an important prerequisite\nfor robots to perform assembly or in-hand manipulation tasks, but object\nocclusion by the robot's own hand greatly increases the difficulty of this\nperceptual task. Here, we propose that combining visual information and\nproprioception with binary, low-resolution tactile contact measurements from\nacross the interior surface of an articulated robotic hand can mitigate this\nissue. The visuo-tactile object-pose-estimation problem is formulated\nprobabilistically in a factor graph. The pose of the object is optimized to\nalign with the three kinds of measurements using a robust cost function to\nreduce the influence of visual or tactile outlier readings. The advantages of\nthe proposed approach are first demonstrated in simulation: a custom 15-DoF\nrobot hand with one binary tactile sensor per link grasps 17 YCB objects while\nobserved by an RGB-D camera. This low-resolution in-hand tactile sensing\nsignificantly improves object-pose estimates under high occlusion and also high\nvisual noise. We also show these benefits through grasping tests with a\npreliminary real version of our tactile hand, obtaining reasonable\nvisuo-tactile estimates of object pose at approximately 13.3 Hz on average.\n","date":"2025-03-25"}
{"id":"2503.19897","title":"Scaling Down Text Encoders of Text-to-Image Diffusion Models","abstract":"  Text encoders in diffusion models have rapidly evolved, transitioning from\nCLIP to T5-XXL. Although this evolution has significantly enhanced the models'\nability to understand complex prompts and generate text, it also leads to a\nsubstantial increase in the number of parameters. Despite T5 series encoders\nbeing trained on the C4 natural language corpus, which includes a significant\namount of non-visual data, diffusion models with T5 encoder do not respond to\nthose non-visual prompts, indicating redundancy in representational power.\nTherefore, it raises an important question: \"Do we really need such a large\ntext encoder?\" In pursuit of an answer, we employ vision-based knowledge\ndistillation to train a series of T5 encoder models. To fully inherit its\ncapabilities, we constructed our dataset based on three criteria: image\nquality, semantic understanding, and text-rendering. Our results demonstrate\nthe scaling down pattern that the distilled T5-base model can generate images\nof comparable quality to those produced by T5-XXL, while being 50 times smaller\nin size. This reduction in model size significantly lowers the GPU requirements\nfor running state-of-the-art models such as FLUX and SD3, making high-quality\ntext-to-image generation more accessible.\n","date":"2025-03-25"}
{"id":"2503.19900","title":"CAFe: Unifying Representation and Generation with\n  Contrastive-Autoregressive Finetuning","abstract":"  The rapid advancement of large vision-language models (LVLMs) has driven\nsignificant progress in multimodal tasks, enabling models to interpret, reason,\nand generate outputs across both visual and textual domains. While excelling in\ngenerative tasks, existing LVLMs often face limitations in tasks requiring\nhigh-fidelity representation learning, such as generating image or text\nembeddings for retrieval. Recent work has proposed finetuning LVLMs for\nrepresentational learning, but the fine-tuned model often loses its generative\ncapabilities due to the representational learning training paradigm. To address\nthis trade-off, we introduce CAFe, a contrastive-autoregressive fine-tuning\nframework that enhances LVLMs for both representation and generative tasks. By\nintegrating a contrastive objective with autoregressive language modeling, our\napproach unifies these traditionally separate tasks, achieving state-of-the-art\nresults in both multimodal retrieval and multimodal generative benchmarks,\nincluding object hallucination (OH) mitigation. CAFe establishes a novel\nframework that synergizes embedding and generative functionalities in a single\nmodel, setting a foundation for future multimodal models that excel in both\nretrieval precision and coherent output generation.\n","date":"2025-03-25"}
{"id":"2503.19901","title":"TokenHSI: Unified Synthesis of Physical Human-Scene Interactions through\n  Task Tokenization","abstract":"  Synthesizing diverse and physically plausible Human-Scene Interactions (HSI)\nis pivotal for both computer animation and embodied AI. Despite encouraging\nprogress, current methods mainly focus on developing separate controllers, each\nspecialized for a specific interaction task. This significantly hinders the\nability to tackle a wide variety of challenging HSI tasks that require the\nintegration of multiple skills, e.g., sitting down while carrying an object. To\naddress this issue, we present TokenHSI, a single, unified transformer-based\npolicy capable of multi-skill unification and flexible adaptation. The key\ninsight is to model the humanoid proprioception as a separate shared token and\ncombine it with distinct task tokens via a masking mechanism. Such a unified\npolicy enables effective knowledge sharing across skills, thereby facilitating\nthe multi-task training. Moreover, our policy architecture supports variable\nlength inputs, enabling flexible adaptation of learned skills to new scenarios.\nBy training additional task tokenizers, we can not only modify the geometries\nof interaction targets but also coordinate multiple skills to address complex\ntasks. The experiments demonstrate that our approach can significantly improve\nversatility, adaptability, and extensibility in various HSI tasks. Website:\nhttps:\/\/liangpan99.github.io\/TokenHSI\/\n","date":"2025-03-25"}
{"id":"2503.19902","title":"ICE: Intrinsic Concept Extraction from a Single Image via Diffusion\n  Models","abstract":"  The inherent ambiguity in defining visual concepts poses significant\nchallenges for modern generative models, such as the diffusion-based\nText-to-Image (T2I) models, in accurately learning concepts from a single\nimage. Existing methods lack a systematic way to reliably extract the\ninterpretable underlying intrinsic concepts. To address this challenge, we\npresent ICE, short for Intrinsic Concept Extraction, a novel framework that\nexclusively utilizes a T2I model to automatically and systematically extract\nintrinsic concepts from a single image. ICE consists of two pivotal stages. In\nthe first stage, ICE devises an automatic concept localization module to\npinpoint relevant text-based concepts and their corresponding masks within the\nimage. This critical stage streamlines concept initialization and provides\nprecise guidance for subsequent analysis. The second stage delves deeper into\neach identified mask, decomposing the object-level concepts into intrinsic\nconcepts and general concepts. This decomposition allows for a more granular\nand interpretable breakdown of visual elements. Our framework demonstrates\nsuperior performance on intrinsic concept extraction from a single image in an\nunsupervised manner. Project page: https:\/\/visual-ai.github.io\/ice\n","date":"2025-03-25"}
{"id":"2503.19903","title":"Scaling Vision Pre-Training to 4K Resolution","abstract":"  High-resolution perception of visual details is crucial for daily tasks.\nCurrent vision pre-training, however, is still limited to low resolutions\n(e.g., 378 x 378 pixels) due to the quadratic cost of processing larger images.\nWe introduce PS3 that scales CLIP-style vision pre-training to 4K resolution\nwith a near-constant cost. Instead of contrastive learning on global image\nrepresentation, PS3 is pre-trained by selectively processing local regions and\ncontrasting them with local detailed captions, enabling high-resolution\nrepresentation learning with greatly reduced computational overhead. The\npre-trained PS3 is able to both encode the global image at low resolution and\nselectively process local high-resolution regions based on their saliency or\nrelevance to a text prompt. When applying PS3 to multi-modal LLM (MLLM), the\nresulting model, named VILA-HD, significantly improves high-resolution visual\nperception compared to baselines without high-resolution vision pre-training\nsuch as AnyRes and S^2 while using up to 4.3x fewer tokens. PS3 also unlocks\nappealing scaling properties of VILA-HD, including scaling up resolution for\nfree and scaling up test-time compute for better performance. Compared to state\nof the arts, VILA-HD outperforms previous MLLMs such as NVILA and Qwen2-VL\nacross multiple benchmarks and achieves better efficiency than latest token\npruning approaches. Finally, we find current benchmarks do not require\n4K-resolution perception, which motivates us to propose 4KPro, a new benchmark\nof image QA at 4K resolution, on which VILA-HD outperforms all previous MLLMs,\nincluding a 14.5% improvement over GPT-4o, and a 3.2% improvement and 2.96x\nspeedup over Qwen2-VL.\n","date":"2025-03-25"}
{"id":"2503.19904","title":"Tracktention: Leveraging Point Tracking to Attend Videos Faster and\n  Better","abstract":"  Temporal consistency is critical in video prediction to ensure that outputs\nare coherent and free of artifacts. Traditional methods, such as temporal\nattention and 3D convolution, may struggle with significant object motion and\nmay not capture long-range temporal dependencies in dynamic scenes. To address\nthis gap, we propose the Tracktention Layer, a novel architectural component\nthat explicitly integrates motion information using point tracks, i.e.,\nsequences of corresponding points across frames. By incorporating these motion\ncues, the Tracktention Layer enhances temporal alignment and effectively\nhandles complex object motions, maintaining consistent feature representations\nover time. Our approach is computationally efficient and can be seamlessly\nintegrated into existing models, such as Vision Transformers, with minimal\nmodification. It can be used to upgrade image-only models to state-of-the-art\nvideo ones, sometimes outperforming models natively designed for video\nprediction. We demonstrate this on video depth prediction and video\ncolorization, where models augmented with the Tracktention Layer exhibit\nsignificantly improved temporal consistency compared to baselines.\n","date":"2025-03-25"}
{"id":"2503.19906","title":"AvatarArtist: Open-Domain 4D Avatarization","abstract":"  This work focuses on open-domain 4D avatarization, with the purpose of\ncreating a 4D avatar from a portrait image in an arbitrary style. We select\nparametric triplanes as the intermediate 4D representation and propose a\npractical training paradigm that takes advantage of both generative adversarial\nnetworks (GANs) and diffusion models. Our design stems from the observation\nthat 4D GANs excel at bridging images and triplanes without supervision yet\nusually face challenges in handling diverse data distributions. A robust 2D\ndiffusion prior emerges as the solution, assisting the GAN in transferring its\nexpertise across various domains. The synergy between these experts permits the\nconstruction of a multi-domain image-triplane dataset, which drives the\ndevelopment of a general 4D avatar creator. Extensive experiments suggest that\nour model, AvatarArtist, is capable of producing high-quality 4D avatars with\nstrong robustness to various source image domains. The code, the data, and the\nmodels will be made publicly available to facilitate future studies.\n","date":"2025-03-25"}
{"id":"2503.19907","title":"FullDiT: Multi-Task Video Generative Foundation Model with Full\n  Attention","abstract":"  Current video generative foundation models primarily focus on text-to-video\ntasks, providing limited control for fine-grained video content creation.\nAlthough adapter-based approaches (e.g., ControlNet) enable additional controls\nwith minimal fine-tuning, they encounter challenges when integrating multiple\nconditions, including: branch conflicts between independently trained adapters,\nparameter redundancy leading to increased computational cost, and suboptimal\nperformance compared to full fine-tuning. To address these challenges, we\nintroduce FullDiT, a unified foundation model for video generation that\nseamlessly integrates multiple conditions via unified full-attention\nmechanisms. By fusing multi-task conditions into a unified sequence\nrepresentation and leveraging the long-context learning ability of full\nself-attention to capture condition dynamics, FullDiT reduces parameter\noverhead, avoids conditions conflict, and shows scalability and emergent\nability. We further introduce FullBench for multi-task video generation\nevaluation. Experiments demonstrate that FullDiT achieves state-of-the-art\nresults, highlighting the efficacy of full-attention in complex multi-task\nvideo generation.\n","date":"2025-03-25"}
{"id":"2503.19910","title":"CoLLM: A Large Language Model for Composed Image Retrieval","abstract":"  Composed Image Retrieval (CIR) is a complex task that aims to retrieve images\nbased on a multimodal query. Typical training data consists of triplets\ncontaining a reference image, a textual description of desired modifications,\nand the target image, which are expensive and time-consuming to acquire. The\nscarcity of CIR datasets has led to zero-shot approaches utilizing synthetic\ntriplets or leveraging vision-language models (VLMs) with ubiquitous\nweb-crawled image-caption pairs. However, these methods have significant\nlimitations: synthetic triplets suffer from limited scale, lack of diversity,\nand unnatural modification text, while image-caption pairs hinder joint\nembedding learning of the multimodal query due to the absence of triplet data.\nMoreover, existing approaches struggle with complex and nuanced modification\ntexts that demand sophisticated fusion and understanding of vision and language\nmodalities. We present CoLLM, a one-stop framework that effectively addresses\nthese limitations. Our approach generates triplets on-the-fly from\nimage-caption pairs, enabling supervised training without manual annotation. We\nleverage Large Language Models (LLMs) to generate joint embeddings of reference\nimages and modification texts, facilitating deeper multimodal fusion.\nAdditionally, we introduce Multi-Text CIR (MTCIR), a large-scale dataset\ncomprising 3.4M samples, and refine existing CIR benchmarks (CIRR and\nFashion-IQ) to enhance evaluation reliability. Experimental results demonstrate\nthat CoLLM achieves state-of-the-art performance across multiple CIR benchmarks\nand settings. MTCIR yields competitive results, with up to 15% performance\nimprovement. Our refined benchmarks provide more reliable evaluation metrics\nfor CIR models, contributing to the advancement of this important field.\n","date":"2025-03-25"}
{"id":"2503.19912","title":"SuperFlow++: Enhanced Spatiotemporal Consistency for Cross-Modal Data\n  Pretraining","abstract":"  LiDAR representation learning has emerged as a promising approach to reducing\nreliance on costly and labor-intensive human annotations. While existing\nmethods primarily focus on spatial alignment between LiDAR and camera sensors,\nthey often overlook the temporal dynamics critical for capturing motion and\nscene continuity in driving scenarios. To address this limitation, we propose\nSuperFlow++, a novel framework that integrates spatiotemporal cues in both\npretraining and downstream tasks using consecutive LiDAR-camera pairs.\nSuperFlow++ introduces four key components: (1) a view consistency alignment\nmodule to unify semantic information across camera views, (2) a dense-to-sparse\nconsistency regularization mechanism to enhance feature robustness across\nvarying point cloud densities, (3) a flow-based contrastive learning approach\nthat models temporal relationships for improved scene understanding, and (4) a\ntemporal voting strategy that propagates semantic information across LiDAR\nscans to improve prediction consistency. Extensive evaluations on 11\nheterogeneous LiDAR datasets demonstrate that SuperFlow++ outperforms\nstate-of-the-art methods across diverse tasks and driving conditions.\nFurthermore, by scaling both 2D and 3D backbones during pretraining, we uncover\nemergent properties that provide deeper insights into developing scalable 3D\nfoundation models. With strong generalizability and computational efficiency,\nSuperFlow++ establishes a new benchmark for data-efficient LiDAR-based\nperception in autonomous driving. The code is publicly available at\nhttps:\/\/github.com\/Xiangxu-0103\/SuperFlow\n","date":"2025-03-25"}
{"id":"2503.19913","title":"PartRM: Modeling Part-Level Dynamics with Large Cross-State\n  Reconstruction Model","abstract":"  As interest grows in world models that predict future states from current\nobservations and actions, accurately modeling part-level dynamics has become\nincreasingly relevant for various applications. Existing approaches, such as\nPuppet-Master, rely on fine-tuning large-scale pre-trained video diffusion\nmodels, which are impractical for real-world use due to the limitations of 2D\nvideo representation and slow processing times. To overcome these challenges,\nwe present PartRM, a novel 4D reconstruction framework that simultaneously\nmodels appearance, geometry, and part-level motion from multi-view images of a\nstatic object. PartRM builds upon large 3D Gaussian reconstruction models,\nleveraging their extensive knowledge of appearance and geometry in static\nobjects. To address data scarcity in 4D, we introduce the PartDrag-4D dataset,\nproviding multi-view observations of part-level dynamics across over 20,000\nstates. We enhance the model's understanding of interaction conditions with a\nmulti-scale drag embedding module that captures dynamics at varying\ngranularities. To prevent catastrophic forgetting during fine-tuning, we\nimplement a two-stage training process that focuses sequentially on motion and\nappearance learning. Experimental results show that PartRM establishes a new\nstate-of-the-art in part-level motion learning and can be applied in\nmanipulation tasks in robotics. Our code, data, and models are publicly\navailable to facilitate future research.\n","date":"2025-03-25"}
{"id":"2503.19914","title":"Learning 3D Object Spatial Relationships from Pre-trained 2D Diffusion\n  Models","abstract":"  We present a method for learning 3D spatial relationships between object\npairs, referred to as object-object spatial relationships (OOR), by leveraging\nsynthetically generated 3D samples from pre-trained 2D diffusion models. We\nhypothesize that images synthesized by 2D diffusion models inherently capture\nplausible and realistic OOR cues, enabling efficient ways to collect a 3D\ndataset to learn OOR for various unbounded object categories. Our approach\nbegins by synthesizing diverse images that capture plausible OOR cues, which we\nthen uplift into 3D samples. Leveraging our diverse collection of plausible 3D\nsamples for the object pairs, we train a score-based OOR diffusion model to\nlearn the distribution of their relative spatial relationships. Additionally,\nwe extend our pairwise OOR to multi-object OOR by enforcing consistency across\npairwise relations and preventing object collisions. Extensive experiments\ndemonstrate the robustness of our method across various object-object spatial\nrelationships, along with its applicability to real-world 3D scene arrangement\ntasks using the OOR diffusion model.\n","date":"2025-03-25"}
{"id":"2503.19916","title":"EventFly: Event Camera Perception from Ground to the Sky","abstract":"  Cross-platform adaptation in event-based dense perception is crucial for\ndeploying event cameras across diverse settings, such as vehicles, drones, and\nquadrupeds, each with unique motion dynamics, viewpoints, and class\ndistributions. In this work, we introduce EventFly, a framework for robust\ncross-platform adaptation in event camera perception. Our approach comprises\nthree key components: i) Event Activation Prior (EAP), which identifies\nhigh-activation regions in the target domain to minimize prediction entropy,\nfostering confident, domain-adaptive predictions; ii) EventBlend, a data-mixing\nstrategy that integrates source and target event voxel grids based on\nEAP-driven similarity and density maps, enhancing feature alignment; and iii)\nEventMatch, a dual-discriminator technique that aligns features from source,\ntarget, and blended domains for better domain-invariant learning. To\nholistically assess cross-platform adaptation abilities, we introduce EXPo, a\nlarge-scale benchmark with diverse samples across vehicle, drone, and quadruped\nplatforms. Extensive experiments validate our effectiveness, demonstrating\nsubstantial gains over popular adaptation methods. We hope this work can pave\nthe way for more adaptive, high-performing event perception across diverse and\ncomplex environments.\n","date":"2025-03-25"}
{"id":"2503.19933","title":"Role of AI Innovation, Clean Energy and Digital Economy towards Net Zero\n  Emission in the United States: An ARDL Approach","abstract":"  The current paper investigates the influences of AI innovation, GDP growth,\nrenewable energy utilization, the digital economy, and industrialization on CO2\nemissions in the USA from 1990 to 2022, incorporating the ARDL methodology. The\noutcomes observe that AI innovation, renewable energy usage, and the digital\neconomy reduce CO2 emissions, while GDP expansion and industrialization\nintensify ecosystem damage. Unit root tests (ADF, PP, and DF-GLS) reveal\nheterogeneous integration levels amongst components, ensuring robustness in the\nARDL analysis. Complementary methods (FMOLS, DOLS, and CCR) validate the\nresults, enhancing their reliability. Pairwise Granger causality assessments\nidentify strong unidirectional connections within CO2 emissions and AI\ninnovation, as well as the digital economy, underscoring their significant\nroles in ecological sustainability. This research highlights the requirement\nfor strategic actions to nurture equitable growth, including advancements in AI\ntechnology, green energy adoption, and environmentally conscious industrial\ndevelopment, to improve environmental quality in the United States.\n","date":"2025-03-24"}
{"id":"2503.19936","title":"VisualQuest: A Diverse Image Dataset for Evaluating Visual Recognition\n  in LLMs","abstract":"  This paper introduces VisualQuest, a novel image dataset designed to assess\nthe ability of large language models (LLMs) to interpret non-traditional,\nstylized imagery. Unlike conventional photographic benchmarks, VisualQuest\nchallenges models with images that incorporate abstract, symbolic, and\nmetaphorical elements, requiring the integration of domain-specific knowledge\nand advanced reasoning. The dataset was meticulously curated through multiple\nstages of filtering, annotation, and standardization to ensure high quality and\ndiversity. Our evaluations using several state-of-the-art multimodal LLMs\nreveal significant performance variations that underscore the importance of\nboth factual background knowledge and inferential capabilities in visual\nrecognition tasks. VisualQuest thus provides a robust and comprehensive\nbenchmark for advancing research in multimodal reasoning and model architecture\ndesign.\n","date":"2025-03-25"}
{"id":"2503.19937","title":"Reverse Prompt: Cracking the Recipe Inside Text-to-Image Generation","abstract":"  Text-to-image generation has become increasingly popular, but achieving the\ndesired images often requires extensive prompt engineering. In this paper, we\nexplore how to decode textual prompts from reference images, a process we refer\nto as image reverse prompt engineering. This technique enables us to gain\ninsights from reference images, understand the creative processes of great\nartists, and generate impressive new images. To address this challenge, we\npropose a method known as automatic reverse prompt optimization (ARPO).\nSpecifically, our method refines an initial prompt into a high-quality prompt\nthrough an iteratively imitative gradient prompt optimization process: 1)\ngenerating a recreated image from the current prompt to instantiate its\nguidance capability; 2) producing textual gradients, which are candidate\nprompts intended to reduce the difference between the recreated image and the\nreference image; 3) updating the current prompt with textual gradients using a\ngreedy search method to maximize the CLIP similarity between prompt and\nreference image. We compare ARPO with several baseline methods, including\nhandcrafted techniques, gradient-based prompt tuning methods, image captioning,\nand data-driven selection method. Both quantitative and qualitative results\ndemonstrate that our ARPO converges quickly to generate high-quality reverse\nprompts. More importantly, we can easily create novel images with diverse\nstyles and content by directly editing these reverse prompts. Code will be made\npublicly available.\n","date":"2025-03-25"}
{"id":"2503.19939","title":"Continual Learning With Quasi-Newton Methods","abstract":"  Catastrophic forgetting remains a major challenge when neural networks learn\ntasks sequentially. Elastic Weight Consolidation (EWC) attempts to address this\nproblem by introducing a Bayesian-inspired regularization loss to preserve\nknowledge of previously learned tasks. However, EWC relies on a Laplace\napproximation where the Hessian is simplified to the diagonal of the Fisher\ninformation matrix, assuming uncorrelated model parameters. This overly\nsimplistic assumption often leads to poor Hessian estimates, limiting its\neffectiveness. To overcome this limitation, we introduce Continual Learning\nwith Sampled Quasi-Newton (CSQN), which leverages Quasi-Newton methods to\ncompute more accurate Hessian approximations. CSQN captures parameter\ninteractions beyond the diagonal without requiring architecture-specific\nmodifications, making it applicable across diverse tasks and architectures.\nExperimental results across four benchmarks demonstrate that CSQN consistently\noutperforms EWC and other state-of-the-art baselines, including rehearsal-based\nmethods. CSQN reduces EWC's forgetting by 50 percent and improves its\nperformance by 8 percent on average. Notably, CSQN achieves superior results on\nthree out of four benchmarks, including the most challenging scenarios,\nhighlighting its potential as a robust solution for continual learning.\n","date":"2025-03-25"}
{"id":"2503.19940","title":"FuXi-RTM: A Physics-Guided Prediction Framework with Radiative Transfer\n  Modeling","abstract":"  Similar to conventional video generation, current deep learning-based weather\nprediction frameworks often lack explicit physical constraints, leading to\nunphysical outputs that limit their reliability for operational forecasting.\nAmong various physical processes requiring proper representation, radiation\nplays a fundamental role as it drives Earth's weather and climate systems.\nHowever, accurate simulation of radiative transfer processes remains\nchallenging for traditional numerical weather prediction (NWP) models due to\ntheir inherent complexity and high computational costs. Here, we propose\nFuXi-RTM, a hybrid physics-guided deep learning framework designed to enhance\nweather forecast accuracy while enforcing physical consistency. FuXi-RTM\nintegrates a primary forecasting model (FuXi) with a fixed deep learning-based\nradiative transfer model (DLRTM) surrogate that efficiently replaces\nconventional radiation parameterization schemes. This represents the first deep\nlearning-based weather forecasting framework to explicitly incorporate physical\nprocess modeling. Evaluated over a comprehensive 5-year dataset, FuXi-RTM\noutperforms its unconstrained counterpart in 88.51% of 3320 variable and lead\ntime combinations, with improvements in radiative flux predictions. By\nincorporating additional physical processes, FuXi-RTM paves the way for\nnext-generation weather forecasting systems that are both accurate and\nphysically consistent.\n","date":"2025-03-25"}
{"id":"2503.19941","title":"Body Discovery of Embodied AI","abstract":"  In the pursuit of realizing artificial general intelligence (AGI), the\nimportance of embodied artificial intelligence (AI) becomes increasingly\napparent. Following this trend, research integrating robots with AGI has become\nprominent. As various kinds of embodiments have been designed, adaptability to\ndiverse embodiments will become important to AGI. We introduce a new challenge,\ntermed \"Body Discovery of Embodied AI\", focusing on tasks of recognizing\nembodiments and summarizing neural signal functionality. The challenge\nencompasses the precise definition of an AI body and the intricate task of\nidentifying embodiments in dynamic environments, where conventional approaches\noften prove inadequate. To address these challenges, we apply causal inference\nmethod and evaluate it by developing a simulator tailored for testing\nalgorithms with virtual environments. Finally, we validate the efficacy of our\nalgorithms through empirical testing, demonstrating their robust performance in\nvarious scenarios based on virtual environments.\n","date":"2025-03-25"}
{"id":"2503.19942","title":"A stochastic gradient descent algorithm with random search directions","abstract":"  Stochastic coordinate descent algorithms are efficient methods in which each\niterate is obtained by fixing most coordinates at their values from the current\niteration, and approximately minimizing the objective with respect to the\nremaining coordinates. However, this approach is usually restricted to\ncanonical basis vectors of $\\mathbb{R}^d$. In this paper, we develop a new\nclass of stochastic gradient descent algorithms with random search directions\nwhich uses the directional derivative of the gradient estimate following more\ngeneral random vectors. We establish the almost sure convergence of these\nalgorithms with decreasing step. We further investigate their central limit\ntheorem and pay particular attention to analyze the impact of the search\ndistributions on the asymptotic covariance matrix. We also provide\nnon-asymptotic $\\mathbb{L}^p$ rates of convergence.\n","date":"2025-03-25"}
{"id":"2503.19943","title":"A Spatiotemporal Radar-Based Precipitation Model for Water Level\n  Prediction and Flood Forecasting","abstract":"  Study Region: Goslar and G\\\"ottingen, Lower Saxony, Germany. Study Focus: In\nJuly 2017, the cities of Goslar and G\\\"ottingen experienced severe flood events\ncharacterized by short warning time of only 20 minutes, resulting in extensive\nregional flooding and significant damage. This highlights the critical need for\na more reliable and timely flood forecasting system. This paper presents a\ncomprehensive study on the impact of radar-based precipitation data on\nforecasting river water levels in Goslar. Additionally, the study examines how\nprecipitation influences water level forecasts in G\\\"ottingen. The analysis\nintegrates radar-derived spatiotemporal precipitation patterns with\nhydrological sensor data obtained from ground stations to evaluate the\neffectiveness of this approach in improving flood prediction capabilities. New\nHydrological Insights for the Region: A key innovation in this paper is the use\nof residual-based modeling to address the non-linearity between precipitation\nimages and water levels, leading to a Spatiotemporal Radar-based Precipitation\nModel with residuals (STRPMr). Unlike traditional hydrological models, our\napproach does not rely on upstream data, making it independent of additional\nhydrological inputs. This independence enhances its adaptability and allows for\nbroader applicability in other regions with RADOLAN precipitation. The deep\nlearning architecture integrates (2+1)D convolutional neural networks for\nspatial and temporal feature extraction with LSTM for timeseries forecasting.\nThe results demonstrate the potential of the STRPMr for capturing extreme\nevents and more accurate flood forecasting.\n","date":"2025-03-25"}
{"id":"2503.19945","title":"Optimizing Breast Cancer Detection in Mammograms: A Comprehensive Study\n  of Transfer Learning, Resolution Reduction, and Multi-View Classification","abstract":"  This study explores open questions in the application of machine learning for\nbreast cancer detection in mammograms. Current approaches often employ a\ntwo-stage transfer learning process: first, adapting a backbone model trained\non natural images to develop a patch classifier, which is then used to create a\nsingle-view whole-image classifier. Additionally, many studies leverage both\nmammographic views to enhance model performance. In this work, we\nsystematically investigate five key questions: (1) Is the intermediate patch\nclassifier essential for optimal performance? (2) Do backbone models that excel\nin natural image classification consistently outperform others on mammograms?\n(3) When reducing mammogram resolution for GPU processing, does the\nlearn-to-resize technique outperform conventional methods? (4) Does\nincorporating both mammographic views in a two-view classifier significantly\nimprove detection accuracy? (5) How do these findings vary when analyzing\nlow-quality versus high-quality mammograms? By addressing these questions, we\ndeveloped models that outperform previous results for both single-view and\ntwo-view classifiers. Our findings provide insights into model architecture and\ntransfer learning strategies contributing to more accurate and efficient\nmammogram analysis.\n","date":"2025-03-25"}
{"id":"2503.19947","title":"Vanishing Depth: A Depth Adapter with Positional Depth Encoding for\n  Generalized Image Encoders","abstract":"  Generalized metric depth understanding is critical for precise vision-guided\nrobotics, which current state-of-the-art (SOTA) vision-encoders do not support.\nTo address this, we propose Vanishing Depth, a self-supervised training\napproach that extends pretrained RGB encoders to incorporate and align metric\ndepth into their feature embeddings. Based on our novel positional depth\nencoding, we enable stable depth density and depth distribution invariant\nfeature extraction. We achieve performance improvements and SOTA results across\na spectrum of relevant RGBD downstream tasks - without the necessity of\nfinetuning the encoder. Most notably, we achieve 56.05 mIoU on SUN-RGBD\nsegmentation, 88.3 RMSE on Void's depth completion, and 83.8 Top 1 accuracy on\nNYUv2 scene classification. In 6D-object pose estimation, we outperform our\npredecessors of DinoV2, EVA-02, and Omnivore and achieve SOTA results for\nnon-finetuned encoders in several related RGBD downstream tasks.\n","date":"2025-03-25"}
{"id":"2503.19948","title":"Test-Time Reasoning Through Visual Human Preferences with VLMs and Soft\n  Rewards","abstract":"  Can Visual Language Models (VLMs) effectively capture human visual\npreferences? This work addresses this question by training VLMs to think about\npreferences at test time, employing reinforcement learning methods inspired by\nDeepSeek R1 and OpenAI O1. Using datasets such as ImageReward and Human\nPreference Score v2 (HPSv2), our models achieve accuracies of 64.9% on the\nImageReward test set (trained on ImageReward official split) and 65.4% on HPSv2\n(trained on approximately 25% of its data). These results match traditional\nencoder-based models while providing transparent reasoning and enhanced\ngeneralization. This approach allows to use not only rich VLM world knowledge,\nbut also its potential to think, yielding interpretable outcomes that help\ndecision-making processes. By demonstrating that human visual preferences\nreasonable by current VLMs, we introduce efficient soft-reward strategies for\nimage ranking, outperforming simplistic selection or scoring methods. This\nreasoning capability enables VLMs to rank arbitrary images-regardless of aspect\nratio or complexity-thereby potentially amplifying the effectiveness of visual\nPreference Optimization. By reducing the need for extensive markup while\nimproving reward generalization and explainability, our findings can be a\nstrong mile-stone that will enhance text-to-vision models even further.\n","date":"2025-03-25"}
{"id":"2503.19949","title":"Automated Video-EEG Analysis in Epilepsy Studies: Advances and\n  Challenges","abstract":"  Epilepsy is typically diagnosed through electroencephalography (EEG) and\nlong-term video-EEG (vEEG) monitoring. The manual analysis of vEEG recordings\nis time-consuming, necessitating automated tools for seizure detection. Recent\nadvancements in machine learning have shown promise in real-time seizure\ndetection and prediction using EEG and video data. However, diversity of\nseizure symptoms, markup ambiguities, and limited availability of multimodal\ndatasets hinder progress. This paper reviews the latest developments in\nautomated video-EEG analysis and discusses the integration of multimodal data.\nWe also propose a novel pipeline for treatment effect estimation from vEEG data\nusing concept-based learning, offering a pathway for future research in this\ndomain.\n","date":"2025-03-25"}
{"id":"2503.19950","title":"LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior\n  Accuracy Preservation","abstract":"  We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV\nCache in large language model (LLM) inference, delivering substantial memory\nsavings while preserving superior performance. Previous methods either assume\nthat later tokens are more important or attempt to predict important tokens\nbased on earlier attention patterns. Both approaches, however, can result in\nperformance bottlenecks or frequent mispredictions.\n  LogQuant takes a different approach. By applying a log-based filtering\nmechanism, it selectively compresses the KV Cache across the entire context,\nachieving better performance with the same or even reduced memory footprint\ncompared to existing methods. In benchmark tests, it enhances throughput by 25%\nand boosts batch size by 60% without increasing memory consumption. For\nchallenging tasks such as Math and Code Completion, LogQuant improves accuracy\nby 40% to 200% at the same compression ratio, outperforming comparable\ntechniques.LogQuant integrates effortlessly with popular inference frameworks\nlike Python's transformers library. Implementation can be available in\nhttps:\/\/github.com\/Concyclics\/LogQuantKV.\n","date":"2025-03-25"}
{"id":"2503.19951","title":"ACVUBench: Audio-Centric Video Understanding Benchmark","abstract":"  Audio often serves as an auxiliary modality in video understanding tasks of\naudio-visual large language models (LLMs), merely assisting in the\ncomprehension of visual information. However, a thorough understanding of\nvideos significantly depends on auditory information, as audio offers critical\ncontext, emotional cues, and semantic meaning that visual data alone often\nlacks. This paper proposes an audio-centric video understanding benchmark\n(ACVUBench) to evaluate the video comprehension capabilities of multimodal LLMs\nwith a particular focus on auditory information. Specifically, ACVUBench\nincorporates 2,662 videos spanning 18 different domains with rich auditory\ninformation, together with over 13k high-quality human annotated or validated\nquestion-answer pairs. Moreover, ACVUBench introduces a suite of carefully\ndesigned audio-centric tasks, holistically testing the understanding of both\naudio content and audio-visual interactions in videos. A thorough evaluation\nacross a diverse range of open-source and proprietary multimodal LLMs is\nperformed, followed by the analyses of deficiencies in audio-visual LLMs. Demos\nare available at https:\/\/github.com\/lark-png\/ACVUBench.\n","date":"2025-03-25"}
{"id":"2503.19953","title":"Self-Supervised Learning of Motion Concepts by Optimizing\n  Counterfactuals","abstract":"  Estimating motion in videos is an essential computer vision problem with many\ndownstream applications, including controllable video generation and robotics.\nCurrent solutions are primarily trained using synthetic data or require tuning\nof situation-specific heuristics, which inherently limits these models'\ncapabilities in real-world contexts. Despite recent developments in large-scale\nself-supervised learning from videos, leveraging such representations for\nmotion estimation remains relatively underexplored. In this work, we develop\nOpt-CWM, a self-supervised technique for flow and occlusion estimation from a\npre-trained next-frame prediction model. Opt-CWM works by learning to optimize\ncounterfactual probes that extract motion information from a base video model,\navoiding the need for fixed heuristics while training on unrestricted video\ninputs. We achieve state-of-the-art performance for motion estimation on\nreal-world videos while requiring no labeled data.\n","date":"2025-03-25"}
{"id":"2503.19976","title":"Thin-Shell-SfT: Fine-Grained Monocular Non-rigid 3D Surface Tracking\n  with Neural Deformation Fields","abstract":"  3D reconstruction of highly deformable surfaces (e.g. cloths) from monocular\nRGB videos is a challenging problem, and no solution provides a consistent and\naccurate recovery of fine-grained surface details. To account for the ill-posed\nnature of the setting, existing methods use deformation models with\nstatistical, neural, or physical priors. They also predominantly rely on\nnonadaptive discrete surface representations (e.g. polygonal meshes), perform\nframe-by-frame optimisation leading to error propagation, and suffer from poor\ngradients of the mesh-based differentiable renderers. Consequently, fine\nsurface details such as cloth wrinkles are often not recovered with the desired\naccuracy. In response to these limitations, we propose ThinShell-SfT, a new\nmethod for non-rigid 3D tracking that represents a surface as an implicit and\ncontinuous spatiotemporal neural field. We incorporate continuous thin shell\nphysics prior based on the Kirchhoff-Love model for spatial regularisation,\nwhich starkly contrasts the discretised alternatives of earlier works. Lastly,\nwe leverage 3D Gaussian splatting to differentiably render the surface into\nimage space and optimise the deformations based on analysis-bysynthesis\nprinciples. Our Thin-Shell-SfT outperforms prior works qualitatively and\nquantitatively thanks to our continuous surface formulation in conjunction with\na specially tailored simulation prior and surface-induced 3D Gaussians. See our\nproject page at https:\/\/4dqv.mpiinf.mpg.de\/ThinShellSfT.\n","date":"2025-03-25"}
{"id":"2503.19979","title":"Untangling the Influence of Typology, Data and Model Architecture on\n  Ranking Transfer Languages for Cross-Lingual POS Tagging","abstract":"  Cross-lingual transfer learning is an invaluable tool for overcoming data\nscarcity, yet selecting a suitable transfer language remains a challenge. The\nprecise roles of linguistic typology, training data, and model architecture in\ntransfer language choice are not fully understood. We take a holistic approach,\nexamining how both dataset-specific and fine-grained typological features\ninfluence transfer language selection for part-of-speech tagging, considering\ntwo different sources for morphosyntactic features. While previous work\nexamines these dynamics in the context of bilingual biLSTMS, we extend our\nanalysis to a more modern transfer learning pipeline: zero-shot prediction with\npretrained multilingual models. We train a series of transfer language ranking\nsystems and examine how different feature inputs influence ranker performance\nacross architectures. Word overlap, type-token ratio, and genealogical distance\nemerge as top features across all architectures. Our findings reveal that a\ncombination of typological and dataset-dependent features leads to the best\nrankings, and that good performance can be obtained with either feature group\non its own.\n","date":"2025-03-25"}
{"id":"2503.19982","title":"SLIP: Spoof-Aware One-Class Face Anti-Spoofing with Language Image\n  Pretraining","abstract":"  Face anti-spoofing (FAS) plays a pivotal role in ensuring the security and\nreliability of face recognition systems. With advancements in vision-language\npretrained (VLP) models, recent two-class FAS techniques have leveraged the\nadvantages of using VLP guidance, while this potential remains unexplored in\none-class FAS methods. The one-class FAS focuses on learning intrinsic liveness\nfeatures solely from live training images to differentiate between live and\nspoof faces. However, the lack of spoof training data can lead one-class FAS\nmodels to inadvertently incorporate domain information irrelevant to the\nlive\/spoof distinction (e.g., facial content), causing performance degradation\nwhen tested with a new application domain. To address this issue, we propose a\nnovel framework called Spoof-aware one-class face anti-spoofing with Language\nImage Pretraining (SLIP). Given that live faces should ideally not be obscured\nby any spoof-attack-related objects (e.g., paper, or masks) and are assumed to\nyield zero spoof cue maps, we first propose an effective language-guided spoof\ncue map estimation to enhance one-class FAS models by simulating whether the\nunderlying faces are covered by attack-related objects and generating\ncorresponding nonzero spoof cue maps. Next, we introduce a novel prompt-driven\nliveness feature disentanglement to alleviate live\/spoof-irrelative domain\nvariations by disentangling live\/spoof-relevant and domain-dependent\ninformation. Finally, we design an effective augmentation strategy by fusing\nlatent features from live images and spoof prompts to generate spoof-like image\nfeatures and thus diversify latent spoof features to facilitate the learning of\none-class FAS. Our extensive experiments and ablation studies support that SLIP\nconsistently outperforms previous one-class FAS methods.\n","date":"2025-03-25"}
{"id":"2503.19988","title":"ExCoT: Optimizing Reasoning for Text-to-SQL with Execution Feedback","abstract":"  Text-to-SQL demands precise reasoning to convert natural language questions\ninto structured queries. While large language models (LLMs) excel in many\nreasoning tasks, their ability to leverage Chain-of-Thought (CoT) reasoning for\ntext-to-SQL remains underexplored. We identify critical limitations: zero-shot\nCoT offers minimal gains, and Direct Preference Optimization (DPO) applied\nwithout CoT yields marginal improvements. We propose ExCoT, a novel framework\nthat iteratively optimizes open-source LLMs by combining CoT reasoning with\noff-policy and on-policy DPO, relying solely on execution accuracy as feedback.\nThis approach eliminates the need for reward models or human-annotated\npreferences.\n  Our experimental results demonstrate significant performance gains: ExCoT\nimproves execution accuracy on BIRD dev set from 57.37% to 68.51% and on Spider\ntest set from 78.81% to 86.59% for LLaMA-3 70B, with Qwen-2.5-Coder\ndemonstrating similar improvements. Our best model achieves state-of-the-art\nperformance in the single-model setting on both BIRD and Spider datasets,\nnotably achieving 68.53% on the BIRD test set.\n","date":"2025-03-25"}
{"id":"2503.19990","title":"LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?","abstract":"  Multi-step spatial reasoning entails understanding and reasoning about\nspatial relationships across multiple sequential steps, which is crucial for\ntackling complex real-world applications, such as robotic manipulation,\nautonomous navigation, and automated assembly. To assess how well current\nMultimodal Large Language Models (MLLMs) have acquired this fundamental\ncapability, we introduce \\textbf{LEGO-Puzzles}, a scalable benchmark designed\nto evaluate both \\textbf{spatial understanding} and \\textbf{sequential\nreasoning} in MLLMs through LEGO-based tasks. LEGO-Puzzles consists of 1,100\ncarefully curated visual question-answering (VQA) samples spanning 11 distinct\ntasks, ranging from basic spatial understanding to complex multi-step\nreasoning. Based on LEGO-Puzzles, we conduct a comprehensive evaluation of\nstate-of-the-art MLLMs and uncover significant limitations in their spatial\nreasoning capabilities: even the most powerful MLLMs can answer only about half\nof the test cases, whereas human participants achieve over 90\\% accuracy. In\naddition to VQA tasks, we evaluate MLLMs' abilities to generate LEGO images\nfollowing assembly illustrations. Our experiments show that only\nGemini-2.0-Flash and GPT-4o exhibit a limited ability to follow these\ninstructions, while other MLLMs either replicate the input image or generate\ncompletely irrelevant outputs. Overall, LEGO-Puzzles exposes critical\ndeficiencies in existing MLLMs' spatial understanding and sequential reasoning\ncapabilities, and underscores the need for further advancements in multimodal\nspatial reasoning.\n","date":"2025-03-25"}
{"id":"2503.20000","title":"The Coralscapes Dataset: Semantic Scene Understanding in Coral Reefs","abstract":"  Coral reefs are declining worldwide due to climate change and local\nstressors. To inform effective conservation or restoration, monitoring at the\nhighest possible spatial and temporal resolution is necessary. Conventional\ncoral reef surveying methods are limited in scalability due to their reliance\non expert labor time, motivating the use of computer vision tools to automate\nthe identification and abundance estimation of live corals from images.\nHowever, the design and evaluation of such tools has been impeded by the lack\nof large high quality datasets. We release the Coralscapes dataset, the first\ngeneral-purpose dense semantic segmentation dataset for coral reefs, covering\n2075 images, 39 benthic classes, and 174k segmentation masks annotated by\nexperts. Coralscapes has a similar scope and the same structure as the widely\nused Cityscapes dataset for urban scene segmentation, allowing benchmarking of\nsemantic segmentation models in a new challenging domain which requires expert\nknowledge to annotate. We benchmark a wide range of semantic segmentation\nmodels, and find that transfer learning from Coralscapes to existing smaller\ndatasets consistently leads to state-of-the-art performance. Coralscapes will\ncatalyze research on efficient, scalable, and standardized coral reef surveying\nmethods based on computer vision, and holds the potential to streamline the\ndevelopment of underwater ecological robotics.\n","date":"2025-03-25"}
{"id":"2503.20001","title":"Unsupervised Learning for Quadratic Assignment","abstract":"  We introduce PLUME search, a data-driven framework that enhances search\nefficiency in combinatorial optimization through unsupervised learning. Unlike\nsupervised or reinforcement learning, PLUME search learns directly from problem\ninstances using a permutation-based loss with a non-autoregressive approach. We\nevaluate its performance on the quadratic assignment problem, a fundamental\nNP-hard problem that encompasses various combinatorial optimization problems.\nExperimental results demonstrate that PLUME search consistently improves\nsolution quality. Furthermore, we study the generalization behavior and show\nthat the learned model generalizes across different densities and sizes.\n","date":"2025-03-25"}
{"id":"2503.20007","title":"Low-resource Machine Translation for Code-switched Kazakh-Russian\n  Language Pair","abstract":"  Machine translation for low resource language pairs is a challenging task.\nThis task could become extremely difficult once a speaker uses code switching.\nWe propose a method to build a machine translation model for code-switched\nKazakh-Russian language pair with no labeled data. Our method is basing on\ngeneration of synthetic data. Additionally, we present the first codeswitching\nKazakh-Russian parallel corpus and the evaluation results, which include a\nmodel achieving 16.48 BLEU almost reaching an existing commercial system and\nbeating it by human evaluation.\n","date":"2025-03-25"}
{"id":"2503.20011","title":"Hyperdimensional Uncertainty Quantification for Multimodal Uncertainty\n  Fusion in Autonomous Vehicles Perception","abstract":"  Uncertainty Quantification (UQ) is crucial for ensuring the reliability of\nmachine learning models deployed in real-world autonomous systems. However,\nexisting approaches typically quantify task-level output prediction uncertainty\nwithout considering epistemic uncertainty at the multimodal feature fusion\nlevel, leading to sub-optimal outcomes. Additionally, popular uncertainty\nquantification methods, e.g., Bayesian approximations, remain challenging to\ndeploy in practice due to high computational costs in training and inference.\nIn this paper, we propose HyperDUM, a novel deterministic uncertainty method\n(DUM) that efficiently quantifies feature-level epistemic uncertainty by\nleveraging hyperdimensional computing. Our method captures the channel and\nspatial uncertainties through channel and patch -wise projection and bundling\ntechniques respectively. Multimodal sensor features are then adaptively\nweighted to mitigate uncertainty propagation and improve feature fusion. Our\nevaluations show that HyperDUM on average outperforms the state-of-the-art\n(SOTA) algorithms by up to 2.01%\/1.27% in 3D Object Detection and up to 1.29%\nimprovement over baselines in semantic segmentation tasks under various types\nof uncertainties. Notably, HyperDUM requires 2.36x less Floating Point\nOperations and up to 38.30x less parameters than SOTA methods, providing an\nefficient solution for real-world autonomous systems.\n","date":"2025-03-25"}
{"id":"2503.20018","title":"Experience Replay Addresses Loss of Plasticity in Continual Learning","abstract":"  Loss of plasticity is one of the main challenges in continual learning with\ndeep neural networks, where neural networks trained via backpropagation\ngradually lose their ability to adapt to new tasks and perform significantly\nworse than their freshly initialized counterparts. The main contribution of\nthis paper is to propose a new hypothesis that experience replay addresses the\nloss of plasticity in continual learning. Here, experience replay is a form of\nmemory. We provide supporting evidence for this hypothesis. In particular, we\ndemonstrate in multiple different tasks, including regression, classification,\nand policy evaluation, that by simply adding an experience replay and\nprocessing the data in the experience replay with Transformers, the loss of\nplasticity disappears. Notably, we do not alter any standard components of deep\nlearning. For example, we do not change backpropagation. We do not modify the\nactivation functions. And we do not use any regularization. We conjecture that\nexperience replay and Transformers can address the loss of plasticity because\nof the in-context learning phenomenon.\n","date":"2025-03-25"}
{"id":"2503.20027","title":"A scalable gene network model of regulatory dynamics in single cells","abstract":"  Single-cell data provide high-dimensional measurements of the transcriptional\nstates of cells, but extracting insights into the regulatory functions of\ngenes, particularly identifying transcriptional mechanisms affected by\nbiological perturbations, remains a challenge. Many perturbations induce\ncompensatory cellular responses, making it difficult to distinguish direct from\nindirect effects on gene regulation. Modeling how gene regulatory functions\nshape the temporal dynamics of these responses is key to improving our\nunderstanding of biological perturbations. Dynamical models based on\ndifferential equations offer a principled way to capture transcriptional\ndynamics, but their application to single-cell data has been hindered by\ncomputational constraints, stochasticity, sparsity, and noise. Existing methods\neither rely on low-dimensional representations or make strong simplifying\nassumptions, limiting their ability to model transcriptional dynamics at scale.\nWe introduce a Functional and Learnable model of Cell dynamicS, FLeCS, that\nincorporates gene network structure into coupled differential equations to\nmodel gene regulatory functions. Given (pseudo)time-series single-cell data,\nFLeCS accurately infers cell dynamics at scale, provides improved functional\ninsights into transcriptional mechanisms perturbed by gene knockouts, both in\nmyeloid differentiation and K562 Perturb-seq experiments, and simulates\nsingle-cell trajectories of A549 cells following small-molecule perturbations.\n","date":"2025-03-25"}
{"id":"2503.20028","title":"OmniNova:A General Multimodal Agent Framework","abstract":"  The integration of Large Language Models (LLMs) with specialized tools\npresents new opportunities for intelligent automation systems. However,\norchestrating multiple LLM-driven agents to tackle complex tasks remains\nchallenging due to coordination difficulties, inefficient resource utilization,\nand inconsistent information flow. We present OmniNova, a modular multi-agent\nautomation framework that combines language models with specialized tools such\nas web search, crawling, and code execution capabilities. OmniNova introduces\nthree key innovations: (1) a hierarchical multi-agent architecture with\ndistinct coordinator, planner, supervisor, and specialist agents; (2) a dynamic\ntask routing mechanism that optimizes agent deployment based on task\ncomplexity; and (3) a multi-layered LLM integration system that allocates\nappropriate models to different cognitive requirements. Our evaluations across\n50 complex tasks in research, data analysis, and web interaction domains\ndemonstrate that OmniNova outperforms existing frameworks in task completion\nrate (87\\% vs. baseline 62\\%), efficiency (41\\% reduced token usage), and\nresult quality (human evaluation score of 4.2\/5 vs. baseline 3.1\/5). We\ncontribute both a theoretical framework for multi-agent system design and an\nopen-source implementation that advances the state-of-the-art in LLM-based\nautomation systems.\n","date":"2025-03-25"}
{"id":"2503.20036","title":"BugCraft: End-to-End Crash Bug Reproduction Using LLM Agents in\n  Minecraft","abstract":"  Reproducing game bugs, in our case crash bugs in continuously evolving games\nlike Minecraft, is a notoriously manual, time-consuming, and challenging\nprocess to automate. Despite the success of LLM-driven bug reproduction in\nother software domains, games, with their complex interactive environments,\nremain largely unaddressed. This paper introduces BugCraft, a novel end-to-end\nframework designed to automate the reproduction of crash bugs in Minecraft\ndirectly from user-submitted bug reports, addressing the critical gap in\nautomated game bug reproduction. BugCraft employs a two-stage approach: first,\na Step Synthesizer leverages LLMs and Minecraft Wiki knowledge to transform bug\nreports into high-quality, structured steps to reproduce (S2R). Second, an\nAction Model, powered by a vision-based LLM agent (GPT-4o) and a custom macro\nAPI, executes these S2R steps within Minecraft to trigger the reported crash.\nTo facilitate evaluation, we introduce BugCraft-Bench, a curated dataset of\nMinecraft crash bug reports. Evaluated on BugCraft-Bench, our framework\nsuccessfully reproduced 30.23% of crash bugs end-to-end. The Step Synthesizer\ndemonstrated a 66.28% accuracy in generating correct bug reproduction plans,\nhighlighting its effectiveness in interpreting and structuring bug report\ninformation. BugCraft demonstrates the feasibility of automated reproduction of\ncrash bugs in complex game environments using LLMs, opening promising avenues\nfor game testing and development. The framework and the BugCraft-Bench dataset\npave the way for future research in automated game bug analysis and hold\npotential for generalization to other interactive game platforms. Finally, we\nmake our code open at https:\/\/bugcraft2025.github.io\/\n","date":"2025-03-25"}
{"id":"2503.20047","title":"Med3DVLM: An Efficient Vision-Language Model for 3D Medical Image\n  Analysis","abstract":"  Vision-language models (VLMs) have shown promise in 2D medical image\nanalysis, but extending them to 3D remains challenging due to the high\ncomputational demands of volumetric data and the difficulty of aligning 3D\nspatial features with clinical text. We present Med3DVLM, a 3D VLM designed to\naddress these challenges through three key innovations: (1) DCFormer, an\nefficient encoder that uses decomposed 3D convolutions to capture fine-grained\nspatial features at scale; (2) SigLIP, a contrastive learning strategy with\npairwise sigmoid loss that improves image-text alignment without relying on\nlarge negative batches; and (3) a dual-stream MLP-Mixer projector that fuses\nlow- and high-level image features with text embeddings for richer multi-modal\nrepresentations. We evaluate our model on the M3D dataset, which includes\nradiology reports and VQA data for 120,084 3D medical images. Results show that\nMed3DVLM achieves superior performance across multiple benchmarks. For\nimage-text retrieval, it reaches 61.00% R@1 on 2,000 samples, significantly\noutperforming the current state-of-the-art M3D model (19.10%). For report\ngeneration, it achieves a METEOR score of 36.42% (vs. 14.38%). In open-ended\nvisual question answering (VQA), it scores 36.76% METEOR (vs. 33.58%), and in\nclosed-ended VQA, it achieves 79.95% accuracy (vs. 75.78%). These results\nhighlight Med3DVLM's ability to bridge the gap between 3D imaging and language,\nenabling scalable, multi-task reasoning across clinical applications. Our code\nis publicly available at https:\/\/github.com\/mirthAI\/Med3DVLM.\n","date":"2025-03-25"}
{"id":"2503.20049","title":"Deep Learning Approaches for Blood Disease Diagnosis Across\n  Hematopoietic Lineages","abstract":"  We present a foundation modeling framework that leverages deep learning to\nuncover latent genetic signatures across the hematopoietic hierarchy. Our\napproach trains a fully connected autoencoder on multipotent progenitor cells,\nreducing over 20,000 gene features to a 256-dimensional latent space that\ncaptures predictive information for both progenitor and downstream\ndifferentiated cells such as monocytes and lymphocytes. We validate the quality\nof these embeddings by training feed-forward, transformer, and graph\nconvolutional architectures for blood disease diagnosis tasks. We also explore\nzero-shot prediction using a progenitor disease state classification model to\nclassify downstream cell conditions. Our models achieve greater than 95%\naccuracy for multi-class classification, and in the zero-shot setting, we\nachieve greater than 0.7 F1-score on the binary classification task. Future\nwork should improve embeddings further to increase robustness on lymphocyte\nclassification specifically.\n","date":"2025-03-25"}
{"id":"2503.20062","title":"Poor Alignment and Steerability of Large Language Models: Evidence from\n  College Admission Essays","abstract":"  People are increasingly using technologies equipped with large language\nmodels (LLM) to write texts for formal communication, which raises two\nimportant questions at the intersection of technology and society: Who do LLMs\nwrite like (model alignment); and can LLMs be prompted to change who they write\nlike (model steerability). We investigate these questions in the high-stakes\ncontext of undergraduate admissions at a selective university by comparing\nlexical and sentence variation between essays written by 30,000 applicants to\ntwo types of LLM-generated essays: one prompted with only the essay question\nused by the human applicants; and another with additional demographic\ninformation about each applicant. We consistently find that both types of\nLLM-generated essays are linguistically distinct from human-authored essays,\nregardless of the specific model and analytical approach. Further, prompting a\nspecific sociodemographic identity is remarkably ineffective in aligning the\nmodel with the linguistic patterns observed in human writing from this identity\ngroup. This holds along the key dimensions of sex, race, first-generation\nstatus, and geographic location. The demographically prompted and unprompted\nsynthetic texts were also more similar to each other than to the human text,\nmeaning that prompting did not alleviate homogenization. These issues of model\nalignment and steerability in current LLMs raise concerns about the use of LLMs\nin high-stakes contexts.\n","date":"2025-03-25"}
{"id":"2503.20066","title":"Learning Scene-Level Signed Directional Distance Function with\n  Ellipsoidal Priors and Neural Residuals","abstract":"  Dense geometric environment representations are critical for autonomous\nmobile robot navigation and exploration. Recent work shows that implicit\ncontinuous representations of occupancy, signed distance, or radiance learned\nusing neural networks offer advantages in reconstruction fidelity, efficiency,\nand differentiability over explicit discrete representations based on meshes,\npoint clouds, and voxels. In this work, we explore a directional formulation of\nsigned distance, called signed directional distance function (SDDF). Unlike\nsigned distance function (SDF) and similar to neural radiance fields (NeRF),\nSDDF has a position and viewing direction as input. Like SDF and unlike NeRF,\nSDDF directly provides distance to the observed surface along the direction,\nrather than integrating along the view ray, allowing efficient view synthesis.\nTo learn and predict scene-level SDDF efficiently, we develop a differentiable\nhybrid representation that combines explicit ellipsoid priors and implicit\nneural residuals. This approach allows the model to effectively handle large\ndistance discontinuities around obstacle boundaries while preserving the\nability for dense high-fidelity prediction. We show that SDDF is competitive\nwith the state-of-the-art neural implicit scene models in terms of\nreconstruction accuracy and rendering efficiency, while allowing differentiable\nview prediction for robot trajectory optimization.\n","date":"2025-03-25"}
{"id":"2503.20068","title":"iNatAg: Multi-Class Classification Models Enabled by a Large-Scale\n  Benchmark Dataset with 4.7M Images of 2,959 Crop and Weed Species","abstract":"  Accurate identification of crop and weed species is critical for precision\nagriculture and sustainable farming. However, it remains a challenging task due\nto a variety of factors -- a high degree of visual similarity among species,\nenvironmental variability, and a continued lack of large, agriculture-specific\nimage data. We introduce iNatAg, a large-scale image dataset which contains\nover 4.7 million images of 2,959 distinct crop and weed species, with precise\nannotations along the taxonomic hierarchy from binary crop\/weed labels to\nspecific species labels. Curated from the broader iNaturalist database, iNatAg\ncontains data from every continent and accurately reflects the variability of\nnatural image captures and environments. Enabled by this data, we train\nbenchmark models built upon the Swin Transformer architecture and evaluate the\nimpact of various modifications such as the incorporation of geospatial data\nand LoRA finetuning. Our best models achieve state-of-the-art performance\nacross all taxonomic classification tasks, achieving 92.38\\% on crop and weed\nclassification. Furthermore, the scale of our dataset enables us to explore\nincorrect misclassifications and unlock new analytic possiblities for plant\nspecies. By combining large-scale species coverage, multi-task labels, and\ngeographic diversity, iNatAg provides a new foundation for building robust,\ngeolocation-aware agricultural classification systems. We release the iNatAg\ndataset publicly through AgML (https:\/\/github.com\/Project-AgML\/AgML), enabling\ndirect access and integration into agricultural machine learning workflows.\n","date":"2025-03-25"}
{"id":"2503.20074","title":"Adaptive Orchestration for Large-Scale Inference on Heterogeneous\n  Accelerator Systems Balancing Cost, Performance, and Resilience","abstract":"  The surge in generative AI workloads has created a need for scalable\ninference systems that can flexibly harness both GPUs and specialized\naccelerators while containing operational costs. This paper proposes a\nhardware-agnostic control loop that adaptively allocates requests across\nheterogeneous accelerators based on real-time cost and capacity signals. The\napproach sustains low latency and high throughput by dynamically shifting\nbetween cost-optimized and capacity-optimized modes, ensuring the most\nefficient use of expensive compute resources under fluctuating availability.\nEvaluated using the Stable Diffusion model, the framework consistently meets\nlatency targets, automatically redirects traffic during capacity shortfalls,\nand capitalizes on lower-cost accelerators when possible. These results\nhighlight how a feedback-driven deployment strategy, spanning the entire\nsoftware and hardware stack, can help organizations efficiently scale\ngenerative AI workloads while maintaining resilience in the face of limited\naccelerator capacity.\n","date":"2025-03-25"}
{"id":"2503.20076","title":"Peer Disambiguation in Self-Reported Surveys using Graph Attention\n  Networks","abstract":"  Studying peer relationships is crucial in solving complex challenges\nunderserved communities face and designing interventions. The effectiveness of\nsuch peer-based interventions relies on accurate network data regarding\nindividual attributes and social influences. However, these datasets are often\ncollected through self-reported surveys, introducing ambiguities in network\nconstruction. These ambiguities make it challenging to fully utilize the\nnetwork data to understand the issues and to design the best interventions. We\npropose and solve two variations of link ambiguities in such network data --\n(i) which among the two candidate links exists, and (ii) if a candidate link\nexists. We design a Graph Attention Network (GAT) that accounts for personal\nattributes and network relationships on real-world data with real and simulated\nambiguities. We also demonstrate that by resolving these ambiguities, we\nimprove network accuracy, and in turn, improve suicide risk prediction. We also\nuncover patterns using GNNExplainer to provide additional insights into vital\nfeatures and relationships. This research demonstrates the potential of Graph\nNeural Networks (GNN) to advance real-world network data analysis facilitating\nmore effective peer interventions across various fields.\n","date":"2025-03-25"}
{"id":"2503.20078","title":"Abstracting Geo-specific Terrains to Scale Up Reinforcement Learning","abstract":"  Multi-agent reinforcement learning (MARL) is increasingly ubiquitous in\ntraining dynamic and adaptive synthetic characters for interactive simulations\non geo-specific terrains. Frameworks such as Unity's ML-Agents help to make\nsuch reinforcement learning experiments more accessible to the simulation\ncommunity. Military training simulations also benefit from advances in MARL,\nbut they have immense computational requirements due to their complex,\ncontinuous, stochastic, partially observable, non-stationary, and\ndoctrine-based nature. Furthermore, these simulations require geo-specific\nterrains, further exacerbating the computational resources problem. In our\nresearch, we leverage Unity's waypoints to automatically generate multi-layered\nrepresentation abstractions of the geo-specific terrains to scale up\nreinforcement learning while still allowing the transfer of learned policies\nbetween different representations. Our early exploratory results on a novel\nMARL scenario, where each side has differing objectives, indicate that\nwaypoint-based navigation enables faster and more efficient learning while\nproducing trajectories similar to those taken by expert human players in CSGO\ngaming environments. This research points out the potential of waypoint-based\nnavigation for reducing the computational costs of developing and training MARL\nmodels for military training simulations, where geo-specific terrains and\ndiffering objectives are crucial.\n","date":"2025-03-25"}
{"id":"2503.20083","title":"Cross-Tokenizer Distillation via Approximate Likelihood Matching","abstract":"  Distillation has shown remarkable success in transferring knowledge from a\nLarge Language Model (LLM) teacher to a student LLM. However, current\ndistillation methods predominantly require the same tokenizer between the\nteacher and the student, restricting their applicability to only a small subset\nof teacher-student pairs. In this work, we develop a cross-tokenizer\ndistillation method to solve this crucial deficiency. Our method is the first\nto enable cross-tokenizer distillation without a next-token prediction loss as\nthe main objective, instead purely maximizing the student predictions'\nsimilarity to the teacher's predictions (known as pure distillation), while\nalso being robust to large mismatches between the teacher and the student\ntokenizer function and vocabulary. Empirically, our method enables\nsubstantially improved performance as tested on two use cases. First, we show\nthat viewing tokenizer transfer as self-distillation enables unprecedently\neffective transfer across tokenizers. We transfer (subword-level) Llama and\nGemma models to byte-level tokenization more effectively than prior methods\ntransfer to a similar subword tokenizer under a comparable training budget.\nTransferring different base models to the same tokenizer also enables\nensembling them (e.g., via averaging their predicted probabilities) which\nboosts performance. Second, we use our cross-tokenizer distillation method to\ndistil a large maths-specialized LLM into a smaller model, achieving\ncompetitive maths problem-solving performance. Overall, our results make\nsubstantial strides toward better adaptability and enhanced interaction between\ndifferent LLMs.\n","date":"2025-03-25"}
{"id":"2503.20084","title":"Can Multi-modal (reasoning) LLMs work as deepfake detectors?","abstract":"  Deepfake detection remains a critical challenge in the era of advanced\ngenerative models, particularly as synthetic media becomes more sophisticated.\nIn this study, we explore the potential of state of the art multi-modal\n(reasoning) large language models (LLMs) for deepfake image detection such as\n(OpenAI O1\/4o, Gemini thinking Flash 2, Deepseek Janus, Grok 3, llama 3.2, Qwen\n2\/2.5 VL, Mistral Pixtral, Claude 3.5\/3.7 sonnet) . We benchmark 12 latest\nmulti-modal LLMs against traditional deepfake detection methods across multiple\ndatasets, including recently published real-world deepfake imagery. To enhance\nperformance, we employ prompt tuning and conduct an in-depth analysis of the\nmodels' reasoning pathways to identify key contributing factors in their\ndecision-making process. Our findings indicate that best multi-modal LLMs\nachieve competitive performance with promising generalization ability with zero\nshot, even surpass traditional deepfake detection pipelines in\nout-of-distribution datasets while the rest of the LLM families performs\nextremely disappointing with some worse than random guess. Furthermore, we\nfound newer model version and reasoning capabilities does not contribute to\nperformance in such niche tasks of deepfake detection while model size do help\nin some cases. This study highlights the potential of integrating multi-modal\nreasoning in future deepfake detection frameworks and provides insights into\nmodel interpretability for robustness in real-world scenarios.\n","date":"2025-03-25"}
{"id":"2503.20087","title":"Random feature-based double Vovk-Azoury-Warmuth algorithm for online\n  multi-kernel learning","abstract":"  We introduce a novel multi-kernel learning algorithm, VAW$^2$, for online\nleast squares regression in reproducing kernel Hilbert spaces (RKHS). VAW$^2$\nleverages random Fourier feature-based functional approximation and the\nVovk-Azoury-Warmuth (VAW) method in a two-level procedure: VAW is used to\nconstruct expert strategies from random features generated for each kernel at\nthe first level, and then again to combine their predictions at the second\nlevel. A theoretical analysis yields a regret bound of $O(T^{1\/2}\\ln T)$ in\nexpectation with respect to artificial randomness, when the number of random\nfeatures scales as $T^{1\/2}$. Empirical results on some benchmark datasets\ndemonstrate that VAW$^2$ achieves superior performance compared to the existing\nonline multi-kernel learning algorithms: Raker and OMKL-GF, and to other\ntheoretically grounded method methods involving convex combination of expert\npredictions at the second level.\n","date":"2025-03-25"}
{"id":"2503.20088","title":"Generative Linguistics, Large Language Models, and the Social Nature of\n  Scientific Success","abstract":"  Chesi's (forthcoming) target paper depicts a generative linguistics in\ncrisis, foreboded by Piantadosi's (2023) declaration that \"modern language\nmodels refute Chomsky's approach to language.\" In order to survive, Chesi\nwarns, generativists must hold themselves to higher standards of formal and\nempirical rigor. This response argues that the crisis described by Chesi and\nPiantadosi actually has little to do with rigor, but is rather a reflection of\ngenerativists' limited social ambitions. Chesi ties the fate of generative\nlinguistics to its intellectual merits, but the current success of language\nmodel research is social in nature as much as it is intellectual. In order to\nthrive, then, generativists must do more than heed Chesi's call for rigor; they\nmust also expand their ambitions by giving outsiders a stake in their future\nsuccess.\n","date":"2025-03-25"}
{"id":"2503.20098","title":"Fundamental Limits of Perfect Concept Erasure","abstract":"  Concept erasure is the task of erasing information about a concept (e.g.,\ngender or race) from a representation set while retaining the maximum possible\nutility -- information from original representations. Concept erasure is useful\nin several applications, such as removing sensitive concepts to achieve\nfairness and interpreting the impact of specific concepts on a model's\nperformance. Previous concept erasure techniques have prioritized robustly\nerasing concepts over retaining the utility of the resultant representations.\nHowever, there seems to be an inherent tradeoff between erasure and retaining\nutility, making it unclear how to achieve perfect concept erasure while\nmaintaining high utility. In this paper, we offer a fresh perspective toward\nsolving this problem by quantifying the fundamental limits of concept erasure\nthrough an information-theoretic lens. Using these results, we investigate\nconstraints on the data distribution and the erasure functions required to\nachieve the limits of perfect concept erasure. Empirically, we show that the\nderived erasure functions achieve the optimal theoretical bounds. Additionally,\nwe show that our approach outperforms existing methods on a range of synthetic\nand real-world datasets using GPT-4 representations.\n","date":"2025-03-25"}
{"id":"2503.20099","title":"AI Identity, Empowerment, and Mindfulness in Mitigating Unethical AI Use","abstract":"  This study examines how AI identity influences psychological empowerment and\nunethical AI behavior among college students, while also exploring the\nmoderating role of IT mindfulness. Findings show that a strong AI identity\nenhances psychological empowerment and academic engagement but can also lead to\nincreased unethical AI practices. Crucially, IT mindfulness acts as an ethical\nsafeguard, promoting sensitivity to ethical concerns and reducing misuse of AI.\nThese insights have implications for educators, policymakers, and AI\ndevelopers, emphasizing For Peer Review the need for a balanced approach that\nencourages digital engagement without compromising student responsibility. The\nstudy also contributes to philosophical discussions of psychological agency,\nsuggesting that empowerment through AI can yield both positive and negative\noutcomes. Mindfulness emerges as essential in guiding ethical AI interactions.\nOverall, the research informs ongoing debates on ethics in education and AI,\noffering strategies to align technological advancement with ethical\naccountability and responsible use.\n","date":"2025-03-25"}
{"id":"2503.20101","title":"EBS-EKF: Accurate and High Frequency Event-based Star Tracking","abstract":"  Event-based sensors (EBS) are a promising new technology for star tracking\ndue to their low latency and power efficiency, but prior work has thus far been\nevaluated exclusively in simulation with simplified signal models. We propose a\nnovel algorithm for event-based star tracking, grounded in an analysis of the\nEBS circuit and an extended Kalman filter (EKF). We quantitatively evaluate our\nmethod using real night sky data, comparing its results with those from a\nspace-ready active-pixel sensor (APS) star tracker. We demonstrate that our\nmethod is an order-of-magnitude more accurate than existing methods due to\nimproved signal modeling and state estimation, while providing more frequent\nupdates and greater motion tolerance than conventional APS trackers. We provide\nall code and the first dataset of events synchronized with APS solutions.\n","date":"2025-03-25"}
{"id":"2503.20102","title":"Extendable Long-Horizon Planning via Hierarchical Multiscale Diffusion","abstract":"  This paper tackles a novel problem, extendable long-horizon planning-enabling\nagents to plan trajectories longer than those in training data without\ncompounding errors. To tackle this, we propose the Hierarchical Multiscale\nDiffuser (HM-Diffuser) and Progressive Trajectory Extension (PTE), an\naugmentation method that iteratively generates longer trajectories by stitching\nshorter ones. HM-Diffuser trains on these extended trajectories using a\nhierarchical structure, efficiently handling tasks across multiple temporal\nscales. Additionally, we introduce Adaptive Plan Pondering and the Recursive\nHM-Diffuser, which consolidate hierarchical layers into a single model to\nprocess temporal scales recursively. Experimental results demonstrate the\neffectiveness of our approach, advancing diffusion-based planners for scalable\nlong-horizon planning.\n","date":"2025-03-25"}
{"id":"2503.20103","title":"Bigger But Not Better: Small Neural Language Models Outperform Large\n  Language Models in Detection of Thought Disorder","abstract":"  Disorganized thinking is a key diagnostic indicator of schizophrenia-spectrum\ndisorders. Recently, clinical estimates of the severity of disorganized\nthinking have been shown to correlate with measures of how difficult speech\ntranscripts would be for large language models (LLMs) to predict. However,\nLLMs' deployment challenges -- including privacy concerns, computational and\nfinancial costs, and lack of transparency of training data -- limit their\nclinical utility. We investigate whether smaller neural language models can\nserve as effective alternatives for detecting positive formal thought disorder,\nusing the same sliding window based perplexity measurements that proved\neffective with larger models. Surprisingly, our results show that smaller\nmodels are more sensitive to linguistic differences associated with formal\nthought disorder than their larger counterparts. Detection capability declines\nbeyond a certain model size and context length, challenging the common\nassumption of ``bigger is better'' for LLM-based applications. Our findings\ngeneralize across audio diaries and clinical interview speech samples from\nindividuals with psychotic symptoms, suggesting a promising direction for\ndeveloping efficient, cost-effective, and privacy-preserving screening tools\nthat can be deployed in both clinical and naturalistic settings.\n","date":"2025-03-25"}
{"id":"2503.20104","title":"\"Is There Anything Else?'': Examining Administrator Influence on\n  Linguistic Features from the Cookie Theft Picture Description Cognitive Test","abstract":"  Alzheimer's Disease (AD) dementia is a progressive neurodegenerative disease\nthat negatively impacts patients' cognitive ability. Previous studies have\ndemonstrated that changes in naturalistic language samples can be useful for\nearly screening of AD dementia. However, the nature of language deficits often\nrequires test administrators to use various speech elicitation techniques\nduring spontaneous language assessments to obtain enough propositional\nutterances from dementia patients. This could lead to the ``observer's effect''\non the downstream analysis that has not been fully investigated. Our study\nseeks to quantify the influence of test administrators on linguistic features\nin dementia assessment with two English corpora the ``Cookie Theft'' picture\ndescription datasets collected at different locations and test administrators\nshow different levels of administrator involvement. Our results show that the\nlevel of test administrator involvement significantly impacts observed\nlinguistic features in patient speech. These results suggest that many of\nsignificant linguistic features in the downstream classification task may be\npartially attributable to differences in the test administration practices\nrather than solely to participants' cognitive status. The variations in test\nadministrator behavior can lead to systematic biases in linguistic data,\npotentially confounding research outcomes and clinical assessments. Our study\nsuggests that there is a need for a more standardized test administration\nprotocol in the development of responsible clinical speech analytics\nframeworks.\n","date":"2025-03-25"}
{"id":"2503.20105","title":"Direct Post-Training Preference Alignment for Multi-Agent Motion\n  Generation Models Using Implicit Feedback from Pre-training Demonstrations","abstract":"  Recent advancements in LLMs have revolutionized motion generation models in\nembodied applications. While LLM-type auto-regressive motion generation models\nbenefit from training scalability, there remains a discrepancy between their\ntoken prediction objectives and human preferences. As a result, models\npre-trained solely with token-prediction objectives often generate behaviors\nthat deviate from what humans would prefer, making post-training preference\nalignment crucial for producing human-preferred motions. Unfortunately,\npost-training alignment requires extensive preference rankings of motions\ngenerated by the pre-trained model, which are costly to annotate, especially in\nmulti-agent settings. Recently, there has been growing interest in leveraging\npre-training demonstrations to scalably generate preference data for\npost-training alignment. However, these methods often adopt an adversarial\nassumption, treating all pre-trained model-generated samples as unpreferred\nexamples. This adversarial approach overlooks the valuable signal provided by\npreference rankings among the model's own generations, ultimately reducing\nalignment effectiveness and potentially leading to misaligned behaviors. In\nthis work, instead of treating all generated samples as equally bad, we\nleverage implicit preferences encoded in pre-training demonstrations to\nconstruct preference rankings among the pre-trained model's generations,\noffering more nuanced preference alignment guidance with zero human cost. We\napply our approach to large-scale traffic simulation and demonstrate its\neffectiveness in improving the realism of pre-trained model's generated\nbehaviors, making a lightweight 1M motion generation model comparable to SOTA\nlarge imitation-based models by relying solely on implicit feedback from\npre-training demonstrations, without additional post-training human preference\nannotations or high computational costs.\n","date":"2025-03-25"}
{"id":"2503.20108","title":"Peepers & Pixels: Human Recognition Accuracy on Low Resolution Faces","abstract":"  Automated one-to-many (1:N) face recognition is a powerful investigative tool\ncommonly used by law enforcement agencies. In this context, potential matches\nresulting from automated 1:N recognition are reviewed by human examiners prior\nto possible use as investigative leads. While automated 1:N recognition can\nachieve near-perfect accuracy under ideal imaging conditions, operational\nscenarios may necessitate the use of surveillance imagery, which is often\ndegraded in various quality dimensions. One important quality dimension is\nimage resolution, typically quantified by the number of pixels on the face. The\ncommon metric for this is inter-pupillary distance (IPD), which measures the\nnumber of pixels between the pupils. Low IPD is known to degrade the accuracy\nof automated face recognition. However, the threshold IPD for reliability in\nhuman face recognition remains undefined. This study aims to explore the\nboundaries of human recognition accuracy by systematically testing accuracy\nacross a range of IPD values. We find that at low IPDs (10px, 5px), human\naccuracy is at or below chance levels (50.7%, 35.9%), even as confidence in\ndecision-making remains relatively high (77%, 70.7%). Our findings indicate\nthat, for low IPD images, human recognition ability could be a limiting factor\nto overall system accuracy.\n","date":"2025-03-25"}
{"id":"2503.20110","title":"Efficient Model Development through Fine-tuning Transfer","abstract":"  Modern LLMs struggle with efficient updates, as each new pretrained model\nversion requires repeating expensive alignment processes. This challenge also\napplies to domain- or language-specific models, where fine-tuning on\nspecialized data must be redone for every new base model release. In this\npaper, we explore the transfer of fine-tuning updates between model versions.\nSpecifically, we derive the diff vector from one source model version, which\nrepresents the weight changes from fine-tuning, and apply it to the base model\nof a different target version. Through empirical evaluations on various\nopen-weight model versions, we show that transferring diff vectors can\nsignificantly improve the target base model, often achieving performance\ncomparable to its fine-tuned counterpart. For example, reusing the fine-tuning\nupdates from Llama 3.0 8B leads to an absolute accuracy improvement of 10.7% on\nGPQA over the base Llama 3.1 8B without additional training, surpassing Llama\n3.1 8B Instruct. In a multilingual model development setting, we show that this\napproach can significantly increase performance on target-language tasks\nwithout retraining, achieving an absolute improvement of 4.7% and 15.5% on\nGlobal MMLU for Malagasy and Turkish, respectively, compared to Llama 3.1 8B\nInstruct. Our controlled experiments reveal that fine-tuning transfer is most\neffective when the source and target models are linearly connected in the\nparameter space. Additionally, we demonstrate that fine-tuning transfer offers\na stronger and more computationally efficient starting point for further\nfine-tuning. Finally, we propose an iterative recycling-then-finetuning\napproach for continuous model development, which improves both efficiency and\neffectiveness. Our findings suggest that fine-tuning transfer is a viable\nstrategy to reduce training costs while maintaining model performance.\n","date":"2025-03-25"}
{"id":"2503.20113","title":"Domain Adaptation Framework for Turning Movement Count Estimation with\n  Limited Data","abstract":"  Urban transportation networks are vital for the efficient movement of people\nand goods, necessitating effective traffic management and planning. An integral\npart of traffic management is understanding the turning movement counts (TMCs)\nat intersections, Accurate TMCs at intersections are crucial for traffic signal\ncontrol, congestion mitigation, and road safety. In general, TMCs are obtained\nusing physical sensors installed at intersections, but this approach can be\ncost-prohibitive and technically challenging, especially for cities with\nextensive road networks. Recent advancements in machine learning and\ndata-driven approaches have offered promising alternatives for estimating TMCs.\nTraffic patterns can vary significantly across different intersections due to\nfactors such as road geometry, traffic signal settings, and local driver\nbehaviors. This domain discrepancy limits the generalizability and accuracy of\nmachine learning models when applied to new or unseen intersections. In\nresponse to these limitations, this research proposes a novel framework\nleveraging domain adaptation (DA) to estimate TMCs at intersections by using\ntraffic controller event-based data, road infrastructure data, and\npoint-of-interest (POI) data. Evaluated on 30 intersections in Tucson, Arizona,\nthe performance of the proposed DA framework was compared with state-of-the-art\nmodels and achieved the lowest values in terms of Mean Absolute Error and Root\nMean Square Error.\n","date":"2025-03-25"}
{"id":"2503.20117","title":"From Interpretation to Correction: A Decentralized Optimization\n  Framework for Exact Convergence in Federated Learning","abstract":"  This work introduces a novel decentralized framework to interpret federated\nlearning (FL) and, consequently, correct the biases introduced by arbitrary\nclient participation and data heterogeneity, which are two typical traits in\npractical FL. Specifically, we first reformulate the core processes of FedAvg -\nclient participation, local updating, and model aggregation - as stochastic\nmatrix multiplications. This reformulation allows us to interpret FedAvg as a\ndecentralized algorithm. Leveraging the decentralized optimization framework,\nwe are able to provide a concise analysis to quantify the impact of arbitrary\nclient participation and data heterogeneity on FedAvg's convergence point. This\ninsight motivates the development of Federated Optimization with Exact\nConvergence via Push-pull Strategy (FOCUS), a novel algorithm inspired by the\ndecentralized algorithm that eliminates these biases and achieves exact\nconvergence without requiring the bounded heterogeneity assumption.\nFurthermore, we theoretically prove that FOCUS exhibits linear convergence\n(exponential decay) for both strongly convex and non-convex functions\nsatisfying the Polyak-Lojasiewicz condition, regardless of the arbitrary nature\nof client participation.\n","date":"2025-03-25"}
{"id":"2503.20118","title":"Zero-Shot Human-Object Interaction Synthesis with Multimodal Priors","abstract":"  Human-object interaction (HOI) synthesis is important for various\napplications, ranging from virtual reality to robotics. However, acquiring 3D\nHOI data is challenging due to its complexity and high cost, limiting existing\nmethods to the narrow diversity of object types and interaction patterns in\ntraining datasets. This paper proposes a novel zero-shot HOI synthesis\nframework without relying on end-to-end training on currently limited 3D HOI\ndatasets. The core idea of our method lies in leveraging extensive HOI\nknowledge from pre-trained Multimodal Models. Given a text description, our\nsystem first obtains temporally consistent 2D HOI image sequences using image\nor video generation models, which are then uplifted to 3D HOI milestones of\nhuman and object poses. We employ pre-trained human pose estimation models to\nextract human poses and introduce a generalizable category-level 6-DoF\nestimation method to obtain the object poses from 2D HOI images. Our estimation\nmethod is adaptive to various object templates obtained from text-to-3D models\nor online retrieval. A physics-based tracking of the 3D HOI kinematic milestone\nis further applied to refine both body motions and object poses, yielding more\nphysically plausible HOI generation results. The experimental results\ndemonstrate that our method is capable of generating open-vocabulary HOIs with\nphysical realism and semantic diversity.\n","date":"2025-03-25"}
{"id":"2503.20120","title":"On the Robustness of Kernel Ridge Regression Using the Cauchy Loss\n  Function","abstract":"  Robust regression aims to develop methods for estimating an unknown\nregression function in the presence of outliers, heavy-tailed distributions, or\ncontaminated data, which can severely impact performance. Most existing\ntheoretical results in robust regression assume that the noise has a finite\nabsolute mean, an assumption violated by certain distributions, such as Cauchy\nand some Pareto noise. In this paper, we introduce a generalized Cauchy noise\nframework that accommodates all noise distributions with finite moments of any\norder, even when the absolute mean is infinite. Within this framework, we study\nthe \\textit{kernel Cauchy ridge regressor} (\\textit{KCRR}), which minimizes a\nregularized empirical Cauchy risk to achieve robustness. To derive the\n$L_2$-risk bound for KCRR, we establish a connection between the excess Cauchy\nrisk and $L_2$-risk for sufficiently large scale parameters of the Cauchy loss,\nwhich reveals that these two risks are equivalent. Furthermore, under the\nassumption that the regression function satisfies H\\\"older smoothness, we\nderive excess Cauchy risk bounds for KCRR, showing improved performance as the\nscale parameter decreases. By considering the twofold effect of the scale\nparameter on the excess Cauchy risk and its equivalence with the $L_2$-risk, we\nestablish the almost minimax-optimal convergence rate for KCRR in terms of\n$L_2$-risk, highlighting the robustness of the Cauchy loss in handling various\ntypes of noise. Finally, we validate the effectiveness of KCRR through\nexperiments on both synthetic and real-world datasets under diverse noise\ncorruption scenarios.\n","date":"2025-03-26"}
{"id":"2503.20124","title":"Synthesizing world models for bilevel planning","abstract":"  Modern reinforcement learning (RL) systems have demonstrated remarkable\ncapabilities in complex environments, such as video games. However, they still\nfall short of achieving human-like sample efficiency and adaptability when\nlearning new domains. Theory-based reinforcement learning (TBRL) is an\nalgorithmic framework specifically designed to address this gap. Modeled on\ncognitive theories, TBRL leverages structured, causal world models - \"theories\"\n- as forward simulators for use in planning, generalization and exploration.\nAlthough current TBRL systems provide compelling explanations of how humans\nlearn to play video games, they face several technical limitations: their\ntheory languages are restrictive, and their planning algorithms are not\nscalable. To address these challenges, we introduce TheoryCoder, an\ninstantiation of TBRL that exploits hierarchical representations of theories\nand efficient program synthesis methods for more powerful learning and\nplanning. TheoryCoder equips agents with general-purpose abstractions (e.g.,\n\"move to\"), which are then grounded in a particular environment by learning a\nlow-level transition model (a Python program synthesized from observations by a\nlarge language model). A bilevel planning algorithm can exploit this\nhierarchical structure to solve large domains. We demonstrate that this\napproach can be successfully applied to diverse and challenging grid-world\ngames, where approaches based on directly synthesizing a policy perform poorly.\nAblation studies demonstrate the benefits of using hierarchical abstractions.\n","date":"2025-03-26"}
{"id":"2503.20126","title":"Can We Make Code Green? Understanding Trade-Offs in LLMs vs. Human Code\n  Optimizations","abstract":"  The rapid technological evolution has accelerated software development for\nvarious domains and use cases, contributing to a growing share of global carbon\nemissions. While recent large language models (LLMs) claim to assist developers\nin optimizing code for performance and energy efficiency, their efficacy in\nreal-world scenarios remains under exploration. In this work, we explore the\neffectiveness of LLMs in reducing the environmental footprint of real-world\nprojects, focusing on software written in Matlab-widely used in both academia\nand industry for scientific and engineering applications. We analyze\nenergy-focused optimization on 400 scripts across 100 top GitHub repositories.\nWe examine potential 2,176 optimizations recommended by leading LLMs, such as\nGPT-3, GPT-4, Llama, and Mixtral, and a senior Matlab developer, on energy\nconsumption, memory usage, execution time consumption, and code correctness.\nThe developer serves as a real-world baseline for comparing typical human and\nLLM-generated optimizations.\n  Mapping these optimizations to 13 high-level themes, we found that LLMs\npropose a broad spectrum of improvements--beyond energy efficiency--including\nimproving code readability and maintainability, memory management, error\nhandling while the developer overlooked some parallel processing, error\nhandling etc. However, our statistical tests reveal that the energy-focused\noptimizations unexpectedly negatively impacted memory usage, with no clear\nbenefits regarding execution time or energy consumption. Our qualitative\nanalysis of energy-time trade-offs revealed that some themes, such as\nvectorization preallocation, were among the common themes shaping these\ntrade-offs. With LLMs becoming ubiquitous in modern software development, our\nstudy serves as a call to action: prioritizing the evaluation of common coding\npractices to identify the green ones.\n","date":"2025-03-26"}
{"id":"2503.20136","title":"Innovative LSGTime Model for Crime Spatiotemporal Prediction Based on\n  MindSpore Framework","abstract":"  With the acceleration of urbanization, the spatiotemporal characteristics of\ncriminal activities have become increasingly complex. Accurate prediction of\ncrime distribution is crucial for optimizing the allocation of police resources\nand preventing crime. This paper proposes LGSTime, a crime spatiotemporal\nprediction model that integrates Long Short-Term Memory (LSTM), Gated Recurrent\nUnit (GRU), and the Multi-head Sparse Self-attention mechanism. LSTM and GRU\ncapture long-term dependencies in crime time series, such as seasonality and\nperiodicity, through their unique gating mechanisms. The Multi-head Sparse\nSelf-attention mechanism, on the other hand, focuses on both temporal and\nspatial features of criminal events simultaneously through parallel processing\nand sparsification techniques, significantly improving computational efficiency\nand prediction accuracy. The integrated model leverages the strengths of each\ntechnique to better handle complex spatiotemporal data. Experimental findings\ndemonstrate that the model attains optimal performance across four real - world\ncrime datasets. In comparison to the CNN model, it exhibits performance\nenhancements of 2.8\\%, 1.9\\%, and 1.4\\% in the Mean Squared Error (MSE), Mean\nAbsolute Error (MAE), and Root Mean Squared Error (RMSE) metrics respectively.\nThese results offer a valuable reference for tackling the challenges in crime\nprediction.\n","date":"2025-03-26"}
{"id":"2503.20138","title":"Unlocking the Value of Decentralized Data: A Federated Dual Learning\n  Approach for Model Aggregation","abstract":"  Artificial Intelligence (AI) technologies have revolutionized numerous\nfields, yet their applications often rely on costly and time-consuming data\ncollection processes. Federated Learning (FL) offers a promising alternative by\nenabling AI models to be trained on decentralized data where data is scattered\nacross clients (distributed nodes). However, existing FL approaches struggle to\nmatch the performance of centralized training due to challenges such as\nheterogeneous data distribution and communication delays, limiting their\npotential for breakthroughs. We observe that many real-world use cases involve\nhybrid data regimes, in which a server (center node) has access to some data\nwhile a large amount of data is distributed across associated clients. To\nimprove the utilization of decentralized data under this regime, address data\nheterogeneity issue, and facilitate asynchronous communication between the\nserver and clients, we propose a dual learning approach that leverages\ncentralized data at the server to guide the merging of model updates from\nclients. Our method accommodates scenarios where server data is out-of-domain\nrelative to decentralized client data, making it applicable to a wide range of\nuse cases. We provide theoretical analysis demonstrating the faster convergence\nof our method compared to existing methods. Furthermore, experimental results\nacross various scenarios show that our approach significantly outperforms\nexisting technologies, highlighting its potential to unlock the value of large\namounts of decentralized data.\n","date":"2025-03-26"}
{"id":"2503.20139","title":"Look Before Leap: Look-Ahead Planning with Uncertainty in Reinforcement\n  Learning","abstract":"  Model-based reinforcement learning (MBRL) has demonstrated superior sample\nefficiency compared to model-free reinforcement learning (MFRL). However, the\npresence of inaccurate models can introduce biases during policy learning,\nresulting in misleading trajectories. The challenge lies in obtaining accurate\nmodels due to limited diverse training data, particularly in regions with\nlimited visits (uncertain regions). Existing approaches passively quantify\nuncertainty after sample generation, failing to actively collect uncertain\nsamples that could enhance state coverage and improve model accuracy. Moreover,\nMBRL often faces difficulties in making accurate multi-step predictions,\nthereby impacting overall performance. To address these limitations, we propose\na novel framework for uncertainty-aware policy optimization with model-based\nexploratory planning. In the model-based planning phase, we introduce an\nuncertainty-aware k-step lookahead planning approach to guide action selection\nat each step. This process involves a trade-off analysis between model\nuncertainty and value function approximation error, effectively enhancing\npolicy performance. In the policy optimization phase, we leverage an\nuncertainty-driven exploratory policy to actively collect diverse training\nsamples, resulting in improved model accuracy and overall performance of the RL\nagent. Our approach offers flexibility and applicability to tasks with varying\nstate\/action spaces and reward structures. We validate its effectiveness\nthrough experiments on challenging robotic manipulation tasks and Atari games,\nsurpassing state-of-the-art methods with fewer interactions, thereby leading to\nsignificant performance improvements.\n","date":"2025-03-26"}
{"id":"2503.20144","title":"Physics-Informed Neural Networks with Unknown Partial Differential\n  Equations: an Application in Multivariate Time Series","abstract":"  A significant advancement in Neural Network (NN) research is the integration\nof domain-specific knowledge through custom loss functions. This approach\naddresses a crucial challenge: how can models utilize physics or mathematical\nprinciples to enhance predictions when dealing with sparse, noisy, or\nincomplete data? Physics-Informed Neural Networks (PINNs) put this idea into\npractice by incorporating physical equations, such as Partial Differential\nEquations (PDEs), as soft constraints. This guidance helps the networks find\nsolutions that align with established laws. Recently, researchers have expanded\nthis framework to include Bayesian NNs (BNNs), which allow for uncertainty\nquantification while still adhering to physical principles. But what happens\nwhen the governing equations of a system are not known? In this work, we\nintroduce methods to automatically extract PDEs from historical data. We then\nintegrate these learned equations into three different modeling approaches:\nPINNs, Bayesian-PINNs (B-PINNs), and Bayesian Linear Regression (BLR). To\nassess these frameworks, we evaluate them on a real-world Multivariate Time\nSeries (MTS) dataset. We compare their effectiveness in forecasting future\nstates under different scenarios: with and without PDE constraints and accuracy\nconsiderations. This research aims to bridge the gap between data-driven\ndiscovery and physics-guided learning, providing valuable insights for\npractical applications.\n","date":"2025-03-26"}
{"id":"2503.20148","title":"Addressing Challenges in Time Series Forecasting: A Comprehensive\n  Comparison of Machine Learning Techniques","abstract":"  The explosion of Time Series (TS) data, driven by advancements in technology,\nnecessitates sophisticated analytical methods. Modern management systems\nincreasingly rely on analyzing this data, highlighting the importance of\neffcient processing techniques. State-of-the-art Machine Learning (ML)\napproaches for TS analysis and forecasting are becoming prevalent. This paper\nbriefly describes and compiles suitable algorithms for TS regression task. We\ncompare these algorithms against each other and the classic ARIMA method using\ndiverse datasets: complete data, data with outliers, and data with missing\nvalues. The focus is on forecasting accuracy, particularly for long-term\npredictions. This research aids in selecting the most appropriate algorithm\nbased on forecasting needs and data characteristics.\n","date":"2025-03-26"}
{"id":"2503.20158","title":"RxRx3-core: Benchmarking drug-target interactions in High-Content\n  Microscopy","abstract":"  High Content Screening (HCS) microscopy datasets have transformed the ability\nto profile cellular responses to genetic and chemical perturbations, enabling\ncell-based inference of drug-target interactions (DTI). However, the adoption\nof representation learning methods for HCS data has been hindered by the lack\nof accessible datasets and robust benchmarks. To address this gap, we present\nRxRx3-core, a curated and compressed subset of the RxRx3 dataset, and an\nassociated DTI benchmarking task. At just 18GB, RxRx3-core significantly\nreduces the size barrier associated with large-scale HCS datasets while\npreserving critical data necessary for benchmarking representation learning\nmodels against a zero-shot DTI prediction task. RxRx3-core includes 222,601\nmicroscopy images spanning 736 CRISPR knockouts and 1,674 compounds at 8\nconcentrations. RxRx3-core is available on HuggingFace and Polaris, along with\npre-trained embeddings and benchmarking code, ensuring accessibility for the\nresearch community. By providing a compact dataset and robust benchmarks, we\naim to accelerate innovation in representation learning methods for HCS data\nand support the discovery of novel biological insights.\n","date":"2025-03-26"}
{"id":"2503.20163","title":"Emotion Detection in Twitter Messages Using Combination of Long\n  Short-Term Memory and Convolutional Deep Neural Networks","abstract":"  One of the most significant issues as attended a lot in recent years is that\nof recognizing the sentiments and emotions in social media texts. The analysis\nof sentiments and emotions is intended to recognize the conceptual information\nsuch as the opinions, feelings, attitudes and emotions of people towards the\nproducts, services, organizations, people, topics, events and features in the\nwritten text. These indicate the greatness of the problem space. In the real\nworld, businesses and organizations are always looking for tools to gather\nideas, emotions, and directions of people about their products, services, or\nevents related to their own. This article uses the Twitter social network, one\nof the most popular social networks with about 420 million active users, to\nextract data. Using this social network, users can share their information and\nopinions about personal issues, policies, products, events, etc. It can be used\nwith appropriate classification of emotional states due to the availability of\nits data. In this study, supervised learning and deep neural network algorithms\nare used to classify the emotional states of Twitter users. The use of deep\nlearning methods to increase the learning capacity of the model is an advantage\ndue to the large amount of available data. Tweets collected on various topics\nare classified into four classes using a combination of two Bidirectional Long\nShort Term Memory network and a Convolutional network. The results obtained\nfrom this study with an average accuracy of 93%, show good results extracted\nfrom the proposed framework and improved accuracy compared to previous work.\n","date":"2025-03-26"}
{"id":"2503.20166","title":"AIGC-assisted Federated Learning for Edge Intelligence: Architecture\n  Design, Research Challenges and Future Directions","abstract":"  Federated learning (FL) can fully leverage large-scale terminal data while\nensuring privacy and security, and is considered as a distributed alternative\nfor the centralized machine learning. However, the issue of data heterogeneity\nposes limitations on FL's performance. To address this challenge, artificial\nintelligence-generated content (AIGC) which is an innovative data synthesis\ntechnique emerges as one potential solution. In this article, we first provide\nan overview of the system architecture, performance metrics, and challenges\nassociated with AIGC-assistant FL system design. We then propose the Generative\nfederated learning (GenFL) architecture and present its workflow, including the\ndesign of aggregation and weight policy. Finally, using the CIFAR10 and\nCIFAR100 datasets, we employ diffusion models to generate dataset and improve\nFL performance. Experiments conducted under various non-independent and\nidentically distributed (non-IID) data distributions demonstrate the\neffectiveness of GenFL on overcoming the bottlenecks in FL caused by data\nheterogeneity. Open research directions in the research of AIGC-assisted FL are\nalso discussed.\n","date":"2025-03-26"}
{"id":"2503.20168","title":"EVolSplat: Efficient Volume-based Gaussian Splatting for Urban View\n  Synthesis","abstract":"  Novel view synthesis of urban scenes is essential for autonomous\ndriving-related applications.Existing NeRF and 3DGS-based methods show\npromising results in achieving photorealistic renderings but require slow,\nper-scene optimization. We introduce EVolSplat, an efficient 3D Gaussian\nSplatting model for urban scenes that works in a feed-forward manner. Unlike\nexisting feed-forward, pixel-aligned 3DGS methods, which often suffer from\nissues like multi-view inconsistencies and duplicated content, our approach\npredicts 3D Gaussians across multiple frames within a unified volume using a 3D\nconvolutional network. This is achieved by initializing 3D Gaussians with noisy\ndepth predictions, and then refining their geometric properties in 3D space and\npredicting color based on 2D textures. Our model also handles distant views and\nthe sky with a flexible hemisphere background model. This enables us to perform\nfast, feed-forward reconstruction while achieving real-time rendering.\nExperimental evaluations on the KITTI-360 and Waymo datasets show that our\nmethod achieves state-of-the-art quality compared to existing feed-forward\n3DGS- and NeRF-based methods.\n","date":"2025-03-26"}
{"id":"2503.20172","title":"Guiding Human-Object Interactions with Rich Geometry and Relations","abstract":"  Human-object interaction (HOI) synthesis is crucial for creating immersive\nand realistic experiences for applications such as virtual reality. Existing\nmethods often rely on simplified object representations, such as the object's\ncentroid or the nearest point to a human, to achieve physically plausible\nmotions. However, these approaches may overlook geometric complexity, resulting\nin suboptimal interaction fidelity. To address this limitation, we introduce\nROG, a novel diffusion-based framework that models the spatiotemporal\nrelationships inherent in HOIs with rich geometric detail. For efficient object\nrepresentation, we select boundary-focused and fine-detail key points from the\nobject mesh, ensuring a comprehensive depiction of the object's geometry. This\nrepresentation is used to construct an interactive distance field (IDF),\ncapturing the robust HOI dynamics. Furthermore, we develop a diffusion-based\nrelation model that integrates spatial and temporal attention mechanisms,\nenabling a better understanding of intricate HOI relationships. This relation\nmodel refines the generated motion's IDF, guiding the motion generation process\nto produce relation-aware and semantically aligned movements. Experimental\nevaluations demonstrate that ROG significantly outperforms state-of-the-art\nmethods in the realism and semantic accuracy of synthesized HOIs.\n","date":"2025-03-26"}
{"id":"2503.20174","title":"Devil is in the Uniformity: Exploring Diverse Learners within\n  Transformer for Image Restoration","abstract":"  Transformer-based approaches have gained significant attention in image\nrestoration, where the core component, i.e, Multi-Head Attention (MHA), plays a\ncrucial role in capturing diverse features and recovering high-quality results.\nIn MHA, heads perform attention calculation independently from uniform split\nsubspaces, and a redundancy issue is triggered to hinder the model from\nachieving satisfactory outputs. In this paper, we propose to improve MHA by\nexploring diverse learners and introducing various interactions between heads,\nwhich results in a Hierarchical multI-head atteNtion driven Transformer model,\ntermed HINT, for image restoration. HINT contains two modules, i.e., the\nHierarchical Multi-Head Attention (HMHA) and the Query-Key Cache Updating\n(QKCU) module, to address the redundancy problem that is rooted in vanilla MHA.\nSpecifically, HMHA extracts diverse contextual features by employing heads to\nlearn from subspaces of varying sizes and containing different information.\nMoreover, QKCU, comprising intra- and inter-layer schemes, further reduces the\nredundancy problem by facilitating enhanced interactions between attention\nheads within and across layers. Extensive experiments are conducted on 12\nbenchmarks across 5 image restoration tasks, including low-light enhancement,\ndehazing, desnowing, denoising, and deraining, to demonstrate the superiority\nof HINT. The source code is available in the supplementary materials.\n","date":"2025-03-26"}
{"id":"2503.20176","title":"Offline Reinforcement Learning with Discrete Diffusion Skills","abstract":"  Skills have been introduced to offline reinforcement learning (RL) as\ntemporal abstractions to tackle complex, long-horizon tasks, promoting\nconsistent behavior and enabling meaningful exploration. While skills in\noffline RL are predominantly modeled within a continuous latent space, the\npotential of discrete skill spaces remains largely underexplored. In this\npaper, we propose a compact discrete skill space for offline RL tasks supported\nby state-of-the-art transformer-based encoder and diffusion-based decoder.\nCoupled with a high-level policy trained via offline RL techniques, our method\nestablishes a hierarchical RL framework where the trained diffusion decoder\nplays a pivotal role. Empirical evaluations show that the proposed algorithm,\nDiscrete Diffusion Skill (DDS), is a powerful offline RL method. DDS performs\ncompetitively on Locomotion and Kitchen tasks and excels on long-horizon tasks,\nachieving at least a 12 percent improvement on AntMaze-v2 benchmarks compared\nto existing offline RL approaches. Furthermore, DDS offers improved\ninterpretability, training stability, and online exploration compared to\nprevious skill-based methods.\n","date":"2025-03-26"}
{"id":"2503.20179","title":"ProtoBERT-LoRA: Parameter-Efficient Prototypical Finetuning for\n  Immunotherapy Study Identification","abstract":"  Identifying immune checkpoint inhibitor (ICI) studies in genomic repositories\nlike Gene Expression Omnibus (GEO) is vital for cancer research yet remains\nchallenging due to semantic ambiguity, extreme class imbalance, and limited\nlabeled data in low-resource settings. We present ProtoBERT-LoRA, a hybrid\nframework that combines PubMedBERT with prototypical networks and Low-Rank\nAdaptation (LoRA) for efficient fine-tuning. The model enforces class-separable\nembeddings via episodic prototype training while preserving biomedical domain\nknowledge. Our dataset was divided as: Training (20 positive, 20 negative),\nPrototype Set (10 positive, 10 negative), Validation (20 positive, 200\nnegative), and Test (71 positive, 765 negative). Evaluated on test dataset,\nProtoBERT-LoRA achieved F1-score of 0.624 (precision: 0.481, recall: 0.887),\noutperforming the rule-based system, machine learning baselines and finetuned\nPubMedBERT. Application to 44,287 unlabeled studies reduced manual review\nefforts by 82%. Ablation studies confirmed that combining prototypes with LoRA\nimproved performance by 29% over stand-alone LoRA.\n","date":"2025-03-26"}
{"id":"2503.20182","title":"Leveraging Implicit Sentiments: Enhancing Reliability and Validity in\n  Psychological Trait Evaluation of LLMs","abstract":"  Recent advancements in Large Language Models (LLMs) have led to their\nincreasing integration into human life. With the transition from mere tools to\nhuman-like assistants, understanding their psychological aspects-such as\nemotional tendencies and personalities-becomes essential for ensuring their\ntrustworthiness. However, current psychological evaluations of LLMs, often\nbased on human psychological assessments like the BFI, face significant\nlimitations. The results from these approaches often lack reliability and have\nlimited validity when predicting LLM behavior in real-world scenarios. In this\nwork, we introduce a novel evaluation instrument specifically designed for\nLLMs, called Core Sentiment Inventory (CSI). CSI is a bilingual tool, covering\nboth English and Chinese, that implicitly evaluates models' sentiment\ntendencies, providing an insightful psychological portrait of LLM across three\ndimensions: optimism, pessimism, and neutrality. Through extensive experiments,\nwe demonstrate that: 1) CSI effectively captures nuanced emotional patterns,\nrevealing significant variation in LLMs across languages and contexts; 2)\nCompared to current approaches, CSI significantly improves reliability,\nyielding more consistent results; and 3) The correlation between CSI scores and\nthe sentiment of LLM's real-world outputs exceeds 0.85, demonstrating its\nstrong validity in predicting LLM behavior. We make CSI public available via:\nhttps:\/\/github.com\/dependentsign\/CSI.\n","date":"2025-03-26"}
{"id":"2503.20184","title":"Spectrum from Defocus: Fast Spectral Imaging with Chromatic Focal Stack","abstract":"  Hyperspectral cameras face harsh trade-offs between spatial, spectral, and\ntemporal resolution in an inherently low-photon regime. Computational imaging\nsystems break through these trade-offs with compressive sensing, but require\ncomplex optics and\/or extensive compute. We present Spectrum from Defocus\n(SfD), a chromatic focal sweep method that recovers state-of-the-art\nhyperspectral images with a small system of off-the-shelf optics and < 1 second\nof compute. Our camera uses two lenses and a grayscale sensor to preserve\nnearly all incident light in a chromatically-aberrated focal stack. Our\nphysics-based iterative algorithm efficiently demixes, deconvolves, and\ndenoises the blurry grayscale focal stack into a sharp spectral image. The\ncombination of photon efficiency, optical simplicity, and physical modeling\nmakes SfD a promising solution for fast, compact, interpretable hyperspectral\nimaging.\n","date":"2025-03-26"}
{"id":"2503.20187","title":"Network Inversion for Generating Confidently Classified Counterfeits","abstract":"  In machine learning, especially with vision classifiers, generating inputs\nthat are confidently classified by the model is essential for understanding its\ndecision boundaries and behavior. However, creating such samples that are\nconfidently classified yet distinct from the training data distribution is a\nchallenge. Traditional methods often modify existing inputs, but they don't\nalways ensure confident classification. In this work, we extend network\ninversion techniques to generate Confidently Classified Counterfeits-synthetic\nsamples that are confidently classified by the model despite being\nsignificantly different from the training data. We achieve this by modifying\nthe generator's conditioning mechanism from soft vector conditioning to one-hot\nvector conditioning and applying Kullback-Leibler divergence (KLD) between the\none-hot vectors and the classifier's output distribution. This encourages the\ngenerator to produce samples that are both plausible and confidently\nclassified. Generating Confidently Classified Counterfeits is crucial for\nensuring the safety and reliability of machine learning systems, particularly\nin safety-critical applications where models must exhibit confidence only on\ndata within the training distribution. By generating such counterfeits, we\nchallenge the assumption that high-confidence predictions are always indicative\nof in-distribution data, providing deeper insights into the model's limitations\nand decision-making process.\n","date":"2025-03-26"}
{"id":"2503.20188","title":"Rethinking Vision-Language Model in Face Forensics: Multi-Modal\n  Interpretable Forged Face Detector","abstract":"  Deepfake detection is a long-established research topic vital for mitigating\nthe spread of malicious misinformation. Unlike prior methods that provide\neither binary classification results or textual explanations separately, we\nintroduce a novel method capable of generating both simultaneously. Our method\nharnesses the multi-modal learning capability of the pre-trained CLIP and the\nunprecedented interpretability of large language models (LLMs) to enhance both\nthe generalization and explainability of deepfake detection. Specifically, we\nintroduce a multi-modal face forgery detector (M2F2-Det) that employs tailored\nface forgery prompt learning, incorporating the pre-trained CLIP to improve\ngeneralization to unseen forgeries. Also, M2F2-Det incorporates an LLM to\nprovide detailed textual explanations of its detection decisions, enhancing\ninterpretability by bridging the gap between natural language and subtle cues\nof facial forgeries. Empirically, we evaluate M2F2-Det on both detection and\nexplanation generation tasks, where it achieves state-of-the-art performance,\ndemonstrating its effectiveness in identifying and explaining diverse\nforgeries.\n","date":"2025-03-26"}
{"id":"2503.20190","title":"Cross-Modal Prototype Allocation: Unsupervised Slide Representation\n  Learning via Patch-Text Contrast in Computational Pathology","abstract":"  With the rapid advancement of pathology foundation models (FMs), the\nrepresentation learning of whole slide images (WSIs) attracts increasing\nattention. Existing studies develop high-quality patch feature extractors and\nemploy carefully designed aggregation schemes to derive slide-level\nrepresentations. However, mainstream weakly supervised slide representation\nlearning methods, primarily based on multiple instance learning (MIL), are\ntailored to specific downstream tasks, which limits their generalizability. To\naddress this issue, some studies explore unsupervised slide representation\nlearning. However, these approaches focus solely on the visual modality of\npatches, neglecting the rich semantic information embedded in textual data. In\nthis work, we propose ProAlign, a cross-modal unsupervised slide representation\nlearning framework. Specifically, we leverage a large language model (LLM) to\ngenerate descriptive text for the prototype types present in a WSI, introducing\npatch-text contrast to construct initial prototype embeddings. Furthermore, we\npropose a parameter-free attention aggregation strategy that utilizes the\nsimilarity between patches and these prototypes to form unsupervised slide\nembeddings applicable to a wide range of downstream tasks. Extensive\nexperiments on four public datasets show that ProAlign outperforms existing\nunsupervised frameworks and achieves performance comparable to some weakly\nsupervised models.\n","date":"2025-03-26"}
{"id":"2503.20191","title":"Maya: Optimizing Deep Learning Training Workloads using Emulated Virtual\n  Accelerators","abstract":"  Training large foundation models costs hundreds of millions of dollars,\nmaking deployment optimization critical. Current approaches require machine\nlearning engineers to manually craft training recipes through error-prone\ntrial-and-error on expensive compute clusters. To enable efficient exploration\nof training configurations, researchers have developed performance modeling\nsystems. However, these systems force users to translate their workloads into\ncustom specification languages, introducing a fundamental semantic gap between\nthe actual workload and its representation. This gap creates an inherent\ntradeoff: systems must either support a narrow set of workloads to maintain\nusability, require complex specifications that limit practical adoption, or\ncompromise prediction accuracy with simplified models.\n  We present Maya, a performance modeling system that eliminates these\ntradeoffs through transparent device emulation. By operating at the narrow\ninterface between training frameworks and accelerator devices, Maya can capture\ncomplete workload behavior without requiring code modifications or\ntranslations. Maya intercepts device API calls from unmodified training code to\ndirectly observe low-level operations, enabling accurate performance prediction\nwhile maintaining both ease of use and generality. Our evaluation shows Maya\nachieves less than 5% prediction error across diverse models and optimization\nstrategies, identifying configurations that reduce training costs by up to 56%\ncompared to existing approaches.\n","date":"2025-03-26"}
{"id":"2503.20193","title":"Nonparametric MLE for Gaussian Location Mixtures: Certified Computation\n  and Generic Behavior","abstract":"  We study the nonparametric maximum likelihood estimator $\\widehat{\\pi}$ for\nGaussian location mixtures in one dimension. It has been known since (Lindsay,\n1983) that given an $n$-point dataset, this estimator always returns a mixture\nwith at most $n$ components, and more recently (Wu-Polyanskiy, 2020) gave a\nsharp $O(\\log n)$ bound for subgaussian data. In this work we study\ncomputational aspects of $\\widehat{\\pi}$. We provide an algorithm which for\nsmall enough $\\varepsilon>0$ computes an $\\varepsilon$-approximation of\n$\\widehat\\pi$ in Wasserstein distance in time $K+Cnk^2\\log\\log(1\/\\varepsilon)$.\nHere $K$ is data-dependent but independent of $\\varepsilon$, while $C$ is an\nabsolute constant and $k=|supp(\\widehat{\\pi})|\\leq n$ is the number of atoms in\n$\\widehat\\pi$. We also certifiably compute the exact value of\n$|supp(\\widehat\\pi)|$ in finite time. These guarantees hold almost surely\nwhenever the dataset $(x_1,\\dots,x_n)\\in [-cn^{1\/4},cn^{1\/4}]$ consists of\nindependent points from a probability distribution with a density (relative to\nLebesgue measure). We also show the distribution of $\\widehat\\pi$ conditioned\nto be $k$-atomic admits a density on the associated $2k-1$ dimensional\nparameter space for all $k\\leq \\sqrt{n}\/3$, and almost sure locally linear\nconvergence of the EM algorithm. One key tool is a classical Fourier analytic\nestimate for non-degenerate curves.\n","date":"2025-03-26"}
{"id":"2503.20194","title":"GAPO: Learning Preferential Prompt through Generative Adversarial Policy\n  Optimization","abstract":"  Recent advances in large language models have highlighted the critical need\nfor precise control over model outputs through predefined constraints. While\nexisting methods attempt to achieve this through either direct\ninstruction-response synthesis or preferential response optimization, they\noften struggle with constraint understanding and adaptation. This limitation\nbecomes particularly evident when handling fine-grained constraints, leading to\neither hallucination or brittle performance. We introduce Generative\nAdversarial Policy Optimization (GAPO), a novel framework that combines\nGAN-based training dynamics with an encoder-only reward model to progressively\nlearn and adapt to increasingly complex constraints. GAPO leverages adversarial\ntraining to automatically generate training samples of varying difficulty while\nutilizing the encoder-only architecture to better capture prompt-response\nrelationships. Extensive experiments demonstrate GAPO's superior performance\nacross multiple benchmarks, particularly in scenarios requiring fine-grained\nconstraint handling, where it significantly outperforms existing methods like\nPPO, DPO, and KTO. Our results suggest that GAPO's unique approach to\npreferential prompt learning offers a more robust and effective solution for\ncontrolling LLM outputs. Code is avaliable in\nhttps:\/\/github.com\/MikeGu721\/GAPO.\n","date":"2025-03-26"}
{"id":"2503.20198","title":"Beyond Words: Advancing Long-Text Image Generation via Multimodal\n  Autoregressive Models","abstract":"  Recent advancements in autoregressive and diffusion models have led to strong\nperformance in image generation with short scene text words. However,\ngenerating coherent, long-form text in images, such as paragraphs in slides or\ndocuments, remains a major challenge for current generative models. We present\nthe first work specifically focused on long text image generation, addressing a\ncritical gap in existing text-to-image systems that typically handle only brief\nphrases or single sentences. Through comprehensive analysis of state-of-the-art\nautoregressive generation models, we identify the image tokenizer as a critical\nbottleneck in text generating quality. To address this, we introduce a novel\ntext-focused, binary tokenizer optimized for capturing detailed scene text\nfeatures. Leveraging our tokenizer, we develop \\ModelName, a multimodal\nautoregressive model that excels in generating high-quality long-text images\nwith unprecedented fidelity. Our model offers robust controllability, enabling\ncustomization of text properties such as font style, size, color, and\nalignment. Extensive experiments demonstrate that \\ModelName~significantly\noutperforms SD3.5 Large~\\cite{sd3} and GPT4o~\\cite{gpt4o} with DALL-E\n3~\\cite{dalle3} in generating long text accurately, consistently, and flexibly.\nBeyond its technical achievements, \\ModelName~opens up exciting opportunities\nfor innovative applications like interleaved document and PowerPoint\ngeneration, establishing a new frontier in long-text image generating.\n","date":"2025-03-26"}
{"id":"2503.20199","title":"Assessing SAM for Tree Crown Instance Segmentation from Drone Imagery","abstract":"  The potential of tree planting as a natural climate solution is often\nundermined by inadequate monitoring of tree planting projects. Current\nmonitoring methods involve measuring trees by hand for each species, requiring\nextensive cost, time, and labour. Advances in drone remote sensing and computer\nvision offer great potential for mapping and characterizing trees from aerial\nimagery, and large pre-trained vision models, such as the Segment Anything\nModel (SAM), may be a particularly compelling choice given limited labeled\ndata. In this work, we compare SAM methods for the task of automatic tree crown\ninstance segmentation in high resolution drone imagery of young tree\nplantations. We explore the potential of SAM for this task, and find that\nmethods using SAM out-of-the-box do not outperform a custom Mask R-CNN, even\nwith well-designed prompts, but that there is potential for methods which tune\nSAM further. We also show that predictions can be improved by adding Digital\nSurface Model (DSM) information as an input.\n","date":"2025-03-26"}
{"id":"2503.20201","title":"Open Deep Search: Democratizing Search with Open-source Reasoning Agents","abstract":"  We introduce Open Deep Search (ODS) to close the increasing gap between the\nproprietary search AI solutions, such as Perplexity's Sonar Reasoning Pro and\nOpenAI's GPT-4o Search Preview, and their open-source counterparts. The main\ninnovation introduced in ODS is to augment the reasoning capabilities of the\nlatest open-source LLMs with reasoning agents that can judiciously use web\nsearch tools to answer queries. Concretely, ODS consists of two components that\nwork with a base LLM chosen by the user: Open Search Tool and Open Reasoning\nAgent. Open Reasoning Agent interprets the given task and completes it by\norchestrating a sequence of actions that includes calling tools, one of which\nis the Open Search Tool. Open Search Tool is a novel web search tool that\noutperforms proprietary counterparts. Together with powerful open-source\nreasoning LLMs, such as DeepSeek-R1, ODS nearly matches and sometimes surpasses\nthe existing state-of-the-art baselines on two benchmarks: SimpleQA and FRAMES.\nFor example, on the FRAMES evaluation benchmark, ODS improves the best existing\nbaseline of the recently released GPT-4o Search Preview by 9.7% in accuracy.\nODS is a general framework for seamlessly augmenting any LLMs -- for example,\nDeepSeek-R1 that achieves 82.4% on SimpleQA and 30.1% on FRAMES -- with search\nand reasoning capabilities to achieve state-of-the-art performance: 88.3% on\nSimpleQA and 75.3% on FRAMES.\n","date":"2025-03-26"}
{"id":"2503.20202","title":"SARGes: Semantically Aligned Reliable Gesture Generation via Intent\n  Chain","abstract":"  Co-speech gesture generation enhances human-computer interaction realism\nthrough speech-synchronized gesture synthesis. However, generating semantically\nmeaningful gestures remains a challenging problem. We propose SARGes, a novel\nframework that leverages large language models (LLMs) to parse speech content\nand generate reliable semantic gesture labels, which subsequently guide the\nsynthesis of meaningful co-speech gestures.First, we constructed a\ncomprehensive co-speech gesture ethogram and developed an LLM-based intent\nchain reasoning mechanism that systematically parses and decomposes gesture\nsemantics into structured inference steps following ethogram criteria,\neffectively guiding LLMs to generate context-aware gesture labels.\nSubsequently, we constructed an intent chain-annotated text-to-gesture label\ndataset and trained a lightweight gesture label generation model, which then\nguides the generation of credible and semantically coherent co-speech gestures.\nExperimental results demonstrate that SARGes achieves highly\nsemantically-aligned gesture labeling (50.2% accuracy) with efficient\nsingle-pass inference (0.4 seconds). The proposed method provides an\ninterpretable intent reasoning pathway for semantic gesture synthesis.\n","date":"2025-03-26"}
{"id":"2503.20205","title":"Generalized Phase Pressure Control Enhanced Reinforcement Learning for\n  Traffic Signal Control","abstract":"  Appropriate traffic state representation is crucial for learning traffic\nsignal control policies. However, most of the current traffic state\nrepresentations are heuristically designed, with insufficient theoretical\nsupport. In this paper, we (1) develop a flexible, efficient, and theoretically\ngrounded method, namely generalized phase pressure (G2P) control, which takes\nonly simple lane features into consideration to decide which phase to be\nactuated; 2) extend the pressure control theory to a general form for\nmulti-homogeneous-lane road networks based on queueing theory; (3) design a new\ntraffic state representation based on the generalized phase state features from\nG2P control; and 4) develop a reinforcement learning (RL)-based algorithm\ntemplate named G2P-XLight, and two RL algorithms, G2P-MPLight and G2P-CoLight,\nby combining the generalized phase state representation with MPLight and\nCoLight, two well-performed RL methods for learning traffic signal control\npolicies. Extensive experiments conducted on multiple real-world datasets\ndemonstrate that G2P control outperforms the state-of-the-art (SOTA) heuristic\nmethod in the transportation field and other recent human-designed heuristic\nmethods; and that the newly proposed G2P-XLight significantly outperforms SOTA\nlearning-based approaches. Our code is available online.\n","date":"2025-03-26"}
{"id":"2503.20207","title":"Reasoning and Learning a Perceptual Metric for Self-Training of\n  Reflective Objects in Bin-Picking with a Low-cost Camera","abstract":"  Bin-picking of metal objects using low-cost RGB-D cameras often suffers from\nsparse depth information and reflective surface textures, leading to errors and\nthe need for manual labeling. To reduce human intervention, we propose a\ntwo-stage framework consisting of a metric learning stage and a self-training\nstage. Specifically, to automatically process data captured by a low-cost\ncamera (LC), we introduce a Multi-object Pose Reasoning (MoPR) algorithm that\noptimizes pose hypotheses under depth, collision, and boundary constraints. To\nfurther refine pose candidates, we adopt a Symmetry-aware Lie-group based\nBayesian Gaussian Mixture Model (SaL-BGMM), integrated with the\nExpectation-Maximization (EM) algorithm, for symmetry-aware filtering.\nAdditionally, we propose a Weighted Ranking Information Noise Contrastive\nEstimation (WR-InfoNCE) loss to enable the LC to learn a perceptual metric from\nreconstructed data, supporting self-training on untrained or even unseen\nobjects. Experimental results show that our approach outperforms several\nstate-of-the-art methods on both the ROBI dataset and our newly introduced\nSelf-ROBI dataset.\n","date":"2025-03-26"}
{"id":"2503.20208","title":"Learning Adaptive Dexterous Grasping from Single Demonstrations","abstract":"  How can robots learn dexterous grasping skills efficiently and apply them\nadaptively based on user instructions? This work tackles two key challenges:\nefficient skill acquisition from limited human demonstrations and\ncontext-driven skill selection. We introduce AdaDexGrasp, a framework that\nlearns a library of grasping skills from a single human demonstration per skill\nand selects the most suitable one using a vision-language model (VLM). To\nimprove sample efficiency, we propose a trajectory following reward that guides\nreinforcement learning (RL) toward states close to a human demonstration while\nallowing flexibility in exploration. To learn beyond the single demonstration,\nwe employ curriculum learning, progressively increasing object pose variations\nto enhance robustness. At deployment, a VLM retrieves the appropriate skill\nbased on user instructions, bridging low-level learned skills with high-level\nintent. We evaluate AdaDexGrasp in both simulation and real-world settings,\nshowing that our approach significantly improves RL efficiency and enables\nlearning human-like grasp strategies across varied object configurations.\nFinally, we demonstrate zero-shot transfer of our learned policies to a\nreal-world PSYONIC Ability Hand, with a 90% success rate across objects,\nsignificantly outperforming the baseline.\n","date":"2025-03-26"}
{"id":"2503.20209","title":"BEAR: A Video Dataset For Fine-grained Behaviors Recognition Oriented\n  with Action and Environment Factors","abstract":"  Behavior recognition is an important task in video representation learning.\nAn essential aspect pertains to effective feature learning conducive to\nbehavior recognition. Recently, researchers have started to study fine-grained\nbehavior recognition, which provides similar behaviors and encourages the model\nto concern with more details of behaviors with effective features for\ndistinction. However, previous fine-grained behaviors limited themselves to\ncontrolling partial information to be similar, leading to an unfair and not\ncomprehensive evaluation of existing works. In this work, we develop a new\nvideo fine-grained behavior dataset, named BEAR, which provides fine-grained\n(i.e. similar) behaviors that uniquely focus on two primary factors defining\nbehavior: Environment and Action. It includes two fine-grained behavior\nprotocols including Fine-grained Behavior with Similar Environments and\nFine-grained Behavior with Similar Actions as well as multiple sub-protocols as\ndifferent scenarios. Furthermore, with this new dataset, we conduct multiple\nexperiments with different behavior recognition models. Our research primarily\nexplores the impact of input modality, a critical element in studying the\nenvironmental and action-based aspects of behavior recognition. Our\nexperimental results yield intriguing insights that have substantial\nimplications for further research endeavors.\n","date":"2025-03-26"}
{"id":"2503.20211","title":"Synthetic-to-Real Self-supervised Robust Depth Estimation via Learning\n  with Motion and Structure Priors","abstract":"  Self-supervised depth estimation from monocular cameras in diverse outdoor\nconditions, such as daytime, rain, and nighttime, is challenging due to the\ndifficulty of learning universal representations and the severe lack of labeled\nreal-world adverse data. Previous methods either rely on synthetic inputs and\npseudo-depth labels or directly apply daytime strategies to adverse conditions,\nresulting in suboptimal results. In this paper, we present the first\nsynthetic-to-real robust depth estimation framework, incorporating motion and\nstructure priors to capture real-world knowledge effectively. In the synthetic\nadaptation, we transfer motion-structure knowledge inside cost volumes for\nbetter robust representation, using a frozen daytime model to train a depth\nestimator in synthetic adverse conditions. In the innovative real adaptation,\nwhich targets to fix synthetic-real gaps, models trained earlier identify the\nweather-insensitive regions with a designed consistency-reweighting strategy to\nemphasize valid pseudo-labels. We introduce a new regularization by gathering\nexplicit depth distributions to constrain the model when facing real-world\ndata. Experiments show that our method outperforms the state-of-the-art across\ndiverse conditions in multi-frame and single-frame evaluations. We achieve\nimprovements of 7.5% and 4.3% in AbsRel and RMSE on average for nuScenes and\nRobotcar datasets (daytime, nighttime, rain). In zero-shot evaluation of\nDrivingStereo (rain, fog), our method generalizes better than the previous\nones.\n","date":"2025-03-26"}
{"id":"2503.20212","title":"Dolphin: A Large-Scale Automatic Speech Recognition Model for Eastern\n  Languages","abstract":"  This report introduces Dolphin, a large-scale multilingual automatic speech\nrecognition (ASR) model that extends the Whisper architecture to support a\nwider range of languages. Our approach integrates in-house proprietary and\nopen-source datasets to refine and optimize Dolphin's performance. The model is\nspecifically designed to achieve notable recognition accuracy for 40 Eastern\nlanguages across East Asia, South Asia, Southeast Asia, and the Middle East,\nwhile also supporting 22 Chinese dialects. Experimental evaluations show that\nDolphin significantly outperforms current state-of-the-art open-source models\nacross various languages. To promote reproducibility and community-driven\ninnovation, we are making our trained models and inference source code publicly\navailable.\n","date":"2025-03-26"}
{"id":"2503.20215","title":"Qwen2.5-Omni Technical Report","abstract":"  In this report, we present Qwen2.5-Omni, an end-to-end multimodal model\ndesigned to perceive diverse modalities, including text, images, audio, and\nvideo, while simultaneously generating text and natural speech responses in a\nstreaming manner. To enable the streaming of multimodal information inputs,\nboth audio and visual encoders utilize a block-wise processing approach. To\nsynchronize the timestamps of video inputs with audio, we organize the audio\nand video sequentially in an interleaved manner and propose a novel position\nembedding approach, named TMRoPE(Time-aligned Multimodal RoPE). To concurrently\ngenerate text and speech while avoiding interference between the two\nmodalities, we propose \\textbf{Thinker-Talker} architecture. In this framework,\nThinker functions as a large language model tasked with text generation, while\nTalker is a dual-track autoregressive model that directly utilizes the hidden\nrepresentations from the Thinker to produce audio tokens as output. Both the\nThinker and Talker models are designed to be trained and inferred in an\nend-to-end manner. For decoding audio tokens in a streaming manner, we\nintroduce a sliding-window DiT that restricts the receptive field, aiming to\nreduce the initial package delay. Qwen2.5-Omni is comparable with the similarly\nsized Qwen2.5-VL and outperforms Qwen2-Audio. Furthermore, Qwen2.5-Omni\nachieves state-of-the-art performance on multimodal benchmarks like Omni-Bench.\nNotably, Qwen2.5-Omni's performance in end-to-end speech instruction following\nis comparable to its capabilities with text inputs, as evidenced by benchmarks\nsuch as MMLU and GSM8K. As for speech generation, Qwen2.5-Omni's streaming\nTalker outperforms most existing streaming and non-streaming alternatives in\nrobustness and naturalness.\n","date":"2025-03-26"}
{"id":"2503.20218","title":"Video Motion Graphs","abstract":"  We present Video Motion Graphs, a system designed to generate realistic human\nmotion videos. Using a reference video and conditional signals such as music or\nmotion tags, the system synthesizes new videos by first retrieving video clips\nwith gestures matching the conditions and then generating interpolation frames\nto seamlessly connect clip boundaries. The core of our approach is HMInterp, a\nrobust Video Frame Interpolation (VFI) model that enables seamless\ninterpolation of discontinuous frames, even for complex motion scenarios like\ndancing. HMInterp i) employs a dual-branch interpolation approach, combining a\nMotion Diffusion Model for human skeleton motion interpolation with a\ndiffusion-based video frame interpolation model for final frame generation. ii)\nadopts condition progressive training to effectively leverage identity strong\nand weak conditions, such as images and pose. These designs ensure both high\nvideo texture quality and accurate motion trajectory. Results show that our\nVideo Motion Graphs outperforms existing generative- and retrieval-based\nmethods for multi-modal conditioned human motion video generation. Project page\ncan be found at https:\/\/h-liu1997.github.io\/Video-Motion-Graphs\/\n","date":"2025-03-26"}
{"id":"2503.20220","title":"DINeMo: Learning Neural Mesh Models with no 3D Annotations","abstract":"  Category-level 3D\/6D pose estimation is a crucial step towards comprehensive\n3D scene understanding, which would enable a broad range of applications in\nrobotics and embodied AI. Recent works explored neural mesh models that\napproach a range of 2D and 3D tasks from an analysis-by-synthesis perspective.\nDespite the largely enhanced robustness to partial occlusion and domain shifts,\nthese methods depended heavily on 3D annotations for part-contrastive learning,\nwhich confines them to a narrow set of categories and hinders efficient\nscaling. In this work, we present DINeMo, a novel neural mesh model that is\ntrained with no 3D annotations by leveraging pseudo-correspondence obtained\nfrom large visual foundation models. We adopt a bidirectional\npseudo-correspondence generation method, which produce pseudo correspondence\nutilize both local appearance features and global context information.\nExperimental results on car datasets demonstrate that our DINeMo outperforms\nprevious zero- and few-shot 3D pose estimation by a wide margin, narrowing the\ngap with fully-supervised methods by 67.3%. Our DINeMo also scales effectively\nand efficiently when incorporating more unlabeled images during training, which\ndemonstrate the advantages over supervised learning methods that rely on 3D\nannotations. Our project page is available at\nhttps:\/\/analysis-by-synthesis.github.io\/DINeMo\/.\n","date":"2025-03-26"}
{"id":"2503.20221","title":"TC-GS: Tri-plane based compression for 3D Gaussian Splatting","abstract":"  Recently, 3D Gaussian Splatting (3DGS) has emerged as a prominent framework\nfor novel view synthesis, providing high fidelity and rapid rendering speed.\nHowever, the substantial data volume of 3DGS and its attributes impede its\npractical utility, requiring compression techniques for reducing memory cost.\nNevertheless, the unorganized shape of 3DGS leads to difficulties in\ncompression. To formulate unstructured attributes into normative distribution,\nwe propose a well-structured tri-plane to encode Gaussian attributes,\nleveraging the distribution of attributes for compression. To exploit the\ncorrelations among adjacent Gaussians, K-Nearest Neighbors (KNN) is used when\ndecoding Gaussian distribution from the Tri-plane. We also introduce Gaussian\nposition information as a prior of the position-sensitive decoder.\nAdditionally, we incorporate an adaptive wavelet loss, aiming to focus on the\nhigh-frequency details as iterations increase. Our approach has achieved\nresults that are comparable to or surpass that of SOTA 3D Gaussians Splatting\ncompression work in extensive experiments across multiple datasets. The codes\nare released at https:\/\/github.com\/timwang2001\/TC-GS.\n","date":"2025-03-26"}
{"id":"2503.20222","title":"Solving 2-D Helmholtz equation in the rectangular, circular, and\n  elliptical domains using neural networks","abstract":"  Physics-informed neural networks offered an alternate way to solve several\ndifferential equations that govern complicated physics. However, their success\nin predicting the acoustic field is limited by the vanishing-gradient problem\nthat occurs when solving the Helmholtz equation. In this paper, a formulation\nis presented that addresses this difficulty. The problem of solving the\ntwo-dimensional Helmholtz equation with the prescribed boundary conditions is\nposed as an unconstrained optimization problem using trial solution method.\nAccording to this method, a trial neural network that satisfies the given\nboundary conditions prior to the training process is constructed using the\ntechnique of transfinite interpolation and the theory of R-functions. This\nansatz is initially applied to the rectangular domain and later extended to the\ncircular and elliptical domains. The acoustic field predicted from the proposed\nformulation is compared with that obtained from the two-dimensional finite\nelement methods. Good agreement is observed in all three domains considered.\nMinor limitations associated with the proposed formulation and their remedies\nare also discussed.\n","date":"2025-03-26"}
{"id":"2503.20227","title":"Advancements in Natural Language Processing: Exploring Transformer-Based\n  Architectures for Text Understanding","abstract":"  Natural Language Processing (NLP) has witnessed a transformative leap with\nthe advent of transformer-based architectures, which have significantly\nenhanced the ability of machines to understand and generate human-like text.\nThis paper explores the advancements in transformer models, such as BERT and\nGPT, focusing on their superior performance in text understanding tasks\ncompared to traditional methods like recurrent neural networks (RNNs). By\nanalyzing statistical properties through visual representations-including\nprobability density functions of text length distributions and feature space\nclassifications-the study highlights the models' proficiency in handling\nlong-range dependencies, adapting to conditional shifts, and extracting\nfeatures for classification, even with overlapping classes. Drawing on recent\n2024 research, including enhancements in multi-hop knowledge graph reasoning\nand context-aware chat interactions, the paper outlines a methodology involving\ndata preparation, model selection, pretraining, fine-tuning, and evaluation.\nThe results demonstrate state-of-the-art performance on benchmarks like GLUE\nand SQuAD, with F1 scores exceeding 90%, though challenges such as high\ncomputational costs persist. This work underscores the pivotal role of\ntransformers in modern NLP and suggests future directions, including efficiency\noptimization and multimodal integration, to further advance language-based AI\nsystems.\n","date":"2025-03-26"}
{"id":"2503.20228","title":"TeleLoRA: Teleporting Model-Specific Alignment Across LLMs","abstract":"  Mitigating Trojans in Large Language Models (LLMs) is one of many tasks where\nalignment data is LLM specific, as different LLMs have different Trojan\ntriggers and trigger behaviors to be removed. In this paper, we introduce\nTeleLoRA (Teleporting Low-Rank Adaptation), a novel framework that synergizes\nmodel-specific alignment data across multiple LLMs to enable zero-shot Trojan\nmitigation on unseen LLMs without alignment data. TeleLoRA learns a unified\ngenerator of LoRA adapter weights by leveraging local activation information\nacross multiple LLMs. This generator is designed to be permutation symmetric to\ngeneralize across models with different architectures and sizes. We optimize\nthe model design for memory efficiency, making it feasible to learn with\nlarge-scale LLMs with minimal computational resources. Experiments on LLM\nTrojan mitigation benchmarks demonstrate that TeleLoRA effectively reduces\nattack success rates while preserving the benign performance of the models.\n","date":"2025-03-26"}
{"id":"2503.20230","title":"TraNCE: Transformative Non-linear Concept Explainer for CNNs","abstract":"  Convolutional neural networks (CNNs) have succeeded remarkably in various\ncomputer vision tasks. However, they are not intrinsically explainable. While\nthe feature-level understanding of CNNs reveals where the models looked,\nconcept-based explainability methods provide insights into what the models saw.\nHowever, their assumption of linear reconstructability of image activations\nfails to capture the intricate relationships within these activations. Their\nFidelity-only approach to evaluating global explanations also presents a new\nconcern. For the first time, we address these limitations with the novel\nTransformative Nonlinear Concept Explainer (TraNCE) for CNNs. Unlike linear\nreconstruction assumptions made by existing methods, TraNCE captures the\nintricate relationships within the activations. This study presents three\noriginal contributions to the CNN explainability literature: (i) An automatic\nconcept discovery mechanism based on variational autoencoders (VAEs). This\ntransformative concept discovery process enhances the identification of\nmeaningful concepts from image activations. (ii) A visualization module that\nleverages the Bessel function to create a smooth transition between\nprototypical image pixels, revealing not only what the CNN saw but also what\nthe CNN avoided, thereby mitigating the challenges of concept duplication as\ndocumented in previous works. (iii) A new metric, the Faith score, integrates\nboth Coherence and Fidelity for a comprehensive evaluation of explainer\nfaithfulness and consistency.\n","date":"2025-03-26"}
{"id":"2503.20231","title":"Dynamics of Algorithmic Content Amplification on TikTok","abstract":"  Intelligent algorithms increasingly shape the content we encounter and engage\nwith online. TikTok's For You feed exemplifies extreme algorithm-driven\ncuration, tailoring the stream of video content almost exclusively based on\nusers' explicit and implicit interactions with the platform. Despite growing\nattention, the dynamics of content amplification on TikTok remain largely\nunquantified. How quickly, and to what extent, does TikTok's algorithm amplify\ncontent aligned with users' interests? To address these questions, we conduct a\nsock-puppet audit, deploying bots with different interests to engage with\nTikTok's \"For You\" feed. Our findings reveal that content aligned with the\nbots' interests undergoes strong amplification, with rapid reinforcement\ntypically occurring within the first 200 videos watched. While amplification is\nconsistently observed across all interests, its intensity varies by interest,\nindicating the emergence of topic-specific biases. Time series analyses and\nMarkov models uncover distinct phases of recommendation dynamics, including\npersistent content reinforcement and a gradual decline in content diversity\nover time. Although TikTok's algorithm preserves some content diversity, we\nfind a strong negative correlation between amplification and exploration: as\nthe amplification of interest-aligned content increases, engagement with unseen\nhashtags declines. These findings contribute to discussions on\nsocio-algorithmic feedback loops in the digital age and the trade-offs between\npersonalization and content diversity.\n","date":"2025-03-26"}
{"id":"2503.20233","title":"Dynamic Learning and Productivity for Data Analysts: A Bayesian Hidden\n  Markov Model Perspective","abstract":"  Data analysts are essential in organizations, transforming raw data into\ninsights that drive decision-making and strategy. This study explores how\nanalysts' productivity evolves on a collaborative platform, focusing on two key\nlearning activities: writing queries and viewing peer queries. While\ntraditional research often assumes static models, where performance improves\nsteadily with cumulative learning, such models fail to capture the dynamic\nnature of real-world learning. To address this, we propose a Hidden Markov\nModel (HMM) that tracks how analysts transition between distinct learning\nstates based on their participation in these activities.\n  Using an industry dataset with 2,001 analysts and 79,797 queries, this study\nidentifies three learning states: novice, intermediate, and advanced.\nProductivity increases as analysts advance to higher states, reflecting the\ncumulative benefits of learning. Writing queries benefits analysts across all\nstates, with the largest gains observed for novices. Viewing peer queries\nsupports novices but may hinder analysts in higher states due to cognitive\noverload or inefficiencies. Transitions between states are also uneven, with\nprogression from intermediate to advanced being particularly challenging. This\nstudy advances understanding of into dynamic learning behavior of knowledge\nworker and offers practical implications for designing systems, optimizing\ntraining, enabling personalized learning, and fostering effective knowledge\nsharing.\n","date":"2025-03-26"}
{"id":"2503.20235","title":"Leveraging 3D Geometric Priors in 2D Rotation Symmetry Detection","abstract":"  Symmetry plays a vital role in understanding structural patterns, aiding\nobject recognition and scene interpretation. This paper focuses on rotation\nsymmetry, where objects remain unchanged when rotated around a central axis,\nrequiring detection of rotation centers and supporting vertices. Traditional\nmethods relied on hand-crafted feature matching, while recent segmentation\nmodels based on convolutional neural networks detect rotation centers but\nstruggle with 3D geometric consistency due to viewpoint distortions. To\novercome this, we propose a model that directly predicts rotation centers and\nvertices in 3D space and projects the results back to 2D while preserving\nstructural integrity. By incorporating a vertex reconstruction stage enforcing\n3D geometric priors -- such as equal side lengths and interior angles -- our\nmodel enhances robustness and accuracy. Experiments on the DENDI dataset show\nsuperior performance in rotation axis detection and validate the impact of 3D\npriors through ablation studies.\n","date":"2025-03-26"}
{"id":"2503.20240","title":"Unconditional Priors Matter! Improving Conditional Generation of\n  Fine-Tuned Diffusion Models","abstract":"  Classifier-Free Guidance (CFG) is a fundamental technique in training\nconditional diffusion models. The common practice for CFG-based training is to\nuse a single network to learn both conditional and unconditional noise\nprediction, with a small dropout rate for conditioning. However, we observe\nthat the joint learning of unconditional noise with limited bandwidth in\ntraining results in poor priors for the unconditional case. More importantly,\nthese poor unconditional noise predictions become a serious reason for\ndegrading the quality of conditional generation. Inspired by the fact that most\nCFG-based conditional models are trained by fine-tuning a base model with\nbetter unconditional generation, we first show that simply replacing the\nunconditional noise in CFG with that predicted by the base model can\nsignificantly improve conditional generation. Furthermore, we show that a\ndiffusion model other than the one the fine-tuned model was trained on can be\nused for unconditional noise replacement. We experimentally verify our claim\nwith a range of CFG-based conditional models for both image and video\ngeneration, including Zero-1-to-3, Versatile Diffusion, DiT, DynamiCrafter, and\nInstructPix2Pix.\n","date":"2025-03-26"}
{"id":"2503.20241","title":"LGR: LLM-Guided Ranking of Frontiers for Object Goal Navigation","abstract":"  Object Goal Navigation (OGN) is a fundamental task for robots and AI, with\nkey applications such as mobile robot image databases (MRID). In particular,\nmapless OGN is essential in scenarios involving unknown or dynamic\nenvironments. This study aims to enhance recent modular mapless OGN systems by\nleveraging the commonsense reasoning capabilities of large language models\n(LLMs). Specifically, we address the challenge of determining the visiting\norder in frontier-based exploration by framing it as a frontier ranking\nproblem. Our approach is grounded in recent findings that, while LLMs cannot\ndetermine the absolute value of a frontier, they excel at evaluating the\nrelative value between multiple frontiers viewed within a single image using\nthe view image as context. We dynamically manage the frontier list by adding\nand removing elements, using an LLM as a ranking model. The ranking results are\nrepresented as reciprocal rank vectors, which are ideal for multi-view,\nmulti-query information fusion. We validate the effectiveness of our method\nthrough evaluations in Habitat-Sim.\n","date":"2025-03-26"}
{"id":"2503.20245","title":"ESSR: An 8K@30FPS Super-Resolution Accelerator With Edge Selective\n  Network","abstract":"  Deep learning-based super-resolution (SR) is challenging to implement in\nresource-constrained edge devices for resolutions beyond full HD due to its\nhigh computational complexity and memory bandwidth requirements. This paper\nintroduces an 8K@30FPS SR accelerator with edge-selective dynamic input\nprocessing. Dynamic processing chooses the appropriate subnets for different\npatches based on simple input edge criteria, achieving a 50\\% MAC reduction\nwith only a 0.1dB PSNR decrease. The quality of reconstruction images is\nguaranteed and maximized its potential with \\textit{resource adaptive model\nswitching} even under resource constraints. In conjunction with\nhardware-specific refinements, the model size is reduced by 84\\% to 51K, but\nwith a decrease of less than 0.6dB PSNR. Additionally, to support dynamic\nprocessing with high utilization, this design incorporates a\n\\textit{configurable group of layer mapping} that synergizes with the\n\\textit{structure-friendly fusion block}, resulting in 77\\% hardware\nutilization and up to 79\\% reduction in feature SRAM access. The\nimplementation, using the TSMC 28nm process, can achieve 8K@30FPS throughput at\n800MHz with a gate count of 2749K, 0.2075W power consumption, and 4797Mpixels\/J\nenergy efficiency, exceeding previous work.\n","date":"2025-03-26"}
{"id":"2503.20248","title":"Incremental Object Keypoint Learning","abstract":"  Existing progress in object keypoint estimation primarily benefits from the\nconventional supervised learning paradigm based on numerous data labeled with\npre-defined keypoints. However, these well-trained models can hardly detect the\nundefined new keypoints in test time, which largely hinders their feasibility\nfor diverse downstream tasks. To handle this, various solutions are explored\nbut still suffer from either limited generalizability or transferability.\nTherefore, in this paper, we explore a novel keypoint learning paradigm in that\nwe only annotate new keypoints in the new data and incrementally train the\nmodel, without retaining any old data, called Incremental object Keypoint\nLearning (IKL). A two-stage learning scheme as a novel baseline tailored to IKL\nis developed. In the first Knowledge Association stage, given the data labeled\nwith only new keypoints, an auxiliary KA-Net is trained to automatically\nassociate the old keypoints to these new ones based on their spatial and\nintrinsic anatomical relations. In the second Mutual Promotion stage, based on\na keypoint-oriented spatial distillation loss, we jointly leverage the\nauxiliary KA-Net and the old model for knowledge consolidation to mutually\npromote the estimation of all old and new keypoints. Owing to the investigation\nof the correlations between new and old keypoints, our proposed method can not\njust effectively mitigate the catastrophic forgetting of old keypoints, but may\neven further improve the estimation of the old ones and achieve a positive\ntransfer beyond anti-forgetting. Such an observation has been solidly verified\nby extensive experiments on different keypoint datasets, where our method\nexhibits superiority in alleviating the forgetting issue and boosting\nperformance while enjoying labeling efficiency even under the low-shot data\nregime.\n","date":"2025-03-26"}
{"id":"2503.20252","title":"LogicQA: Logical Anomaly Detection with Vision Language Model Generated\n  Questions","abstract":"  Anomaly Detection (AD) focuses on detecting samples that differ from the\nstandard pattern, making it a vital tool in process control. Logical anomalies\nmay appear visually normal yet violate predefined constraints on object\npresence, arrangement, or quantity, depending on reasoning and explainability.\nWe introduce LogicQA, a framework that enhances AD by providing industrial\noperators with explanations for logical anomalies. LogicQA compiles\nautomatically generated questions into a checklist and collects responses to\nidentify violations of logical constraints. LogicQA is training-free,\nannotation-free, and operates in a few-shot setting. We achieve\nstate-of-the-art (SOTA) Logical AD performance on public benchmarks, MVTec LOCO\nAD, with an AUROC of 87.6 percent and an F1-max of 87.0 percent along with the\nexplanations of anomalies. Also, our approach has shown outstanding performance\non semiconductor SEM corporate data, further validating its effectiveness in\nindustrial applications.\n","date":"2025-03-26"}
{"id":"2503.20258","title":"Mamba-3D as Masked Autoencoders for Accurate and Data-Efficient Analysis\n  of Medical Ultrasound Videos","abstract":"  Ultrasound videos are an important form of clinical imaging data, and deep\nlearning-based automated analysis can improve diagnostic accuracy and clinical\nefficiency. However, the scarcity of labeled data and the inherent challenges\nof video analysis have impeded the advancement of related methods. In this\nwork, we introduce E-ViM$^3$, a data-efficient Vision Mamba network that\npreserves the 3D structure of video data, enhancing long-range dependencies and\ninductive biases to better model space-time correlations. With our design of\nEnclosure Global Tokens (EGT), the model captures and aggregates global\nfeatures more effectively than competing methods. To further improve data\nefficiency, we employ masked video modeling for self-supervised pre-training,\nwith the proposed Spatial-Temporal Chained (STC) masking strategy designed to\nadapt to various video scenarios. Experiments demonstrate that E-ViM$^3$\nperforms as the state-of-the-art in two high-level semantic analysis tasks\nacross four datasets of varying sizes: EchoNet-Dynamic, CAMUS, MICCAI-BUV, and\nWHBUS. Furthermore, our model achieves competitive performance with limited\nlabels, highlighting its potential impact on real-world clinical applications.\n","date":"2025-03-26"}
{"id":"2503.20264","title":"Revisit Time Series Classification Benchmark: The Impact of Temporal\n  Information for Classification","abstract":"  Time series classification is usually regarded as a distinct task from\ntabular data classification due to the importance of temporal information.\nHowever, in this paper, by performing permutation tests that disrupt temporal\ninformation on the UCR time series classification archive, the most widely used\nbenchmark for time series classification, we identify a significant proportion\nof datasets where temporal information has little to no impact on\nclassification. Many of these datasets are tabular in nature or rely mainly on\ntabular features, leading to potentially biased evaluations of time series\nclassifiers focused on temporal information. To address this, we propose UCR\nAugmented, a benchmark based on the UCR time series classification archive\ndesigned to evaluate classifiers' ability to extract and utilize temporal\ninformation. Testing classifiers from seven categories on this benchmark\nrevealed notable shifts in performance rankings. Some previously overlooked\napproaches perform well, while others see their performance decline\nsignificantly when temporal information is crucial. UCR Augmented provides a\nmore robust framework for assessing time series classifiers, ensuring fairer\nevaluations. Our code is available at\nhttps:\/\/github.com\/YunruiZhang\/Revisit-Time-Series-Classification-Benchmark.\n","date":"2025-03-26"}
{"id":"2503.20268","title":"EGVD: Event-Guided Video Diffusion Model for Physically Realistic\n  Large-Motion Frame Interpolation","abstract":"  Video frame interpolation (VFI) in scenarios with large motion remains\nchallenging due to motion ambiguity between frames. While event cameras can\ncapture high temporal resolution motion information, existing event-based VFI\nmethods struggle with limited training data and complex motion patterns. In\nthis paper, we introduce Event-Guided Video Diffusion Model (EGVD), a novel\nframework that leverages the powerful priors of pre-trained stable video\ndiffusion models alongside the precise temporal information from event cameras.\nOur approach features a Multi-modal Motion Condition Generator (MMCG) that\neffectively integrates RGB frames and event signals to guide the diffusion\nprocess, producing physically realistic intermediate frames. We employ a\nselective fine-tuning strategy that preserves spatial modeling capabilities\nwhile efficiently incorporating event-guided temporal information. We\nincorporate input-output normalization techniques inspired by recent advances\nin diffusion modeling to enhance training stability across varying noise\nlevels. To improve generalization, we construct a comprehensive dataset\ncombining both real and simulated event data across diverse scenarios.\nExtensive experiments on both real and simulated datasets demonstrate that EGVD\nsignificantly outperforms existing methods in handling large motion and\nchallenging lighting conditions, achieving substantial improvements in\nperceptual quality metrics (27.4% better LPIPS on Prophesee and 24.1% on BSRGB)\nwhile maintaining competitive fidelity measures. Code and datasets available\nat: https:\/\/github.com\/OpenImagingLab\/EGVD.\n","date":"2025-03-26"}
{"id":"2503.20271","title":"ViLBench: A Suite for Vision-Language Process Reward Modeling","abstract":"  Process-supervised reward models serve as a fine-grained function that\nprovides detailed step-wise feedback to model responses, facilitating effective\nselection of reasoning trajectories for complex tasks. Despite its advantages,\nevaluation on PRMs remains less explored, especially in the multimodal domain.\nTo address this gap, this paper first benchmarks current vision large language\nmodels (VLLMs) as two types of reward models: output reward models (ORMs) and\nprocess reward models (PRMs) on multiple vision-language benchmarks, which\nreveal that neither ORM nor PRM consistently outperforms across all tasks, and\nsuperior VLLMs do not necessarily yield better rewarding performance. To\nfurther advance evaluation, we introduce ViLBench, a vision-language benchmark\ndesigned to require intensive process reward signals. Notably, OpenAI's GPT-4o\nwith Chain-of-Thought (CoT) achieves only 27.3% accuracy, indicating the\nbenchmark's challenge for current VLLMs. Lastly, we preliminarily showcase a\npromising pathway towards bridging the gap between general VLLMs and reward\nmodels -- by collecting 73.6K vision-language process reward data using an\nenhanced tree-search algorithm, our 3B model is able to achieve an average\nimprovement of 3.3% over standard CoT and up to 2.5% compared to its untrained\ncounterpart on ViLBench by selecting OpenAI o1's generations. We release the\nimplementations at https:\/\/ucsc-vlaa.github.io\/ViLBench with our code, model,\nand data.\n","date":"2025-03-26"}
{"id":"2503.20272","title":"An $(\\epsilon,\\delta)$-accurate level set estimation with a stopping\n  criterion","abstract":"  The level set estimation problem seeks to identify regions within a set of\ncandidate points where an unknown and costly to evaluate function's value\nexceeds a specified threshold, providing an efficient alternative to exhaustive\nevaluations of function values. Traditional methods often use sequential\noptimization strategies to find $\\epsilon$-accurate solutions, which permit a\nmargin around the threshold contour but frequently lack effective stopping\ncriteria, leading to excessive exploration and inefficiencies. This paper\nintroduces an acquisition strategy for level set estimation that incorporates a\nstopping criterion, ensuring the algorithm halts when further exploration is\nunlikely to yield improvements, thereby reducing unnecessary function\nevaluations. We theoretically prove that our method satisfies\n$\\epsilon$-accuracy with a confidence level of $1 - \\delta$, addressing a key\ngap in existing approaches. Furthermore, we show that this also leads to\nguarantees on the lower bounds of performance metrics such as F-score.\nNumerical experiments demonstrate that the proposed acquisition function\nachieves comparable precision to existing methods while confirming that the\nstopping criterion effectively terminates the algorithm once adequate\nexploration is completed.\n","date":"2025-03-26"}
{"id":"2503.20278","title":"The cell as a token: high-dimensional geometry in language models and\n  cell embeddings","abstract":"  Single-cell sequencing technology maps cells to a high-dimensional space\nencoding their internal activity. This process mirrors parallel developments in\nmachine learning, where large language models ingest unstructured text by\nconverting words into discrete tokens embedded within a high-dimensional vector\nspace. This perspective explores how advances in understanding the structure of\nlanguage embeddings can inform ongoing efforts to analyze and visualize single\ncell datasets. We discuss how the context of tokens influences the geometry of\nembedding space, and the role of low-dimensional manifolds in shaping this\nspace's robustness and interpretability. We highlight new developments in\nlanguage modeling, such as interpretability probes and in-context reasoning,\nthat can inform future efforts to construct and consolidate cell atlases.\n","date":"2025-03-26"}
{"id":"2503.20279","title":"sudo rm -rf agentic_security","abstract":"  Large Language Models (LLMs) are increasingly deployed as computer-use\nagents, autonomously performing tasks within real desktop or web environments.\nWhile this evolution greatly expands practical use cases for humans, it also\ncreates serious security exposures. We present SUDO (Screen-based Universal\nDetox2Tox Offense), a novel attack framework that systematically bypasses\nrefusal trained safeguards in commercial computer-use agents, such as Claude\nComputer Use. The core mechanism, Detox2Tox, transforms harmful requests (that\nagents initially reject) into seemingly benign requests via detoxification,\nsecures detailed instructions from advanced vision language models (VLMs), and\nthen reintroduces malicious content via toxification just before execution.\nUnlike conventional jailbreaks, SUDO iteratively refines its attacks based on a\nbuilt-in refusal feedback, making it increasingly effective against robust\npolicy filters. In extensive tests spanning 50 real-world tasks and multiple\nstate-of-the-art VLMs, SUDO achieves a stark attack success rate of 24% (with\nno refinement), and up to 41% (by its iterative refinement) in Claude Computer\nUse. By revealing these vulnerabilities and demonstrating the ease with which\nthey can be exploited in real-world computing environments, this paper\nhighlights an immediate need for robust, context-aware safeguards. WARNING:\nThis paper includes harmful or offensive model outputs.\n","date":"2025-03-26"}
{"id":"2503.20281","title":"Are We There Yet? Unraveling the State-of-the-Art Graph Network\n  Intrusion Detection Systems","abstract":"  Network Intrusion Detection Systems (NIDS) are vital for ensuring enterprise\nsecurity. Recently, Graph-based NIDS (GIDS) have attracted considerable\nattention because of their capability to effectively capture the complex\nrelationships within the graph structures of data communications. Despite their\npromise, the reproducibility and replicability of these GIDS remain largely\nunexplored, posing challenges for developing reliable and robust detection\nsystems. This study bridges this gap by designing a systematic approach to\nevaluate state-of-the-art GIDS, which includes critically assessing, extending,\nand clarifying the findings of these systems. We further assess the robustness\nof GIDS under adversarial attacks. Evaluations were conducted on three public\ndatasets as well as a newly collected large-scale enterprise dataset. Our\nfindings reveal significant performance discrepancies, highlighting challenges\nrelated to dataset scale, model inputs, and implementation settings. We\ndemonstrate difficulties in reproducing and replicating results, particularly\nconcerning false positive rates and robustness against adversarial attacks.\nThis work provides valuable insights and recommendations for future research,\nemphasizing the importance of rigorous reproduction and replication studies in\ndeveloping robust and generalizable GIDS solutions.\n","date":"2025-03-26"}
{"id":"2503.20282","title":"Faster Parameter-Efficient Tuning with Token Redundancy Reduction","abstract":"  Parameter-efficient tuning (PET) aims to transfer pre-trained foundation\nmodels to downstream tasks by learning a small number of parameters. Compared\nto traditional fine-tuning, which updates the entire model, PET significantly\nreduces storage and transfer costs for each task regardless of exponentially\nincreasing pre-trained model capacity. However, most PET methods inherit the\ninference latency of their large backbone models and often introduce additional\ncomputational overhead due to additional modules (e.g. adapters), limiting\ntheir practicality for compute-intensive applications. In this paper, we\npropose Faster Parameter-Efficient Tuning (FPET), a novel approach that\nenhances inference speed and training efficiency while maintaining high storage\nefficiency. Specifically, we introduce a plug-and-play token redundancy\nreduction module delicately designed for PET. This module refines tokens from\nthe self-attention layer using an adapter to learn the accurate similarity\nbetween tokens and cuts off the tokens through a fully-differentiable token\nmerging strategy, which uses a straight-through estimator for optimal token\nreduction. Experimental results prove that our FPET achieves faster inference\nand higher memory efficiency than the pre-trained backbone while keeping\ncompetitive performance on par with state-of-the-art PET methods.\n","date":"2025-03-26"}
{"id":"2503.20285","title":"Model-Based Offline Reinforcement Learning with Adversarial Data\n  Augmentation","abstract":"  Model-based offline Reinforcement Learning (RL) constructs environment models\nfrom offline datasets to perform conservative policy optimization. Existing\napproaches focus on learning state transitions through ensemble models,\nrollouting conservative estimation to mitigate extrapolation errors. However,\nthe static data makes it challenging to develop a robust policy, and offline\nagents cannot access the environment to gather new data. To address these\nchallenges, we introduce Model-based Offline Reinforcement learning with\nAdversariaL data augmentation (MORAL). In MORAL, we replace the fixed horizon\nrollout by employing adversaria data augmentation to execute alternating\nsampling with ensemble models to enrich training data. Specifically, this\nadversarial process dynamically selects ensemble models against policy for\nbiased sampling, mitigating the optimistic estimation of fixed models, thus\nrobustly expanding the training data for policy optimization. Moreover, a\ndifferential factor is integrated into the adversarial process for\nregularization, ensuring error minimization in extrapolations. This\ndata-augmented optimization adapts to diverse offline tasks without rollout\nhorizon tuning, showing remarkable applicability. Extensive experiments on D4RL\nbenchmark demonstrate that MORAL outperforms other model-based offline RL\nmethods in terms of policy learning and sample efficiency.\n","date":"2025-03-26"}
{"id":"2503.20286","title":"Bridging Evolutionary Multiobjective Optimization and GPU Acceleration\n  via Tensorization","abstract":"  Evolutionary multiobjective optimization (EMO) has made significant strides\nover the past two decades. However, as problem scales and complexities\nincrease, traditional EMO algorithms face substantial performance limitations\ndue to insufficient parallelism and scalability. While most work has focused on\nalgorithm design to address these challenges, little attention has been given\nto hardware acceleration, thereby leaving a clear gap between EMO algorithms\nand advanced computing devices, such as GPUs. To bridge the gap, we propose to\nparallelize EMO algorithms on GPUs via the tensorization methodology. By\nemploying tensorization, the data structures and operations of EMO algorithms\nare transformed into concise tensor representations, which seamlessly enables\nautomatic utilization of GPU computing. We demonstrate the effectiveness of our\napproach by applying it to three representative EMO algorithms: NSGA-III,\nMOEA\/D, and HypE. To comprehensively assess our methodology, we introduce a\nmultiobjective robot control benchmark using a GPU-accelerated physics engine.\nOur experiments show that the tensorized EMO algorithms achieve speedups of up\nto 1113x compared to their CPU-based counterparts, while maintaining solution\nquality and effectively scaling population sizes to hundreds of thousands.\nFurthermore, the tensorized EMO algorithms efficiently tackle complex\nmultiobjective robot control tasks, producing high-quality solutions with\ndiverse behaviors. Source codes are available at\nhttps:\/\/github.com\/EMI-Group\/evomo.\n","date":"2025-03-26"}
{"id":"2503.20287","title":"InsViE-1M: Effective Instruction-based Video Editing with Elaborate\n  Dataset Construction","abstract":"  Instruction-based video editing allows effective and interactive editing of\nvideos using only instructions without extra inputs such as masks or\nattributes. However, collecting high-quality training triplets (source video,\nedited video, instruction) is a challenging task. Existing datasets mostly\nconsist of low-resolution, short duration, and limited amount of source videos\nwith unsatisfactory editing quality, limiting the performance of trained\nediting models. In this work, we present a high-quality Instruction-based Video\nEditing dataset with 1M triplets, namely InsViE-1M. We first curate\nhigh-resolution and high-quality source videos and images, then design an\neffective editing-filtering pipeline to construct high-quality editing triplets\nfor model training. For a source video, we generate multiple edited samples of\nits first frame with different intensities of classifier-free guidance, which\nare automatically filtered by GPT-4o with carefully crafted guidelines. The\nedited first frame is propagated to subsequent frames to produce the edited\nvideo, followed by another round of filtering for frame quality and motion\nevaluation. We also generate and filter a variety of video editing triplets\nfrom high-quality images. With the InsViE-1M dataset, we propose a multi-stage\nlearning strategy to train our InsViE model, progressively enhancing its\ninstruction following and editing ability. Extensive experiments demonstrate\nthe advantages of our InsViE-1M dataset and the trained model over\nstate-of-the-art works. Codes are available at InsViE.\n","date":"2025-03-26"}
{"id":"2503.20289","title":"RelTriple: Learning Plausible Indoor Layouts by Integrating Relationship\n  Triples into the Diffusion Process","abstract":"  The generation of indoor furniture layouts has significant applications in\naugmented reality, smart homes, and architectural design. Successful furniture\narrangement requires proper physical relationships (e.g., collision avoidance)\nand spacing relationships between furniture and their functional zones to be\nrespected. However, manually defined relationships are almost always incomplete\nand can produce unrealistic layouts. This work instead extracts spacing\nrelationships automatically based on a hierarchical analysis and adopts the\nDelaunay Triangulation to produce important triple relationships. Compared to\npairwise relationship modeling, triple relationships account for interactions\nand space utilization among multiple objects. To this end, we introduce\nRelTriple, a novel approach that enhances furniture distribution by learning\nspacing relationships between objects and regions. We formulate triple\nrelationships as object-to-object (O2O) losses and object-to-region (O2R)\nlosses and integrate them directly into the training process of generative\ndiffusion. Our approach consistently improves over existing state-of-the-art\nmethods in visual results evaluation metrics on unconditional layout\ngeneration, floorplan-conditioned layout generation, and scene rearrangement,\nachieving at least 12% on the introduced spatial relationship metric and\nsuperior spatial coherence and practical usability.\n","date":"2025-03-26"}
{"id":"2503.20290","title":"QualiSpeech: A Speech Quality Assessment Dataset with Natural Language\n  Reasoning and Descriptions","abstract":"  This paper explores a novel perspective to speech quality assessment by\nleveraging natural language descriptions, offering richer, more nuanced\ninsights than traditional numerical scoring methods. Natural language feedback\nprovides instructive recommendations and detailed evaluations, yet existing\ndatasets lack the comprehensive annotations needed for this approach. To bridge\nthis gap, we introduce QualiSpeech, a comprehensive low-level speech quality\nassessment dataset encompassing 11 key aspects and detailed natural language\ncomments that include reasoning and contextual insights. Additionally, we\npropose the QualiSpeech Benchmark to evaluate the low-level speech\nunderstanding capabilities of auditory large language models (LLMs).\nExperimental results demonstrate that finetuned auditory LLMs can reliably\ngenerate detailed descriptions of noise and distortion, effectively identifying\ntheir types and temporal characteristics. The results further highlight the\npotential for incorporating reasoning to enhance the accuracy and reliability\nof quality assessments. The dataset will be released at\nhttps:\/\/huggingface.co\/datasets\/tsinghua-ee\/QualiSpeech.\n","date":"2025-03-26"}
{"id":"2503.20291","title":"CryoSAMU: Enhancing 3D Cryo-EM Density Maps of Protein Structures at\n  Intermediate Resolution with Structure-Aware Multimodal U-Nets","abstract":"  Enhancing cryogenic electron microscopy (cryo-EM) 3D density maps at\nintermediate resolution (4-8 {\\AA}) is crucial in protein structure\ndetermination. Recent advances in deep learning have led to the development of\nautomated approaches for enhancing experimental cryo-EM density maps. Yet,\nthese methods are not optimized for intermediate-resolution maps and rely on\nmap density features alone. To address this, we propose CryoSAMU, a novel\nmethod designed to enhance 3D cryo-EM density maps of protein structures using\nstructure-aware multimodal U-Nets and trained on curated\nintermediate-resolution density maps. We comprehensively evaluate CryoSAMU\nacross various metrics and demonstrate its competitive performance compared to\nstate-of-the-art methods. Notably, CryoSAMU achieves significantly faster\nprocessing speed, showing promise for future practical applications. Our code\nis available at https:\/\/github.com\/chenwei-zhang\/CryoSAMU.\n","date":"2025-03-26"}
{"id":"2503.20294","title":"Context-Aware Weakly Supervised Image Manipulation Localization with SAM\n  Refinement","abstract":"  Malicious image manipulation poses societal risks, increasing the importance\nof effective image manipulation detection methods. Recent approaches in image\nmanipulation detection have largely been driven by fully supervised approaches,\nwhich require labor-intensive pixel-level annotations. Thus, it is essential to\nexplore weakly supervised image manipulation localization methods that only\nrequire image-level binary labels for training. However, existing weakly\nsupervised image manipulation methods overlook the importance of edge\ninformation for accurate localization, leading to suboptimal localization\nperformance. To address this, we propose a Context-Aware Boundary Localization\n(CABL) module to aggregate boundary features and learn context-inconsistency\nfor localizing manipulated areas. Furthermore, by leveraging Class Activation\nMapping (CAM) and Segment Anything Model (SAM), we introduce the CAM-Guided SAM\nRefinement (CGSR) module to generate more accurate manipulation localization\nmaps. By integrating two modules, we present a novel weakly supervised\nframework based on a dual-branch Transformer-CNN architecture. Our method\nachieves outstanding localization performance across multiple datasets.\n","date":"2025-03-26"}
{"id":"2503.20297","title":"Traversing Distortion-Perception Tradeoff using a Single Score-Based\n  Generative Model","abstract":"  The distortion-perception (DP) tradeoff reveals a fundamental conflict\nbetween distortion metrics (e.g., MSE and PSNR) and perceptual quality. Recent\nresearch has increasingly concentrated on evaluating denoising algorithms\nwithin the DP framework. However, existing algorithms either prioritize\nperceptual quality by sacrificing acceptable distortion, or focus on minimizing\nMSE for faithful restoration. When the goal shifts or noisy measurements vary,\nadapting to different points on the DP plane needs retraining or even\nre-designing the model. Inspired by recent advances in solving inverse problems\nusing score-based generative models, we explore the potential of flexibly and\noptimally traversing DP tradeoffs using a single pre-trained score-based model.\nSpecifically, we introduce a variance-scaled reverse diffusion process and\ntheoretically characterize the marginal distribution. We then prove that the\nproposed sample process is an optimal solution to the DP tradeoff for\nconditional Gaussian distribution. Experimental results on two-dimensional and\nimage datasets illustrate that a single score network can effectively and\nflexibly traverse the DP tradeoff for general denoising problems.\n","date":"2025-03-26"}
{"id":"2503.20301","title":"Attribute-formed Class-specific Concept Space: Endowing Language\n  Bottleneck Model with Better Interpretability and Scalability","abstract":"  Language Bottleneck Models (LBMs) are proposed to achieve interpretable image\nrecognition by classifying images based on textual concept bottlenecks.\nHowever, current LBMs simply list all concepts together as the bottleneck\nlayer, leading to the spurious cue inference problem and cannot generalized to\nunseen classes. To address these limitations, we propose the Attribute-formed\nLanguage Bottleneck Model (ALBM). ALBM organizes concepts in the\nattribute-formed class-specific space, where concepts are descriptions of\nspecific attributes for specific classes. In this way, ALBM can avoid the\nspurious cue inference problem by classifying solely based on the essential\nconcepts of each class. In addition, the cross-class unified attribute set also\nensures that the concept spaces of different classes have strong correlations,\nas a result, the learned concept classifier can be easily generalized to unseen\nclasses. Moreover, to further improve interpretability, we propose Visual\nAttribute Prompt Learning (VAPL) to extract visual features on fine-grained\nattributes. Furthermore, to avoid labor-intensive concept annotation, we\npropose the Description, Summary, and Supplement (DSS) strategy to\nautomatically generate high-quality concept sets with a complete and precise\nattribute. Extensive experiments on 9 widely used few-shot benchmarks\ndemonstrate the interpretability, transferability, and performance of our\napproach. The code and collected concept sets are available at\nhttps:\/\/github.com\/tiggers23\/ALBM.\n","date":"2025-03-26"}
{"id":"2503.20302","title":"A Multilingual, Culture-First Approach to Addressing Misgendering in LLM\n  Applications","abstract":"  Misgendering is the act of referring to someone by a gender that does not\nmatch their chosen identity. It marginalizes and undermines a person's sense of\nself, causing significant harm. English-based approaches have clear-cut\napproaches to avoiding misgendering, such as the use of the pronoun ``they''.\nHowever, other languages pose unique challenges due to both grammatical and\ncultural constructs. In this work we develop methodologies to assess and\nmitigate misgendering across 42 languages and dialects using a\nparticipatory-design approach to design effective and appropriate guardrails\nacross all languages. We test these guardrails in a standard large language\nmodel-based application (meeting transcript summarization), where both the data\ngeneration and the annotation steps followed a human-in-the-loop approach. We\nfind that the proposed guardrails are very effective in reducing misgendering\nrates across all languages in the summaries generated, and without incurring\nloss of quality. Our human-in-the-loop approach demonstrates a method to\nfeasibly scale inclusive and responsible AI-based solutions across multiple\nlanguages and cultures.\n","date":"2025-03-26"}
{"id":"2503.20306","title":"3D Convolutional Neural Networks for Improved Detection of Intracranial\n  bleeding in CT Imaging","abstract":"  Background: Intracranial bleeding (IB) is a life-threatening condition caused\nby traumatic brain injuries, including epidural, subdural, subarachnoid, and\nintraparenchymal hemorrhages. Rapid and accurate detection is crucial to\nprevent severe complications. Traditional imaging can be slow and prone to\nvariability, especially in high-pressure scenarios. Artificial Intelligence\n(AI) provides a solution by quickly analyzing medical images, identifying\nsubtle hemorrhages, and flagging urgent cases. By enhancing diagnostic speed\nand accuracy, AI improves workflows and patient care. This article explores\nAI's role in transforming IB detection in emergency settings.\n  Methods: A U-shaped 3D Convolutional Neural Network (CNN) automates IB\ndetection and classification in volumetric CT scans. Advanced preprocessing,\nincluding CLAHE and intensity normalization, enhances image quality. The\narchitecture preserves spatial and contextual details for precise segmentation.\nA dataset of 2,912 annotated CT scans was used for training and evaluation.\n  Results: The model achieved high performance across major bleed types, with\nprecision, recall, and accuracy exceeding 90 percent in most cases 96 percent\nprecision for epidural hemorrhages and 94 percent accuracy for subarachnoid\nhemorrhages. Its ability to classify and localize hemorrhages highlights its\nclinical reliability.\n  Conclusion: This U-shaped 3D CNN offers a scalable solution for automating IB\ndetection, reducing diagnostic delays, and improving emergency care outcomes.\nFuture work will expand dataset diversity, optimize real-time processing, and\nintegrate multimodal data for enhanced clinical applicability.\n","date":"2025-03-26"}
{"id":"2503.20308","title":"Perceptually Accurate 3D Talking Head Generation: New Definitions,\n  Speech-Mesh Representation, and Evaluation Metrics","abstract":"  Recent advancements in speech-driven 3D talking head generation have made\nsignificant progress in lip synchronization. However, existing models still\nstruggle to capture the perceptual alignment between varying speech\ncharacteristics and corresponding lip movements. In this work, we claim that\nthree criteria -- Temporal Synchronization, Lip Readability, and Expressiveness\n-- are crucial for achieving perceptually accurate lip movements. Motivated by\nour hypothesis that a desirable representation space exists to meet these three\ncriteria, we introduce a speech-mesh synchronized representation that captures\nintricate correspondences between speech signals and 3D face meshes. We found\nthat our learned representation exhibits desirable characteristics, and we plug\nit into existing models as a perceptual loss to better align lip movements to\nthe given speech. In addition, we utilize this representation as a perceptual\nmetric and introduce two other physically grounded lip synchronization metrics\nto assess how well the generated 3D talking heads align with these three\ncriteria. Experiments show that training 3D talking head generation models with\nour perceptual loss significantly improve all three aspects of perceptually\naccurate lip synchronization. Codes and datasets are available at\nhttps:\/\/perceptual-3d-talking-head.github.io\/.\n","date":"2025-03-26"}
{"id":"2503.20309","title":"Instruction-Oriented Preference Alignment for Enhancing Multi-Modal\n  Comprehension Capability of MLLMs","abstract":"  Preference alignment has emerged as an effective strategy to enhance the\nperformance of Multimodal Large Language Models (MLLMs) following supervised\nfine-tuning. While existing preference alignment methods predominantly target\nhallucination factors, they overlook the factors essential for multi-modal\ncomprehension capabilities, often narrowing their improvements on hallucination\nmitigation. To bridge this gap, we propose Instruction-oriented Preference\nAlignment (IPA), a scalable framework designed to automatically construct\nalignment preferences grounded in instruction fulfillment efficacy. Our method\ninvolves an automated preference construction coupled with a dedicated\nverification process that identifies instruction-oriented factors, avoiding\nsignificant variability in response representations. Additionally, IPA\nincorporates a progressive preference collection pipeline, further recalling\nchallenging samples through model self-evolution and reference-guided\nrefinement. Experiments conducted on Qwen2VL-7B demonstrate IPA's effectiveness\nacross multiple benchmarks, including hallucination evaluation, visual question\nanswering, and text understanding tasks, highlighting its capability to enhance\ngeneral comprehension.\n","date":"2025-03-26"}
{"id":"2503.20310","title":"Enabling Heterogeneous Adversarial Transferability via Feature\n  Permutation Attacks","abstract":"  Adversarial attacks in black-box settings are highly practical, with\ntransfer-based attacks being the most effective at generating adversarial\nexamples (AEs) that transfer from surrogate models to unseen target models.\nHowever, their performance significantly degrades when transferring across\nheterogeneous architectures -- such as CNNs, MLPs, and Vision Transformers\n(ViTs) -- due to fundamental architectural differences. To address this, we\npropose Feature Permutation Attack (FPA), a zero-FLOP, parameter-free method\nthat enhances adversarial transferability across diverse architectures. FPA\nintroduces a novel feature permutation (FP) operation, which rearranges pixel\nvalues in selected feature maps to simulate long-range dependencies,\neffectively making CNNs behave more like ViTs and MLPs. This enhances feature\ndiversity and improves transferability both across heterogeneous architectures\nand within homogeneous CNNs. Extensive evaluations on 14 state-of-the-art\narchitectures show that FPA achieves maximum absolute gains in attack success\nrates of 7.68% on CNNs, 14.57% on ViTs, and 14.48% on MLPs, outperforming\nexisting black-box attacks. Additionally, FPA is highly generalizable and can\nseamlessly integrate with other transfer-based attacks to further boost their\nperformance. Our findings establish FPA as a robust, efficient, and\ncomputationally lightweight strategy for enhancing adversarial transferability\nacross heterogeneous architectures.\n","date":"2025-03-26"}
{"id":"2503.20314","title":"Wan: Open and Advanced Large-Scale Video Generative Models","abstract":"  This report presents Wan, a comprehensive and open suite of video foundation\nmodels designed to push the boundaries of video generation. Built upon the\nmainstream diffusion transformer paradigm, Wan achieves significant\nadvancements in generative capabilities through a series of innovations,\nincluding our novel VAE, scalable pre-training strategies, large-scale data\ncuration, and automated evaluation metrics. These contributions collectively\nenhance the model's performance and versatility. Specifically, Wan is\ncharacterized by four key features: Leading Performance: The 14B model of Wan,\ntrained on a vast dataset comprising billions of images and videos,\ndemonstrates the scaling laws of video generation with respect to both data and\nmodel size. It consistently outperforms the existing open-source models as well\nas state-of-the-art commercial solutions across multiple internal and external\nbenchmarks, demonstrating a clear and significant performance superiority.\nComprehensiveness: Wan offers two capable models, i.e., 1.3B and 14B\nparameters, for efficiency and effectiveness respectively. It also covers\nmultiple downstream applications, including image-to-video, instruction-guided\nvideo editing, and personal video generation, encompassing up to eight tasks.\nConsumer-Grade Efficiency: The 1.3B model demonstrates exceptional resource\nefficiency, requiring only 8.19 GB VRAM, making it compatible with a wide range\nof consumer-grade GPUs. Openness: We open-source the entire series of Wan,\nincluding source code and all models, with the goal of fostering the growth of\nthe video generation community. This openness seeks to significantly expand the\ncreative possibilities of video production in the industry and provide academia\nwith high-quality video foundation models. All the code and models are\navailable at https:\/\/github.com\/Wan-Video\/Wan2.1.\n","date":"2025-03-26"}
{"id":"2503.20315","title":"SpikeDerain: Unveiling Clear Videos from Rainy Sequences Using Color\n  Spike Streams","abstract":"  Restoring clear frames from rainy videos presents a significant challenge due\nto the rapid motion of rain streaks. Traditional frame-based visual sensors,\nwhich capture scene content synchronously, struggle to capture the fast-moving\ndetails of rain accurately. In recent years, neuromorphic sensors have\nintroduced a new paradigm for dynamic scene perception, offering microsecond\ntemporal resolution and high dynamic range. However, existing multimodal\nmethods that fuse event streams with RGB images face difficulties in handling\nthe complex spatiotemporal interference of raindrops in real scenes, primarily\ndue to hardware synchronization errors and computational redundancy. In this\npaper, we propose a Color Spike Stream Deraining Network (SpikeDerain), capable\nof reconstructing spike streams of dynamic scenes and accurately removing rain\nstreaks. To address the challenges of data scarcity in real continuous rainfall\nscenes, we design a physically interpretable rain streak synthesis model that\ngenerates parameterized continuous rain patterns based on arbitrary background\nimages. Experimental results demonstrate that the network, trained with this\nsynthetic data, remains highly robust even under extreme rainfall conditions.\nThese findings highlight the effectiveness and robustness of our method across\nvarying rainfall levels and datasets, setting new standards for video deraining\ntasks. The code will be released soon.\n","date":"2025-03-26"}
{"id":"2503.20316","title":"AI-Driven MRI Spine Pathology Detection: A Comprehensive Deep Learning\n  Approach for Automated Diagnosis in Diverse Clinical Settings","abstract":"  Study Design: This study presents the development of an autonomous AI system\nfor MRI spine pathology detection, trained on a dataset of 2 million MRI spine\nscans sourced from diverse healthcare facilities across India. The AI system\nintegrates advanced architectures, including Vision Transformers, U-Net with\ncross-attention, MedSAM, and Cascade R-CNN, enabling comprehensive\nclassification, segmentation, and detection of 43 distinct spinal pathologies.\nThe dataset is balanced across age groups, genders, and scanner manufacturers\nto ensure robustness and adaptability. Subgroup analyses were conducted to\nvalidate the model's performance across different patient demographics, imaging\nconditions, and equipment types.\n  Performance: The AI system achieved up to 97.9 percent multi-pathology\ndetection, demonstrating consistent performance across age, gender, and\nmanufacturer subgroups. The normal vs. abnormal classification achieved 98.0\npercent accuracy, and the system was deployed across 13 major healthcare\nenterprises in India, encompassing diagnostic centers, large hospitals, and\ngovernment facilities. During deployment, it processed approximately 100,000\nplus MRI spine scans, leading to reduced reporting times and increased\ndiagnostic efficiency by automating the identification of common spinal\nconditions.\n  Conclusion: The AI system's high precision and recall validate its capability\nas a reliable tool for autonomous normal\/abnormal classification, pathology\nsegmentation, and detection. Its scalability and adaptability address critical\ndiagnostic gaps, optimize radiology workflows, and improve patient care across\nvaried healthcare environments in India.\n","date":"2025-03-26"}
{"id":"2503.20318","title":"EditCLIP: Representation Learning for Image Editing","abstract":"  We introduce EditCLIP, a novel representation-learning approach for image\nediting. Our method learns a unified representation of edits by jointly\nencoding an input image and its edited counterpart, effectively capturing their\ntransformation. To evaluate its effectiveness, we employ EditCLIP to solve two\ntasks: exemplar-based image editing and automated edit evaluation. In\nexemplar-based image editing, we replace text-based instructions in\nInstructPix2Pix with EditCLIP embeddings computed from a reference exemplar\nimage pair. Experiments demonstrate that our approach outperforms\nstate-of-the-art methods while being more efficient and versatile. For\nautomated evaluation, EditCLIP assesses image edits by measuring the similarity\nbetween the EditCLIP embedding of a given image pair and either a textual\nediting instruction or the EditCLIP embedding of another reference image pair.\nExperiments show that EditCLIP aligns more closely with human judgments than\nexisting CLIP-based metrics, providing a reliable measure of edit quality and\nstructural preservation.\n","date":"2025-03-26"}
{"id":"2503.20320","title":"Iterative Prompting with Persuasion Skills in Jailbreaking Large\n  Language Models","abstract":"  Large language models (LLMs) are designed to align with human values in their\nresponses. This study exploits LLMs with an iterative prompting technique where\neach prompt is systematically modified and refined across multiple iterations\nto enhance its effectiveness in jailbreaking attacks progressively. This\ntechnique involves analyzing the response patterns of LLMs, including GPT-3.5,\nGPT-4, LLaMa2, Vicuna, and ChatGLM, allowing us to adjust and optimize prompts\nto evade the LLMs' ethical and security constraints. Persuasion strategies\nenhance prompt effectiveness while maintaining consistency with malicious\nintent. Our results show that the attack success rates (ASR) increase as the\nattacking prompts become more refined with the highest ASR of 90% for GPT4 and\nChatGLM and the lowest ASR of 68% for LLaMa2. Our technique outperforms\nbaseline techniques (PAIR and PAP) in ASR and shows comparable performance with\nGCG and ArtPrompt.\n","date":"2025-03-26"}
{"id":"2503.20321","title":"Recovering Dynamic 3D Sketches from Videos","abstract":"  Understanding 3D motion from videos presents inherent challenges due to the\ndiverse types of movement, ranging from rigid and deformable objects to\narticulated structures. To overcome this, we propose Liv3Stroke, a novel\napproach for abstracting objects in motion with deformable 3D strokes. The\ndetailed movements of an object may be represented by unstructured motion\nvectors or a set of motion primitives using a pre-defined articulation from a\ntemplate model. Just as a free-hand sketch can intuitively visualize scenes or\nintentions with a sparse set of lines, we utilize a set of parametric 3D curves\nto capture a set of spatially smooth motion elements for general objects with\nunknown structures. We first extract noisy, 3D point cloud motion guidance from\nvideo frames using semantic features, and our approach deforms a set of curves\nto abstract essential motion features as a set of explicit 3D representations.\nSuch abstraction enables an understanding of prominent components of motions\nwhile maintaining robustness to environmental factors. Our approach allows\ndirect analysis of 3D object movements from video, tackling the uncertainty\nthat typically occurs when translating real-world motion into recorded footage.\nThe project page is accessible via: https:\/\/jaeah.me\/liv3stroke_web\n","date":"2025-03-26"}
{"id":"2503.20322","title":"Dynamic Pyramid Network for Efficient Multimodal Large Language Model","abstract":"  Multimodal large language models (MLLMs) have demonstrated impressive\nperformance in various vision-language (VL) tasks, but their expensive\ncomputations still limit the real-world application. To address this issue,\nrecent efforts aim to compress the visual features to save the computational\ncosts of MLLMs. However, direct visual compression methods, e.g. efficient\nprojectors, inevitably destroy the visual semantics in MLLM, especially in\ndifficult samples. To overcome this shortcoming, we propose a novel dynamic\npyramid network (DPN) for efficient MLLMs. Specifically, DPN formulates MLLM as\na hierarchical structure where visual features are gradually compressed with\nincreasing depth. In this case, even with a high compression ratio,\nfine-grained visual information can still be perceived in shallow layers. To\nmaximize the benefit of DPN, we further propose an innovative Dynamic Pooling\nExperts (DPE) that can dynamically choose the optimal visual compression rate\naccording to input features. With this design, harder samples will be assigned\nlarger computations, thus preserving the model performance. To validate our\napproach, we conduct extensive experiments on two popular MLLMs and ten\nbenchmarks. Experimental results show that DPN can save up to 56% average FLOPs\non LLaVA while further achieving +0.74% performance gains. Besides, the\ngeneralization ability of DPN is also validated on the existing high-resolution\nMLLM called LLaVA-HR. Our source codes are anonymously released at\nhttps:\/\/github.com\/aihao2000\/DPN-LLaVA.\n","date":"2025-03-26"}
{"id":"2503.20328","title":"Euclidean Distance to Convex Polyhedra and Application to Class\n  Representation in Spectral Images","abstract":"  With the aim of estimating the abundance map from observations only, linear\nunmixing approaches are not always suitable to spectral images, especially when\nthe number of bands is too small or when the spectra of the observed data are\ntoo correlated. To address this issue in the general case, we present a novel\napproach which provides an adapted spatial density function based on any\narbitrary linear classifier. A robust mathematical formulation for computing\nthe Euclidean distance to polyhedral sets is presented, along with an efficient\nalgorithm that provides the exact minimum-norm point in a polyhedron. An\nempirical evaluation on the widely-used Samson hyperspectral dataset\ndemonstrates that the proposed method surpasses state-of-the-art approaches in\nreconstructing abundance maps. Furthermore, its application to spectral images\nof a Lithium-ion battery, incompatible with linear unmixing models, validates\nthe method's generality and effectiveness.\n","date":"2025-03-26"}
{"id":"2503.20337","title":"Progressive Focused Transformer for Single Image Super-Resolution","abstract":"  Transformer-based methods have achieved remarkable results in image\nsuper-resolution tasks because they can capture non-local dependencies in\nlow-quality input images. However, this feature-intensive modeling approach is\ncomputationally expensive because it calculates the similarities between\nnumerous features that are irrelevant to the query features when obtaining\nattention weights. These unnecessary similarity calculations not only degrade\nthe reconstruction performance but also introduce significant computational\noverhead. How to accurately identify the features that are important to the\ncurrent query features and avoid similarity calculations between irrelevant\nfeatures remains an urgent problem. To address this issue, we propose a novel\nand effective Progressive Focused Transformer (PFT) that links all isolated\nattention maps in the network through Progressive Focused Attention (PFA) to\nfocus attention on the most important tokens. PFA not only enables the network\nto capture more critical similar features, but also significantly reduces the\ncomputational cost of the overall network by filtering out irrelevant features\nbefore calculating similarities. Extensive experiments demonstrate the\neffectiveness of the proposed method, achieving state-of-the-art performance on\nvarious single image super-resolution benchmarks.\n","date":"2025-03-26"}
{"id":"2503.20341","title":"Wasserstein Distributionally Robust Bayesian Optimization with\n  Continuous Context","abstract":"  We address the challenge of sequential data-driven decision-making under\ncontext distributional uncertainty. This problem arises in numerous real-world\nscenarios where the learner optimizes black-box objective functions in the\npresence of uncontrollable contextual variables. We consider the setting where\nthe context distribution is uncertain but known to lie within an ambiguity set\ndefined as a ball in the Wasserstein distance. We propose a novel algorithm for\nWasserstein Distributionally Robust Bayesian Optimization that can handle\ncontinuous context distributions while maintaining computational tractability.\nOur theoretical analysis combines recent results in self-normalized\nconcentration in Hilbert spaces and finite-sample bounds for distributionally\nrobust optimization to establish sublinear regret bounds that match\nstate-of-the-art results. Through extensive comparisons with existing\napproaches on both synthetic and real-world problems, we demonstrate the\nsimplicity, effectiveness, and practical applicability of our proposed method.\n","date":"2025-03-26"}
{"id":"2503.20348","title":"VideoGEM: Training-free Action Grounding in Videos","abstract":"  Vision-language foundation models have shown impressive capabilities across\nvarious zero-shot tasks, including training-free localization and grounding,\nprimarily focusing on localizing objects in images. However, leveraging those\ncapabilities to localize actions and events in videos is challenging, as\nactions have less physical outline and are usually described by higher-level\nconcepts. In this work, we propose VideoGEM, the first training-free spatial\naction grounding method based on pretrained image- and video-language\nbackbones. Namely, we adapt the self-self attention formulation of GEM to\nspatial activity grounding. We observe that high-level semantic concepts, such\nas actions, usually emerge in the higher layers of the image- and\nvideo-language models. We, therefore, propose a layer weighting in the\nself-attention path to prioritize higher layers. Additionally, we introduce a\ndynamic weighting method to automatically tune layer weights to capture each\nlayer`s relevance to a specific prompt. Finally, we introduce a prompt\ndecomposition, processing action, verb, and object prompts separately,\nresulting in a better spatial localization of actions. We evaluate the proposed\napproach on three image- and video-language backbones, CLIP, OpenCLIP, and\nViCLIP, and on four video grounding datasets, V-HICO, DALY,\nYouCook-Interactions, and GroundingYouTube, showing that the proposed\ntraining-free approach is able to outperform current trained state-of-the-art\napproaches for spatial video grounding.\n","date":"2025-03-26"}
{"id":"2503.20349","title":"Consistency Trajectory Matching for One-Step Generative Super-Resolution","abstract":"  Current diffusion-based super-resolution (SR) approaches achieve commendable\nperformance at the cost of high inference overhead. Therefore, distillation\ntechniques are utilized to accelerate the multi-step teacher model into\none-step student model. Nevertheless, these methods significantly raise\ntraining costs and constrain the performance of the student model by the\nteacher model. To overcome these tough challenges, we propose Consistency\nTrajectory Matching for Super-Resolution (CTMSR), a distillation-free strategy\nthat is able to generate photo-realistic SR results in one step. Concretely, we\nfirst formulate a Probability Flow Ordinary Differential Equation (PF-ODE)\ntrajectory to establish a deterministic mapping from low-resolution (LR) images\nwith noise to high-resolution (HR) images. Then we apply the Consistency\nTraining (CT) strategy to directly learn the mapping in one step, eliminating\nthe necessity of pre-trained diffusion model. To further enhance the\nperformance and better leverage the ground-truth during the training process,\nwe aim to align the distribution of SR results more closely with that of the\nnatural images. To this end, we propose to minimize the discrepancy between\ntheir respective PF-ODE trajectories from the LR image distribution by our\nmeticulously designed Distribution Trajectory Matching (DTM) loss, resulting in\nimproved realism of our recovered HR images. Comprehensive experimental results\ndemonstrate that the proposed methods can attain comparable or even superior\ncapabilities on both synthetic and real datasets while maintaining minimal\ninference latency.\n","date":"2025-03-26"}
{"id":"2503.20354","title":"SURGEON: Memory-Adaptive Fully Test-Time Adaptation via Dynamic\n  Activation Sparsity","abstract":"  Despite the growing integration of deep models into mobile terminals, the\naccuracy of these models declines significantly due to various deployment\ninterferences. Test-time adaptation (TTA) has emerged to improve the\nperformance of deep models by adapting them to unlabeled target data online.\nYet, the significant memory cost, particularly in resource-constrained\nterminals, impedes the effective deployment of most backward-propagation-based\nTTA methods. To tackle memory constraints, we introduce SURGEON, a method that\nsubstantially reduces memory cost while preserving comparable accuracy\nimprovements during fully test-time adaptation (FTTA) without relying on\nspecific network architectures or modifications to the original training\nprocedure. Specifically, we propose a novel dynamic activation sparsity\nstrategy that directly prunes activations at layer-specific dynamic ratios\nduring adaptation, allowing for flexible control of learning ability and memory\ncost in a data-sensitive manner. Among this, two metrics, Gradient Importance\nand Layer Activation Memory, are considered to determine the layer-wise pruning\nratios, reflecting accuracy contribution and memory efficiency, respectively.\nExperimentally, our method surpasses the baselines by not only reducing memory\nusage but also achieving superior accuracy, delivering SOTA performance across\ndiverse datasets, architectures, and tasks.\n","date":"2025-03-26"}
{"id":"2503.20355","title":"CNN+Transformer Based Anomaly Traffic Detection in UAV Networks for\n  Emergency Rescue","abstract":"  The unmanned aerial vehicle (UAV) network has gained significant attentions\nin recent years due to its various applications. However, the traffic security\nbecomes the key threatening public safety issue in an emergency rescue system\ndue to the increasing vulnerability of UAVs to cyber attacks in environments\nwith high heterogeneities. Hence, in this paper, we propose a novel anomaly\ntraffic detection architecture for UAV networks based on the software-defined\nnetworking (SDN) framework and blockchain technology. Specifically, SDN\nseparates the control and data plane to enhance the network manageability and\nsecurity. Meanwhile, the blockchain provides decentralized identity\nauthentication and data security records. Beisdes, a complete security\narchitecture requires an effective mechanism to detect the time-series based\nabnormal traffic. Thus, an integrated algorithm combining convolutional neural\nnetworks (CNNs) and Transformer (CNN+Transformer) for anomaly traffic detection\nis developed, which is called CTranATD. Finally, the simulation results show\nthat the proposed CTranATD algorithm is effective and outperforms the\nindividual CNN, Transformer, and LSTM algorithms for detecting anomaly traffic.\n","date":"2025-03-26"}
{"id":"2503.20362","title":"Self-ReS: Self-Reflection in Large Vision-Language Models for Long Video\n  Understanding","abstract":"  Large Vision-Language Models (LVLMs) demonstrate remarkable performance in\nshort-video tasks such as video question answering, but struggle in long-video\nunderstanding. The linear frame sampling strategy, conventionally used by\nLVLMs, fails to account for the non-linear distribution of key events in video\ndata, often introducing redundant or irrelevant information in longer contexts\nwhile risking the omission of critical events in shorter ones. To address this,\nwe propose SelfReS, a non-linear spatiotemporal self-reflective sampling method\nthat dynamically selects key video fragments based on user prompts. Unlike\nprior approaches, SelfReS leverages the inherently sparse attention maps of\nLVLMs to define reflection tokens, enabling relevance-aware token selection\nwithout requiring additional training or external modules. Experiments\ndemonstrate that SelfReS can be seamlessly integrated into strong base LVLMs,\nimproving long-video task accuracy and achieving up to 46% faster inference\nspeed within the same GPU memory budget.\n","date":"2025-03-26"}
{"id":"2503.20368","title":"Pluggable Style Representation Learning for Multi-Style Transfer","abstract":"  Due to the high diversity of image styles, the scalability to various styles\nplays a critical role in real-world applications. To accommodate a large amount\nof styles, previous multi-style transfer approaches rely on enlarging the model\nsize while arbitrary-style transfer methods utilize heavy backbones. However,\nthe additional computational cost introduced by more model parameters hinders\nthese methods to be deployed on resource-limited devices. To address this\nchallenge, in this paper, we develop a style transfer framework by decoupling\nthe style modeling and transferring. Specifically, for style modeling, we\npropose a style representation learning scheme to encode the style information\ninto a compact representation. Then, for style transferring, we develop a\nstyle-aware multi-style transfer network (SaMST) to adapt to diverse styles\nusing pluggable style representations. In this way, our framework is able to\naccommodate diverse image styles in the learned style representations without\nintroducing additional overhead during inference, thereby maintaining\nefficiency. Experiments show that our style representation can extract accurate\nstyle information. Moreover, qualitative and quantitative results demonstrate\nthat our method achieves state-of-the-art performance in terms of both accuracy\nand efficiency. The codes are available in\nhttps:\/\/github.com\/The-Learning-And-Vision-Atelier-LAVA\/SaMST.\n","date":"2025-03-26"}
{"id":"2503.20382","title":"RSRWKV: A Linear-Complexity 2D Attention Mechanism for Efficient Remote\n  Sensing Vision Task","abstract":"  High-resolution remote sensing analysis faces challenges in global context\nmodeling due to scene complexity and scale diversity. While CNNs excel at local\nfeature extraction via parameter sharing, their fixed receptive fields\nfundamentally restrict long-range dependency modeling. Vision Transformers\n(ViTs) effectively capture global semantic relationships through self-attention\nmechanisms but suffer from quadratic computational complexity relative to image\nresolution, creating critical efficiency bottlenecks for high-resolution\nimagery. The RWKV model's linear-complexity sequence modeling achieves\nbreakthroughs in NLP but exhibits anisotropic limitations in vision tasks due\nto its 1D scanning mechanism. To address these challenges, we propose RSRWKV,\nfeaturing a novel 2D-WKV scanning mechanism that bridges sequential processing\nand 2D spatial reasoning while maintaining linear complexity. This enables\nisotropic context aggregation across multiple directions. The MVC-Shift module\nenhances multi-scale receptive field coverage, while the ECA module strengthens\ncross-channel feature interaction and semantic saliency modeling. Experimental\nresults demonstrate RSRWKV's superior performance over CNN and Transformer\nbaselines in classification, detection, and segmentation tasks on NWPU\nRESISC45, VHR-10.v2, and GLH-Water datasets, offering a scalable solution for\nhigh-resolution remote sensing analysis.\n","date":"2025-03-26"}
{"id":"2503.20384","title":"MoLe-VLA: Dynamic Layer-skipping Vision Language Action Model via\n  Mixture-of-Layers for Efficient Robot Manipulation","abstract":"  Multimodal Large Language Models (MLLMs) excel in understanding complex\nlanguage and visual data, enabling generalist robotic systems to interpret\ninstructions and perform embodied tasks. Nevertheless, their real-world\ndeployment is hindered by substantial computational and storage demands. Recent\ninsights into the homogeneous patterns in the LLM layer have inspired\nsparsification techniques to address these challenges, such as early exit and\ntoken pruning. However, these methods often neglect the critical role of the\nfinal layers that encode the semantic information most relevant to downstream\nrobotic tasks. Aligning with the recent breakthrough of the Shallow Brain\nHypothesis (SBH) in neuroscience and the mixture of experts in model\nsparsification, we conceptualize each LLM layer as an expert and propose a\nMixture-of-Layers Vision-Language-Action model (MoLe-VLA, or simply MoLe)\narchitecture for dynamic LLM layer activation. We introduce a Spatial-Temporal\nAware Router (STAR) for MoLe to selectively activate only parts of the layers\nbased on the robot's current state, mimicking the brain's distinct signal\npathways specialized for cognition and causal reasoning. Additionally, to\ncompensate for the cognitive ability of LLMs lost in MoLe, we devise a\nCognition Self-Knowledge Distillation (CogKD) framework. CogKD enhances the\nunderstanding of task demands and improves the generation of task-relevant\naction sequences by leveraging cognitive features. Extensive experiments\nconducted in both RLBench simulation and real-world environments demonstrate\nthe superiority of MoLe-VLA in both efficiency and performance. Specifically,\nMoLe-VLA achieves an 8% improvement in the mean success rate across ten tasks\nwhile reducing computational costs by up to x5.6 compared to standard LLMs.\n","date":"2025-03-26"}
{"id":"2503.20394","title":"FastFT: Accelerating Reinforced Feature Transformation via Advanced\n  Exploration Strategies","abstract":"  Feature Transformation is crucial for classic machine learning that aims to\ngenerate feature combinations to enhance the performance of downstream tasks\nfrom a data-centric perspective. Current methodologies, such as manual\nexpert-driven processes, iterative-feedback techniques, and\nexploration-generative tactics, have shown promise in automating such data\nengineering workflow by minimizing human involvement. However, three challenges\nremain in those frameworks: (1) It predominantly depends on downstream task\nperformance metrics, as assessment is time-consuming, especially for large\ndatasets. (2) The diversity of feature combinations will hardly be guaranteed\nafter random exploration ends. (3) Rare significant transformations lead to\nsparse valuable feedback that hinders the learning processes or leads to less\neffective results. In response to these challenges, we introduce FastFT, an\ninnovative framework that leverages a trio of advanced strategies.We first\ndecouple the feature transformation evaluation from the outcomes of the\ngenerated datasets via the performance predictor. To address the issue of\nreward sparsity, we developed a method to evaluate the novelty of generated\ntransformation sequences. Incorporating this novelty into the reward function\naccelerates the model's exploration of effective transformations, thereby\nimproving the search productivity. Additionally, we combine novelty and\nperformance to create a prioritized memory buffer, ensuring that essential\nexperiences are effectively revisited during exploration. Our extensive\nexperimental evaluations validate the performance, efficiency, and traceability\nof our proposed framework, showcasing its superiority in handling complex\nfeature transformation tasks.\n","date":"2025-03-26"}
{"id":"2503.20398","title":"Including local feature interactions in deep non-negative matrix\n  factorization networks improves performance","abstract":"  The brain uses positive signals as a means of signaling. Forward interactions\nin the early visual cortex are also positive, realized by excitatory synapses.\nOnly local interactions also include inhibition. Non-negative matrix\nfactorization (NMF) captures the biological constraint of positive long-range\ninteractions and can be implemented with stochastic spikes. While NMF can serve\nas an abstract formalization of early neural processing in the visual system,\nthe performance of deep convolutional networks with NMF modules does not match\nthat of CNNs of similar size. However, when the local NMF modules are each\nfollowed by a module that mixes the NMF's positive activities, the performances\non the benchmark data exceed that of vanilla deep convolutional networks of\nsimilar size. This setting can be considered a biologically more plausible\nemulation of the processing in cortical (hyper-)columns with the potential to\nimprove the performance of deep networks.\n","date":"2025-03-26"}
{"id":"2503.20400","title":"Multi-dataset and Transfer Learning Using Gene Expression Knowledge\n  Graphs","abstract":"  Gene expression datasets offer insights into gene regulation mechanisms,\nbiochemical pathways, and cellular functions. Additionally, comparing gene\nexpression profiles between disease and control patients can deepen the\nunderstanding of disease pathology. Therefore, machine learning has been used\nto process gene expression data, with patient diagnosis emerging as one of the\nmost popular applications. Although gene expression data can provide valuable\ninsights, challenges arise because the number of patients in expression\ndatasets is usually limited, and the data from different datasets with\ndifferent gene expressions cannot be easily combined. This work proposes a\nnovel methodology to address these challenges by integrating multiple gene\nexpression datasets and domain-specific knowledge using knowledge graphs, a\nunique tool for biomedical data integration. Then, vector representations are\nproduced using knowledge graph embedding techniques, which are used as inputs\nfor a graph neural network and a multi-layer perceptron. We evaluate the\nefficacy of our methodology in three settings: single-dataset learning,\nmulti-dataset learning, and transfer learning. The experimental results show\nthat combining gene expression datasets and domain-specific knowledge improves\npatient diagnosis in all three settings.\n","date":"2025-03-26"}
{"id":"2503.20403","title":"Comparative analysis and evaluation of ageing forecasting methods for\n  semiconductor devices in online health monitoring","abstract":"  Semiconductor devices, especially MOSFETs (Metal-oxide-semiconductor\nfield-effect transistor), are crucial in power electronics, but their\nreliability is affected by aging processes influenced by cycling and\ntemperature. The primary aging mechanism in discrete semiconductors and power\nmodules is the bond wire lift-off, caused by crack growth due to thermal\nfatigue. The process is empirically characterized by exponential growth and an\nabrupt end of life, making long-term aging forecasts challenging. This research\npresents a comprehensive comparative assessment of different forecasting\nmethods for MOSFET failure forecasting applications. Classical tracking,\nstatistical forecasting and Neural Network (NN) based forecasting models are\nimplemented along with novel Temporal Fusion Transformers (TFTs). A\ncomprehensive comparison is performed assessing their MOSFET ageing forecasting\nability for different forecasting horizons. For short-term predictions, all\nalgorithms result in acceptable results, with the best results produced by\nclassical NN forecasting models at the expense of higher computations. For\nlong-term forecasting, only the TFT is able to produce valid outcomes owing to\nthe ability to integrate covariates from the expected future conditions.\nAdditionally, TFT attention points identify key ageing turning points, which\nindicate new failure modes or accelerated ageing phases.\n","date":"2025-03-26"}
{"id":"2503.20410","title":"Learning Data-Driven Uncertainty Set Partitions for Robust and Adaptive\n  Energy Forecasting with Missing Data","abstract":"  Short-term forecasting models typically assume the availability of input data\n(features) when they are deployed and in use. However, equipment failures,\ndisruptions, cyberattacks, may lead to missing features when such models are\nused operationally, which could negatively affect forecast accuracy, and result\nin suboptimal operational decisions. In this paper, we use adaptive robust\noptimization and adversarial machine learning to develop forecasting models\nthat seamlessly handle missing data operationally. We propose linear- and\nneural network-based forecasting models with parameters that adapt to available\nfeatures, combining linear adaptation with a novel algorithm for learning\ndata-driven uncertainty set partitions. The proposed adaptive models do not\nrely on identifying historical missing data patterns and are suitable for\nreal-time operations under stringent time constraints. Extensive numerical\nexperiments on short-term wind power forecasting considering horizons from 15\nminutes to 4 hours ahead illustrate that our proposed adaptive models are on\npar with imputation when data are missing for very short periods (e.g., when\nonly the latest measurement is missing) whereas they significantly outperform\nimputation when data are missing for longer periods. We further provide\ninsights by showcasing how linear adaptation and data-driven partitions (even\nwith a few subsets) approach the performance of the optimal, yet impractical,\nmethod of retraining for every possible realization of missing data.\n","date":"2025-03-26"}
{"id":"2503.20414","title":"Active Data Sampling and Generation for Bias Remediation","abstract":"  Adequate sampling space coverage is the keystone to effectively train\ntrustworthy Machine Learning models. Unfortunately, real data do carry several\ninherent risks due to the many potential biases they exhibit when gathered\nwithout a proper random sampling over the reference population, and most of the\ntimes this is way too expensive or time consuming to be a viable option.\nDepending on how training data have been gathered, unmitigated biases can lead\nto harmful or discriminatory consequences that ultimately hinders large scale\napplicability of pre-trained models and undermine their truthfulness or\nfairness expectations. In this paper, a mixed active sampling and data\ngeneration strategy -- called samplation -- is proposed as a mean to compensate\nduring fine-tuning of a pre-trained classifer the unfair classifications it\nproduces, assuming that the training data come from a non-probabilistic\nsampling schema. Given a pre-trained classifier, first a fairness metric is\nevaluated on a test set, then new reservoirs of labeled data are generated and\nfinally a number of reversely-biased artificial samples are generated for the\nfine-tuning of the model. Using as case study Deep Models for visual semantic\nrole labeling, the proposed method has been able to fully cure a simulated\ngender bias starting from a 90\/10 imbalance, with only a small percentage of\nnew data and with a minor effect on accuracy.\n","date":"2025-03-26"}
{"id":"2503.20417","title":"CFunModel: A \"Funny\" Language Model Capable of Chinese Humor Generation\n  and Processing","abstract":"  Humor plays a significant role in daily language communication. With the\nrapid development of large language models (LLMs), natural language processing\nhas made significant strides in understanding and generating various genres of\ntexts. However, most LLMs exhibit poor performance in generating and processing\nChinese humor. In this study, we introduce a comprehensive Chinese\nhumor-related dataset, the Chinese Fun Set (CFunSet). This dataset aggregates\nexisting Chinese humor datasets and includes over 20,000 jokes collected from\nTieba-JokeBar, a Chinese online platform known for joke sharing. The resulting\ncorpus comprises more than 160,000 entries. Leveraging CFunSet, we developed\nthe Chinese Fun Model (CFunModel), the first large language model designed to\nhandle various Chinese humor-related tasks including Crosstalk Response\nSelection, Humor Recognition, Joke Generation, etc. Experimental results\ndemonstrate that CFunModel outperforms popular large language models in these\ntasks. Our CFunSet is available at\nhttps:\/\/huggingface.co\/datasets\/ZhenghanYU\/CFunSet and CFunModel is available\nat https:\/\/huggingface.co\/ZhenghanYU\/CFunModel. A demostration video of our\nwork is available at https:\/\/youtu.be\/MOsISOJ66Ms.\n","date":"2025-03-26"}
{"id":"2503.20418","title":"ITA-MDT: Image-Timestep-Adaptive Masked Diffusion Transformer Framework\n  for Image-Based Virtual Try-On","abstract":"  This paper introduces ITA-MDT, the Image-Timestep-Adaptive Masked Diffusion\nTransformer Framework for Image-Based Virtual Try-On (IVTON), designed to\novercome the limitations of previous approaches by leveraging the Masked\nDiffusion Transformer (MDT) for improved handling of both global garment\ncontext and fine-grained details. The IVTON task involves seamlessly\nsuperimposing a garment from one image onto a person in another, creating a\nrealistic depiction of the person wearing the specified garment. Unlike\nconventional diffusion-based virtual try-on models that depend on large\npre-trained U-Net architectures, ITA-MDT leverages a lightweight, scalable\ntransformer-based denoising diffusion model with a mask latent modeling scheme,\nachieving competitive results while reducing computational overhead. A key\ncomponent of ITA-MDT is the Image-Timestep Adaptive Feature Aggregator (ITAFA),\na dynamic feature aggregator that combines all of the features from the image\nencoder into a unified feature of the same size, guided by diffusion timestep\nand garment image complexity. This enables adaptive weighting of features,\nallowing the model to emphasize either global information or fine-grained\ndetails based on the requirements of the denoising stage. Additionally, the\nSalient Region Extractor (SRE) module is presented to identify complex region\nof the garment to provide high-resolution local information to the denoising\nmodel as an additional condition alongside the global information of the full\ngarment image. This targeted conditioning strategy enhances detail preservation\nof fine details in highly salient garment regions, optimizing computational\nresources by avoiding unnecessarily processing entire garment image.\nComparative evaluations confirms that ITA-MDT improves efficiency while\nmaintaining strong performance, reaching state-of-the-art results in several\nmetrics.\n","date":"2025-03-26"}
{"id":"2503.20419","title":"Cherry Yield Forecast: Harvest Prediction for Individual Sweet Cherry\n  Trees","abstract":"  This paper is part of a publication series from the For5G project that has\nthe goal of creating digital twins of sweet cherry trees. At the beginning a\nbrief overview of the revious work in this project is provided. Afterwards the\nfocus shifts to a crucial problem in the fruit farming domain: the difficulty\nof making reliable yield predictions early in the season. Following three Satin\nsweet cherry trees along the year 2023 enabled the collection of accurate\nground truth data about the development of cherries from dormancy until\nharvest. The methodology used to collect this data is presented, along with its\nvaluation and visualization. The predictive power of counting objects at all\nrelevant vegetative stages of the fruit development cycle in cherry trees with\nregards to yield predictions is investigated. It is found that all investigated\nfruit states are suitable for yield predictions based on linear regression.\nConceptionally, there is a trade-off between earliness and external events with\nthe potential to invalidate the prediction. Considering this, two optimal\ntimepoints are suggested that are opening cluster stage before the start of the\nflowering and the early fruit stage right after the second fruit drop. However,\nboth timepoints are challenging to solve with automated procedures based on\nimage data. Counting developing cherries based on images is exceptionally\ndifficult due to the small fruit size and their tendency to be occluded by\nleaves. It was not possible to obtain satisfying results relying on a\nstate-of-the-art fruit-counting method. Counting the elements within a bursting\nbud is also challenging, even when using high resolution cameras. It is\nconcluded that accurate yield prediction for sweet cherry trees is possible\nwhen objects are manually counted and that automated features extraction with\nsimilar accuracy remains an open problem yet to be solved.\n","date":"2025-03-26"}
{"id":"2503.20421","title":"TempTest: Local Normalization Distortion and the Detection of\n  Machine-generated Text","abstract":"  Existing methods for the zero-shot detection of machine-generated text are\ndominated by three statistical quantities: log-likelihood, log-rank, and\nentropy. As language models mimic the distribution of human text ever closer,\nthis will limit our ability to build effective detection algorithms. To combat\nthis, we introduce a method for detecting machine-generated text that is\nentirely agnostic of the generating language model. This is achieved by\ntargeting a defect in the way that decoding strategies, such as temperature or\ntop-k sampling, normalize conditional probability measures. This method can be\nrigorously theoretically justified, is easily explainable, and is conceptually\ndistinct from existing methods for detecting machine-generated text. We\nevaluate our detector in the white and black box settings across various\nlanguage models, datasets, and passage lengths. We also study the effect of\nparaphrasing attacks on our detector and the extent to which it is biased\nagainst non-native speakers. In each of these settings, the performance of our\ntest is at least comparable to that of other state-of-the-art text detectors,\nand in some cases, we strongly outperform these baselines.\n","date":"2025-03-26"}
{"id":"2503.20425","title":"Perspective-Shifted Neuro-Symbolic World Models: A Framework for\n  Socially-Aware Robot Navigation","abstract":"  Navigating in environments alongside humans requires agents to reason under\nuncertainty and account for the beliefs and intentions of those around them.\nUnder a sequential decision-making framework, egocentric navigation can\nnaturally be represented as a Markov Decision Process (MDP). However, social\nnavigation additionally requires reasoning about the hidden beliefs of others,\ninherently leading to a Partially Observable Markov Decision Process (POMDP),\nwhere agents lack direct access to others' mental states. Inspired by Theory of\nMind and Epistemic Planning, we propose (1) a neuro-symbolic model-based\nreinforcement learning architecture for social navigation, addressing the\nchallenge of belief tracking in partially observable environments; and (2) a\nperspective-shift operator for belief estimation, leveraging recent work on\nInfluence-based Abstractions (IBA) in structured multi-agent settings.\n","date":"2025-03-26"}
{"id":"2503.20428","title":"Evaluating Facial Expression Recognition Datasets for Deep Learning: A\n  Benchmark Study with Novel Similarity Metrics","abstract":"  This study investigates the key characteristics and suitability of widely\nused Facial Expression Recognition (FER) datasets for training deep learning\nmodels. In the field of affective computing, FER is essential for interpreting\nhuman emotions, yet the performance of FER systems is highly contingent on the\nquality and diversity of the underlying datasets. To address this issue, we\ncompiled and analyzed 24 FER datasets, including those targeting specific age\ngroups such as children, adults, and the elderly, and processed them through a\ncomprehensive normalization pipeline. In addition, we enriched the datasets\nwith automatic annotations for age and gender, enabling a more nuanced\nevaluation of their demographic properties. To further assess dataset efficacy,\nwe introduce three novel metricsLocal, Global, and Paired Similarity, which\nquantitatively measure dataset difficulty, generalization capability, and\ncross-dataset transferability. Benchmark experiments using state-of-the-art\nneural networks reveal that large-scale, automatically collected datasets\n(e.g., AffectNet, FER2013) tend to generalize better, despite issues with\nlabeling noise and demographic biases, whereas controlled datasets offer higher\nannotation quality but limited variability. Our findings provide actionable\nrecommendations for dataset selection and design, advancing the development of\nmore robust, fair, and effective FER systems.\n","date":"2025-03-26"}
{"id":"2503.20429","title":"Latent Beam Diffusion Models for Decoding Image Sequences","abstract":"  While diffusion models excel at generating high-quality images from text\nprompts, they struggle with visual consistency in image sequences. Existing\nmethods generate each image independently, leading to disjointed narratives - a\nchallenge further exacerbated in non-linear storytelling, where scenes must\nconnect beyond adjacent frames. We introduce a novel beam search strategy for\nlatent space exploration, enabling conditional generation of full image\nsequences with beam search decoding. Unlike prior approaches that use fixed\nlatent priors, our method dynamically searches for an optimal sequence of\nlatent representations, ensuring coherent visual transitions. To address beam\nsearch's quadratic complexity, we integrate a cross-attention mechanism that\nefficiently scores search paths and enables pruning, prioritizing alignment\nwith both textual prompts and visual context. Human evaluations confirm that\nour approach outperforms baseline methods, producing full sequences with\nsuperior coherence, visual continuity, and textual alignment. By bridging\nadvances in search optimization and latent space refinement, this work sets a\nnew standard for structured image sequence generation.\n","date":"2025-03-26"}
{"id":"2503.20436","title":"Siformer: Feature-isolated Transformer for Efficient Skeleton-based Sign\n  Language Recognition","abstract":"  Sign language recognition (SLR) refers to interpreting sign language glosses\nfrom given videos automatically. This research area presents a complex\nchallenge in computer vision because of the rapid and intricate movements\ninherent in sign languages, which encompass hand gestures, body postures, and\neven facial expressions. Recently, skeleton-based action recognition has\nattracted increasing attention due to its ability to handle variations in\nsubjects and backgrounds independently. However, current skeleton-based SLR\nmethods exhibit three limitations: 1) they often neglect the importance of\nrealistic hand poses, where most studies train SLR models on non-realistic\nskeletal representations; 2) they tend to assume complete data availability in\nboth training or inference phases, and capture intricate relationships among\ndifferent body parts collectively; 3) these methods treat all sign glosses\nuniformly, failing to account for differences in complexity levels regarding\nskeletal representations. To enhance the realism of hand skeletal\nrepresentations, we present a kinematic hand pose rectification method for\nenforcing constraints. Mitigating the impact of missing data, we propose a\nfeature-isolated mechanism to focus on capturing local spatial-temporal\ncontext. This method captures the context concurrently and independently from\nindividual features, thus enhancing the robustness of the SLR model.\nAdditionally, to adapt to varying complexity levels of sign glosses, we develop\nan input-adaptive inference approach to optimise computational efficiency and\naccuracy. Experimental results demonstrate the effectiveness of our approach,\nas evidenced by achieving a new state-of-the-art (SOTA) performance on WLASL100\nand LSA64. For WLASL100, we achieve a top-1 accuracy of 86.50\\%, marking a\nrelative improvement of 2.39% over the previous SOTA. For LSA64, we achieve a\ntop-1 accuracy of 99.84%.\n","date":"2025-03-26"}
{"id":"2503.20442","title":"The Crucial Role of Problem Formulation in Real-World Reinforcement\n  Learning","abstract":"  Reinforcement Learning (RL) offers promising solutions for control tasks in\nindustrial cyber-physical systems (ICPSs), yet its real-world adoption remains\nlimited. This paper demonstrates how seemingly small but well-designed\nmodifications to the RL problem formulation can substantially improve\nperformance, stability, and sample efficiency. We identify and investigate key\nelements of RL problem formulation and show that these enhance both learning\nspeed and final policy quality. Our experiments use a one-degree-of-freedom\n(1-DoF) helicopter testbed, the Quanser Aero~2, which features non-linear\ndynamics representative of many industrial settings. In simulation, the\nproposed problem design principles yield more reliable and efficient training,\nand we further validate these results by training the agent directly on\nphysical hardware. The encouraging real-world outcomes highlight the potential\nof RL for ICPS, especially when careful attention is paid to the design\nprinciples of problem formulation. Overall, our study underscores the crucial\nrole of thoughtful problem formulation in bridging the gap between RL research\nand the demands of real-world industrial systems.\n","date":"2025-03-26"}
{"id":"2503.20446","title":"Attention Xception UNet (AXUNet): A Novel Combination of CNN and\n  Self-Attention for Brain Tumor Segmentation","abstract":"  Accurate segmentation of glioma brain tumors is crucial for diagnosis and\ntreatment planning. Deep learning techniques offer promising solutions, but\noptimal model architectures remain under investigation. We used the BraTS 2021\ndataset, selecting T1 with contrast enhancement (T1CE), T2, and\nFluid-Attenuated Inversion Recovery (FLAIR) sequences for model development.\nThe proposed Attention Xception UNet (AXUNet) architecture integrates an\nXception backbone with dot-product self-attention modules, inspired by\nstate-of-the-art (SOTA) large language models such as Google Bard and OpenAI\nChatGPT, within a UNet-shaped model. We compared AXUNet with SOTA models.\nComparative evaluation on the test set demonstrated improved results over\nbaseline models. Inception-UNet and Xception-UNet achieved mean Dice scores of\n90.88 and 93.24, respectively. Attention ResUNet (AResUNet) attained a mean\nDice score of 92.80, with the highest score of 84.92 for enhancing tumor (ET)\namong all models. Attention Gate UNet (AGUNet) yielded a mean Dice score of\n90.38. AXUNet outperformed all models with a mean Dice score of 93.73. It\ndemonstrated superior Dice scores across whole tumor (WT) and tumor core (TC)\nregions, achieving 92.59 for WT, 86.81 for TC, and 84.89 for ET. The\nintegration of the Xception backbone and dot-product self-attention mechanisms\nin AXUNet showcases enhanced performance in capturing spatial and contextual\ninformation. The findings underscore the potential utility of AXUNet in\nfacilitating precise tumor delineation.\n","date":"2025-03-26"}
{"id":"2503.20454","title":"Lipschitz Constant Meets Condition Number: Learning Robust and Compact\n  Deep Neural Networks","abstract":"  Recent research has revealed that high compression of Deep Neural Networks\n(DNNs), e.g., massive pruning of the weight matrix of a DNN, leads to a severe\ndrop in accuracy and susceptibility to adversarial attacks. Integration of\nnetwork pruning into an adversarial training framework has been proposed to\npromote adversarial robustness. It has been observed that a highly pruned\nweight matrix tends to be ill-conditioned, i.e., increasing the condition\nnumber of the weight matrix. This phenomenon aggravates the vulnerability of a\nDNN to input noise. Although a highly pruned weight matrix is considered to be\nable to lower the upper bound of the local Lipschitz constant to tolerate large\ndistortion, the ill-conditionedness of such a weight matrix results in a\nnon-robust DNN model. To overcome this challenge, this work develops novel\njoint constraints to adjust the weight distribution of networks, namely, the\nTransformed Sparse Constraint joint with Condition Number Constraint (TSCNC),\nwhich copes with smoothing distribution and differentiable constraint functions\nto reduce condition number and thus avoid the ill-conditionedness of weight\nmatrices. Furthermore, our theoretical analyses unveil the relevance between\nthe condition number and the local Lipschitz constant of the weight matrix,\nnamely, the sharply increasing condition number becomes the dominant factor\nthat restricts the robustness of over-sparsified models. Extensive experiments\nare conducted on several public datasets, and the results show that the\nproposed constraints significantly improve the robustness of a DNN with high\npruning rates.\n","date":"2025-03-26"}
{"id":"2503.20466","title":"Data-driven Seasonal Climate Predictions via Variational Inference and\n  Transformers","abstract":"  Most operational climate services providers base their seasonal predictions\non initialised general circulation models (GCMs) or statistical techniques that\nfit past observations. GCMs require substantial computational resources, which\nlimits their capacity. In contrast, statistical methods often lack robustness\ndue to short historical records. Recent works propose machine learning methods\ntrained on climate model output, leveraging larger sample sizes and simulated\nscenarios. Yet, many of these studies focus on prediction tasks that might be\nrestricted in spatial extent or temporal coverage, opening a gap with existing\noperational predictions. Thus, the present study evaluates the effectiveness of\na methodology that combines variational inference with transformer models to\npredict fields of seasonal anomalies. The predictions cover all four seasons\nand are initialised one month before the start of each season. The model was\ntrained on climate model output from CMIP6 and tested using ERA5 reanalysis\ndata. We analyse the method's performance in predicting interannual anomalies\nbeyond the climate change-induced trend. We also test the proposed methodology\nin a regional context with a use case focused on Europe. While climate change\ntrends dominate the skill of temperature predictions, the method presents\nadditional skill over the climatological forecast in regions influenced by\nknown teleconnections. We reach similar conclusions based on the validation of\nprecipitation predictions. Despite underperforming SEAS5 in most tropics, our\nmodel offers added value in numerous extratropical inland regions. This work\ndemonstrates the effectiveness of training generative models on climate model\noutput for seasonal predictions, providing skilful predictions beyond the\ninduced climate change trend at time scales and lead times relevant for user\napplications.\n","date":"2025-03-26"}
{"id":"2503.20472","title":"From Trial to Triumph: Advancing Long Video Understanding via Visual\n  Context Sample Scaling and Self-reward Alignment","abstract":"  Multi-modal Large language models (MLLMs) show remarkable ability in video\nunderstanding. Nevertheless, understanding long videos remains challenging as\nthe models can only process a finite number of frames in a single inference,\npotentially omitting crucial visual information. To address the challenge, we\npropose generating multiple predictions through visual context sampling,\nfollowed by a scoring mechanism to select the final prediction. Specifically,\nwe devise a bin-wise sampling strategy that enables MLLMs to generate diverse\nanswers based on various combinations of keyframes, thereby enriching the\nvisual context. To determine the final prediction from the sampled answers, we\nemploy a self-reward by linearly combining three scores: (1) a frequency score\nindicating the prevalence of each option, (2) a marginal confidence score\nreflecting the inter-intra sample certainty of MLLM predictions, and (3) a\nreasoning score for different question types, including clue-guided answering\nfor global questions and temporal self-refocusing for local questions. The\nfrequency score ensures robustness through majority correctness, the\nconfidence-aligned score reflects prediction certainty, and the typed-reasoning\nscore addresses cases with sparse key visual information using tailored\nstrategies. Experiments show that this approach covers the correct answer for a\nhigh percentage of long video questions, on seven datasets show that our method\nimproves the performance of three MLLMs.\n","date":"2025-03-26"}
{"id":"2503.20479","title":"A multi-agentic framework for real-time, autonomous freeform metasurface\n  design","abstract":"  Innovation in nanophotonics currently relies on human experts who synergize\nspecialized knowledge in photonics and coding with simulation and optimization\nalgorithms, entailing design cycles that are time-consuming, computationally\ndemanding, and frequently suboptimal. We introduce MetaChat, a multi-agentic\ndesign framework that can translate semantically described photonic design\ngoals into high-performance, freeform device layouts in an automated, nearly\nreal-time manner. Multi-step reasoning is enabled by our Agentic Iterative\nMonologue (AIM) paradigm, which coherently interfaces agents with code-based\ntools, other specialized agents, and human designers. Design acceleration is\nfacilitated by Feature-wise Linear Modulation-conditioned Maxwell surrogate\nsolvers that support the generalized evaluation of metasurface structures. We\nuse freeform dielectric metasurfaces as a model system and demonstrate with\nMetaChat the design of multi-objective, multi-wavelength metasurfaces orders of\nmagnitude faster than conventional methods. These concepts present a scientific\ncomputing blueprint for utilizing specialist design agents, surrogate solvers,\nand human interactions to drive multi-physics innovation and discovery.\n","date":"2025-03-26"}
{"id":"2503.20483","title":"Dissecting and Mitigating Diffusion Bias via Mechanistic\n  Interpretability","abstract":"  Diffusion models have demonstrated impressive capabilities in synthesizing\ndiverse content. However, despite their high-quality outputs, these models\noften perpetuate social biases, including those related to gender and race.\nThese biases can potentially contribute to harmful real-world consequences,\nreinforcing stereotypes and exacerbating inequalities in various social\ncontexts. While existing research on diffusion bias mitigation has\npredominantly focused on guiding content generation, it often neglects the\nintrinsic mechanisms within diffusion models that causally drive biased\noutputs. In this paper, we investigate the internal processes of diffusion\nmodels, identifying specific decision-making mechanisms, termed bias features,\nembedded within the model architecture. By directly manipulating these\nfeatures, our method precisely isolates and adjusts the elements responsible\nfor bias generation, permitting granular control over the bias levels in the\ngenerated content. Through experiments on both unconditional and conditional\ndiffusion models across various social bias attributes, we demonstrate our\nmethod's efficacy in managing generation distribution while preserving image\nquality. We also dissect the discovered model mechanism, revealing different\nintrinsic features controlling fine-grained aspects of generation, boosting\nfurther research on mechanistic interpretability of diffusion models.\n","date":"2025-03-26"}
{"id":"2503.20484","title":"Contrastive Learning Guided Latent Diffusion Model for Image-to-Image\n  Translation","abstract":"  The diffusion model has demonstrated superior performance in synthesizing\ndiverse and high-quality images for text-guided image translation. However,\nthere remains room for improvement in both the formulation of text prompts and\nthe preservation of reference image content. First, variations in target text\nprompts can significantly influence the quality of the generated images, and it\nis often challenging for users to craft an optimal prompt that fully captures\nthe content of the input image. Second, while existing models can introduce\ndesired modifications to specific regions of the reference image, they\nfrequently induce unintended alterations in areas that should remain unchanged.\nTo address these challenges, we propose pix2pix-zeroCon, a zero-shot\ndiffusion-based method that eliminates the need for additional training by\nleveraging patch-wise contrastive loss. Specifically, we automatically\ndetermine the editing direction in the text embedding space based on the\nreference image and target prompts. Furthermore, to ensure precise content and\nstructural preservation in the edited image, we introduce cross-attention\nguiding loss and patch-wise contrastive loss between the generated and original\nimage embeddings within a pre-trained diffusion model. Notably, our approach\nrequires no additional training and operates directly on a pre-trained\ntext-to-image diffusion model. Extensive experiments demonstrate that our\nmethod surpasses existing models in image-to-image translation, achieving\nenhanced fidelity and controllability.\n","date":"2025-03-26"}
{"id":"2503.20485","title":"Underwater Image Enhancement by Convolutional Spiking Neural Networks","abstract":"  Underwater image enhancement (UIE) is fundamental for marine applications,\nincluding autonomous vision-based navigation. Deep learning methods using\nconvolutional neural networks (CNN) and vision transformers advanced UIE\nperformance. Recently, spiking neural networks (SNN) have gained attention for\ntheir lightweight design, energy efficiency, and scalability. This paper\nintroduces UIE-SNN, the first SNN-based UIE algorithm to improve visibility of\nunderwater images. UIE-SNN is a 19- layered convolutional spiking\nencoder-decoder framework with skip connections, directly trained using\nsurrogate gradient-based backpropagation through time (BPTT) strategy. We\nexplore and validate the influence of training datasets on energy reduction, a\nunique advantage of UIE-SNN architecture, in contrast to the conventional\nlearning-based architectures, where energy consumption is model-dependent.\nUIE-SNN optimizes the loss function in latent space representation to\nreconstruct clear underwater images. Our algorithm performs on par with its\nnon-spiking counterpart methods in terms of PSNR and structural similarity\nindex (SSIM) at reduced timesteps ($T=5$) and energy consumption of $85\\%$. The\nalgorithm is trained on two publicly available benchmark datasets, UIEB and\nEUVP, and tested on unseen images from UIEB, EUVP, LSUI, U45, and our custom\nUIE dataset. The UIE-SNN algorithm achieves PSNR of \\(17.7801~dB\\) and SSIM of\n\\(0.7454\\) on UIEB, and PSNR of \\(23.1725~dB\\) and SSIM of \\(0.7890\\) on EUVP.\nUIE-SNN achieves this algorithmic performance with fewer operators (\\(147.49\\)\nGSOPs) and energy (\\(0.1327~J\\)) compared to its non-spiking counterpart\n(GFLOPs = \\(218.88\\) and Energy=\\(1.0068~J\\)). Compared with existing SOTA UIE\nmethods, UIE-SNN achieves an average of \\(6.5\\times\\) improvement in energy\nefficiency. The source code is available at\n\\href{https:\/\/github.com\/vidya-rejul\/UIE-SNN.git}{UIE-SNN}.\n","date":"2025-03-26"}
{"id":"2503.20488","title":"Adaptive Local Clustering over Attributed Graphs","abstract":"  Given a graph $G$ and a seed node $v_s$, the objective of local graph\nclustering (LGC) is to identify a subgraph $C_s \\in G$ (a.k.a. local cluster)\nsurrounding $v_s$ in time roughly linear with the size of $C_s$. This approach\nyields personalized clusters without needing to access the entire graph, which\nmakes it highly suitable for numerous applications involving large graphs.\nHowever, most existing solutions merely rely on the topological connectivity\nbetween nodes in $G$, rendering them vulnerable to missing or noisy links that\nare commonly present in real-world graphs.\n  To address this issue, this paper resorts to leveraging the complementary\nnature of graph topology and node attributes to enhance local clustering\nquality. To effectively exploit the attribute information, we first formulate\nthe LGC as an estimation of the bidirectional diffusion distribution (BDD),\nwhich is specialized for capturing the multi-hop affinity between nodes in the\npresence of attributes. Furthermore, we propose LACA, an efficient and\neffective approach for LGC that achieves superb empirical performance on\nmultiple real datasets while maintaining strong locality. The core components\nof LACA include (i) a fast and theoretically-grounded preprocessing technique\nfor node attributes, (ii) an adaptive algorithm for diffusing any vectors over\n$G$ with rigorous theoretical guarantees and expedited convergence, and (iii)\nan effective three-step scheme for BDD approximation. Extensive experiments,\ncomparing 17 competitors on 8 real datasets, show that LACA outperforms all\ncompetitors in terms of result quality measured against ground truth local\nclusters, while also being up to orders of magnitude faster. The code is\navailable at https:\/\/github.com\/HaoranZ99\/alac.\n","date":"2025-03-26"}
{"id":"2503.20491","title":"VPO: Aligning Text-to-Video Generation Models with Prompt Optimization","abstract":"  Video generation models have achieved remarkable progress in text-to-video\ntasks. These models are typically trained on text-video pairs with highly\ndetailed and carefully crafted descriptions, while real-world user inputs\nduring inference are often concise, vague, or poorly structured. This gap makes\nprompt optimization crucial for generating high-quality videos. Current methods\noften rely on large language models (LLMs) to refine prompts through in-context\nlearning, but suffer from several limitations: they may distort user intent,\nomit critical details, or introduce safety risks. Moreover, they optimize\nprompts without considering the impact on the final video quality, which can\nlead to suboptimal results. To address these issues, we introduce VPO, a\nprincipled framework that optimizes prompts based on three core principles:\nharmlessness, accuracy, and helpfulness. The generated prompts faithfully\npreserve user intents and, more importantly, enhance the safety and quality of\ngenerated videos. To achieve this, VPO employs a two-stage optimization\napproach. First, we construct and refine a supervised fine-tuning (SFT) dataset\nbased on principles of safety and alignment. Second, we introduce both\ntext-level and video-level feedback to further optimize the SFT model with\npreference learning. Our extensive experiments demonstrate that VPO\nsignificantly improves safety, alignment, and video quality compared to\nbaseline methods. Moreover, VPO shows strong generalization across video\ngeneration models. Furthermore, we demonstrate that VPO could outperform and be\ncombined with RLHF methods on video generation models, underscoring the\neffectiveness of VPO in aligning video generation models. Our code and data are\npublicly available at https:\/\/github.com\/thu-coai\/VPO.\n","date":"2025-03-26"}
{"id":"2503.20492","title":"Towards Efficient and General-Purpose Few-Shot Misclassification\n  Detection for Vision-Language Models","abstract":"  Reliable prediction by classifiers is crucial for their deployment in high\nsecurity and dynamically changing situations. However, modern neural networks\noften exhibit overconfidence for misclassified predictions, highlighting the\nneed for confidence estimation to detect errors. Despite the achievements\nobtained by existing methods on small-scale datasets, they all require training\nfrom scratch and there are no efficient and effective misclassification\ndetection (MisD) methods, hindering practical application towards large-scale\nand ever-changing datasets. In this paper, we pave the way to exploit vision\nlanguage model (VLM) leveraging text information to establish an efficient and\ngeneral-purpose misclassification detection framework. By harnessing the power\nof VLM, we construct FSMisD, a Few-Shot prompt learning framework for MisD to\nrefrain from training from scratch and therefore improve tuning efficiency. To\nenhance misclassification detection ability, we use adaptive pseudo sample\ngeneration and a novel negative loss to mitigate the issue of overconfidence by\npushing category prompts away from pseudo features. We conduct comprehensive\nexperiments with prompt learning methods and validate the generalization\nability across various datasets with domain shift. Significant and consistent\nimprovement demonstrates the effectiveness, efficiency and generalizability of\nour approach.\n","date":"2025-03-26"}
{"id":"2503.20493","title":"Automated and Risk-Aware Engine Control Calibration Using Constrained\n  Bayesian Optimization","abstract":"  Decarbonization of the transport sector sets increasingly strict demands to\nmaximize thermal efficiency and minimize greenhouse gas emissions of Internal\nCombustion Engines. This has led to complex engines with a surge in the number\nof corresponding tunable parameters in actuator set points and control\nsettings. Automated calibration is therefore essential to keep development time\nand costs at acceptable levels. In this work, an innovative self-learning\ncalibration method is presented based on in-cylinder pressure curve shaping.\nThis method combines Principal Component Decomposition with constrained\nBayesian Optimization. To realize maximal thermal engine efficiency, the\noptimization problem aims at minimizing the difference between the actual\nin-cylinder pressure curve and an Idealized Thermodynamic Cycle. By\ncontinuously updating a Gaussian Process Regression model of the pressure's\nPrincipal Components weights using measurements of the actual operating\nconditions, the mean in-cylinder pressure curve as well as its uncertainty\nbounds are learned. This information drives the optimization of calibration\nparameters, which are automatically adapted while dealing with the risks and\nuncertainties associated with operational safety and combustion stability. This\ndata-driven method does not require prior knowledge of the system. The proposed\nmethod is successfully demonstrated in simulation using a Reactivity Controlled\nCompression Ignition engine model. The difference between the Gross Indicated\nEfficiency of the optimal solution found and the true optimum is 0.017%. For\nthis complex engine, the optimal solution was found after 64.4s, which is\nrelatively fast compared to conventional calibration methods.\n","date":"2025-03-26"}
{"id":"2503.20496","title":"Enhancing Depression Detection via Question-wise Modality Fusion","abstract":"  Depression is a highly prevalent and disabling condition that incurs\nsubstantial personal and societal costs. Current depression diagnosis involves\ndetermining the depression severity of a person through self-reported\nquestionnaires or interviews conducted by clinicians. This often leads to\ndelayed treatment and involves substantial human resources. Thus, several works\ntry to automate the process using multimodal data. However, they usually\noverlook the following: i) The variable contribution of each modality for each\nquestion in the questionnaire and ii) Using ordinal classification for the\ntask. This results in sub-optimal fusion and training methods. In this work, we\npropose a novel Question-wise Modality Fusion (QuestMF) framework trained with\na novel Imbalanced Ordinal Log-Loss (ImbOLL) function to tackle these issues.\nThe performance of our framework is comparable to the current state-of-the-art\nmodels on the E-DAIC dataset and enhances interpretability by predicting scores\nfor each question. This will help clinicians identify an individual's symptoms,\nallowing them to customise their interventions accordingly. We also make the\ncode for the QuestMF framework publicly available.\n","date":"2025-03-26"}
{"id":"2503.20500","title":"Design and Evaluation of Neural Network-Based Receiver Architectures for\n  Reliable Communication","abstract":"  Neural network-based receivers leverage deep learning to optimize signal\ndetection and decoding, significantly improving bit-error rate (BER) and\nblock-error rate (BLER) in challenging environments. This study evaluates\nvarious architectures and compares their BER and BLER performance across\ndifferent noise levels. Two novel models, the Dual Attention Transformer (DAT)\nand the Residual Dual Non-Local Attention Network (RDNLA), integrate\nself-attention and residual learning to enhance signal reconstruction. These\nmodels bypass conventional channel estimation and equalization by directly\npredicting log-likelihood ratios (LLRs) from received signals, with noise\nvariance as an additional input. Simulations show that DAT and RDNLA outperform\ntraditional and other neural receiver models under varying signal-to-noise\nratios (SNR), while their computational efficiency supports their feasibility\nfor next-generation communication systems.\n","date":"2025-03-26"}
{"id":"2503.20502","title":"MLLM-Selector: Necessity and Diversity-driven High-Value Data Selection\n  for Enhanced Visual Instruction Tuning","abstract":"  Visual instruction tuning (VIT) has emerged as a crucial technique for\nenabling multi-modal large language models (MLLMs) to follow user instructions\nadeptly. Yet, a significant gap persists in understanding the attributes of\nhigh-quality instruction tuning data and frameworks for its automated\nselection. To address this, we introduce MLLM-Selector, an automated approach\nthat identifies valuable data for VIT by weighing necessity and diversity. Our\nprocess starts by randomly sampling a subset from the VIT data pool to\nfine-tune a pretrained model, thus creating a seed model with an initial\nability to follow instructions. Then, leveraging the seed model, we calculate\nnecessity scores for each sample in the VIT data pool to identify samples\npivotal for enhancing model performance. Our findings underscore the importance\nof mixing necessity and diversity in data choice, leading to the creation of\nMLLM-Selector, our methodology that fuses necessity scoring with strategic\nsampling for superior data refinement. Empirical results indicate that within\nidentical experimental conditions, MLLM-Selector surpasses LLaVA-1.5 in some\nbenchmarks with less than 1% of the data and consistently exceeds performance\nacross all validated benchmarks when using less than 50%.\n","date":"2025-03-26"}
{"id":"2503.20504","title":"Vision-Amplified Semantic Entropy for Hallucination Detection in Medical\n  Visual Question Answering","abstract":"  Multimodal large language models (MLLMs) have demonstrated significant\npotential in medical Visual Question Answering (VQA). Yet, they remain prone to\nhallucinations-incorrect responses that contradict input images, posing\nsubstantial risks in clinical decision-making. Detecting these hallucinations\nis essential for establishing trust in MLLMs among clinicians and patients,\nthereby enabling their real-world adoption. Current hallucination detection\nmethods, especially semantic entropy (SE), have demonstrated promising\nhallucination detection capacity for LLMs. However, adapting SE to medical\nMLLMs by incorporating visual perturbations presents a dilemma. Weak\nperturbations preserve image content and ensure clinical validity, but may be\noverlooked by medical MLLMs, which tend to over rely on language priors. In\ncontrast, strong perturbations can distort essential diagnostic features,\ncompromising clinical interpretation. To address this issue, we propose Vision\nAmplified Semantic Entropy (VASE), which incorporates weak image\ntransformations and amplifies the impact of visual input, to improve\nhallucination detection in medical VQA. We first estimate the semantic\npredictive distribution under weak visual transformations to preserve clinical\nvalidity, and then amplify visual influence by contrasting this distribution\nwith that derived from a distorted image. The entropy of the resulting\ndistribution is estimated as VASE. Experiments on two medical open-ended VQA\ndatasets demonstrate that VASE consistently outperforms existing hallucination\ndetection methods.\n","date":"2025-03-26"}
{"id":"2503.20505","title":"Riemannian Optimization on Relaxed Indicator Matrix Manifold","abstract":"  The indicator matrix plays an important role in machine learning, but\noptimizing it is an NP-hard problem. We propose a new relaxation of the\nindicator matrix and prove that this relaxation forms a manifold, which we call\nthe Relaxed Indicator Matrix Manifold (RIM manifold). Based on Riemannian\ngeometry, we develop a Riemannian toolbox for optimization on the RIM manifold.\nSpecifically, we provide several methods of Retraction, including a fast\nRetraction method to obtain geodesics. We point out that the RIM manifold is a\ngeneralization of the double stochastic manifold, and it is much faster than\nexisting methods on the double stochastic manifold, which has a complexity of\n\\( \\mathcal{O}(n^3) \\), while RIM manifold optimization is \\( \\mathcal{O}(n) \\)\nand often yields better results. We conducted extensive experiments, including\nimage denoising, with millions of variables to support our conclusion, and\napplied the RIM manifold to Ratio Cut, achieving clustering results that\noutperform the state-of-the-art methods. Our Code in\n\\href{https:\/\/github.com\/Yuan-Jinghui\/Riemannian-Optimization-on-Relaxed-Indicator-Matrix-Manifold}{https:\/\/github.com\/Yuan-Jinghui\/Riemannian-Optimization-on-Relaxed-Indicator-Matrix-Manifold}.\n","date":"2025-03-26"}
{"id":"2503.20507","title":"Harmonia: A Multi-Agent Reinforcement Learning Approach to Data\n  Placement and Migration in Hybrid Storage Systems","abstract":"  Hybrid storage systems (HSS) combine multiple storage devices with diverse\ncharacteristics to achieve high performance and capacity at low cost. The\nperformance of an HSS highly depends on the effectiveness of two key policies:\n(1) the data-placement policy, which determines the best-fit storage device for\nincoming data, and (2) the data-migration policy, which rearranges stored data\nacross the devices to sustain high HSS performance. Prior works focus on\nimproving only data placement or only data migration in HSS, which leads to\nsub-optimal HSS performance. Unfortunately, no prior work tries to optimize\nboth policies together. Our goal is to design a holistic data-management\ntechnique for HSS that optimizes both data-placement and data-migration\npolicies to fully exploit the potential of an HSS. We propose Harmonia, a\nmulti-agent reinforcement learning (RL)-based data-management technique that\nemploys two light-weight autonomous RL agents, a data-placement agent and a\ndata-migration agent, which adapt their policies for the current workload and\nHSS configuration, and coordinate with each other to improve overall HSS\nperformance. We evaluate Harmonia on a real HSS with up to four heterogeneous\nstorage devices with diverse characteristics. Our evaluation using 17\ndata-intensive workloads on performance-optimized (cost-optimized) HSS with two\nstorage devices shows that, on average, Harmonia (1) outperforms the\nbest-performing prior approach by 49.5% (31.7%), (2) bridges the performance\ngap between the best-performing prior work and Oracle by 64.2% (64.3%). On an\nHSS with three (four) devices, Harmonia outperforms the best-performing prior\nwork by 37.0% (42.0%). Harmonia's performance benefits come with low latency\n(240ns for inference) and storage overheads (206 KiB for both RL agents\ntogether). We plan to open-source Harmonia's implementation to aid future\nresearch on HSS.\n","date":"2025-03-26"}
{"id":"2503.20508","title":"Explainable ICD Coding via Entity Linking","abstract":"  Clinical coding is a critical task in healthcare, although traditional\nmethods for automating clinical coding may not provide sufficient explicit\nevidence for coders in production environments. This evidence is crucial, as\nmedical coders have to make sure there exists at least one explicit passage in\nthe input health record that justifies the attribution of a code. We therefore\npropose to reframe the task as an entity linking problem, in which each\ndocument is annotated with its set of codes and respective textual evidence,\nenabling better human-machine collaboration. By leveraging parameter-efficient\nfine-tuning of Large Language Models (LLMs), together with constrained\ndecoding, we introduce three approaches to solve this problem that prove\neffective at disambiguating clinical mentions and that perform well in few-shot\nscenarios.\n","date":"2025-03-26"}
{"id":"2503.20516","title":"Small Object Detection: A Comprehensive Survey on Challenges, Techniques\n  and Real-World Applications","abstract":"  Small object detection (SOD) is a critical yet challenging task in computer\nvision, with applications like spanning surveillance, autonomous systems,\nmedical imaging, and remote sensing. Unlike larger objects, small objects\ncontain limited spatial and contextual information, making accurate detection\ndifficult. Challenges such as low resolution, occlusion, background\ninterference, and class imbalance further complicate the problem. This survey\nprovides a comprehensive review of recent advancements in SOD using deep\nlearning, focusing on articles published in Q1 journals during 2024-2025. We\nanalyzed challenges, state-of-the-art techniques, datasets, evaluation metrics,\nand real-world applications. Recent advancements in deep learning have\nintroduced innovative solutions, including multi-scale feature extraction,\nSuper-Resolution (SR) techniques, attention mechanisms, and transformer-based\narchitectures. Additionally, improvements in data augmentation, synthetic data\ngeneration, and transfer learning have addressed data scarcity and domain\nadaptation issues. Furthermore, emerging trends such as lightweight neural\nnetworks, knowledge distillation (KD), and self-supervised learning offer\npromising directions for improving detection efficiency, particularly in\nresource-constrained environments like Unmanned Aerial Vehicles (UAV)-based\nsurveillance and edge computing. We also review widely used datasets, along\nwith standard evaluation metrics such as mean Average Precision (mAP) and\nsize-specific AP scores. The survey highlights real-world applications,\nincluding traffic monitoring, maritime surveillance, industrial defect\ndetection, and precision agriculture. Finally, we discuss open research\nchallenges and future directions, emphasizing the need for robust domain\nadaptation techniques, better feature fusion strategies, and real-time\nperformance optimization.\n","date":"2025-03-26"}
{"id":"2503.20518","title":"Exploring the Effect of Robotic Embodiment and Empathetic Tone of LLMs\n  on Empathy Elicitation","abstract":"  This study investigates the elicitation of empathy toward a third party\nthrough interaction with social agents. Participants engaged with either a\nphysical robot or a voice-enabled chatbot, both driven by a large language\nmodel (LLM) programmed to exhibit either an empathetic tone or remain neutral.\nThe interaction is focused on a fictional character, Katie Banks, who is in a\nchallenging situation and in need of financial donations. The willingness to\nhelp Katie, measured by the number of hours participants were willing to\nvolunteer, along with their perceptions of the agent, were assessed for 60\nparticipants. Results indicate that neither robotic embodiment nor empathetic\ntone significantly influenced participants' willingness to volunteer. While the\nLLM effectively simulated human empathy, fostering genuine empathetic responses\nin participants proved challenging.\n","date":"2025-03-26"}
{"id":"2503.20519","title":"MAR-3D: Progressive Masked Auto-regressor for High-Resolution 3D\n  Generation","abstract":"  Recent advances in auto-regressive transformers have revolutionized\ngenerative modeling across different domains, from language processing to\nvisual generation, demonstrating remarkable capabilities. However, applying\nthese advances to 3D generation presents three key challenges: the unordered\nnature of 3D data conflicts with sequential next-token prediction paradigm,\nconventional vector quantization approaches incur substantial compression loss\nwhen applied to 3D meshes, and the lack of efficient scaling strategies for\nhigher resolution latent prediction. To address these challenges, we introduce\nMAR-3D, which integrates a pyramid variational autoencoder with a cascaded\nmasked auto-regressive transformer (Cascaded MAR) for progressive latent\nupscaling in the continuous space. Our architecture employs random masking\nduring training and auto-regressive denoising in random order during inference,\nnaturally accommodating the unordered property of 3D latent tokens.\nAdditionally, we propose a cascaded training strategy with condition\naugmentation that enables efficiently up-scale the latent token resolution with\nfast convergence. Extensive experiments demonstrate that MAR-3D not only\nachieves superior performance and generalization capabilities compared to\nexisting methods but also exhibits enhanced scaling capabilities compared to\njoint distribution modeling approaches (e.g., diffusion transformers).\n","date":"2025-03-26"}
{"id":"2503.20523","title":"GAIA-2: A Controllable Multi-View Generative World Model for Autonomous\n  Driving","abstract":"  Generative models offer a scalable and flexible paradigm for simulating\ncomplex environments, yet current approaches fall short in addressing the\ndomain-specific requirements of autonomous driving - such as multi-agent\ninteractions, fine-grained control, and multi-camera consistency. We introduce\nGAIA-2, Generative AI for Autonomy, a latent diffusion world model that unifies\nthese capabilities within a single generative framework. GAIA-2 supports\ncontrollable video generation conditioned on a rich set of structured inputs:\nego-vehicle dynamics, agent configurations, environmental factors, and road\nsemantics. It generates high-resolution, spatiotemporally consistent\nmulti-camera videos across geographically diverse driving environments (UK, US,\nGermany). The model integrates both structured conditioning and external latent\nembeddings (e.g., from a proprietary driving model) to facilitate flexible and\nsemantically grounded scene synthesis. Through this integration, GAIA-2 enables\nscalable simulation of both common and rare driving scenarios, advancing the\nuse of generative world models as a core tool in the development of autonomous\nsystems. Videos are available at https:\/\/wayve.ai\/thinking\/gaia-2.\n","date":"2025-03-26"}
{"id":"2503.20527","title":"StableToolBench-MirrorAPI: Modeling Tool Environments as Mirrors of\n  7,000+ Real-World APIs","abstract":"  The rapid advancement of large language models (LLMs) has spurred significant\ninterest in tool learning, where LLMs are augmented with external tools to\ntackle complex tasks. However, existing tool environments face challenges in\nbalancing stability, scalability, and realness, particularly for benchmarking\npurposes. To address this problem, we propose MirrorAPI, a novel framework that\ntrains specialized LLMs to accurately simulate real API responses, effectively\nacting as \"mirrors\" to tool environments. Using a comprehensive dataset of\nrequest-response pairs from 7,000+ APIs, we employ supervised fine-tuning and\nchain-of-thought reasoning to enhance simulation fidelity. MirrorAPI achieves\nsuperior accuracy and stability compared to state-of-the-art methods, as\ndemonstrated by its performance on the newly constructed MirrorAPI-Bench and\nits integration into StableToolBench.\n","date":"2025-03-26"}
{"id":"2503.20533","title":"Accelerate Parallelizable Reasoning via Parallel Decoding within One\n  Sequence","abstract":"  Recent advances in reasoning models have demonstrated significant\nimprovements in accuracy, particularly for complex tasks such as mathematical\nreasoning, by employing detailed and comprehensive reasoning processes.\nHowever, generating these lengthy reasoning sequences is computationally\nexpensive and time-consuming. To address this inefficiency, we leverage the\ninherent parallelizability of certain tasks to accelerate the reasoning\nprocess. Specifically, when multiple parallel reasoning branches exist, we\ndecode multiple tokens per step using a specialized attention mask, processing\nthem within a single sequence, avoiding additional memory usage. Experimental\nresults show that our method achieves over 100% speedup in decoding time while\nmaintaining the answer quality.\n","date":"2025-03-26"}
{"id":"2503.20537","title":"TD-BFR: Truncated Diffusion Model for Efficient Blind Face Restoration","abstract":"  Diffusion-based methodologies have shown significant potential in blind face\nrestoration (BFR), leveraging their robust generative capabilities. However,\nthey are often criticized for two significant problems: 1) slow training and\ninference speed, and 2) inadequate recovery of fine-grained facial details. To\naddress these problems, we propose a novel Truncated Diffusion model for\nefficient Blind Face Restoration (TD-BFR), a three-stage paradigm tailored for\nthe progressive resolution of degraded images. Specifically, TD-BFR utilizes an\ninnovative truncated sampling method, starting from low-quality (LQ) images at\nlow resolution to enhance sampling speed, and then introduces an adaptive\ndegradation removal module to handle unknown degradations and connect the\ngeneration processes across different resolutions. Additionally, we further\nadapt the priors of pre-trained diffusion models to recover rich facial\ndetails. Our method efficiently restores high-quality images in a\ncoarse-to-fine manner and experimental results demonstrate that TD-BFR is, on\naverage, \\textbf{4.75$\\times$} faster than current state-of-the-art\ndiffusion-based BFR methods while maintaining competitive quality.\n","date":"2025-03-26"}
{"id":"2503.20540","title":"Beyond Intermediate States: Explaining Visual Redundancy through\n  Language","abstract":"  Multi-modal Large Langue Models (MLLMs) often process thousands of visual\ntokens, which consume a significant portion of the context window and impose a\nsubstantial computational burden. Prior work has empirically explored visual\ntoken pruning methods based on MLLMs' intermediate states (e.g., attention\nscores). However, they have limitations in precisely defining visual redundancy\ndue to their inability to capture the influence of visual tokens on MLLMs'\nvisual understanding (i.e., the predicted probabilities for textual token\ncandidates). To address this issue, we manipulate the visual input and\ninvestigate variations in the textual output from both token-centric and\ncontext-centric perspectives, achieving intuitive and comprehensive analysis.\nExperimental results reveal that visual tokens with low ViT-[cls] association\nand low text-to-image attention scores can contain recognizable information and\nsignificantly contribute to images' overall information. To develop a more\nreliable method for identifying and pruning redundant visual tokens, we\nintegrate these two perspectives and introduce a context-independent condition\nto identify redundant prototypes from training images, which probes the\nredundancy of each visual token during inference. Extensive experiments on\nsingle-image, multi-image and video comprehension tasks demonstrate the\neffectiveness of our method, notably achieving 90% to 110% of the performance\nwhile pruning 80% to 90% of visual tokens.\n","date":"2025-03-26"}
{"id":"2503.20541","title":"Fast, Modular, and Differentiable Framework for Machine\n  Learning-Enhanced Molecular Simulations","abstract":"  We present an end-to-end differentiable molecular simulation framework\n(DIMOS) for molecular dynamics and Monte Carlo simulations. DIMOS easily\nintegrates machine-learning-based interatomic potentials and implements\nclassical force fields including particle-mesh Ewald electrostatics. Thanks to\nits modularity, both classical and machine-learning-based approaches can be\neasily combined into a hybrid description of the system (ML\/MM). By supporting\nkey molecular dynamics features such as efficient neighborlists and constraint\nalgorithms for larger time steps, the framework bridges the gap between\nhand-optimized simulation engines and the flexibility of a PyTorch\nimplementation. The superior performance and the high versatility is probed in\ndifferent benchmarks and applications, with speed-up factors of up to\n$170\\times$. The advantage of differentiability is demonstrated by an\nend-to-end optimization of the proposal distribution in a Markov Chain Monte\nCarlo simulation based on Hamiltonian Monte Carlo. Using these optimized\nsimulation parameters a $3\\times$ acceleration is observed in comparison to\nad-hoc chosen simulation parameters. The code is available at\nhttps:\/\/github.com\/nec-research\/DIMOS.\n","date":"2025-03-26"}
{"id":"2503.20546","title":"Regression-Based Estimation of Causal Effects in the Presence of\n  Selection Bias and Confounding","abstract":"  We consider the problem of estimating the expected causal effect $E[Y|do(X)]$\nfor a target variable $Y$ when treatment $X$ is set by intervention, focusing\non continuous random variables. In settings without selection bias or\nconfounding, $E[Y|do(X)] = E[Y|X]$, which can be estimated using standard\nregression methods. However, regression fails when systematic missingness\ninduced by selection bias, or confounding distorts the data. Boeken et al.\n[2023] show that when training data is subject to selection, proxy variables\nunaffected by this process can, under certain constraints, be used to correct\nfor selection bias to estimate $E[Y|X]$, and hence $E[Y|do(X)]$, reliably. When\ndata is additionally affected by confounding, however, this equality is no\nlonger valid.\n  Building on these results, we consider a more general setting and propose a\nframework that incorporates both selection bias and confounding. Specifically,\nwe derive theoretical conditions ensuring identifiability and recoverability of\ncausal effects under access to external data and proxy variables. We further\nintroduce a two-step regression estimator (TSR), capable of exploiting proxy\nvariables to adjust for selection bias while accounting for confounding. We\nshow that TSR coincides with prior work if confounding is absent, but achieves\na lower variance. Extensive simulation studies validate TSR's correctness for\nscenarios which may include both selection bias and confounding with proxy\nvariables.\n","date":"2025-03-26"}
{"id":"2503.20552","title":"Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and\n  Throughput via Attention Disaggregation","abstract":"  In large language model (LLM) serving systems, executing each request\nconsists of two phases: the compute-intensive prefill phase and the\nmemory-intensive decoding phase. To prevent performance interference between\nthe two phases, current LLM serving systems typically adopt prefill-decoding\ndisaggregation, where the two phases are split across separate machines.\nHowever, we observe this approach leads to significant resource\nunderutilization. Specifically, prefill instances that are compute-intensive\nsuffer from low memory utilization, while decoding instances that are\nmemory-intensive experience low compute utilization. To address this problem,\nthis paper proposes Adrenaline, an attention disaggregation and offloading\nmechanism designed to enhance resource utilization and performance in LLM\nserving systems. Adrenaline's key innovation lies in disaggregating part of the\nattention computation in the decoding phase and offloading them to prefill\ninstances. The memory-bound nature of decoding-phase attention computation\ninherently enables an effective offloading strategy, yielding two complementary\nadvantages: 1) improved memory capacity and bandwidth utilization in prefill\ninstances, and 2) increased decoding batch sizes that enhance compute\nutilization in decoding instances, collectively boosting overall system\nperformance. Adrenaline achieves these gains through three key techniques:\nlow-latency decoding synchronization, resource-efficient prefill colocation,\nand load-aware offloading scheduling. Experimental results show that Adrenaline\nachieves 2.28x higher memory capacity and 2.07x better memory bandwidth\nutilization in prefill instances, up to 1.67x improvements in compute\nutilization for decoding instances, and 1.68x higher overall inference\nthroughput compared to state-of-the-art systems.\n","date":"2025-03-26"}
{"id":"2503.20556","title":"A Retrieval-Based Approach to Medical Procedure Matching in Romanian","abstract":"  Accurately mapping medical procedure names from healthcare providers to\nstandardized terminology used by insurance companies is a crucial yet complex\ntask. Inconsistencies in naming conventions lead to missclasified procedures,\ncausing administrative inefficiencies and insurance claim problems in private\nhealthcare settings. Many companies still use human resources for manual\nmapping, while there is a clear opportunity for automation. This paper proposes\na retrieval-based architecture leveraging sentence embeddings for medical name\nmatching in the Romanian healthcare system. This challenge is significantly\nmore difficult in underrepresented languages such as Romanian, where existing\npretrained language models lack domain-specific adaptation to medical text. We\nevaluate multiple embedding models, including Romanian, multilingual, and\nmedical-domain-specific representations, to identify the most effective\nsolution for this task. Our findings contribute to the broader field of medical\nNLP for low-resource languages such as Romanian.\n","date":"2025-03-26"}
{"id":"2503.20561","title":"A Theoretical Framework for Prompt Engineering: Approximating Smooth\n  Functions with Transformer Prompts","abstract":"  Prompt engineering has emerged as a powerful technique for guiding large\nlanguage models (LLMs) toward desired responses, significantly enhancing their\nperformance across diverse tasks. Beyond their role as static predictors, LLMs\nincreasingly function as intelligent agents, capable of reasoning,\ndecision-making, and adapting dynamically to complex environments. However, the\ntheoretical underpinnings of prompt engineering remain largely unexplored. In\nthis paper, we introduce a formal framework demonstrating that transformer\nmodels, when provided with carefully designed prompts, can act as a\nconfigurable computational system by emulating a ``virtual'' neural network\nduring inference. Specifically, input prompts effectively translate into the\ncorresponding network configuration, enabling LLMs to adjust their internal\ncomputations dynamically. Building on this construction, we establish an\napproximation theory for $\\beta$-times differentiable functions, proving that\ntransformers can approximate such functions with arbitrary precision when\nguided by appropriately structured prompts. Moreover, our framework provides\ntheoretical justification for several empirically successful prompt engineering\ntechniques, including the use of longer, structured prompts, filtering\nirrelevant information, enhancing prompt token diversity, and leveraging\nmulti-agent interactions. By framing LLMs as adaptable agents rather than\nstatic models, our findings underscore their potential for autonomous reasoning\nand problem-solving, paving the way for more robust and theoretically grounded\nadvancements in prompt engineering and AI agent design.\n","date":"2025-03-26"}
{"id":"2503.20563","title":"TerraTorch: The Geospatial Foundation Models Toolkit","abstract":"  TerraTorch is a fine-tuning and benchmarking toolkit for Geospatial\nFoundation Models built on PyTorch Lightning and tailored for satellite,\nweather, and climate data. It integrates domain-specific data modules,\npre-defined tasks, and a modular model factory that pairs any backbone with\ndiverse decoder heads. These components allow researchers and practitioners to\nfine-tune supported models in a no-code fashion by simply editing a training\nconfiguration. By consolidating best practices for model development and\nincorporating the automated hyperparameter optimization extension Iterate,\nTerraTorch reduces the expertise and time required to fine-tune or benchmark\nmodels on new Earth Observation use cases. Furthermore, TerraTorch directly\nintegrates with GEO-Bench, allowing for systematic and reproducible\nbenchmarking of Geospatial Foundation Models. TerraTorch is open sourced under\nApache 2.0, available at https:\/\/github.com\/IBM\/terratorch, and can be\ninstalled via pip install terratorch.\n","date":"2025-03-26"}
{"id":"2503.20568","title":"Low-resource Information Extraction with the European Clinical Case\n  Corpus","abstract":"  We present E3C-3.0, a multilingual dataset in the medical domain, comprising\nclinical cases annotated with diseases and test-result relations. The dataset\nincludes both native texts in five languages (English, French, Italian, Spanish\nand Basque) and texts translated and projected from the English source into\nfive target languages (Greek, Italian, Polish, Slovak, and Slovenian). A\nsemi-automatic approach has been implemented, including automatic annotation\nprojection based on Large Language Models (LLMs) and human revision. We present\nseveral experiments showing that current state-of-the-art LLMs can benefit from\nbeing fine-tuned on the E3C-3.0 dataset. We also show that transfer learning in\ndifferent languages is very effective, mitigating the scarcity of data.\nFinally, we compare performance both on native data and on projected data. We\nrelease the data at\nhttps:\/\/huggingface.co\/collections\/NLP-FBK\/e3c-projected-676a7d6221608d60e4e9fd89 .\n","date":"2025-03-26"}
{"id":"2503.20571","title":"Exploring Robustness of Cortical Morphometry in the presence of white\n  matter lesions, using Diffusion Models for Lesion Filling","abstract":"  Cortical thickness measurements from magnetic resonance imaging, an important\nbiomarker in many neurodegenerative and neurological disorders, are derived by\nmany tools from an initial voxel-wise tissue segmentation. White matter (WM)\nhypointensities in T1-weighted imaging, such as those arising from multiple\nsclerosis or small vessel disease, are known to affect the output of brain\nsegmentation methods and therefore bias cortical thickness measurements. These\neffects are well-documented among traditional brain segmentation tools but have\nnot been studied extensively in tools based on deep-learning segmentations,\nwhich promise to be more robust. In this paper, we explore the potential of\ndeep learning to enhance the accuracy and efficiency of cortical thickness\nmeasurement in the presence of WM lesions, using a high-quality lesion filling\nalgorithm leveraging denoising diffusion networks.\n  A pseudo-3D U-Net architecture trained on the OASIS dataset to generate\nsynthetic healthy tissue, conditioned on binary lesion masks derived from the\nMSSEG dataset, allows realistic removal of white matter lesions in multiple\nsclerosis patients. By applying morphometry methods to patient images before\nand after lesion filling, we analysed robustness of global and regional\ncortical thickness measurements in the presence of white matter lesions.\nMethods based on a deep learning-based segmentation of the brain (Fastsurfer,\nDL+DiReCT, ANTsPyNet) exhibited greater robustness than those using classical\nsegmentation methods (Freesurfer, ANTs).\n","date":"2025-03-26"}
{"id":"2503.20576","title":"Optimizing Case-Based Reasoning System for Functional Test Script\n  Generation with Large Language Models","abstract":"  In this work, we explore the potential of large language models (LLMs) for\ngenerating functional test scripts, which necessitates understanding the\ndynamically evolving code structure of the target software. To achieve this, we\npropose a case-based reasoning (CBR) system utilizing a 4R cycle (i.e.,\nretrieve, reuse, revise, and retain), which maintains and leverages a case bank\nof test intent descriptions and corresponding test scripts to facilitate LLMs\nfor test script generation. To improve user experience further, we introduce\nRe4, an optimization method for the CBR system, comprising reranking-based\nretrieval finetuning and reinforced reuse finetuning. Specifically, we first\nidentify positive examples with high semantic and script similarity, providing\nreliable pseudo-labels for finetuning the retriever model without costly\nlabeling. Then, we apply supervised finetuning, followed by a reinforcement\nlearning finetuning stage, to align LLMs with our production scenarios,\nensuring the faithful reuse of retrieved cases. Extensive experimental results\non two product development units from Huawei Datacom demonstrate the\nsuperiority of the proposed CBR+Re4. Notably, we also show that the proposed\nRe4 method can help alleviate the repetitive generation issues with LLMs.\n","date":"2025-03-26"}
{"id":"2503.20583","title":"Feature Statistics with Uncertainty Help Adversarial Robustness","abstract":"  Despite the remarkable success of deep neural networks (DNNs), the security\nthreat of adversarial attacks poses a significant challenge to the reliability\nof DNNs. By introducing randomness into different parts of DNNs, stochastic\nmethods can enable the model to learn some uncertainty, thereby improving model\nrobustness efficiently. In this paper, we theoretically discover a universal\nphenomenon that adversarial attacks will shift the distributions of feature\nstatistics. Motivated by this theoretical finding, we propose a robustness\nenhancement module called Feature Statistics with Uncertainty (FSU). It\nresamples channel-wise feature means and standard deviations of examples from\nmultivariate Gaussian distributions, which helps to reconstruct the attacked\nexamples and calibrate the shifted distributions. The calibration recovers some\ndomain characteristics of the data for classification, thereby mitigating the\ninfluence of perturbations and weakening the ability of attacks to deceive\nmodels. The proposed FSU module has universal applicability in training,\nattacking, predicting and fine-tuning, demonstrating impressive robustness\nenhancement ability at trivial additional time cost. For example, against\npowerful optimization-based CW attacks, by incorporating FSU into attacking and\npredicting phases, it endows many collapsed state-of-the-art models with\n50%-80% robust accuracy on CIFAR10, CIFAR100 and SVHN.\n","date":"2025-03-26"}
{"id":"2503.20588","title":"Synthetic Data Augmentation for Cross-domain Implicit Discourse Relation\n  Recognition","abstract":"  Implicit discourse relation recognition (IDRR) -- the task of identifying the\nimplicit coherence relation between two text spans -- requires deep semantic\nunderstanding. Recent studies have shown that zero- or few-shot approaches\nsignificantly lag behind supervised models, but LLMs may be useful for\nsynthetic data augmentation, where LLMs generate a second argument following a\nspecified coherence relation. We applied this approach in a cross-domain\nsetting, generating discourse continuations using unlabelled target-domain data\nto adapt a base model which was trained on source-domain labelled data.\nEvaluations conducted on a large-scale test set revealed that different\nvariations of the approach did not result in any significant improvements. We\nconclude that LLMs often fail to generate useful samples for IDRR, and\nemphasize the importance of considering both statistical significance and\ncomparability when evaluating IDRR models.\n","date":"2025-03-26"}
{"id":"2503.20595","title":"Diffusion Counterfactuals for Image Regressors","abstract":"  Counterfactual explanations have been successfully applied to create human\ninterpretable explanations for various black-box models. They are handy for\ntasks in the image domain, where the quality of the explanations benefits from\nrecent advances in generative models. Although counterfactual explanations have\nbeen widely applied to classification models, their application to regression\ntasks remains underexplored. We present two methods to create counterfactual\nexplanations for image regression tasks using diffusion-based generative models\nto address challenges in sparsity and quality: 1) one based on a Denoising\nDiffusion Probabilistic Model that operates directly in pixel-space and 2)\nanother based on a Diffusion Autoencoder operating in latent space. Both\nproduce realistic, semantic, and smooth counterfactuals on CelebA-HQ and a\nsynthetic data set, providing easily interpretable insights into the\ndecision-making process of the regression model and reveal spurious\ncorrelations. We find that for regression counterfactuals, changes in features\ndepend on the region of the predicted value. Large semantic changes are needed\nfor significant changes in predicted values, making it harder to find sparse\ncounterfactuals than with classifiers. Moreover, pixel space counterfactuals\nare more sparse while latent space counterfactuals are of higher quality and\nallow bigger semantic changes.\n","date":"2025-03-26"}
{"id":"2503.20607","title":"A decision-theoretic approach to dealing with uncertainty in quantum\n  mechanics","abstract":"  We provide a decision-theoretic framework for dealing with uncertainty in\nquantum mechanics. This uncertainty is two-fold: on the one hand there may be\nuncertainty about the state the quantum system is in, and on the other hand, as\nis essential to quantum mechanical uncertainty, even if the quantum state is\nknown, measurements may still produce an uncertain outcome. In our framework,\nmeasurements therefore play the role of acts with an uncertain outcome and our\nsimple decision-theoretic postulates ensure that Born's rule is encapsulated in\nthe utility functions associated with such acts. This approach allows us to\nuncouple (precise) probability theory from quantum mechanics, in the sense that\nit leaves room for a more general, so-called imprecise probabilities approach.\nWe discuss the mathematical implications of our findings, which allow us to\ngive a decision-theoretic foundation to recent seminal work by Benavoli,\nFacchini and Zaffalon, and we compare our approach to earlier and different\napproaches by Deutsch and Wallace.\n","date":"2025-03-26"}
{"id":"2503.20612","title":"IAP: Improving Continual Learning of Vision-Language Models via\n  Instance-Aware Prompting","abstract":"  Recent pre-trained vision-language models (PT-VLMs) often face a Multi-Domain\nClass-Incremental Learning (MCIL) scenario in practice, where several classes\nand domains of multi-modal tasks are incrementally arrived. Without access to\npreviously learned tasks and unseen tasks, memory-constrained MCIL suffers from\nforward and backward forgetting. To alleviate the above challenges,\nparameter-efficient fine-tuning techniques (PEFT), such as prompt tuning, are\nemployed to adapt the PT-VLM to the diverse incrementally learned tasks. To\nachieve effective new task adaptation, existing methods only consider the\neffect of PEFT strategy selection, but neglect the influence of PEFT parameter\nsetting (e.g., prompting). In this paper, we tackle the challenge of optimizing\nprompt designs for diverse tasks in MCIL and propose an Instance-Aware\nPrompting (IAP) framework. Specifically, our Instance-Aware Gated Prompting\n(IA-GP) module enhances adaptation to new tasks while mitigating forgetting by\ndynamically assigning prompts across transformer layers at the instance level.\nOur Instance-Aware Class-Distribution-Driven Prompting (IA-CDDP) improves the\ntask adaptation process by determining an accurate task-label-related\nconfidence score for each instance. Experimental evaluations across 11\ndatasets, using three performance metrics, demonstrate the effectiveness of our\nproposed method. Code can be found at https:\/\/github.com\/FerdinandZJU\/IAP.\n","date":"2025-03-26"}
{"id":"2503.20613","title":"State-Aware Perturbation Optimization for Robust Deep Reinforcement\n  Learning","abstract":"  Recently, deep reinforcement learning (DRL) has emerged as a promising\napproach for robotic control. However, the deployment of DRL in real-world\nrobots is hindered by its sensitivity to environmental perturbations. While\nexisting whitebox adversarial attacks rely on local gradient information and\napply uniform perturbations across all states to evaluate DRL robustness, they\nfail to account for temporal dynamics and state-specific vulnerabilities. To\ncombat the above challenge, we first conduct a theoretical analysis of\nwhite-box attacks in DRL by establishing the adversarial victim-dynamics Markov\ndecision process (AVD-MDP), to derive the necessary and sufficient conditions\nfor a successful attack. Based on this, we propose a selective state-aware\nreinforcement adversarial attack method, named STAR, to optimize perturbation\nstealthiness and state visitation dispersion. STAR first employs a soft\nmask-based state-targeting mechanism to minimize redundant perturbations,\nenhancing stealthiness and attack effectiveness. Then, it incorporates an\ninformation-theoretic optimization objective to maximize mutual information\nbetween perturbations, environmental states, and victim actions, ensuring a\ndispersed state-visitation distribution that steers the victim agent into\nvulnerable states for maximum return reduction. Extensive experiments\ndemonstrate that STAR outperforms state-of-the-art benchmarks.\n","date":"2025-03-26"}
{"id":"2503.20618","title":"ProFed: a Benchmark for Proximity-based non-IID Federated Learning","abstract":"  In recent years, cro:flFederated learning (FL) has gained significant\nattention within the machine learning community. Although various FL algorithms\nhave been proposed in the literature, their performance often degrades when\ndata across clients is non-independently and identically distributed (non-IID).\nThis skewness in data distribution often emerges from geographic patterns, with\nnotable examples including regional linguistic variations in text data or\nlocalized traffic patterns in urban environments. Such scenarios result in IID\ndata within specific regions but non-IID data across regions. However, existing\nFL algorithms are typically evaluated by randomly splitting non-IID data across\ndevices, disregarding their spatial distribution. To address this gap, we\nintroduce ProFed, a benchmark that simulates data splits with varying degrees\nof skewness across different regions. We incorporate several skewness methods\nfrom the literature and apply them to well-known datasets, including MNIST,\nFashionMNIST, CIFAR-10, and CIFAR-100. Our goal is to provide researchers with\na standardized framework to evaluate FL algorithms more effectively and\nconsistently against established baselines.\n","date":"2025-03-26"}
{"id":"2503.20623","title":"Collaborative Storytelling and LLM: A Linguistic Analysis of\n  Automatically-Generated Role-Playing Game Sessions","abstract":"  Role-playing games (RPG) are games in which players interact with one another\nto create narratives. The role of players in the RPG is largely based on the\ninteraction between players and their characters. This emerging form of shared\nnarrative, primarily oral, is receiving increasing attention. In particular,\nmany authors investigated the use of an LLM as an actor in the game. In this\npaper, we aim to discover to what extent the language of Large Language Models\n(LLMs) exhibit oral or written features when asked to generate an RPG session\nwithout human interference. We will conduct a linguistic analysis of the\nlexical and syntactic features of the generated texts and compare the results\nwith analyses of conversations, transcripts of human RPG sessions, and books.\nWe found that LLMs exhibit a pattern that is distinct from all other text\ncategories, including oral conversations, human RPG sessions and books. Our\nanalysis has shown how training influences the way LLMs express themselves and\nprovides important indications of the narrative capabilities of these tools.\n","date":"2025-03-26"}
{"id":"2503.20630","title":"$\\beta$-GNN: A Robust Ensemble Approach Against Graph Structure\n  Perturbation","abstract":"  Graph Neural Networks (GNNs) are playing an increasingly important role in\nthe efficient operation and security of computing systems, with applications in\nworkload scheduling, anomaly detection, and resource management. However, their\nvulnerability to network perturbations poses a significant challenge. We\npropose $\\beta$-GNN, a model enhancing GNN robustness without sacrificing clean\ndata performance. $\\beta$-GNN uses a weighted ensemble, combining any GNN with\na multi-layer perceptron. A learned dynamic weight, $\\beta$, modulates the\nGNN's contribution. This $\\beta$ not only weights GNN influence but also\nindicates data perturbation levels, enabling proactive mitigation. Experimental\nresults on diverse datasets show $\\beta$-GNN's superior adversarial accuracy\nand attack severity quantification. Crucially, $\\beta$-GNN avoids perturbation\nassumptions, preserving clean data structure and performance.\n","date":"2025-03-26"}
{"id":"2503.20631","title":"Robust Flower Cluster Matching Using The Unscented Transform","abstract":"  Monitoring flowers over time is essential for precision robotic pollination\nin agriculture. To accomplish this, a continuous spatial-temporal observation\nof plant growth can be done using stationary RGB-D cameras. However, image\nregistration becomes a serious challenge due to changes in the visual\nappearance of the plant caused by the pollination process and occlusions from\ngrowth and camera angles. Plants flower in a manner that produces distinct\nclusters on branches. This paper presents a method for matching flower clusters\nusing descriptors generated from RGB-D data and considers allowing for spatial\nuncertainty within the cluster. The proposed approach leverages the Unscented\nTransform to efficiently estimate plant descriptor uncertainty tolerances,\nenabling a robust image-registration process despite temporal changes. The\nUnscented Transform is used to handle the nonlinear transformations by\npropagating the uncertainty of flower positions to determine the variations in\nthe descriptor domain. A Monte Carlo simulation is used to validate the\nUnscented Transform results, confirming our method's effectiveness for flower\ncluster matching. Therefore, it can facilitate improved robotics pollination in\ndynamic environments.\n","date":"2025-03-26"}
{"id":"2503.20633","title":"Enhancing Multi-modal Models with Heterogeneous MoE Adapters for\n  Fine-tuning","abstract":"  Multi-modal models excel in cross-modal tasks but are computationally\nexpensive due to their billions of parameters. Parameter-efficient fine-tuning\n(PEFT) offers a solution by adding small trainable components while freezing\npre-trained parameters. However, existing methods primarily focus on uni-modal\nprocessing, overlooking the critical modal fusion needed for multi-modal tasks.\nTo fill this gap, we propose heterogeneous mixture of experts adapters that\nextend the traditional PEFT framework to support multi-modal expert\ncombinations and improve information interaction. Additionally, our approach\nmodifies the affine linear expert design to enable efficient modal fusion in a\nlow-rank space, achieving competitive performance with only 5-8\\% of the\nparameters fine-tuned. Experiments across eight downstream tasks, including\nvisual-audio and text-visual, demonstrate the superior performance of the\napproach.\n","date":"2025-03-26"}
{"id":"2503.20634","title":"Procedural Knowledge Ontology (PKO)","abstract":"  Processes, workflows and guidelines are core to ensure the correct\nfunctioning of industrial companies: for the successful operations of factory\nlines, machinery or services, often industry operators rely on their past\nexperience and know-how. The effect is that this Procedural Knowledge (PK)\nremains tacit and, as such, difficult to exploit efficiently and effectively.\nThis paper presents PKO, the Procedural Knowledge Ontology, which enables the\nexplicit modeling of procedures and their executions, by reusing and extending\nexisting ontologies. PKO is built on requirements collected from three\nheterogeneous industrial use cases and can be exploited by any AI and\ndata-driven tools that rely on a shared and interoperable representation to\nsupport the governance of PK throughout its life cycle. We describe its\nstructure and design methodology, and outline its relevance, quality, and\nimpact by discussing applications leveraging PKO for PK elicitation and\nexploitation.\n","date":"2025-03-26"}
{"id":"2503.20639","title":"PVLens: Enhancing Pharmacovigilance Through Automated Label Extraction","abstract":"  Reliable drug safety reference databases are essential for pharmacovigilance,\nyet existing resources like SIDER are outdated and static. We introduce PVLens,\nan automated system that extracts labeled safety information from FDA\nStructured Product Labels (SPLs) and maps terms to MedDRA. PVLens integrates\nautomation with expert oversight through a web-based review tool. In validation\nagainst 97 drug labels, PVLens achieved an F1 score of 0.882, with high recall\n(0.983) and moderate precision (0.799). By offering a scalable, more accurate\nand continuously updated alternative to SIDER, PVLens enhances real-time\npharamcovigilance with improved accuracy and contemporaneous insights.\n","date":"2025-03-26"}
{"id":"2503.20641","title":"Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging","abstract":"  The transition from System 1 to System 2 reasoning in large language models\n(LLMs) has marked significant advancements in handling complex tasks through\ndeliberate, iterative thinking. However, this progress often comes at the cost\nof efficiency, as models tend to overthink, generating redundant reasoning\nsteps without proportional improvements in output quality. Long-to-Short (L2S)\nreasoning has emerged as a promising solution to this challenge, aiming to\nbalance reasoning depth with practical efficiency. While existing approaches,\nsuch as supervised fine-tuning (SFT), reinforcement learning (RL), and prompt\nengineering, have shown potential, they are either computationally expensive or\nunstable. Model merging, on the other hand, offers a cost-effective and robust\nalternative by integrating the quick-thinking capabilities of System 1 models\nwith the methodical reasoning of System 2 models. In this work, we present a\ncomprehensive empirical study on model merging for L2S reasoning, exploring\ndiverse methodologies, including task-vector-based, SVD-based, and\nactivation-informed merging. Our experiments reveal that model merging can\nreduce average response length by up to 55% while preserving or even improving\nbaseline performance. We also identify a strong correlation between model scale\nand merging efficacy with extensive evaluations on 1.5B\/7B\/14B\/32B models.\nFurthermore, we investigate the merged model's ability to self-critique and\nself-correct, as well as its adaptive response length based on task complexity.\nOur findings highlight model merging as a highly efficient and effective\nparadigm for L2S reasoning, offering a practical solution to the overthinking\nproblem while maintaining the robustness of System 2 reasoning. This work can\nbe found on Github https:\/\/github.com\/hahahawu\/Long-to-Short-via-Model-Merging.\n","date":"2025-03-26"}
{"id":"2503.20642","title":"Representation Improvement in Latent Space for Search-Based Testing of\n  Autonomous Robotic Systems","abstract":"  Testing autonomous robotic systems, such as self-driving cars and unmanned\naerial vehicles, is challenging due to their interaction with highly\nunpredictable environments. A common practice is to first conduct\nsimulation-based testing, which, despite reducing real-world risks, remains\ntime-consuming and resource-intensive due to the vast space of possible test\nscenarios. A number of search-based approaches were proposed to generate test\nscenarios more efficiently. A key aspect of any search-based test generation\napproach is the choice of representation used during the search process.\nHowever, existing methods for improving test scenario representation remain\nlimited. We propose RILaST (Representation Improvement in Latent Space for\nSearch-Based Testing) approach, which enhances test representation by mapping\nit to the latent space of a variational autoencoder. We evaluate RILaST on two\nuse cases, including autonomous drone and autonomous lane-keeping assist\nsystem. The obtained results show that RILaST allows finding between 3 to 4.6\ntimes more failures than baseline approaches, achieving a high level of test\ndiversity.\n","date":"2025-03-26"}
{"id":"2503.20644","title":"MMGen: Unified Multi-modal Image Generation and Understanding in One Go","abstract":"  A unified diffusion framework for multi-modal generation and understanding\nhas the transformative potential to achieve seamless and controllable image\ndiffusion and other cross-modal tasks. In this paper, we introduce MMGen, a\nunified framework that integrates multiple generative tasks into a single\ndiffusion model. This includes: (1) multi-modal category-conditioned\ngeneration, where multi-modal outputs are generated simultaneously through a\nsingle inference process, given category information; (2) multi-modal visual\nunderstanding, which accurately predicts depth, surface normals, and\nsegmentation maps from RGB images; and (3) multi-modal conditioned generation,\nwhich produces corresponding RGB images based on specific modality conditions\nand other aligned modalities. Our approach develops a novel diffusion\ntransformer that flexibly supports multi-modal output, along with a simple\nmodality-decoupling strategy to unify various tasks. Extensive experiments and\napplications demonstrate the effectiveness and superiority of MMGen across\ndiverse tasks and conditions, highlighting its potential for applications that\nrequire simultaneous generation and understanding.\n","date":"2025-03-26"}
{"id":"2503.20648","title":"TN-Eval: Rubric and Evaluation Protocols for Measuring the Quality of\n  Behavioral Therapy Notes","abstract":"  Behavioral therapy notes are important for both legal compliance and patient\ncare. Unlike progress notes in physical health, quality standards for\nbehavioral therapy notes remain underdeveloped. To address this gap, we\ncollaborated with licensed therapists to design a comprehensive rubric for\nevaluating therapy notes across key dimensions: completeness, conciseness, and\nfaithfulness. Further, we extend a public dataset of behavioral health\nconversations with therapist-written notes and LLM-generated notes, and apply\nour evaluation framework to measure their quality. We find that: (1) A\nrubric-based manual evaluation protocol offers more reliable and interpretable\nresults than traditional Likert-scale annotations. (2) LLMs can mimic human\nevaluators in assessing completeness and conciseness but struggle with\nfaithfulness. (3) Therapist-written notes often lack completeness and\nconciseness, while LLM-generated notes contain hallucination. Surprisingly, in\na blind test, therapists prefer and judge LLM-generated notes to be superior to\ntherapist-written notes.\n","date":"2025-03-26"}
{"id":"2503.20652","title":"Imitating Radiological Scrolling: A Global-Local Attention Model for 3D\n  Chest CT Volumes Multi-Label Anomaly Classification","abstract":"  The rapid increase in the number of Computed Tomography (CT) scan\nexaminations has created an urgent need for automated tools, such as organ\nsegmentation, anomaly classification, and report generation, to assist\nradiologists with their growing workload. Multi-label classification of\nThree-Dimensional (3D) CT scans is a challenging task due to the volumetric\nnature of the data and the variety of anomalies to be detected. Existing deep\nlearning methods based on Convolutional Neural Networks (CNNs) struggle to\ncapture long-range dependencies effectively, while Vision Transformers require\nextensive pre-training, posing challenges for practical use. Additionally,\nthese existing methods do not explicitly model the radiologist's navigational\nbehavior while scrolling through CT scan slices, which requires both global\ncontext understanding and local detail awareness. In this study, we present\nCT-Scroll, a novel global-local attention model specifically designed to\nemulate the scrolling behavior of radiologists during the analysis of 3D CT\nscans. Our approach is evaluated on two public datasets, demonstrating its\nefficacy through comprehensive experiments and an ablation study that\nhighlights the contribution of each model component.\n","date":"2025-03-26"}
{"id":"2503.20653","title":"UWarp: A Whole Slide Image Registration Pipeline to Characterize\n  Scanner-Induced Local Domain Shift","abstract":"  Histopathology slide digitization introduces scanner-induced domain shift\nthat can significantly impact computational pathology models based on deep\nlearning methods. In the state-of-the-art, this shift is often characterized at\na broad scale (slide-level or dataset-level) but not patch-level, which limits\nour comprehension of the impact of localized tissue characteristics on the\naccuracy of the deep learning models. To address this challenge, we present a\ndomain shift analysis framework based on UWarp, a novel registration tool\ndesigned to accurately align histological slides scanned under varying\nconditions. UWarp employs a hierarchical registration approach, combining\nglobal affine transformations with fine-grained local corrections to achieve\nrobust tissue patch alignment. We evaluate UWarp using two private datasets,\nCypathLung and BosomShieldBreast, containing whole slide images scanned by\nmultiple devices. Our experiments demonstrate that UWarp outperforms existing\nopen-source registration methods, achieving a median target registration error\n(TRE) of less than 4 pixels (<1 micrometer at 40x magnification) while\nsignificantly reducing computational time. Additionally, we apply UWarp to\ncharacterize scanner-induced local domain shift in the predictions of\nBreast-NEOprAIdict, a deep learning model for breast cancer pathological\nresponse prediction. We find that prediction variability is strongly correlated\nwith tissue density on a given patch. Our findings highlight the importance of\nlocalized domain shift analysis and suggest that UWarp can serve as a valuable\ntool for improving model robustness and domain adaptation strategies in\ncomputational pathology.\n","date":"2025-03-26"}
{"id":"2503.20654","title":"AccidentSim: Generating Physically Realistic Vehicle Collision Videos\n  from Real-World Accident Reports","abstract":"  Collecting real-world vehicle accident videos for autonomous driving research\nis challenging due to their rarity and complexity. While existing driving video\ngeneration methods may produce visually realistic videos, they often fail to\ndeliver physically realistic simulations because they lack the capability to\ngenerate accurate post-collision trajectories. In this paper, we introduce\nAccidentSim, a novel framework that generates physically realistic vehicle\ncollision videos by extracting and utilizing the physical clues and contextual\ninformation available in real-world vehicle accident reports. Specifically,\nAccidentSim leverages a reliable physical simulator to replicate post-collision\nvehicle trajectories from the physical and contextual information in the\naccident reports and to build a vehicle collision trajectory dataset. This\ndataset is then used to fine-tune a language model, enabling it to respond to\nuser prompts and predict physically consistent post-collision trajectories\nacross various driving scenarios based on user descriptions. Finally, we employ\nNeural Radiance Fields (NeRF) to render high-quality backgrounds, merging them\nwith the foreground vehicles that exhibit physically realistic trajectories to\ngenerate vehicle collision videos. Experimental results demonstrate that the\nvideos produced by AccidentSim excel in both visual and physical authenticity.\n","date":"2025-03-26"}
{"id":"2503.20658","title":"Probabilistic Forecasting for Network Resource Analysis in Integrated\n  Terrestrial and Non-Terrestrial Networks","abstract":"  Efficient resource management is critical for Non-Terrestrial Networks (NTNs)\nto provide consistent, high-quality service in remote and under-served regions.\nWhile traditional single-point prediction methods, such as Long-Short Term\nMemory (LSTM), have been used in terrestrial networks, they often fall short in\nNTNs due to the complexity of satellite dynamics, signal latency and coverage\nvariability. Probabilistic forecasting, which quantifies the uncertainties of\nthe predictions, is a robust alternative. In this paper, we evaluate the\napplication of probabilistic forecasting techniques, in particular SFF, to NTN\nresource allocation scenarios. Our results show their effectiveness in\npredicting bandwidth and capacity requirements in different NTN segments of\nprobabilistic forecasting compared to single-point prediction techniques such\nas LSTM. The results show the potential of black probabilistic forecasting\nmodels to provide accurate and reliable predictions and to quantify their\nuncertainty, making them indispensable for optimizing NTN resource allocation.\nAt the end of the paper, we also present application scenarios and a\nstandardization roadmap for the use of probabilistic forecasting in integrated\nTerrestrial Network (TN)-NTN environments.\n","date":"2025-03-26"}
{"id":"2503.20660","title":"DR-PETS: Learning-Based Control With Planning in Adversarial\n  Environments","abstract":"  Ensuring robustness against epistemic, possibly adversarial, perturbations is\nessential for reliable real-world decision-making. While the Probabilistic\nEnsembles with Trajectory Sampling (PETS) algorithm inherently handles\nuncertainty via ensemble-based probabilistic models, it lacks guarantees\nagainst structured adversarial or worst-case uncertainty distributions. To\naddress this, we propose DR-PETS, a distributionally robust extension of PETS\nthat certifies robustness against adversarial perturbations. We formalize\nuncertainty via a p-Wasserstein ambiguity set, enabling worst-case-aware\nplanning through a min-max optimization framework. While PETS passively\naccounts for stochasticity, DR-PETS actively optimizes robustness via a\ntractable convex approximation integrated into PETS planning loop. Experiments\non pendulum stabilization and cart-pole balancing show that DR-PETS certifies\nrobustness against adversarial parameter perturbations, achieving consistent\nperformance in worst-case scenarios where PETS deteriorates.\n","date":"2025-03-26"}
{"id":"2503.20662","title":"AutoRad-Lung: A Radiomic-Guided Prompting Autoregressive Vision-Language\n  Model for Lung Nodule Malignancy Prediction","abstract":"  Lung cancer remains one of the leading causes of cancer-related mortality\nworldwide. A crucial challenge for early diagnosis is differentiating uncertain\ncases with similar visual characteristics and closely annotation scores. In\nclinical practice, radiologists rely on quantitative, hand-crafted Radiomic\nfeatures extracted from Computed Tomography (CT) images, while recent research\nhas primarily focused on deep learning solutions. More recently,\nVision-Language Models (VLMs), particularly Contrastive Language-Image\nPre-Training (CLIP)-based models, have gained attention for their ability to\nintegrate textual knowledge into lung cancer diagnosis. While CLIP-Lung models\nhave shown promising results, we identified the following potential\nlimitations: (a) dependence on radiologists' annotated attributes, which are\ninherently subjective and error-prone, (b) use of textual information only\nduring training, limiting direct applicability at inference, and (c)\nConvolutional-based vision encoder with randomly initialized weights, which\ndisregards prior knowledge. To address these limitations, we introduce\nAutoRad-Lung, which couples an autoregressively pre-trained VLM, with prompts\ngenerated from hand-crafted Radiomics. AutoRad-Lung uses the vision encoder of\nthe Large-Scale Autoregressive Image Model (AIMv2), pre-trained using a\nmulti-modal autoregressive objective. Given that lung tumors are typically\nsmall, irregularly shaped, and visually similar to healthy tissue, AutoRad-Lung\noffers significant advantages over its CLIP-based counterparts by capturing\npixel-level differences. Additionally, we introduce conditional context\noptimization, which dynamically generates context-specific prompts based on\ninput Radiomics, improving cross-modal alignment.\n","date":"2025-03-26"}
{"id":"2503.20663","title":"ARMO: Autoregressive Rigging for Multi-Category Objects","abstract":"  Recent advancements in large-scale generative models have significantly\nimproved the quality and diversity of 3D shape generation. However, most\nexisting methods focus primarily on generating static 3D models, overlooking\nthe potentially dynamic nature of certain shapes, such as humanoids, animals,\nand insects. To address this gap, we focus on rigging, a fundamental task in\nanimation that establishes skeletal structures and skinning for 3D models. In\nthis paper, we introduce OmniRig, the first large-scale rigging dataset,\ncomprising 79,499 meshes with detailed skeleton and skinning information.\nUnlike traditional benchmarks that rely on predefined standard poses (e.g.,\nA-pose, T-pose), our dataset embraces diverse shape categories, styles, and\nposes. Leveraging this rich dataset, we propose ARMO, a novel rigging framework\nthat utilizes an autoregressive model to predict both joint positions and\nconnectivity relationships in a unified manner. By treating the skeletal\nstructure as a complete graph and discretizing it into tokens, we encode the\njoints using an auto-encoder to obtain a latent embedding and an autoregressive\nmodel to predict the tokens. A mesh-conditioned latent diffusion model is used\nto predict the latent embedding for conditional skeleton generation. Our method\naddresses the limitations of regression-based approaches, which often suffer\nfrom error accumulation and suboptimal connectivity estimation. Through\nextensive experiments on the OmniRig dataset, our approach achieves\nstate-of-the-art performance in skeleton prediction, demonstrating improved\ngeneralization across diverse object categories. The code and dataset will be\nmade public for academic use upon acceptance.\n","date":"2025-03-26"}
{"id":"2503.20666","title":"TAMA: A Human-AI Collaborative Thematic Analysis Framework Using\n  Multi-Agent LLMs for Clinical Interviews","abstract":"  Thematic analysis (TA) is a widely used qualitative approach for uncovering\nlatent meanings in unstructured text data. TA provides valuable insights in\nhealthcare but is resource-intensive. Large Language Models (LLMs) have been\nintroduced to perform TA, yet their applications in healthcare remain\nunexplored. Here, we propose TAMA: A Human-AI Collaborative Thematic Analysis\nframework using Multi-Agent LLMs for clinical interviews. We leverage the\nscalability and coherence of multi-agent systems through structured\nconversations between agents and coordinate the expertise of cardiac experts in\nTA. Using interview transcripts from parents of children with Anomalous Aortic\nOrigin of a Coronary Artery (AAOCA), a rare congenital heart disease, we\ndemonstrate that TAMA outperforms existing LLM-assisted TA approaches,\nachieving higher thematic hit rate, coverage, and distinctiveness. TAMA\ndemonstrates strong potential for automated TA in clinical settings by\nleveraging multi-agent LLM systems with human-in-the-loop integration by\nenhancing quality while significantly reducing manual workload.\n","date":"2025-03-26"}
{"id":"2503.20672","title":"BizGen: Advancing Article-level Visual Text Rendering for Infographics\n  Generation","abstract":"  Recently, state-of-the-art text-to-image generation models, such as Flux and\nIdeogram 2.0, have made significant progress in sentence-level visual text\nrendering. In this paper, we focus on the more challenging scenarios of\narticle-level visual text rendering and address a novel task of generating\nhigh-quality business content, including infographics and slides, based on user\nprovided article-level descriptive prompts and ultra-dense layouts. The\nfundamental challenges are twofold: significantly longer context lengths and\nthe scarcity of high-quality business content data.\n  In contrast to most previous works that focus on a limited number of\nsub-regions and sentence-level prompts, ensuring precise adherence to\nultra-dense layouts with tens or even hundreds of sub-regions in business\ncontent is far more challenging. We make two key technical contributions: (i)\nthe construction of scalable, high-quality business content dataset, i.e.,\nInfographics-650K, equipped with ultra-dense layouts and prompts by\nimplementing a layer-wise retrieval-augmented infographic generation scheme;\nand (ii) a layout-guided cross attention scheme, which injects tens of\nregion-wise prompts into a set of cropped region latent space according to the\nultra-dense layouts, and refine each sub-regions flexibly during inference\nusing a layout conditional CFG.\n  We demonstrate the strong results of our system compared to previous SOTA\nsystems such as Flux and SD3 on our BizEval prompt set. Additionally, we\nconduct thorough ablation experiments to verify the effectiveness of each\ncomponent. We hope our constructed Infographics-650K and BizEval can encourage\nthe broader community to advance the progress of business content generation.\n","date":"2025-03-26"}
{"id":"2503.20673","title":"Mitigating Low-Level Visual Hallucinations Requires Self-Awareness:\n  Database, Model and Training Strategy","abstract":"  The rapid development of multimodal large language models has resulted in\nremarkable advancements in visual perception and understanding, consolidating\nseveral tasks into a single visual question-answering framework. However, these\nmodels are prone to hallucinations, which limit their reliability as artificial\nintelligence systems. While this issue is extensively researched in natural\nlanguage processing and image captioning, there remains a lack of investigation\nof hallucinations in Low-level Visual Perception and Understanding (HLPU),\nespecially in the context of image quality assessment tasks. We consider that\nthese hallucinations arise from an absence of clear self-awareness within the\nmodels. To address this issue, we first introduce the HLPU instruction\ndatabase, the first instruction database specifically focused on hallucinations\nin low-level vision tasks. This database contains approximately 200K\nquestion-answer pairs and comprises four subsets, each covering different types\nof instructions. Subsequently, we propose the Self-Awareness Failure\nElimination (SAFEQA) model, which utilizes image features, salient region\nfeatures and quality features to improve the perception and comprehension\nabilities of the model in low-level vision tasks. Furthermore, we propose the\nEnhancing Self-Awareness Preference Optimization (ESA-PO) framework to increase\nthe model's awareness of knowledge boundaries, thereby mitigating the incidence\nof hallucination. Finally, we conduct comprehensive experiments on low-level\nvision tasks, with the results demonstrating that our proposed method\nsignificantly enhances self-awareness of the model in these tasks and reduces\nhallucinations. Notably, our proposed method improves both accuracy and\nself-awareness of the proposed model and outperforms close-source models in\nterms of various evaluation metrics.\n","date":"2025-03-26"}
{"id":"2503.20676","title":"Inductive Link Prediction on N-ary Relational Facts via Semantic\n  Hypergraph Reasoning","abstract":"  N-ary relational facts represent semantic correlations among more than two\nentities. While recent studies have developed link prediction (LP) methods to\ninfer missing relations for knowledge graphs (KGs) containing n-ary relational\nfacts, they are generally limited to transductive settings. Fully inductive\nsettings, where predictions are made on previously unseen entities, remain a\nsignificant challenge. As existing methods are mainly entity embedding-based,\nthey struggle to capture entity-independent logical rules. To fill in this gap,\nwe propose an n-ary subgraph reasoning framework for fully inductive link\nprediction (ILP) on n-ary relational facts. This framework reasons over local\nsubgraphs and has a strong inductive inference ability to capture n-ary\npatterns. Specifically, we introduce a novel graph structure, the n-ary\nsemantic hypergraph, to facilitate subgraph extraction. Moreover, we develop a\nsubgraph aggregating network, NS-HART, to effectively mine complex semantic\ncorrelations within subgraphs. Theoretically, we provide a thorough analysis\nfrom the score function optimization perspective to shed light on NS-HART's\neffectiveness for n-ary ILP tasks. Empirically, we conduct extensive\nexperiments on a series of inductive benchmarks, including transfer reasoning\n(with and without entity features) and pairwise subgraph reasoning. The results\nhighlight the superiority of the n-ary subgraph reasoning framework and the\nexceptional inductive ability of NS-HART. The source code of this paper has\nbeen made publicly available at\nhttps:\/\/github.com\/yin-gz\/Nary-Inductive-SubGraph.\n","date":"2025-03-26"}
{"id":"2503.20678","title":"Asset price movement prediction using empirical mode decomposition and\n  Gaussian mixture models","abstract":"  We investigated the use of Empirical Mode Decomposition (EMD) combined with\nGaussian Mixture Models (GMM), feature engineering and machine learning\nalgorithms to optimize trading decisions. We used five, two, and one year\nsamples of hourly candle data for GameStop, Tesla, and XRP (Ripple) markets\nrespectively. Applying a 15 hour rolling window for each market, we collected\nseveral features based on a linear model and other classical features to\npredict the next hour's movement. Subsequently, a GMM filtering approach was\nused to identify clusters among these markets. For each cluster, we applied the\nEMD algorithm to extract high, medium, low and trend components from each\nfeature collected. A simple thresholding algorithm was applied to classify\nmarket movements based on the percentage change in each market's close price.\nWe then evaluated the performance of various machine learning models, including\nRandom Forests (RF) and XGBoost, in classifying market movements. A naive\nrandom selection of trading decisions was used as a benchmark, which assumed\nequal probabilities for each outcome, and a temporal cross-validation approach\nwas used to test models on 40%, 30%, and 20% of the dataset. Our results\nindicate that transforming selected features using EMD improves performance,\nparticularly for ensemble learning algorithms like Random Forest and XGBoost,\nas measured by accumulated profit. Finally, GMM filtering expanded the range of\nlearning algorithm and data source combinations that outperformed the top\npercentile of the random baseline.\n","date":"2025-03-26"}
{"id":"2503.20680","title":"Vision as LoRA","abstract":"  We introduce Vision as LoRA (VoRA), a novel paradigm for transforming an LLM\ninto an MLLM. Unlike prevalent MLLM architectures that rely on external vision\nmodules for vision encoding, VoRA internalizes visual capabilities by\nintegrating vision-specific LoRA layers directly into the LLM. This design\nallows the added parameters to be seamlessly merged into the LLM during\ninference, eliminating structural complexity and minimizing computational\noverhead. Moreover, inheriting the LLM's ability of handling flexible context,\nVoRA can process inputs at arbitrary resolutions.\n  To further strengthen VoRA's visual capabilities, we introduce a block-wise\ndistillation method that transfers visual priors from a pre-trained ViT into\nthe LoRA layers, effectively accelerating training by injecting visual\nknowledge. Additionally, we apply bi-directional attention masks to better\ncapture the context information of an image. We successfully demonstrate that\nwith additional pre-training data, VoRA can perform comparably with\nconventional encode-based MLLMs. All training data, codes, and model weights\nwill be released at https:\/\/github.com\/Hon-Wong\/VoRA.\n","date":"2025-03-26"}
{"id":"2503.20681","title":"Benchmarking Machine Learning Methods for Distributed Acoustic Sensing","abstract":"  Distributed acoustic sensing (DAS) technology represents an innovative\nfiber-optic-based sensing methodology that enables real-time acoustic signal\nmonitoring through the detection of minute perturbations along optical fibers.\nThis sensing approach offers compelling advantages, including extensive\nmeasurement ranges, exceptional spatial resolution, and an expansive dynamic\nmeasurement spectrum.\n  The integration of machine learning (ML) paradigms presents transformative\npotential for DAS technology, encompassing critical domains such as data\naugmentation, sophisticated preprocessing techniques, and advanced acoustic\nevent classification and recognition. By leveraging ML algorithms, DAS systems\ncan transition from traditional data processing methodologies to more automated\nand intelligent analytical frameworks.\n  The computational intelligence afforded by ML-enhanced DAS technologies\nfacilitates unprecedented monitoring capabilities across diverse critical\ninfrastructure sectors. Particularly noteworthy are the technology's\napplications in transportation infrastructure, energy management systems, and\nNatural disaster monitoring frameworks, where the precision of data acquisition\nand the reliability of intelligent decision-making mechanisms are paramount.\n  This research critically examines the comparative performance characteristics\nof classical machine learning methodologies and state-of-the-art deep learning\nmodels in the context of DAS data recognition and interpretation, offering\ncomprehensive insights into the evolving landscape of intelligent sensing\ntechnologies.\n","date":"2025-03-26"}
{"id":"2503.20682","title":"GLRD: Global-Local Collaborative Reason and Debate with PSL for 3D\n  Open-Vocabulary Detection","abstract":"  The task of LiDAR-based 3D Open-Vocabulary Detection (3D OVD) requires the\ndetector to learn to detect novel objects from point clouds without\noff-the-shelf training labels. Previous methods focus on the learning of\nobject-level representations and ignore the scene-level information, thus it is\nhard to distinguish objects with similar classes. In this work, we propose a\nGlobal-Local Collaborative Reason and Debate with PSL (GLRD) framework for the\n3D OVD task, considering both local object-level information and global\nscene-level information. Specifically, LLM is utilized to perform common sense\nreasoning based on object-level and scene-level information, where the\ndetection result is refined accordingly. To further boost the LLM's ability of\nprecise decisions, we also design a probabilistic soft logic solver (OV-PSL) to\nsearch for the optimal solution, and a debate scheme to confirm the class of\nconfusable objects. In addition, to alleviate the uneven distribution of\nclasses, a static balance scheme (SBC) and a dynamic balance scheme (DBC) are\ndesigned. In addition, to reduce the influence of noise in data and training,\nwe further propose Reflected Pseudo Labels Generation (RPLG) and\nBackground-Aware Object Localization (BAOL). Extensive experiments conducted on\nScanNet and SUN RGB-D demonstrate the superiority of GLRD, where absolute\nimprovements in mean average precision are $+2.82\\%$ on SUN RGB-D and $+3.72\\%$\non ScanNet in the partial open-vocabulary setting. In the full open-vocabulary\nsetting, the absolute improvements in mean average precision are $+4.03\\%$ on\nScanNet and $+14.11\\%$ on SUN RGB-D.\n","date":"2025-03-26"}
{"id":"2503.20685","title":"Flip Learning: Weakly Supervised Erase to Segment Nodules in Breast\n  Ultrasound","abstract":"  Accurate segmentation of nodules in both 2D breast ultrasound (BUS) and 3D\nautomated breast ultrasound (ABUS) is crucial for clinical diagnosis and\ntreatment planning. Therefore, developing an automated system for nodule\nsegmentation can enhance user independence and expedite clinical analysis.\nUnlike fully-supervised learning, weakly-supervised segmentation (WSS) can\nstreamline the laborious and intricate annotation process. However, current WSS\nmethods face challenges in achieving precise nodule segmentation, as many of\nthem depend on inaccurate activation maps or inefficient pseudo-mask generation\nalgorithms. In this study, we introduce a novel multi-agent reinforcement\nlearning-based WSS framework called Flip Learning, which relies solely on 2D\/3D\nboxes for accurate segmentation. Specifically, multiple agents are employed to\nerase the target from the box to facilitate classification tag flipping, with\nthe erased region serving as the predicted segmentation mask. The key\ncontributions of this research are as follows: (1) Adoption of a\nsuperpixel\/supervoxel-based approach to encode the standardized environment,\ncapturing boundary priors and expediting the learning process. (2) Introduction\nof three meticulously designed rewards, comprising a classification score\nreward and two intensity distribution rewards, to steer the agents' erasing\nprocess precisely, thereby avoiding both under- and over-segmentation. (3)\nImplementation of a progressive curriculum learning strategy to enable agents\nto interact with the environment in a progressively challenging manner, thereby\nenhancing learning efficiency. Extensively validated on the large in-house BUS\nand ABUS datasets, our Flip Learning method outperforms state-of-the-art WSS\nmethods and foundation models, and achieves comparable performance as\nfully-supervised learning algorithms.\n","date":"2025-03-26"}
{"id":"2503.20688","title":"Graph-Enhanced Model-Free Reinforcement Learning Agents for Efficient\n  Power Grid Topological Control","abstract":"  The increasing complexity of power grid management, driven by the emergence\nof prosumers and the demand for cleaner energy solutions, has needed innovative\napproaches to ensure stability and efficiency. This paper presents a novel\napproach within the model-free framework of reinforcement learning, aimed at\noptimizing power network operations without prior expert knowledge. We\nintroduce a masked topological action space, enabling agents to explore diverse\nstrategies for cost reduction while maintaining reliable service using the\nstate logic as a guide for choosing proper actions. Through extensive\nexperimentation across 20 different scenarios in a simulated 5-substation\nenvironment, we demonstrate that our approach achieves a consistent reduction\nin power losses, while ensuring grid stability against potential blackouts. The\nresults underscore the effectiveness of combining dynamic observation\nformalization with opponent-based training, showing a viable way for autonomous\nmanagement solutions in modern energy systems or even for building a\nfoundational model for this field.\n","date":"2025-03-26"}
{"id":"2503.20694","title":"A Low-complexity Structured Neural Network Approach to Intelligently\n  Realize Wideband Multi-beam Beamformers","abstract":"  True-time-delay (TTD) beamformers can produce wideband, squint-free beams in\nboth analog and digital signal domains, unlike frequency-dependent FFT beams.\nOur previous work showed that TTD beamformers can be efficiently realized using\nthe elements of delay Vandermonde matrix (DVM), answering the longstanding\nbeam-squint problem. Thus, building on our work on classical algorithms based\non DVM, we propose neural network (NN) architecture to realize wideband\nmulti-beam beamformers using structure-imposed weight matrices and submatrices.\nThe structure and sparsity of the weight matrices and submatrices are shown to\nreduce the space and computational complexities of the NN greatly. The proposed\nnetwork architecture has O(pLM logM) complexity compared to a conventional\nfully connected L-layers network with O(M2L) complexity, where M is the number\nof nodes in each layer of the network, p is the number of submatrices per\nlayer, and M >> p. We will show numerical simulations in the 24 GHz to 32 GHz\nrange to demonstrate the numerical feasibility of realizing wideband multi-beam\nbeamformers using the proposed neural architecture. We also show the complexity\nreduction of the proposed NN and compare that with fully connected NNs, to show\nthe efficiency of the proposed architecture without sacrificing accuracy. The\naccuracy of the proposed NN architecture was shown using the mean squared\nerror, which is based on an objective function of the weight matrices and\nbeamformed signals of antenna arrays, while also normalizing nodes. The\nproposed NN architecture shows a low-complexity NN realizing wideband\nmulti-beam beamformers in real-time for low-complexity intelligent systems.\n","date":"2025-03-26"}
{"id":"2503.20697","title":"Semi-supervised Node Importance Estimation with Informative Distribution\n  Modeling for Uncertainty Regularization","abstract":"  Node importance estimation, a classical problem in network analysis,\nunderpins various web applications. Previous methods either exploit intrinsic\ntopological characteristics, e.g., graph centrality, or leverage additional\ninformation, e.g., data heterogeneity, for node feature enhancement. However,\nthese methods follow the supervised learning setting, overlooking the fact that\nground-truth node-importance data are usually partially labeled in practice. In\nthis work, we propose the first semi-supervised node importance estimation\nframework, i.e., EASING, to improve learning quality for unlabeled data in\nheterogeneous graphs. Different from previous approaches, EASING explicitly\ncaptures uncertainty to reflect the confidence of model predictions. To jointly\nestimate the importance values and uncertainties, EASING incorporates DJE, a\ndeep encoder-decoder neural architecture. DJE introduces distribution modeling\nfor graph nodes, where the distribution representations derive both importance\nand uncertainty estimates. Additionally, DJE facilitates effective pseudo-label\ngeneration for the unlabeled data to enrich the training samples. Based on\nlabeled and pseudo-labeled data, EASING develops effective semi-supervised\nheteroscedastic learning with varying node uncertainty regularization.\nExtensive experiments on three real-world datasets highlight the superior\nperformance of EASING compared to competing methods. Codes are available via\nhttps:\/\/github.com\/yankai-chen\/EASING.\n","date":"2025-03-26"}
{"id":"2503.20698","title":"MMMORRF: Multimodal Multilingual Modularized Reciprocal Rank Fusion","abstract":"  Videos inherently contain multiple modalities, including visual events, text\noverlays, sounds, and speech, all of which are important for retrieval.\nHowever, state-of-the-art multimodal language models like VAST and LanguageBind\nare built on vision-language models (VLMs), and thus overly prioritize visual\nsignals. Retrieval benchmarks further reinforce this bias by focusing on visual\nqueries and neglecting other modalities. We create a search system MMMORRF that\nextracts text and features from both visual and audio modalities and integrates\nthem with a novel modality-aware weighted reciprocal rank fusion. MMMORRF is\nboth effective and efficient, demonstrating practicality in searching videos\nbased on users' information needs instead of visual descriptive queries. We\nevaluate MMMORRF on MultiVENT 2.0 and TVR, two multimodal benchmarks designed\nfor more targeted information needs, and find that it improves nDCG@20 by 81%\nover leading multimodal encoders and 37% over single-modality retrieval,\ndemonstrating the value of integrating diverse modalities.\n","date":"2025-03-26"}
{"id":"2503.20701","title":"UniEDU: A Unified Language and Vision Assistant for Education\n  Applications","abstract":"  Education materials for K-12 students often consist of multiple modalities,\nsuch as text and images, posing challenges for models to fully understand\nnuanced information in these materials. In this paper, we propose a unified\nlanguage and vision assistant UniEDU designed for various educational\napplications, including knowledge recommendation, knowledge tracing, time cost\nprediction, and user answer prediction, all within a single model. Unlike\nconventional task-specific models, UniEDU offers a unified solution that excels\nacross multiple educational tasks while maintaining strong generalization\ncapabilities. Its adaptability makes it well-suited for real-world deployment\nin diverse learning environments. Furthermore, UniEDU is optimized for\nindustry-scale deployment by significantly reducing computational\noverhead-achieving approximately a 300\\% increase in efficiency-while\nmaintaining competitive performance with minimal degradation compared to fully\nfine-tuned models. This work represents a significant step toward creating\nversatile AI systems tailored to the evolving demands of education.\n","date":"2025-03-26"}
{"id":"2503.20711","title":"Demand Estimation with Text and Image Data","abstract":"  We propose a demand estimation method that leverages unstructured text and\nimage data to infer substitution patterns. Using pre-trained deep learning\nmodels, we extract embeddings from product images and textual descriptions and\nincorporate them into a random coefficients logit model. This approach enables\nresearchers to estimate demand even when they lack data on product attributes\nor when consumers value hard-to-quantify attributes, such as visual design or\nfunctional benefits. Using data from a choice experiment, we show that our\napproach outperforms standard attribute-based models in counterfactual\npredictions of consumers' second choices. We also apply it across 40 product\ncategories on Amazon and consistently find that text and image data help\nidentify close substitutes within each category.\n","date":"2025-03-26"}
{"id":"2503.20715","title":"From Annotation to Adaptation: Metrics, Synthetic Data, and Aspect\n  Extraction for Aspect-Based Sentiment Analysis with Large Language Models","abstract":"  This study examines the performance of Large Language Models (LLMs) in\nAspect-Based Sentiment Analysis (ABSA), with a focus on implicit aspect\nextraction in a novel domain. Using a synthetic sports feedback dataset, we\nevaluate open-weight LLMs' ability to extract aspect-polarity pairs and propose\na metric to facilitate the evaluation of aspect extraction with generative\nmodels. Our findings highlight both the potential and limitations of LLMs in\nthe ABSA task.\n","date":"2025-03-26"}
{"id":"2503.20719","title":"Learning Straight Flows by Learning Curved Interpolants","abstract":"  Flow matching models typically use linear interpolants to define the\nforward\/noise addition process. This, together with the independent coupling\nbetween noise and target distributions, yields a vector field which is often\nnon-straight. Such curved fields lead to a slow inference\/generation process.\nIn this work, we propose to learn flexible (potentially curved) interpolants in\norder to learn straight vector fields to enable faster generation. We formulate\nthis via a multi-level optimization problem and propose an efficient\napproximate procedure to solve it. Our framework provides an end-to-end and\nsimulation-free optimization procedure, which can be leveraged to learn\nstraight line generative trajectories.\n","date":"2025-03-26"}
{"id":"2503.20722","title":"A weakly-supervised deep learning model for fast localisation and\n  delineation of the skeleton, internal organs, and spinal canal on Whole-Body\n  Diffusion-Weighted MRI (WB-DWI)","abstract":"  Background: Apparent Diffusion Coefficient (ADC) values and Total Diffusion\nVolume (TDV) from Whole-body diffusion-weighted MRI (WB-DWI) are recognized\ncancer imaging biomarkers. However, manual disease delineation for ADC and TDV\nmeasurements is unfeasible in clinical practice, demanding automation. As a\nfirst step, we propose an algorithm to generate fast and reproducible\nprobability maps of the skeleton, adjacent internal organs (liver, spleen,\nurinary bladder, and kidneys), and spinal canal. Methods: We developed an\nautomated deep-learning pipeline based on a 3D patch-based Residual U-Net\narchitecture that localizes and delineates these anatomical structures on\nWB-DWI. The algorithm was trained using \"soft-labels\" (non-binary\nsegmentations) derived from a computationally intensive atlas-based approach.\nFor training and validation, we employed a multi-center WB-DWI dataset\ncomprising 532 scans from patients with Advanced Prostate Cancer (APC) or\nMultiple Myeloma (MM), with testing on 45 patients. Results: Our\nweakly-supervised deep learning model achieved an average dice\nscore\/precision\/recall of 0.66\/0.6\/0.73 for skeletal delineations,\n0.8\/0.79\/0.81 for internal organs, and 0.85\/0.79\/0.94 for spinal canal, with\nsurface distances consistently below 3 mm. Relative median ADC and\nlog-transformed volume differences between automated and manual expert-defined\nfull-body delineations were below 10% and 4%, respectively. The computational\ntime for generating probability maps was 12x faster than the atlas-based\nregistration algorithm (25 s vs. 5 min). An experienced radiologist rated the\nmodel's accuracy \"good\" or \"excellent\" on test datasets. Conclusion: Our model\noffers fast and reproducible probability maps for localizing and delineating\nbody regions on WB-DWI, enabling ADC and TDV quantification, potentially\nsupporting clinicians in disease staging and treatment response assessment.\n","date":"2025-03-26"}
{"id":"2503.20724","title":"Dynamic Motion Blending for Versatile Motion Editing","abstract":"  Text-guided motion editing enables high-level semantic control and iterative\nmodifications beyond traditional keyframe animation. Existing methods rely on\nlimited pre-collected training triplets, which severely hinders their\nversatility in diverse editing scenarios. We introduce MotionCutMix, an online\ndata augmentation technique that dynamically generates training triplets by\nblending body part motions based on input text. While MotionCutMix effectively\nexpands the training distribution, the compositional nature introduces\nincreased randomness and potential body part incoordination. To model such a\nrich distribution, we present MotionReFit, an auto-regressive diffusion model\nwith a motion coordinator. The auto-regressive architecture facilitates\nlearning by decomposing long sequences, while the motion coordinator mitigates\nthe artifacts of motion composition. Our method handles both spatial and\ntemporal motion edits directly from high-level human instructions, without\nrelying on additional specifications or Large Language Models. Through\nextensive experiments, we show that MotionReFit achieves state-of-the-art\nperformance in text-guided motion editing.\n","date":"2025-03-26"}
{"id":"2503.20725","title":"Continual learning via probabilistic exchangeable sequence modelling","abstract":"  Continual learning (CL) refers to the ability to continuously learn and\naccumulate new knowledge while retaining useful information from past\nexperiences. Although numerous CL methods have been proposed in recent years,\nit is not straightforward to deploy them directly to real-world decision-making\nproblems due to their computational cost and lack of uncertainty\nquantification. To address these issues, we propose CL-BRUNO, a probabilistic,\nNeural Process-based CL model that performs scalable and tractable Bayesian\nupdate and prediction. Our proposed approach uses deep-generative models to\ncreate a unified probabilistic framework capable of handling different types of\nCL problems such as task- and class-incremental learning, allowing users to\nintegrate information across different CL scenarios using a single model. Our\napproach is able to prevent catastrophic forgetting through distributional and\nfunctional regularisation without the need of retaining any previously seen\nsamples, making it appealing to applications where data privacy or storage\ncapacity is of concern. Experiments show that CL-BRUNO outperforms existing\nmethods on both natural image and biomedical data sets, confirming its\neffectiveness in real-world applications.\n","date":"2025-03-26"}
{"id":"2503.20730","title":"Benchmarking and optimizing organism wide single-cell RNA alignment\n  methods","abstract":"  Many methods have been proposed for removing batch effects and aligning\nsingle-cell RNA (scRNA) datasets. However, performance is typically evaluated\nbased on multiple parameters and few datasets, creating challenges in assessing\nwhich method is best for aligning data at scale. Here, we introduce the\nK-Neighbors Intersection (KNI) score, a single score that both penalizes batch\neffects and measures accuracy at cross-dataset cell-type label prediction\nalongside carefully curated small (scMARK) and large (scREF) benchmarks\ncomprising 11 and 46 human scRNA studies respectively, where we have\nstandardized author labels. Using the KNI score, we evaluate and optimize\napproaches for cross-dataset single-cell RNA integration. We introduce Batch\nAdversarial single-cell Variational Inference (BA-scVI), as a new variant of\nscVI that uses adversarial training to penalize batch-effects in the encoder\nand decoder, and show this approach outperforms other methods. In the resulting\naligned space, we find that the granularity of cell-type groupings is\nconserved, supporting the notion that whole-organism cell-type maps can be\ncreated by a single model without loss of information.\n","date":"2025-03-26"}
{"id":"2503.20731","title":"RecTable: Fast Modeling Tabular Data with Rectified Flow","abstract":"  Score-based or diffusion models generate high-quality tabular data,\nsurpassing GAN-based and VAE-based models. However, these methods require\nsubstantial training time. In this paper, we introduce RecTable, which uses the\nrectified flow modeling, applied in such as text-to-image generation and\ntext-to-video generation. RecTable features a simple architecture consisting of\na few stacked gated linear unit blocks. Additionally, our training strategies\nare also simple, incorporating a mixed-type noise distribution and a\nlogit-normal timestep distribution. Our experiments demonstrate that RecTable\nachieves competitive performance compared to the several state-of-the-art\ndiffusion and score-based models while reducing the required training time. Our\ncode is available at https:\/\/github.com\/fmp453\/rectable.\n","date":"2025-03-26"}
{"id":"2503.20734","title":"SChanger: Change Detection from a Semantic Change and Spatial\n  Consistency Perspective","abstract":"  Change detection is a key task in Earth observation applications. Recently,\ndeep learning methods have demonstrated strong performance and widespread\napplication. However, change detection faces data scarcity due to the\nlabor-intensive process of accurately aligning remote sensing images of the\nsame area, which limits the performance of deep learning algorithms. To address\nthe data scarcity issue, we develop a fine-tuning strategy called the Semantic\nChange Network (SCN). We initially pre-train the model on single-temporal\nsupervised tasks to acquire prior knowledge of instance feature extraction. The\nmodel then employs a shared-weight Siamese architecture and extended Temporal\nFusion Module (TFM) to preserve this prior knowledge and is fine-tuned on\nchange detection tasks. The learned semantics for identifying all instances is\nchanged to focus on identifying only the changes. Meanwhile, we observe that\nthe locations of changes between the two images are spatially identical, a\nconcept we refer to as spatial consistency. We introduce this inductive bias\nthrough an attention map that is generated by large-kernel convolutions and\napplied to the features from both time points. This enhances the modeling of\nmulti-scale changes and helps capture underlying relationships in change\ndetection semantics. We develop a binary change detection model utilizing these\ntwo strategies. The model is validated against state-of-the-art methods on six\ndatasets, surpassing all benchmark methods and achieving F1 scores of 92.87%,\n86.43%, 68.95%, 97.62%, 84.58%, and 93.20% on the LEVIR-CD, LEVIR-CD+,\nS2Looking, CDD, SYSU-CD, and WHU-CD datasets, respectively.\n","date":"2025-03-26"}
{"id":"2503.20737","title":"Ontology-based Semantic Similarity Measures for Clustering Medical\n  Concepts in Drug Safety","abstract":"  Semantic similarity measures (SSMs) are widely used in biomedical research\nbut remain underutilized in pharmacovigilance. This study evaluates six\nontology-based SSMs for clustering MedDRA Preferred Terms (PTs) in drug safety\ndata. Using the Unified Medical Language System (UMLS), we assess each method's\nability to group PTs around medically meaningful centroids. A high-throughput\nframework was developed with a Java API and Python and R interfaces support\nlarge-scale similarity computations. Results show that while path-based methods\nperform moderately with F1 scores of 0.36 for WUPALMER and 0.28 for LCH,\nintrinsic information content (IC)-based measures, especially INTRINSIC-LIN and\nSOKAL, consistently yield better clustering accuracy (F1 score of 0.403).\nValidated against expert review and standard MedDRA queries (SMQs), our\nfindings highlight the promise of IC-based SSMs in enhancing pharmacovigilance\nworkflows by improving early signal detection and reducing manual review.\n","date":"2025-03-26"}
{"id":"2503.20739","title":"Emotion Detection and Music Recommendation System","abstract":"  As artificial intelligence becomes more and more ingrained in daily life, we\npresent a novel system that uses deep learning for music recommendation and\nemotion-based detection. Through the use of facial recognition and the DeepFace\nframework, our method analyses human emotions in real-time and then plays music\nthat reflects the mood it has discovered. The system uses a webcam to take\npictures, analyses the most common facial expression, and then pulls a playlist\nfrom local storage that corresponds to the mood it has detected. An engaging\nand customised experience is ensured by allowing users to manually change the\nsong selection via a dropdown menu or navigation buttons. By continuously\nlooping over the playlist, the technology guarantees continuity. The objective\nof our system is to improve emotional well-being through music therapy by\noffering a responsive and automated music-selection experience.\n","date":"2025-03-26"}
{"id":"2503.20742","title":"Quantum Neural Network Restatement of Markov Jump Process","abstract":"  Despite the many challenges in exploratory data analysis, artificial neural\nnetworks have motivated strong interests in scientists and researchers both in\ntheoretical as well as practical applications. Among sources of such popularity\nof artificial neural networks the ability of modeling non-linear dynamical\nsystems, generalization, and adaptation possibilities should be mentioned.\nDespite this, there is still significant debate about the role of various\nunderlying stochastic processes in stabilizing a unique structure for data\nlearning and prediction. One of such obstacles to the theoretical and numerical\nstudy of machine intelligent systems is the curse of dimensionality and the\nsampling from high-dimensional probability distributions. In general, this\ncurse prevents efficient description of states, providing a significant\ncomplexity barrier for the system to be efficiently described and studied. In\nthis strand of research, direct treatment and description of such abstract\nnotions of learning theory in terms of quantum information be one of the most\nfavorable candidates. Hence, the subject matter of these articles is devoted to\nproblems of design, adaptation and the formulations of computationally hard\nproblems in terms of quantum mechanical systems. In order to characterize the\nmicroscopic description of such dynamics in the language of inferential\nstatistics, covariance matrix estimation of d-dimensional Gaussian densities\nand Bayesian interpretation of eigenvalue problem for dynamical systems is\nassessed.\n","date":"2025-03-26"}
{"id":"2503.20744","title":"High Quality Diffusion Distillation on a Single GPU with Relative and\n  Absolute Position Matching","abstract":"  We introduce relative and absolute position matching (RAPM), a diffusion\ndistillation method resulting in high quality generation that can be trained\nefficiently on a single GPU. Recent diffusion distillation research has\nachieved excellent results for high-resolution text-to-image generation with\nmethods such as phased consistency models (PCM) and improved distribution\nmatching distillation (DMD2). However, these methods generally require many\nGPUs (e.g.~8-64) and significant batchsizes (e.g.~128-2048) during training,\nresulting in memory and compute requirements that are beyond the resources of\nsome researchers. RAPM provides effective single-GPU diffusion distillation\ntraining with a batchsize of 1. The new method attempts to mimic the sampling\ntrajectories of the teacher model by matching the relative and absolute\npositions. The design of relative positions is inspired by PCM. Two\ndiscriminators are introduced accordingly in RAPM, one for matching relative\npositions and the other for absolute positions. Experimental results on\nStableDiffusion (SD) V1.5 and SDXL indicate that RAPM with 4 timesteps produces\ncomparable FID scores as the best method with 1 timestep under very limited\ncomputational resources.\n","date":"2025-03-26"}
{"id":"2503.20745","title":"MATHGLANCE: Multimodal Large Language Models Do Not Know Where to Look\n  in Mathematical Diagrams","abstract":"  Diagrams serve as a fundamental form of visual language, representing complex\nconcepts and their inter-relationships through structured symbols, shapes, and\nspatial arrangements. Unlike natural images, their inherently symbolic and\nabstract nature poses significant challenges for Multimodal Large Language\nModels (MLLMs). However, current benchmarks conflate perceptual and reasoning\ntasks, making it difficult to assess whether MLLMs genuinely understand\nmathematical diagrams beyond superficial pattern recognition. To address this\ngap, we introduce MATHGLANCE, a benchmark specifically designed to isolate and\nevaluate mathematical perception in MLLMs. MATHGLANCE comprises 1.2K images and\n1.6K carefully curated questions spanning four perception tasks: shape\nclassification, object counting, relationship identification, and object\ngrounding, covering diverse domains including plane geometry, solid geometry,\nand graphical representations. Our evaluation of MLLMs reveals that their\nability to understand diagrams is notably limited, particularly in fine-grained\ngrounding tasks. In response, we construct GeoPeP, a perception-oriented\ndataset of 200K structured geometry image-text pairs explicitly annotated with\ngeometric primitives and precise spatial relationships. Training MLLM on GeoPeP\nleads to significant gains in perceptual accuracy, which in turn substantially\nimproves mathematical reasoning. Our benchmark and dataset establish critical\nstandards for evaluating and advancing multimodal mathematical understanding,\nproviding valuable resources and insights to foster future MLLM research.\n","date":"2025-03-26"}
{"id":"2503.20746","title":"PhysGen3D: Crafting a Miniature Interactive World from a Single Image","abstract":"  Envisioning physically plausible outcomes from a single image requires a deep\nunderstanding of the world's dynamics. To address this, we introduce PhysGen3D,\na novel framework that transforms a single image into an amodal,\ncamera-centric, interactive 3D scene. By combining advanced image-based\ngeometric and semantic understanding with physics-based simulation, PhysGen3D\ncreates an interactive 3D world from a static image, enabling us to \"imagine\"\nand simulate future scenarios based on user input. At its core, PhysGen3D\nestimates 3D shapes, poses, physical and lighting properties of objects,\nthereby capturing essential physical attributes that drive realistic object\ninteractions. This framework allows users to specify precise initial\nconditions, such as object speed or material properties, for enhanced control\nover generated video outcomes. We evaluate PhysGen3D's performance against\nclosed-source state-of-the-art (SOTA) image-to-video models, including Pika,\nKling, and Gen-3, showing PhysGen3D's capacity to generate videos with\nrealistic physics while offering greater flexibility and fine-grained control.\nOur results show that PhysGen3D achieves a unique balance of photorealism,\nphysical plausibility, and user-driven interactivity, opening new possibilities\nfor generating dynamic, physics-grounded video from an image.\n","date":"2025-03-26"}
{"id":"2503.20748","title":"UniSTD: Towards Unified Spatio-Temporal Learning across Diverse\n  Disciplines","abstract":"  Traditional spatiotemporal models generally rely on task-specific\narchitectures, which limit their generalizability and scalability across\ndiverse tasks due to domain-specific design requirements. In this paper, we\nintroduce \\textbf{UniSTD}, a unified Transformer-based framework for\nspatiotemporal modeling, which is inspired by advances in recent foundation\nmodels with the two-stage pretraining-then-adaption paradigm. Specifically, our\nwork demonstrates that task-agnostic pretraining on 2D vision and vision-text\ndatasets can build a generalizable model foundation for spatiotemporal\nlearning, followed by specialized joint training on spatiotemporal datasets to\nenhance task-specific adaptability. To improve the learning capabilities across\ndomains, our framework employs a rank-adaptive mixture-of-expert adaptation by\nusing fractional interpolation to relax the discrete variables so that can be\noptimized in the continuous space. Additionally, we introduce a temporal module\nto incorporate temporal dynamics explicitly. We evaluate our approach on a\nlarge-scale dataset covering 10 tasks across 4 disciplines, demonstrating that\na unified spatiotemporal model can achieve scalable, cross-task learning and\nsupport up to 10 tasks simultaneously within one model while reducing training\ncosts in multi-domain applications. Code will be available at\nhttps:\/\/github.com\/1hunters\/UniSTD.\n","date":"2025-03-26"}
{"id":"2503.20749","title":"Beyond Believability: Accurate Human Behavior Simulation with Fine-Tuned\n  LLMs","abstract":"  Recent research shows that LLMs can simulate ``believable'' human behaviors\nto power LLM agents via prompt-only methods. In this work, we focus on\nevaluating and improving LLM's objective ``accuracy'' rather than the\nsubjective ``believability'' in the web action generation task, leveraging a\nlarge-scale, real-world dataset collected from online shopping human actions.\nWe present the first comprehensive quantitative evaluation of state-of-the-art\nLLMs (e.g., DeepSeek-R1, Llama, and Claude) on the task of web action\ngeneration. Our results show that fine-tuning LLMs on real-world behavioral\ndata substantially improves their ability to generate actions compared to\nprompt-only methods. Furthermore, incorporating synthesized reasoning traces\ninto model training leads to additional performance gains, demonstrating the\nvalue of explicit rationale in behavior modeling. This work establishes a new\nbenchmark for evaluating LLMs in behavior simulation and offers actionable\ninsights into how real-world action data and reasoning augmentation can enhance\nthe fidelity of LLM agents.\n","date":"2025-03-26"}
{"id":"2503.20750","title":"Optimal Scaling Laws for Efficiency Gains in a Theoretical\n  Transformer-Augmented Sectional MoE Framework","abstract":"  This paper introduces a theoretical framework for a Transformer-augmented,\nsectional Mixture-of-Experts (MoE) architecture that aims to enhance\ncomputational efficiency while preserving model scalability. Unlike\nconventional MoE models, which route entire token embeddings to selected\nexperts, our approach portions the embedding dimension itself -- assigning\nsegments of each token's representation to dedicated experts. To combat losses\nin token representation, we utilize a pre-expert transformer layer to recompute\nattention across tokens and reduce the sequence length dimensionality. We\nextend our theory by deriving optimal scaling laws that a non-linear\nrelationship between the number of experts and factors such as model\ndimensionality, sequence length, and system overhead. These formulations yield\nclosed-form and numerically-solvable expressions for identifying the optimal\nexpert count under given architectural and hardware constraints. As a result,\nour framework not only provides theoretical bounds for computing efficiency\nwith varying frameworks but also guides practical design choices for scaling\nlarge models effectively. While empirical validation is pending, we present a\ncomprehensive experimental road map to evaluate the framework's efficiency,\nscalability, and practicality in future work.\n","date":"2025-03-26"}
{"id":"2503.20752","title":"Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning","abstract":"  Visual reasoning abilities play a crucial role in understanding complex\nmultimodal data, advancing both domain-specific applications and artificial\ngeneral intelligence (AGI). Existing methods improve VLM reasoning via\nChain-of-Thought (CoT) supervised fine-tuning, using meticulously annotated\ntraining data to enhance visual reasoning capabilities. However, this training\nparadigm may lead to overfitting and cognitive rigidity, restricting the\nmodel's ability to transfer visual reasoning skills across domains and limiting\nits real-world applicability. To address these limitations, we propose\nReason-RFT, a novel reinforcement fine-tuning framework that significantly\nenhances generalization capabilities in visual reasoning tasks. Reason-RFT\nintroduces a two-phase training framework for visual reasoning: (1) Supervised\nFine-Tuning (SFT) with curated Chain-of-Thought (CoT) data activates the\nreasoning potential of Vision-Language Models (VLMs), followed by (2) Group\nRelative Policy Optimization (GRPO)-based reinforcement learning that generates\nmultiple reasoning-response pairs, significantly enhancing generalization in\nvisual reasoning tasks. To evaluate Reason-RFT's visual reasoning capabilities,\nwe reconstructed a comprehensive dataset spanning visual counting, structure\nperception, and spatial transformation. Experimental results demonstrate\nReasoning-RFT's three key advantages: (1) Performance Enhancement: achieving\nstate-of-the-art results across multiple tasks, outperforming most mainstream\nopen-source and proprietary models; (2) Generalization Superiority:\nconsistently maintaining robust performance across diverse tasks and domains,\noutperforming alternative training paradigms; (3) Data Efficiency: excelling in\nfew-shot learning scenarios while surpassing full-dataset SFT baselines.\nProject website: https:\/\/tanhuajie.github.io\/ReasonRFT\n","date":"2025-03-26"}
{"id":"2503.20756","title":"ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving\n  Systems","abstract":"  Recent advancements in Large Multimodal Models (LMMs) have shown promise in\nAutonomous Driving Systems (ADS). However, their direct application to ADS is\nhindered by challenges such as misunderstanding of traffic knowledge, complex\nroad conditions, and diverse states of vehicle. To address these challenges, we\npropose the use of Knowledge Editing, which enables targeted modifications to a\nmodel's behavior without the need for full retraining. Meanwhile, we introduce\nADS-Edit, a multimodal knowledge editing dataset specifically designed for ADS,\nwhich includes various real-world scenarios, multiple data types, and\ncomprehensive evaluation metrics. We conduct comprehensive experiments and\nderive several interesting conclusions. We hope that our work will contribute\nto the further advancement of knowledge editing applications in the field of\nautonomous driving. Code and data are available in\nhttps:\/\/github.com\/zjunlp\/EasyEdit.\n","date":"2025-03-26"}
{"id":"2503.20757","title":"MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree\n  Search","abstract":"  We introduce MCTS-RAG, a novel approach that enhances the reasoning\ncapabilities of small language models on knowledge-intensive tasks by\nleveraging retrieval-augmented generation (RAG) to provide relevant context and\nMonte Carlo Tree Search (MCTS) to refine reasoning paths. MCTS-RAG dynamically\nintegrates retrieval and reasoning through an iterative decision-making\nprocess. Unlike standard RAG methods, which typically retrieve information\nindependently from reasoning and thus integrate knowledge suboptimally, or\nconventional MCTS reasoning, which depends solely on internal model knowledge\nwithout external facts, MCTS-RAG combines structured reasoning with adaptive\nretrieval. This integrated approach enhances decision-making, reduces\nhallucinations, and ensures improved factual accuracy and response consistency.\nThe experimental results on multiple reasoning and knowledge-intensive datasets\ndatasets (i.e., ComplexWebQA, GPQA, and FoolMeTwice) show that our method\nenables small-scale LMs to achieve performance comparable to frontier LLMs like\nGPT-4o by effectively scaling inference-time compute, setting a new standard\nfor reasoning in small-scale models.\n","date":"2025-03-26"}
{"id":"2503.20758","title":"MindfulLIME: A Stable Solution for Explanations of Machine Learning\n  Models with Enhanced Localization Precision -- A Medical Image Case Study","abstract":"  Ensuring transparency in machine learning decisions is critically important,\nespecially in sensitive sectors such as healthcare, finance, and justice.\nDespite this, some popular explainable algorithms, such as Local Interpretable\nModel-agnostic Explanations (LIME), often produce unstable explanations due to\nthe random generation of perturbed samples. Random perturbation introduces\nsmall changes or noise to modified instances of the original data, leading to\ninconsistent explanations. Even slight variations in the generated samples\nsignificantly affect the explanations provided by such models, undermining\ntrust and hindering the adoption of interpretable models. To address this\nchallenge, we propose MindfulLIME, a novel algorithm that intelligently\ngenerates purposive samples using a graph-based pruning algorithm and\nuncertainty sampling. MindfulLIME substantially improves the consistency of\nvisual explanations compared to random sampling approaches. Our experimental\nevaluation, conducted on a widely recognized chest X-ray dataset, confirms\nMindfulLIME's stability with a 100% success rate in delivering reliable\nexplanations under identical conditions. Additionally, MindfulLIME improves the\nlocalization precision of visual explanations by reducing the distance between\nthe generated explanations and the actual local annotations compared to LIME.\nWe also performed comprehensive experiments considering various segmentation\nalgorithms and sample numbers, focusing on stability, quality, and efficiency.\nThe results demonstrate the outstanding performance of MindfulLIME across\ndifferent segmentation settings, generating fewer high-quality samples within a\nreasonable processing time. By addressing the stability limitations of LIME in\nimage data, MindfulLIME enhances the trustworthiness and interpretability of\nmachine learning models in specific medical imaging applications, a critical\ndomain.\n","date":"2025-03-25"}
{"id":"2503.20762","title":"ASGO: Adaptive Structured Gradient Optimization","abstract":"  Training deep neural networks (DNNs) is a structured optimization problem,\nbecause the parameters are naturally represented by matrices and tensors rather\nthan simple vectors. Under this structural representation, it has been widely\nobserved that gradients are low-rank and Hessians are approximately block-wise\ndiagonal. These structured properties are crucial for designing efficient\noptimization algorithms but may not be utilized by current popular optimizers\nlike Adam. In this paper, we present a novel optimization algorithm ASGO that\ncapitalizes on these properties by employing a preconditioner that is\nadaptively updated using structured gradients. By fine-grained theoretical\nanalysis, ASGO is proven to achieve superior convergence rates compared to\nexisting structured gradient methods. Based on the convergence theory, we\nfurther demonstrate that ASGO can benefit from the low-rank and block-wise\ndiagonal properties. We also discuss practical modifications of ASGO and\nempirically verify the effectiveness of the algorithm on language model tasks.\n","date":"2025-03-26"}
{"id":"2503.20767","title":"Reliable algorithm selection for machine learning-guided design","abstract":"  Algorithms for machine learning-guided design, or design algorithms, use\nmachine learning-based predictions to propose novel objects with desired\nproperty values. Given a new design task -- for example, to design novel\nproteins with high binding affinity to a therapeutic target -- one must choose\na design algorithm and specify any hyperparameters and predictive and\/or\ngenerative models involved. How can these decisions be made such that the\nresulting designs are successful? This paper proposes a method for design\nalgorithm selection, which aims to select design algorithms that will produce a\ndistribution of design labels satisfying a user-specified success criterion --\nfor example, that at least ten percent of designs' labels exceed a threshold.\nIt does so by combining designs' predicted property values with held-out\nlabeled data to reliably forecast characteristics of the label distributions\nproduced by different design algorithms, building upon techniques from\nprediction-powered inference. The method is guaranteed with high probability to\nreturn design algorithms that yield successful label distributions (or the null\nset if none exist), if the density ratios between the design and labeled data\ndistributions are known. We demonstrate the method's effectiveness in simulated\nprotein and RNA design tasks, in settings with either known or estimated\ndensity ratios.\n","date":"2025-03-26"}
{"id":"2503.20768","title":"An Empirical Study of the Impact of Federated Learning on Machine\n  Learning Model Accuracy","abstract":"  Federated Learning (FL) enables distributed ML model training on private user\ndata at the global scale. Despite the potential of FL demonstrated in many\ndomains, an in-depth view of its impact on model accuracy remains unclear. In\nthis paper, we investigate, systematically, how this learning paradigm can\naffect the accuracy of state-of-the-art ML models for a variety of ML tasks. We\npresent an empirical study that involves various data types: text, image,\naudio, and video, and FL configuration knobs: data distribution, FL scale,\nclient sampling, and local and global computations. Our experiments are\nconducted in a unified FL framework to achieve high fidelity, with substantial\nhuman efforts and resource investments. Based on the results, we perform a\nquantitative analysis of the impact of FL, and highlight challenging scenarios\nwhere applying FL degrades the accuracy of the model drastically and identify\ncases where the impact is negligible. The detailed and extensive findings can\nbenefit practical deployments and future development of FL.\n","date":"2025-03-26"}
{"id":"2503.20771","title":"Disentangled Source-Free Personalization for Facial Expression\n  Recognition with Neutral Target Data","abstract":"  Facial Expression Recognition (FER) from videos is a crucial task in various\napplication areas, such as human-computer interaction and health monitoring\n(e.g., pain, depression, fatigue, and stress). Beyond the challenges of\nrecognizing subtle emotional or health states, the effectiveness of deep FER\nmodels is often hindered by the considerable variability of expressions among\nsubjects. Source-free domain adaptation (SFDA) methods are employed to adapt a\npre-trained source model using only unlabeled target domain data, thereby\navoiding data privacy and storage issues. Typically, SFDA methods adapt to a\ntarget domain dataset corresponding to an entire population and assume it\nincludes data from all recognition classes. However, collecting such\ncomprehensive target data can be difficult or even impossible for FER in\nhealthcare applications. In many real-world scenarios, it may be feasible to\ncollect a short neutral control video (displaying only neutral expressions) for\ntarget subjects before deployment. These videos can be used to adapt a model to\nbetter handle the variability of expressions among subjects. This paper\nintroduces the Disentangled Source-Free Domain Adaptation (DSFDA) method to\naddress the SFDA challenge posed by missing target expression data. DSFDA\nleverages data from a neutral target control video for end-to-end generation\nand adaptation of target data with missing non-neutral data. Our method learns\nto disentangle features related to expressions and identity while generating\nthe missing non-neutral target data, thereby enhancing model accuracy.\nAdditionally, our self-supervision strategy improves model adaptation by\nreconstructing target images that maintain the same identity and source\nexpression. Experimental results on the challenging BioVid and UNBC-McMaster\npain datasets indicate that our DSFDA approach can outperform state-of-the-art\nadaptation method.\n","date":"2025-03-26"}
{"id":"2503.20776","title":"Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile\n  Gaussian Feature Fields","abstract":"  Recent advancements in 2D and multimodal models have achieved remarkable\nsuccess by leveraging large-scale training on extensive datasets. However,\nextending these achievements to enable free-form interactions and high-level\nsemantic operations with complex 3D\/4D scenes remains challenging. This\ndifficulty stems from the limited availability of large-scale, annotated 3D\/4D\nor multi-view datasets, which are crucial for generalizable vision and language\ntasks such as open-vocabulary and prompt-based segmentation, language-guided\nediting, and visual question answering (VQA). In this paper, we introduce\nFeature4X, a universal framework designed to extend any functionality from 2D\nvision foundation model into the 4D realm, using only monocular video input,\nwhich is widely available from user-generated content. The \"X\" in Feature4X\nrepresents its versatility, enabling any task through adaptable,\nmodel-conditioned 4D feature field distillation. At the core of our framework\nis a dynamic optimization strategy that unifies multiple model capabilities\ninto a single representation. Additionally, to the best of our knowledge,\nFeature4X is the first method to distill and lift the features of video\nfoundation models (e.g., SAM2, InternVideo2) into an explicit 4D feature field\nusing Gaussian Splatting. Our experiments showcase novel view segment anything,\ngeometric and appearance scene editing, and free-form VQA across all time\nsteps, empowered by LLMs in feedback loops. These advancements broaden the\nscope of agentic AI applications by providing a foundation for scalable,\ncontextually and spatiotemporally aware systems capable of immersive dynamic 4D\nscene interaction.\n","date":"2025-03-26"}
{"id":"2503.20781","title":"BASKET: A Large-Scale Video Dataset for Fine-Grained Skill Estimation","abstract":"  We present BASKET, a large-scale basketball video dataset for fine-grained\nskill estimation. BASKET contains 4,477 hours of video capturing 32,232\nbasketball players from all over the world. Compared to prior skill estimation\ndatasets, our dataset includes a massive number of skilled participants with\nunprecedented diversity in terms of gender, age, skill level, geographical\nlocation, etc. BASKET includes 20 fine-grained basketball skills, challenging\nmodern video recognition models to capture the intricate nuances of player\nskill through in-depth video analysis. Given a long highlight video (8-10\nminutes) of a particular player, the model needs to predict the skill level\n(e.g., excellent, good, average, fair, poor) for each of the 20 basketball\nskills. Our empirical analysis reveals that the current state-of-the-art video\nmodels struggle with this task, significantly lagging behind the human\nbaseline. We believe that BASKET could be a useful resource for developing new\nvideo models with advanced long-range, fine-grained recognition capabilities.\nIn addition, we hope that our dataset will be useful for domain-specific\napplications such as fair basketball scouting, personalized player development,\nand many others. Dataset and code are available at\nhttps:\/\/github.com\/yulupan00\/BASKET.\n","date":"2025-03-26"}
{"id":"2503.20782","title":"Zero-Shot Audio-Visual Editing via Cross-Modal Delta Denoising","abstract":"  In this paper, we introduce zero-shot audio-video editing, a novel task that\nrequires transforming original audio-visual content to align with a specified\ntextual prompt without additional model training. To evaluate this task, we\ncurate a benchmark dataset, AvED-Bench, designed explicitly for zero-shot\naudio-video editing. AvED-Bench includes 110 videos, each with a 10-second\nduration, spanning 11 categories from VGGSound. It offers diverse prompts and\nscenarios that require precise alignment between auditory and visual elements,\nenabling robust evaluation. We identify limitations in existing zero-shot audio\nand video editing methods, particularly in synchronization and coherence\nbetween modalities, which often result in inconsistent outcomes. To address\nthese challenges, we propose AvED, a zero-shot cross-modal delta denoising\nframework that leverages audio-video interactions to achieve synchronized and\ncoherent edits. AvED demonstrates superior results on both AvED-Bench and the\nrecent OAVE dataset to validate its generalization capabilities. Results are\navailable at https:\/\/genjib.github.io\/project_page\/AVED\/index.html\n","date":"2025-03-26"}
{"id":"2503.20783","title":"Understanding R1-Zero-Like Training: A Critical Perspective","abstract":"  DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can\ndirectly enhance the reasoning capabilities of LLMs without supervised\nfine-tuning. In this work, we critically examine R1-Zero-like training by\nanalyzing its two core components: base models and RL. We investigate a wide\nrange of base models, including DeepSeek-V3-Base, to understand how pretraining\ncharacteristics influence RL performance. Our analysis reveals that\nDeepSeek-V3-Base already exhibit ''Aha moment'', while Qwen2.5 base models\ndemonstrate strong reasoning capabilities even without prompt templates,\nsuggesting potential pretraining biases. Additionally, we identify an\noptimization bias in Group Relative Policy Optimization (GRPO), which\nartificially increases response length (especially for incorrect outputs)\nduring training. To address this, we introduce Dr. GRPO, an unbiased\noptimization method that improves token efficiency while maintaining reasoning\nperformance. Leveraging these insights, we present a minimalist R1-Zero recipe\nthat achieves 43.3% accuracy on AIME 2024 with a 7B base model, establishing a\nnew state-of-the-art. Our code is available at\nhttps:\/\/github.com\/sail-sg\/understand-r1-zero.\n","date":"2025-03-26"}
{"id":"2503.20784","title":"FB-4D: Spatial-Temporal Coherent Dynamic 3D Content Generation with\n  Feature Banks","abstract":"  With the rapid advancements in diffusion models and 3D generation techniques,\ndynamic 3D content generation has become a crucial research area. However,\nachieving high-fidelity 4D (dynamic 3D) generation with strong spatial-temporal\nconsistency remains a challenging task. Inspired by recent findings that\npretrained diffusion features capture rich correspondences, we propose FB-4D, a\nnovel 4D generation framework that integrates a Feature Bank mechanism to\nenhance both spatial and temporal consistency in generated frames. In FB-4D, we\nstore features extracted from previous frames and fuse them into the process of\ngenerating subsequent frames, ensuring consistent characteristics across both\ntime and multiple views. To ensure a compact representation, the Feature Bank\nis updated by a proposed dynamic merging mechanism. Leveraging this Feature\nBank, we demonstrate for the first time that generating additional reference\nsequences through multiple autoregressive iterations can continuously improve\ngeneration performance. Experimental results show that FB-4D significantly\noutperforms existing methods in terms of rendering quality, spatial-temporal\nconsistency, and robustness. It surpasses all multi-view generation tuning-free\napproaches by a large margin and achieves performance on par with\ntraining-based methods.\n","date":"2025-03-26"}
{"id":"2503.20785","title":"Free4D: Tuning-free 4D Scene Generation with Spatial-Temporal\n  Consistency","abstract":"  We present Free4D, a novel tuning-free framework for 4D scene generation from\na single image. Existing methods either focus on object-level generation,\nmaking scene-level generation infeasible, or rely on large-scale multi-view\nvideo datasets for expensive training, with limited generalization ability due\nto the scarcity of 4D scene data. In contrast, our key insight is to distill\npre-trained foundation models for consistent 4D scene representation, which\noffers promising advantages such as efficiency and generalizability. 1) To\nachieve this, we first animate the input image using image-to-video diffusion\nmodels followed by 4D geometric structure initialization. 2) To turn this\ncoarse structure into spatial-temporal consistent multiview videos, we design\nan adaptive guidance mechanism with a point-guided denoising strategy for\nspatial consistency and a novel latent replacement strategy for temporal\ncoherence. 3) To lift these generated observations into consistent 4D\nrepresentation, we propose a modulation-based refinement to mitigate\ninconsistencies while fully leveraging the generated information. The resulting\n4D representation enables real-time, controllable rendering, marking a\nsignificant advancement in single-image-based 4D scene generation.\n","date":"2025-03-26"}
{"id":"2503.20786","title":"Mobile-MMLU: A Mobile Intelligence Language Understanding Benchmark","abstract":"  Rapid advancements in large language models (LLMs) have increased interest in\ndeploying them on mobile devices for on-device AI applications. Mobile users\ninteract differently with LLMs compared to desktop users, creating unique\nexpectations and data biases. Current benchmark datasets primarily target at\nserver and desktop environments, and there is a notable lack of extensive\ndatasets specifically designed for mobile contexts. Additionally, mobile\ndevices face strict limitations in storage and computing resources,\nconstraining model size and capabilities, thus requiring optimized efficiency\nand prioritized knowledge. To address these challenges, we introduce\nMobile-MMLU, a large-scale benchmark dataset tailored for mobile intelligence.\nIt consists of 16,186 questions across 80 mobile-related fields, designed to\nevaluate LLM performance in realistic mobile scenarios. A challenging subset,\nMobile-MMLU-Pro, provides advanced evaluation similar in size to MMLU-Pro but\nsignificantly more difficult than our standard full set. Both benchmarks use\nmultiple-choice, order-invariant questions focused on practical mobile\ninteractions, such as recipe suggestions, travel planning, and essential daily\ntasks. The dataset emphasizes critical mobile-specific metrics like inference\nlatency, energy consumption, memory usage, and response quality, offering\ncomprehensive insights into model performance under mobile constraints.\nMoreover, it prioritizes privacy and adaptability, assessing models' ability to\nperform on-device processing, maintain user privacy, and adapt to personalized\nusage patterns. Mobile-MMLU family offers a standardized framework for\ndeveloping and comparing mobile-optimized LLMs, enabling advancements in\nproductivity and decision-making within mobile computing environments. Our code\nand data are available at: https:\/\/github.com\/VILA-Lab\/Mobile-MMLU.\n","date":"2025-03-26"}
{"id":"2503.20797","title":"\"Whose Side Are You On?\" Estimating Ideology of Political and News\n  Content Using Large Language Models and Few-shot Demonstration Selection","abstract":"  The rapid growth of social media platforms has led to concerns about\nradicalization, filter bubbles, and content bias. Existing approaches to\nclassifying ideology are limited in that they require extensive human effort,\nthe labeling of large datasets, and are not able to adapt to evolving\nideological contexts. This paper explores the potential of Large Language\nModels (LLMs) for classifying the political ideology of online content in the\ncontext of the two-party US political spectrum through in-context learning\n(ICL). Our extensive experiments involving demonstration selection in\nlabel-balanced fashion, conducted on three datasets comprising news articles\nand YouTube videos, reveal that our approach significantly outperforms\nzero-shot and traditional supervised methods. Additionally, we evaluate the\ninfluence of metadata (e.g., content source and descriptions) on ideological\nclassification and discuss its implications. Finally, we show how providing the\nsource for political and non-political content influences the LLM's\nclassification.\n","date":"2025-03-23"}
{"id":"2503.20798","title":"Payload-Aware Intrusion Detection with CMAE and Large Language Models","abstract":"  Intrusion Detection Systems (IDS) are crucial for identifying malicious\ntraffic, yet traditional signature-based methods struggle with zero-day attacks\nand high false positive rates. AI-driven packet-capture analysis offers a\npromising alternative. However, existing approaches rely heavily on flow-based\nor statistical features, limiting their ability to detect fine-grained attack\npatterns. This study proposes Xavier-CMAE, an enhanced Convolutional Multi-Head\nAttention Ensemble (CMAE) model that improves detection accuracy while reducing\ncomputational overhead. By replacing Word2Vec embeddings with a Hex2Int\ntokenizer and Xavier initialization, Xavier-CMAE eliminates pre-training,\naccelerates training, and achieves 99.971% accuracy with a 0.018% false\npositive rate, outperforming Word2Vec-based methods. Additionally, we introduce\nLLM-CMAE, which integrates pre-trained Large Language Model (LLM) tokenizers\ninto CMAE. While LLMs enhance feature extraction, their computational cost\nhinders real-time detection. LLM-CMAE balances efficiency and performance,\nreaching 99.969% accuracy with a 0.019% false positive rate. This work advances\nAI-powered IDS by (1) introducing a payload-based detection framework, (2)\nenhancing efficiency with Xavier-CMAE, and (3) integrating LLM tokenizers for\nimproved real-time detection.\n","date":"2025-03-23"}
{"id":"2503.20800","title":"Evidencing Unauthorized Training Data from AI Generated Content using\n  Information Isotopes","abstract":"  In light of scaling laws, many AI institutions are intensifying efforts to\nconstruct advanced AIs on extensive collections of high-quality human data.\nHowever, in a rush to stay competitive, some institutions may inadvertently or\neven deliberately include unauthorized data (like privacy- or intellectual\nproperty-sensitive content) for AI training, which infringes on the rights of\ndata owners. Compounding this issue, these advanced AI services are typically\nbuilt on opaque cloud platforms, which restricts access to internal information\nduring AI training and inference, leaving only the generated outputs available\nfor forensics. Thus, despite the introduction of legal frameworks by various\ncountries to safeguard data rights, uncovering evidence of data misuse in\nmodern opaque AI applications remains a significant challenge. In this paper,\ninspired by the ability of isotopes to trace elements within chemical\nreactions, we introduce the concept of information isotopes and elucidate their\nproperties in tracing training data within opaque AI systems. Furthermore, we\npropose an information isotope tracing method designed to identify and provide\nevidence of unauthorized data usage by detecting the presence of target\ninformation isotopes in AI generations. We conduct experiments on ten AI models\n(including GPT-4o, Claude-3.5, and DeepSeek) and four benchmark datasets in\ncritical domains (medical data, copyrighted books, and news). Results show that\nour method can distinguish training datasets from non-training datasets with\n99\\% accuracy and significant evidence (p-value$<0.001$) by examining a data\nentry equivalent in length to a research paper. The findings show the potential\nof our work as an inclusive tool for empowering individuals, including those\nwithout expertise in AI, to safeguard their data rights in the rapidly evolving\nera of AI advancements and applications.\n","date":"2025-03-24"}
{"id":"2503.20801","title":"SE-GNN: Seed Expanded-Aware Graph Neural Network with Iterative\n  Optimization for Semi-supervised Entity Alignment","abstract":"  Entity alignment aims to use pre-aligned seed pairs to find other equivalent\nentities from different knowledge graphs (KGs) and is widely used in graph\nfusion-related fields. However, as the scale of KGs increases, manually\nannotating pre-aligned seed pairs becomes difficult. Existing research utilizes\nentity embeddings obtained by aggregating single structural information to\nidentify potential seed pairs, thus reducing the reliance on pre-aligned seed\npairs. However, due to the structural heterogeneity of KGs, the quality of\npotential seed pairs obtained using only a single structural information is not\nideal. In addition, although existing research improves the quality of\npotential seed pairs through semi-supervised iteration, they underestimate the\nimpact of embedding distortion produced by noisy seed pairs on the alignment\neffect. In order to solve the above problems, we propose a seed expanded-aware\ngraph neural network with iterative optimization for semi-supervised entity\nalignment, named SE-GNN. First, we utilize the semantic attributes and\nstructural features of entities, combined with a conditional filtering\nmechanism, to obtain high-quality initial potential seed pairs. Next, we\ndesigned a local and global awareness mechanism. It introduces initial\npotential seed pairs and combines local and global information to obtain a more\ncomprehensive entity embedding representation, which alleviates the impact of\nKGs structural heterogeneity and lays the foundation for the optimization of\ninitial potential seed pairs. Then, we designed the threshold nearest neighbor\nembedding correction strategy. It combines the similarity threshold and the\nbidirectional nearest neighbor method as a filtering mechanism to select\niterative potential seed pairs and also uses an embedding correction strategy\nto eliminate the embedding distortion.\n","date":"2025-03-24"}
{"id":"2503.20802","title":"CEFW: A Comprehensive Evaluation Framework for Watermark in Large\n  Language Models","abstract":"  Text watermarking provides an effective solution for identifying synthetic\ntext generated by large language models. However, existing techniques often\nfocus on satisfying specific criteria while ignoring other key aspects, lacking\na unified evaluation. To fill this gap, we propose the Comprehensive Evaluation\nFramework for Watermark (CEFW), a unified framework that comprehensively\nevaluates watermarking methods across five key dimensions: ease of detection,\nfidelity of text quality, minimal embedding cost, robustness to adversarial\nattacks, and imperceptibility to prevent imitation or forgery. By assessing\nwatermarks according to all these key criteria, CEFW offers a thorough\nevaluation of their practicality and effectiveness. Moreover, we introduce a\nsimple and effective watermarking method called Balanced Watermark (BW), which\nguarantees robustness and imperceptibility through balancing the way watermark\ninformation is added. Extensive experiments show that BW outperforms existing\nmethods in overall performance across all evaluation dimensions. We release our\ncode to the community for future research.\nhttps:\/\/github.com\/DrankXs\/BalancedWatermark.\n","date":"2025-03-24"}
{"id":"2503.20803","title":"Leveraging VAE-Derived Latent Spaces for Enhanced Malware Detection with\n  Machine Learning Classifiers","abstract":"  This paper assesses the performance of five machine learning classifiers:\nDecision Tree, Naive Bayes, LightGBM, Logistic Regression, and Random Forest\nusing latent representations learned by a Variational Autoencoder from malware\ndatasets. Results from the experiments conducted on different training-test\nsplits with different random seeds reveal that all the models perform well in\ndetecting malware with ensemble methods (LightGBM and Random Forest) performing\nslightly better than the rest. In addition, the use of latent features reduces\nthe computational cost of the model and the need for extensive hyperparameter\ntuning for improved efficiency of the model for deployment. Statistical tests\nshow that these improvements are significant, and thus, the practical relevance\nof integrating latent space representation with traditional classifiers for\neffective malware detection in cybersecurity is established.\n","date":"2025-03-24"}
{"id":"2503.20804","title":"AED: Automatic Discovery of Effective and Diverse Vulnerabilities for\n  Autonomous Driving Policy with Large Language Models","abstract":"  Assessing the safety of autonomous driving policy is of great importance, and\nreinforcement learning (RL) has emerged as a powerful method for discovering\ncritical vulnerabilities in driving policies. However, existing RL-based\napproaches often struggle to identify vulnerabilities that are both\neffective-meaning the autonomous vehicle is genuinely responsible for the\naccidents-and diverse-meaning they span various failure types. To address these\nchallenges, we propose AED, a framework that uses large language models (LLMs)\nto automatically discover effective and diverse vulnerabilities in autonomous\ndriving policies. We first utilize an LLM to automatically design reward\nfunctions for RL training. Then we let the LLM consider a diverse set of\naccident types and train adversarial policies for different accident types in\nparallel. Finally, we use preference-based learning to filter ineffective\naccidents and enhance the effectiveness of each vulnerability. Experiments\nacross multiple simulated traffic scenarios and tested policies show that AED\nuncovers a broader range of vulnerabilities and achieves higher attack success\nrates compared with expert-designed rewards, thereby reducing the need for\nmanual reward engineering and improving the diversity and effectiveness of\nvulnerability discovery.\n","date":"2025-03-24"}
{"id":"2503.20807","title":"Fundamental Safety-Capability Trade-offs in Fine-tuning Large Language\n  Models","abstract":"  Fine-tuning Large Language Models (LLMs) on some task-specific datasets has\nbeen a primary use of LLMs. However, it has been empirically observed that this\napproach to enhancing capability inevitably compromises safety, a phenomenon\nalso known as the safety-capability trade-off in LLM fine-tuning. This paper\npresents a theoretical framework for understanding the interplay between safety\nand capability in two primary safety-aware LLM fine-tuning strategies,\nproviding new insights into the effects of data similarity, context overlap,\nand alignment loss landscape. Our theoretical results characterize the\nfundamental limits of the safety-capability trade-off in LLM fine-tuning, which\nare also validated by numerical experiments.\n","date":"2025-03-24"}
{"id":"2503.20808","title":"Dynamic Allocation Hypernetwork with Adaptive Model Recalibration for\n  Federated Continual Learning","abstract":"  Federated continual learning (FCL) offers an emerging pattern to facilitate\nthe applicability of federated learning (FL) in real-world scenarios, where\ntasks evolve dynamically and asynchronously across clients, especially in\nmedical scenario. Existing server-side FCL methods in nature domain construct a\ncontinually learnable server model by client aggregation on all-involved tasks.\nHowever, they are challenged by: (1) Catastrophic forgetting for previously\nlearned tasks, leading to error accumulation in server model, making it\ndifficult to sustain comprehensive knowledge across all tasks. (2) Biased\noptimization due to asynchronous tasks handled across different clients,\nleading to the collision of optimization targets of different clients at the\nsame time steps. In this work, we take the first step to propose a novel\nserver-side FCL pattern in medical domain, Dynamic Allocation Hypernetwork with\nadaptive model recalibration (FedDAH). It is to facilitate collaborative\nlearning under the distinct and dynamic task streams across clients. To\nalleviate the catastrophic forgetting, we propose a dynamic allocation\nhypernetwork (DAHyper) where a continually updated hypernetwork is designed to\nmanage the mapping between task identities and their associated model\nparameters, enabling the dynamic allocation of the model across clients. For\nthe biased optimization, we introduce a novel adaptive model recalibration\n(AMR) to incorporate the candidate changes of historical models into current\nserver updates, and assign weights to identical tasks across different time\nsteps based on the similarity for continual optimization. Extensive experiments\non the AMOS dataset demonstrate the superiority of our FedDAH to other FCL\nmethods on sites with different task streams. The code is\navailable:https:\/\/github.com\/jinlab-imvr\/FedDAH.\n","date":"2025-03-25"}
{"id":"2503.20822","title":"Synthetic Video Enhances Physical Fidelity in Video Synthesis","abstract":"  We investigate how to enhance the physical fidelity of video generation\nmodels by leveraging synthetic videos derived from computer graphics pipelines.\nThese rendered videos respect real-world physics, such as maintaining 3D\nconsistency, and serve as a valuable resource that can potentially improve\nvideo generation models. To harness this potential, we propose a solution that\ncurates and integrates synthetic data while introducing a method to transfer\nits physical realism to the model, significantly reducing unwanted artifacts.\nThrough experiments on three representative tasks emphasizing physical\nconsistency, we demonstrate its efficacy in enhancing physical fidelity. While\nour model still lacks a deep understanding of physics, our work offers one of\nthe first empirical demonstrations that synthetic video enhances physical\nfidelity in video synthesis. Website: https:\/\/kevinz8866.github.io\/simulation\/\n","date":"2025-03-26"}
{"id":"2503.20824","title":"Exploiting Temporal State Space Sharing for Video Semantic Segmentation","abstract":"  Video semantic segmentation (VSS) plays a vital role in understanding the\ntemporal evolution of scenes. Traditional methods often segment videos\nframe-by-frame or in a short temporal window, leading to limited temporal\ncontext, redundant computations, and heavy memory requirements. To this end, we\nintroduce a Temporal Video State Space Sharing (TV3S) architecture to leverage\nMamba state space models for temporal feature sharing. Our model features a\nselective gating mechanism that efficiently propagates relevant information\nacross video frames, eliminating the need for a memory-heavy feature pool. By\nprocessing spatial patches independently and incorporating shifted operation,\nTV3S supports highly parallel computation in both training and inference\nstages, which reduces the delay in sequential state space processing and\nimproves the scalability for long video sequences. Moreover, TV3S incorporates\ninformation from prior frames during inference, achieving long-range temporal\ncoherence and superior adaptability to extended sequences. Evaluations on the\nVSPW and Cityscapes datasets reveal that our approach outperforms current\nstate-of-the-art methods, establishing a new standard for VSS with consistent\nresults across long video sequences. By achieving a good balance between\naccuracy and efficiency, TV3S shows a significant advancement in spatiotemporal\nmodeling, paving the way for efficient video analysis. The code is publicly\navailable at https:\/\/github.com\/Ashesham\/TV3S.git.\n","date":"2025-03-26"}
{"id":"2503.20825","title":"Debiasing Kernel-Based Generative Models","abstract":"  We propose a novel two-stage framework of generative models named Debiasing\nKernel-Based Generative Models (DKGM) with the insights from kernel density\nestimation (KDE) and stochastic approximation. In the first stage of DKGM, we\nemploy KDE to bypass the obstacles in estimating the density of data without\nlosing too much image quality. One characteristic of KDE is oversmoothing,\nwhich makes the generated image blurry. Therefore, in the second stage, we\nformulate the process of reducing the blurriness of images as a statistical\ndebiasing problem and develop a novel iterative algorithm to improve image\nquality, which is inspired by the stochastic approximation. Extensive\nexperiments illustrate that the image quality of DKGM on CIFAR10 is comparable\nto state-of-the-art models such as diffusion models and GAN models. The\nperformance of DKGM on CelebA 128x128 and LSUN (Church) 128x128 is also\ncompetitive. We conduct extra experiments to exploit how the bandwidth in KDE\naffects the sample diversity and debiasing effect of DKGM. The connections\nbetween DKGM and score-based models are also discussed.\n","date":"2025-03-26"}
{"id":"2503.20827","title":"Multimodal Image Matching based on Frequency-domain Information of Local\n  Energy Response","abstract":"  Complicated nonlinear intensity differences, nonlinear local geometric\ndistortions, noises and rotation transformation are main challenges in\nmultimodal image matching. In order to solve these problems, we propose a\nmethod based on Frequency-domain Information of Local Energy Response called\nFILER. The core of FILER is the local energy response model based on\nfrequency-domain information, which can overcome the effect of nonlinear\nintensity differences. To improve the robustness to local nonlinear geometric\ndistortions and noises, we design a new edge structure enhanced feature\ndetector and convolutional feature weighted descriptor, respectively. In\naddition, FILER overcomes the sensitivity of the frequency-domain information\nto the rotation angle and achieves rotation invariance. Extensive experiments\nmultimodal image pairs show that FILER outperforms other state-of-the-art\nalgorithms and has good robustness and universality.\n","date":"2025-03-26"}
{"id":"2503.20830","title":"MedSegNet10: A Publicly Accessible Network Repository for Split\n  Federated Medical Image Segmentation","abstract":"  Machine Learning (ML) and Deep Learning (DL) have shown significant promise\nin healthcare, particularly in medical image segmentation, which is crucial for\naccurate disease diagnosis and treatment planning. Despite their potential,\nchallenges such as data privacy concerns, limited annotated data, and\ninadequate training data persist. Decentralized learning approaches such as\nfederated learning (FL), split learning (SL), and split federated learning\n(SplitFed\/SFL) address these issues effectively. This paper introduces\n\"MedSegNet10,\" a publicly accessible repository designed for medical image\nsegmentation using split-federated learning. MedSegNet10 provides a collection\nof pre-trained neural network architectures optimized for various medical image\ntypes, including microscopic images of human blastocysts, dermatoscopic images\nof skin lesions, and endoscopic images of lesions, polyps, and ulcers, with\napplications extending beyond these examples. By leveraging SplitFed's\nbenefits, MedSegNet10 allows collaborative training on privately stored,\nhorizontally split data, ensuring privacy and integrity. This repository\nsupports researchers, practitioners, trainees, and data scientists, aiming to\nadvance medical image segmentation while maintaining patient data privacy. The\nrepository is available at: https:\/\/vault.sfu.ca\/index.php\/s\/ryhf6t12O0sobuX\n(password upon request to the authors).\n","date":"2025-03-26"}
{"id":"2503.20831","title":"Advancing Vulnerability Classification with BERT: A Multi-Objective\n  Learning Model","abstract":"  The rapid increase in cybersecurity vulnerabilities necessitates automated\ntools for analyzing and classifying vulnerability reports. This paper presents\na novel Vulnerability Report Classifier that leverages the BERT (Bidirectional\nEncoder Representations from Transformers) model to perform multi-label\nclassification of Common Vulnerabilities and Exposures (CVE) reports from the\nNational Vulnerability Database (NVD). The classifier predicts both the\nseverity (Low, Medium, High, Critical) and vulnerability types (e.g., Buffer\nOverflow, XSS) from textual descriptions. We introduce a custom training\npipeline using a combined loss function-Cross-Entropy for severity and Binary\nCross-Entropy with Logits for types-integrated into a Hugging Face Trainer\nsubclass. Experiments on recent NVD data demonstrate promising results, with\ndecreasing evaluation loss across epochs. The system is deployed via a REST API\nand a Streamlit UI, enabling real-time vulnerability analysis. This work\ncontributes a scalable, open-source solution for cybersecurity practitioners to\nautomate vulnerability triage.\n","date":"2025-03-26"}
{"id":"2503.20835","title":"Comprehensive Manuscript Assessment with Text Summarization Using 69707\n  articles","abstract":"  Rapid and efficient assessment of the future impact of research articles is a\nsignificant concern for both authors and reviewers. The most common standard\nfor measuring the impact of academic papers is the number of citations. In\nrecent years, numerous efforts have been undertaken to predict citation counts\nwithin various citation windows. However, most of these studies focus solely on\na specific academic field or require early citation counts for prediction,\nrendering them impractical for the early-stage evaluation of papers. In this\nwork, we harness Scopus to curate a significantly comprehensive and large-scale\ndataset of information from 69707 scientific articles sourced from 99 journals\nspanning multiple disciplines. We propose a deep learning methodology for the\nimpact-based classification tasks, which leverages semantic features extracted\nfrom the manuscripts and paper metadata. To summarize the semantic features,\nsuch as titles and abstracts, we employ a Transformer-based language model to\nencode semantic features and design a text fusion layer to capture shared\ninformation between titles and abstracts. We specifically focus on the\nfollowing impact-based prediction tasks using information of scientific\nmanuscripts in pre-publication stage: (1) The impact of journals in which the\nmanuscripts will be published. (2) The future impact of manuscripts themselves.\nExtensive experiments on our datasets demonstrate the superiority of our\nproposed model for impact-based prediction tasks. We also demonstrate\npotentials in generating manuscript's feedback and improvement suggestions.\n","date":"2025-03-26"}
{"id":"2503.20836","title":"Named Entity Recognition in Context","abstract":"  We present the Named Entity Recognition system developed by the Edit Dunhuang\nteam for the EvaHan2025 competition. Our approach integrates three core\ncomponents: (1) Pindola, a modern transformer-based bidirectional encoder\npretrained on a large corpus of Classical Chinese texts; (2) a retrieval module\nthat fetches relevant external context for each target sequence; and (3) a\ngenerative reasoning step that summarizes retrieved context in Classical\nChinese for more robust entity disambiguation. Using this approach, we achieve\nan average F1 score of 85.58, improving upon the competition baseline by nearly\n5 points.\n","date":"2025-03-26"}
{"id":"2503.20839","title":"TAR: Teacher-Aligned Representations via Contrastive Learning for\n  Quadrupedal Locomotion","abstract":"  Quadrupedal locomotion via Reinforcement Learning (RL) is commonly addressed\nusing the teacher-student paradigm, where a privileged teacher guides a\nproprioceptive student policy. However, key challenges such as representation\nmisalignment between the privileged teacher and the proprioceptive-only\nstudent, covariate shift due to behavioral cloning, and lack of deployable\nadaptation lead to poor generalization in real-world scenarios. We propose\nTeacher-Aligned Representations via Contrastive Learning (TAR), a framework\nthat leverages privileged information with self-supervised contrastive learning\nto bridge this gap. By aligning representations to a privileged teacher in\nsimulation via contrastive objectives, our student policy learns structured\nlatent spaces and exhibits robust generalization to Out-of-Distribution (OOD)\nscenarios, surpassing the fully privileged \"Teacher\". Results showed\naccelerated training by 2x compared to state-of-the-art baselines to achieve\npeak performance. OOD scenarios showed better generalization by 40 percent on\naverage compared to existing methods. Additionally, TAR transitions seamlessly\ninto learning during deployment without requiring privileged states, setting a\nnew benchmark in sample-efficient, adaptive locomotion and enabling continual\nfine-tuning in real-world scenarios. Open-source code and videos are available\nat https:\/\/ammousa.github.io\/TARLoco\/.\n","date":"2025-03-26"}
{"id":"2503.20842","title":"Anti Robot Speciesism","abstract":"  Humanoid robots are a form of embodied artificial intelligence (AI) that\nlooks and acts more and more like humans. Powered by generative AI and advances\nin robotics, humanoid robots can speak and interact with humans rather\nnaturally but are still easily recognizable as robots. But how will we treat\nhumanoids when they seem indistinguishable from humans in appearance and mind?\nWe find a tendency (called \"anti-robot\" speciesism) to deny such robots\nhumanlike capabilities, driven by motivations to accord members of the human\nspecies preferential treatment. Six experiments show that robots are denied\nhumanlike attributes, simply because they are not biological beings and because\nhumans want to avoid feelings of cognitive dissonance when utilizing such\nrobots for unsavory tasks. Thus, people do not rationally attribute\ncapabilities to perfectly humanlike robots but deny them capabilities as it\nsuits them.\n","date":"2025-03-26"}
{"id":"2503.20844","title":"Robust Deep Reinforcement Learning in Robotics via Adaptive\n  Gradient-Masked Adversarial Attacks","abstract":"  Deep reinforcement learning (DRL) has emerged as a promising approach for\nrobotic control, but its realworld deployment remains challenging due to its\nvulnerability to environmental perturbations. Existing white-box adversarial\nattack methods, adapted from supervised learning, fail to effectively target\nDRL agents as they overlook temporal dynamics and indiscriminately perturb all\nstate dimensions, limiting their impact on long-term rewards. To address these\nchallenges, we propose the Adaptive Gradient-Masked Reinforcement (AGMR)\nAttack, a white-box attack method that combines DRL with a gradient-based soft\nmasking mechanism to dynamically identify critical state dimensions and\noptimize adversarial policies. AGMR selectively allocates perturbations to the\nmost impactful state features and incorporates a dynamic adjustment mechanism\nto balance exploration and exploitation during training. Extensive experiments\ndemonstrate that AGMR outperforms state-of-the-art adversarial attack methods\nin degrading the performance of the victim agent and enhances the victim\nagent's robustness through adversarial defense mechanisms.\n","date":"2025-03-26"}
{"id":"2503.20846","title":"Generating Synthetic Data with Formal Privacy Guarantees: State of the\n  Art and the Road Ahead","abstract":"  Privacy-preserving synthetic data offers a promising solution to harness\nsegregated data in high-stakes domains where information is compartmentalized\nfor regulatory, privacy, or institutional reasons. This survey provides a\ncomprehensive framework for understanding the landscape of privacy-preserving\nsynthetic data, presenting the theoretical foundations of generative models and\ndifferential privacy followed by a review of state-of-the-art methods across\ntabular data, images, and text. Our synthesis of evaluation approaches\nhighlights the fundamental trade-off between utility for down-stream tasks and\nprivacy guarantees, while identifying critical research gaps: the lack of\nrealistic benchmarks representing specialized domains and insufficient\nempirical evaluations required to contextualise formal guarantees.\n  Through empirical analysis of four leading methods on five real-world\ndatasets from specialized domains, we demonstrate significant performance\ndegradation under realistic privacy constraints ($\\epsilon \\leq 4$), revealing\na substantial gap between results reported on general domain benchmarks and\nperformance on domain-specific data. %Our findings highlight key challenges\nincluding unaccounted privacy leakage, insufficient empirical verification of\nformal guarantees, and a critical deficit of realistic benchmarks. These\nchallenges underscore the need for robust evaluation frameworks, standardized\nbenchmarks for specialized domains, and improved techniques to address the\nunique requirements of privacy-sensitive fields such that this technology can\ndeliver on its considerable potential.\n","date":"2025-03-26"}
{"id":"2503.20848","title":"The Backfiring Effect of Weak AI Safety Regulation","abstract":"  Recent policy proposals aim to improve the safety of general-purpose AI, but\nthere is little understanding of the efficacy of different regulatory\napproaches to AI safety. We present a strategic model that explores the\ninteractions between the regulator, the general-purpose AI technology creators,\nand domain specialists--those who adapt the AI for specific applications. Our\nanalysis examines how different regulatory measures, targeting different parts\nof the development chain, affect the outcome of the development process. In\nparticular, we assume AI technology is described by two key attributes: safety\nand performance. The regulator first sets a minimum safety standard that\napplies to one or both players, with strict penalties for non-compliance. The\ngeneral-purpose creator then develops the technology, establishing its initial\nsafety and performance levels. Next, domain specialists refine the AI for their\nspecific use cases, and the resulting revenue is distributed between the\nspecialist and generalist through an ex-ante bargaining process. Our analysis\nof this game reveals two key insights: First, weak safety regulation imposed\nonly on the domain specialists can backfire. While it might seem logical to\nregulate use cases (as opposed to the general-purpose technology), our analysis\nshows that weak regulations targeting domain specialists alone can\nunintentionally reduce safety. This effect persists across a wide range of\nsettings. Second, in sharp contrast to the previous finding, we observe that\nstronger, well-placed regulation can in fact benefit all players subjected to\nit. When regulators impose appropriate safety standards on both AI creators and\ndomain specialists, the regulation functions as a commitment mechanism, leading\nto safety and performance gains, surpassing what is achieved under no\nregulation or regulating one player only.\n","date":"2025-03-26"}
{"id":"2503.20850","title":"Both Direct and Indirect Evidence Contribute to Dative Alternation\n  Preferences in Language Models","abstract":"  Language models (LMs) tend to show human-like preferences on a number of\nsyntactic phenomena, but the extent to which these are attributable to direct\nexposure to the phenomena or more general properties of language is unclear. We\nexplore this with the English dative alternation (DO: \"gave Y the X\" vs. PO:\n\"gave the X to Y\"), using a controlled rearing paradigm wherein we iteratively\ntrain small LMs on systematically manipulated input. We focus on properties\nthat affect the choice of alternant: length and animacy. Both properties are\ndirectly present in datives but also reflect more global tendencies for shorter\nelements to precede longer ones and animates to precede inanimates. First, by\nmanipulating and ablating datives for these biases in the input, we show that\ndirect evidence of length and animacy matters, but easy-first preferences\npersist even without such evidence. Then, using LMs trained on systematically\nperturbed datasets to manipulate global length effects (re-linearizing\nsentences globally while preserving dependency structure), we find that dative\npreferences can emerge from indirect evidence. We conclude that LMs' emergent\nsyntactic preferences come from a mix of direct and indirect sources.\n","date":"2025-03-26"}
{"id":"2503.20853","title":"Unified Multimodal Discrete Diffusion","abstract":"  Multimodal generative models that can understand and generate across multiple\nmodalities are dominated by autoregressive (AR) approaches, which process\ntokens sequentially from left to right, or top to bottom. These models jointly\nhandle images, text, video, and audio for various tasks such as image\ncaptioning, question answering, and image generation. In this work, we explore\ndiscrete diffusion models as a unified generative formulation in the joint text\nand image domain, building upon their recent success in text generation.\nDiscrete diffusion models offer several advantages over AR models, including\nimproved control over quality versus diversity of generated samples, the\nability to perform joint multimodal inpainting (across both text and image\ndomains), and greater controllability in generation through guidance.\nLeveraging these benefits, we present the first Unified Multimodal Discrete\nDiffusion (UniDisc) model which is capable of jointly understanding and\ngenerating text and images for a variety of downstream tasks. We compare\nUniDisc to multimodal AR models, performing a scaling analysis and\ndemonstrating that UniDisc outperforms them in terms of both performance and\ninference-time compute, enhanced controllability, editability, inpainting, and\nflexible trade-off between inference time and generation quality. Code and\nadditional visualizations are available at https:\/\/unidisc.github.io.\n","date":"2025-03-26"}
{"id":"2503.20871","title":"VinaBench: Benchmark for Faithful and Consistent Visual Narratives","abstract":"  Visual narrative generation transforms textual narratives into sequences of\nimages illustrating the content of the text. However, generating visual\nnarratives that are faithful to the input text and self-consistent across\ngenerated images remains an open challenge, due to the lack of knowledge\nconstraints used for planning the stories. In this work, we propose a new\nbenchmark, VinaBench, to address this challenge. Our benchmark annotates the\nunderlying commonsense and discourse constraints in visual narrative samples,\noffering systematic scaffolds for learning the implicit strategies of visual\nstorytelling. Based on the incorporated narrative constraints, we further\npropose novel metrics to closely evaluate the consistency of generated\nnarrative images and the alignment of generations with the input textual\nnarrative. Our results across three generative vision models demonstrate that\nlearning with VinaBench's knowledge constraints effectively improves the\nfaithfulness and cohesion of generated visual narratives.\n","date":"2025-03-26"}
{"id":"2503.20879","title":"Quantum advantage for learning shallow neural networks with natural data\n  distributions","abstract":"  The application of quantum computers to machine learning tasks is an exciting\npotential direction to explore in search of quantum advantage. In the absence\nof large quantum computers to empirically evaluate performance, theoretical\nframeworks such as the quantum probably approximately correct (PAC) and quantum\nstatistical query (QSQ) models have been proposed to study quantum algorithms\nfor learning classical functions. Despite numerous works investigating quantum\nadvantage in these models, we nevertheless only understand it at two extremes:\neither exponential quantum advantages for uniform input distributions or no\nadvantage for potentially adversarial distributions. In this work, we study the\ngap between these two regimes by designing an efficient quantum algorithm for\nlearning periodic neurons in the QSQ model over a broad range of non-uniform\ndistributions, which includes Gaussian, generalized Gaussian, and logistic\ndistributions. To our knowledge, our work is also the first result in quantum\nlearning theory for classical functions that explicitly considers real-valued\nfunctions. Recent advances in classical learning theory prove that learning\nperiodic neurons is hard for any classical gradient-based algorithm, giving us\nan exponential quantum advantage over such algorithms, which are the standard\nworkhorses of machine learning. Moreover, in some parameter regimes, the\nproblem remains hard for classical statistical query algorithms and even\ngeneral classical algorithms learning under small amounts of noise.\n","date":"2025-03-26"}
{"id":"2503.20880","title":"BioX-CPath: Biologically-driven Explainable Diagnostics for Multistain\n  IHC Computational Pathology","abstract":"  The development of biologically interpretable and explainable models remains\na key challenge in computational pathology, particularly for multistain\nimmunohistochemistry (IHC) analysis. We present BioX-CPath, an explainable\ngraph neural network architecture for whole slide image (WSI) classification\nthat leverages both spatial and semantic features across multiple stains. At\nits core, BioX-CPath introduces a novel Stain-Aware Attention Pooling (SAAP)\nmodule that generates biologically meaningful, stain-aware patient embeddings.\nOur approach achieves state-of-the-art performance on both Rheumatoid Arthritis\nand Sjogren's Disease multistain datasets. Beyond performance metrics,\nBioX-CPath provides interpretable insights through stain attention scores,\nentropy measures, and stain interaction scores, that permit measuring model\nalignment with known pathological mechanisms. This biological grounding,\ncombined with strong classification performance, makes BioX-CPath particularly\nsuitable for clinical applications where interpretability is key. Source code\nand documentation can be found at: https:\/\/github.com\/AmayaGS\/BioX-CPath.\n","date":"2025-03-26"}
{"id":"2503.20884","title":"Robust Federated Learning Against Poisoning Attacks: A GAN-Based Defense\n  Framework","abstract":"  Federated Learning (FL) enables collaborative model training across\ndecentralized devices without sharing raw data, but it remains vulnerable to\npoisoning attacks that compromise model integrity. Existing defenses often rely\non external datasets or predefined heuristics (e.g. number of malicious\nclients), limiting their effectiveness and scalability. To address these\nlimitations, we propose a privacy-preserving defense framework that leverages a\nConditional Generative Adversarial Network (cGAN) to generate synthetic data at\nthe server for authenticating client updates, eliminating the need for external\ndatasets. Our framework is scalable, adaptive, and seamlessly integrates into\nFL workflows. Extensive experiments on benchmark datasets demonstrate its\nrobust performance against a variety of poisoning attacks, achieving high True\nPositive Rate (TPR) and True Negative Rate (TNR) of malicious and benign\nclients, respectively, while maintaining model accuracy. The proposed framework\noffers a practical and effective solution for securing federated learning\nsystems.\n","date":"2025-03-26"}
{"id":"2503.20897","title":"Feature Modulation for Semi-Supervised Domain Generalization without\n  Domain Labels","abstract":"  Semi-supervised domain generalization (SSDG) leverages a small fraction of\nlabeled data alongside unlabeled data to enhance model generalization. Most of\nthe existing SSDG methods rely on pseudo-labeling (PL) for unlabeled data,\noften assuming access to domain labels-a privilege not always available.\nHowever, domain shifts introduce domain noise, leading to inconsistent PLs that\ndegrade model performance. Methods derived from FixMatch suffer particularly\nfrom lower PL accuracy, reducing the effectiveness of unlabeled data. To\naddress this, we tackle the more challenging domain-label agnostic SSDG, where\ndomain labels for unlabeled data are not available during training. First, we\npropose a feature modulation strategy that enhances class-discriminative\nfeatures while suppressing domain-specific information. This modulation shifts\nfeatures toward Similar Average Representations-a modified version of class\nprototypes-that are robust across domains, encouraging the classifier to\ndistinguish between closely related classes and feature extractor to form\ntightly clustered, domain-invariant representations. Second, to mitigate domain\nnoise and improve pseudo-label accuracy, we introduce a loss-scaling function\nthat dynamically lowers the fixed confidence threshold for pseudo-labels,\noptimizing the use of unlabeled data. With these key innovations, our approach\nachieves significant improvements on four major domain generalization\nbenchmarks-even without domain labels. We will make the code available.\n","date":"2025-03-26"}
{"id":"2503.20903","title":"Assessing Generative Models for Structured Data","abstract":"  Synthetic tabular data generation has emerged as a promising method to\naddress limited data availability and privacy concerns. With the sharp increase\nin the performance of large language models in recent years, researchers have\nbeen interested in applying these models to the generation of tabular data.\nHowever, little is known about the quality of the generated tabular data from\nlarge language models. The predominant method for assessing the quality of\nsynthetic tabular data is the train-synthetic-test-real approach, where the\nartificial examples are compared to the original by how well machine learning\nmodels, trained separately on the real and synthetic sets, perform in some\ndownstream tasks. This method does not directly measure how closely the\ndistribution of generated data approximates that of the original. This paper\nintroduces rigorous methods for directly assessing synthetic tabular data\nagainst real data by looking at inter-column dependencies within the data. We\nfind that large language models (GPT-2), both when queried via few-shot\nprompting and when fine-tuned, and GAN (CTGAN) models do not produce data with\ndependencies that mirror the original real data. Results from this study can\ninform future practice in synthetic data generation to improve data quality.\n","date":"2025-03-26"}
{"id":"2503.20913","title":"TransDiffSBDD: Causality-Aware Multi-Modal Structure-Based Drug Design","abstract":"  Structure-based drug design (SBDD) is a critical task in drug discovery,\nrequiring the generation of molecular information across two distinct\nmodalities: discrete molecular graphs and continuous 3D coordinates. However,\nexisting SBDD methods often overlook two key challenges: (1) the multi-modal\nnature of this task and (2) the causal relationship between these modalities,\nlimiting their plausibility and performance. To address both challenges, we\npropose TransDiffSBDD, an integrated framework combining autoregressive\ntransformers and diffusion models for SBDD. Specifically, the autoregressive\ntransformer models discrete molecular information, while the diffusion model\nsamples continuous distributions, effectively resolving the first challenge. To\naddress the second challenge, we design a hybrid-modal sequence for\nprotein-ligand complexes that explicitly respects the causality between\nmodalities. Experiments on the CrossDocked2020 benchmark demonstrate that\nTransDiffSBDD outperforms existing baselines.\n","date":"2025-03-26"}
{"id":"2503.20914","title":"D4R -- Exploring and Querying Relational Graphs Using Natural Language\n  and Large Language Models -- the Case of Historical Documents","abstract":"  D4R is a digital platform designed to assist non-technical users,\nparticularly historians, in exploring textual documents through advanced\ngraphical tools for text analysis and knowledge extraction. By leveraging a\nlarge language model, D4R translates natural language questions into Cypher\nqueries, enabling the retrieval of data from a Neo4J database. A user-friendly\ngraphical interface allows for intuitive interaction, enabling users to\nnavigate and analyse complex relational data extracted from unstructured\ntextual documents. Originally designed to bridge the gap between AI\ntechnologies and historical research, D4R's capabilities extend to various\nother domains. A demonstration video and a live software demo are available.\n","date":"2025-03-26"}
{"id":"2503.20919","title":"GatedxLSTM: A Multimodal Affective Computing Approach for Emotion\n  Recognition in Conversations","abstract":"  Affective Computing (AC) is essential for advancing Artificial General\nIntelligence (AGI), with emotion recognition serving as a key component.\nHowever, human emotions are inherently dynamic, influenced not only by an\nindividual's expressions but also by interactions with others, and\nsingle-modality approaches often fail to capture their full dynamics.\nMultimodal Emotion Recognition (MER) leverages multiple signals but\ntraditionally relies on utterance-level analysis, overlooking the dynamic\nnature of emotions in conversations. Emotion Recognition in Conversation (ERC)\naddresses this limitation, yet existing methods struggle to align multimodal\nfeatures and explain why emotions evolve within dialogues. To bridge this gap,\nwe propose GatedxLSTM, a novel speech-text multimodal ERC model that explicitly\nconsiders voice and transcripts of both the speaker and their conversational\npartner(s) to identify the most influential sentences driving emotional shifts.\nBy integrating Contrastive Language-Audio Pretraining (CLAP) for improved\ncross-modal alignment and employing a gating mechanism to emphasise emotionally\nimpactful utterances, GatedxLSTM enhances both interpretability and\nperformance. Additionally, the Dialogical Emotion Decoder (DED) refines emotion\npredictions by modelling contextual dependencies. Experiments on the IEMOCAP\ndataset demonstrate that GatedxLSTM achieves state-of-the-art (SOTA)\nperformance among open-source methods in four-class emotion classification.\nThese results validate its effectiveness for ERC applications and provide an\ninterpretability analysis from a psychological perspective.\n","date":"2025-03-26"}
{"id":"2503.20925","title":"Prototype Guided Backdoor Defense","abstract":"  Deep learning models are susceptible to {\\em backdoor attacks} involving\nmalicious attackers perturbing a small subset of training data with a {\\em\ntrigger} to causes misclassifications. Various triggers have been used,\nincluding semantic triggers that are easily realizable without requiring the\nattacker to manipulate the image. The emergence of generative AI has eased the\ngeneration of varied poisoned samples. Robustness across types of triggers is\ncrucial to effective defense. We propose Prototype Guided Backdoor Defense\n(PGBD), a robust post-hoc defense that scales across different trigger types,\nincluding previously unsolved semantic triggers. PGBD exploits displacements in\nthe geometric spaces of activations to penalize movements toward the trigger.\nThis is done using a novel sanitization loss of a post-hoc fine-tuning step.\nThe geometric approach scales easily to all types of attacks. PGBD achieves\nbetter performance across all settings. We also present the first defense\nagainst a new semantic attack on celebrity face images. Project page:\n\\hyperlink{https:\/\/venkatadithya9.github.io\/pgbd.github.io\/}{this https URL}.\n","date":"2025-03-26"}
{"id":"2503.20929","title":"Global and Local Structure Learning for Sparse Tensor Completion","abstract":"  How can we accurately complete tensors by learning relationships of\ndimensions along each mode? Tensor completion, a widely studied problem, is to\npredict missing entries in incomplete tensors. Tensor decomposition methods,\nfundamental tensor analysis tools, have been actively developed to solve tensor\ncompletion tasks. However, standard tensor decomposition models have not been\ndesigned to learn relationships of dimensions along each mode, which limits to\naccurate tensor completion. Also, previously developed tensor decomposition\nmodels have required prior knowledge between relations within dimensions to\nmodel the relations, expensive to obtain.\n  This paper proposes TGL (Tensor Decomposition Learning Global and Local\nStructures) to accurately predict missing entries in tensors. TGL reconstructs\na tensor with factor matrices which learn local structures with GNN without\nprior knowledges. Extensive experiments are conducted to evaluate TGL with\nbaselines and datasets.\n","date":"2025-03-26"}
{"id":"2503.20936","title":"LATTE-MV: Learning to Anticipate Table Tennis Hits from Monocular Videos","abstract":"  Physical agility is a necessary skill in competitive table tennis, but by no\nmeans sufficient. Champions excel in this fast-paced and highly dynamic\nenvironment by anticipating their opponent's intent - buying themselves the\nnecessary time to react. In this work, we take one step towards designing such\nan anticipatory agent. Previous works have developed systems capable of\nreal-time table tennis gameplay, though they often do not leverage\nanticipation. Among the works that forecast opponent actions, their approaches\nare limited by dataset size and variety. Our paper contributes (1) a scalable\nsystem for reconstructing monocular video of table tennis matches in 3D and (2)\nan uncertainty-aware controller that anticipates opponent actions. We\ndemonstrate in simulation that our policy improves the ball return rate against\nhigh-speed hits from 49.9% to 59.0% as compared to a baseline non-anticipatory\npolicy.\n","date":"2025-03-26"}
{"id":"2503.20939","title":"Hacia la interpretabilidad de la detecci\\'on anticipada de riesgos de\n  depresi\\'on utilizando grandes modelos de lenguaje","abstract":"  Early Detection of Risks (EDR) on the Web involves identifying at-risk users\nas early as possible. Although Large Language Models (LLMs) have proven to\nsolve various linguistic tasks efficiently, assessing their reasoning ability\nin specific domains is crucial. In this work, we propose a method for solving\ndepression-related EDR using LLMs on Spanish texts, with responses that can be\ninterpreted by humans. We define a reasoning criterion to analyze users through\na specialist, apply in-context learning to the Gemini model, and evaluate its\nperformance both quantitatively and qualitatively. The results show that\naccurate predictions can be obtained, supported by explanatory reasoning,\nproviding a deeper understanding of the solution. Our approach offers new\nperspectives for addressing EDR problems by leveraging the power of LLMs.\n","date":"2025-03-26"}
{"id":"2503.20950","title":"DEMENTIA-PLAN: An Agent-Based Framework for Multi-Knowledge Graph\n  Retrieval-Augmented Generation in Dementia Care","abstract":"  Mild-stage dementia patients primarily experience two critical symptoms:\nsevere memory loss and emotional instability. To address these challenges, we\npropose DEMENTIA-PLAN, an innovative retrieval-augmented generation framework\nthat leverages large language models to enhance conversational support. Our\nmodel employs a multiple knowledge graph architecture, integrating various\ndimensional knowledge representations including daily routine graphs and life\nmemory graphs. Through this multi-graph architecture, DEMENTIA-PLAN\ncomprehensively addresses both immediate care needs and facilitates deeper\nemotional resonance through personal memories, helping stabilize patient mood\nwhile providing reliable memory support. Our notable innovation is the\nself-reflection planning agent, which systematically coordinates knowledge\nretrieval and semantic integration across multiple knowledge graphs, while\nscoring retrieved content from daily routine and life memory graphs to\ndynamically adjust their retrieval weights for optimized response generation.\nDEMENTIA-PLAN represents a significant advancement in the clinical application\nof large language models for dementia care, bridging the gap between AI tools\nand caregivers interventions.\n","date":"2025-03-26"}
{"id":"2503.20952","title":"TS-Inverse: A Gradient Inversion Attack Tailored for Federated Time\n  Series Forecasting Models","abstract":"  Federated learning (FL) for time series forecasting (TSF) enables clients\nwith privacy-sensitive time series (TS) data to collaboratively learn accurate\nforecasting models, for example, in energy load prediction. Unfortunately,\nprivacy risks in FL persist, as servers can potentially reconstruct clients'\ntraining data through gradient inversion attacks (GIA). Although GIA is\ndemonstrated for image classification tasks, little is known about time series\nregression tasks. In this paper, we first conduct an extensive empirical study\non inverting TS data across 4 TSF models and 4 datasets, identifying the unique\nchallenges of reconstructing both observations and targets of TS data. We then\npropose TS-Inverse, a novel GIA that improves the inversion of TS data by (i)\nlearning a gradient inversion model that outputs quantile predictions, (ii) a\nunique loss function that incorporates periodicity and trend regularization,\nand (iii) regularization according to the quantile predictions. Our evaluations\ndemonstrate a remarkable performance of TS-Inverse, achieving at least a 2x-10x\nimprovement in terms of the sMAPE metric over existing GIA methods on TS data.\nCode repository: https:\/\/github.com\/Capsar\/ts-inverse\n","date":"2025-03-26"}
{"id":"2503.20953","title":"Clean & Clear: Feasibility of Safe LLM Clinical Guidance","abstract":"  Background:\n  Clinical guidelines are central to safe evidence-based medicine in modern\nhealthcare, providing diagnostic criteria, treatment options and monitoring\nadvice for a wide range of illnesses. LLM-empowered chatbots have shown great\npromise in Healthcare Q&A tasks, offering the potential to provide quick and\naccurate responses to medical inquiries.\n  Our main objective was the development and preliminary assessment of an\nLLM-empowered chatbot software capable of reliably answering clinical guideline\nquestions using University College London Hospital (UCLH) clinical guidelines.\n  Methods: We used the open-weight Llama-3.1-8B LLM to extract relevant\ninformation from the UCLH guidelines to answer questions. Our approach\nhighlights the safety and reliability of referencing information over its\ninterpretation and response generation. Seven doctors from the ward assessed\nthe chatbot's performance by comparing its answers to the gold standard.\n  Results: Our chatbot demonstrates promising performance in terms of\nrelevance, with ~73% of its responses rated as very relevant, showcasing a\nstrong understanding of the clinical context. Importantly, our chatbot achieves\na recall of 0.98 for extracted guideline lines, substantially minimising the\nrisk of missing critical information. Approximately 78% of responses were rated\nsatisfactory in terms of completeness. A small portion (~14.5%) contained minor\nunnecessary information, indicating occasional lapses in precision. The\nchatbot' showed high efficiency, with an average completion time of 10 seconds,\ncompared to 30 seconds for human respondents. Evaluation of clinical reasoning\nshowed that 72% of the chatbot's responses were without flaws. Our chatbot\ndemonstrates significant potential to speed up and improve the process of\naccessing locally relevant clinical information for healthcare professionals.\n","date":"2025-03-26"}
{"id":"2503.20959","title":"Sociotechnical Effects of Machine Translation","abstract":"  While the previous chapters have shown how machine translation (MT) can be\nuseful, in this chapter we discuss some of the side-effects and risks that are\nassociated, and how they might be mitigated. With the move to neural MT and\napproaches using Large Language Models (LLMs), there is an associated impact on\nclimate change, as the models built by multinational corporations are massive.\nThey are hugely expensive to train, consume large amounts of electricity, and\noutput huge volumes of kgCO2 to boot. However, smaller models which still\nperform to a high level of quality can be built with much lower carbon\nfootprints, and tuning pre-trained models saves on the requirement to train\nfrom scratch. We also discuss the possible detrimental effects of MT on\ntranslators and other users. The topics of copyright and ownership of data are\ndiscussed, as well as ethical considerations on data and MT use. Finally, we\nshow how if done properly, using MT in crisis scenarios can save lives, and we\nprovide a method of how this might be done.\n","date":"2025-03-26"}
{"id":"2503.20960","title":"Multi-Modal Framing Analysis of News","abstract":"  Automated frame analysis of political communication is a popular task in\ncomputational social science that is used to study how authors select aspects\nof a topic to frame its reception. So far, such studies have been narrow, in\nthat they use a fixed set of pre-defined frames and focus only on the text,\nignoring the visual contexts in which those texts appear. Especially for\nframing in the news, this leaves out valuable information about editorial\nchoices, which include not just the written article but also accompanying\nphotographs. To overcome such limitations, we present a method for conducting\nmulti-modal, multi-label framing analysis at scale using large\n(vision-)language models. Grounding our work in framing theory, we extract\nlatent meaning embedded in images used to convey a certain point and contrast\nthat to the text by comparing the respective frames used. We also identify\nhighly partisan framing of topics with issue-specific frame analysis found in\nprior qualitative work. We demonstrate a method for doing scalable integrative\nframing analysis of both text and image in news, providing a more complete\npicture for understanding media bias.\n","date":"2025-03-26"}
{"id":"2503.20967","title":"Eyes Tell the Truth: GazeVal Highlights Shortcomings of Generative AI in\n  Medical Imaging","abstract":"  The demand for high-quality synthetic data for model training and\naugmentation has never been greater in medical imaging. However, current\nevaluations predominantly rely on computational metrics that fail to align with\nhuman expert recognition. This leads to synthetic images that may appear\nrealistic numerically but lack clinical authenticity, posing significant\nchallenges in ensuring the reliability and effectiveness of AI-driven medical\ntools. To address this gap, we introduce GazeVal, a practical framework that\nsynergizes expert eye-tracking data with direct radiological evaluations to\nassess the quality of synthetic medical images. GazeVal leverages gaze patterns\nof radiologists as they provide a deeper understanding of how experts perceive\nand interact with synthetic data in different tasks (i.e., diagnostic or Turing\ntests). Experiments with sixteen radiologists revealed that 96.6% of the\ngenerated images (by the most recent state-of-the-art AI algorithm) were\nidentified as fake, demonstrating the limitations of generative AI in producing\nclinically accurate images.\n","date":"2025-03-26"}
{"id":"2503.20968","title":"Reinforcement Learning for Efficient Toxicity Detection in Competitive\n  Online Video Games","abstract":"  Online platforms take proactive measures to detect and address undesirable\nbehavior, aiming to focus these resource-intensive efforts where such behavior\nis most prevalent. This article considers the problem of efficient sampling for\ntoxicity detection in competitive online video games. To make optimal\nmonitoring decisions, video game service operators need estimates of the\nlikelihood of toxic behavior. If no model is available for these predictions,\none must be estimated in real time. To close this gap, we propose a contextual\nbandit algorithm that makes monitoring decisions based on a small set of\nvariables that, according to domain expertise, are associated with toxic\nbehavior. This algorithm balances exploration and exploitation to optimize\nlong-term outcomes and is deliberately designed for easy deployment in\nproduction. Using data from the popular first-person action game Call of Duty:\nModern Warfare III, we show that our algorithm consistently outperforms\nbaseline algorithms that rely solely on players' past behavior. This finding\nhas substantive implications for the nature of toxicity. It also illustrates\nhow domain expertise can be harnessed to help video game service operators\nidentify and mitigate toxicity, ultimately fostering a safer and more enjoyable\ngaming experience.\n","date":"2025-03-26"}
{"id":"2503.20975","title":"Competitive Multi-armed Bandit Games for Resource Sharing","abstract":"  In modern resource-sharing systems, multiple agents access limited resources\nwith unknown stochastic conditions to perform tasks. When multiple agents\naccess the same resource (arm) simultaneously, they compete for successful\nusage, leading to contention and reduced rewards. This motivates our study of\ncompetitive multi-armed bandit (CMAB) games. In this paper, we study a new\nN-player K-arm competitive MAB game, where non-myopic players (agents) compete\nwith each other to form diverse private estimations of unknown arms over time.\nTheir possible collisions on same arms and time-varying nature of arm rewards\nmake the policy analysis more involved than existing studies for myopic\nplayers. We explicitly analyze the threshold-based structures of social optimum\nand existing selfish policy, showing that the latter causes prolonged\nconvergence time $\\Omega(\\frac{K}{\\eta^2}\\ln({\\frac{KN}{\\delta}}))$, while\nsocially optimal policy with coordinated communication reduces it to\n$\\mathcal{O}(\\frac{K}{N\\eta^2}\\ln{(\\frac{K}{\\delta})})$. Based on the\ncomparison, we prove that the competition among selfish players for the best\narm can result in an infinite price of anarchy (PoA), indicating an arbitrarily\nlarge efficiency loss compared to social optimum. We further prove that no\ninformational (non-monetary) mechanism (including Bayesian persuasion) can\nreduce the infinite PoA, as the strategic misreporting by non-myopic players\nundermines such approaches. To address this, we propose a Combined\nInformational and Side-Payment (CISP) mechanism, which provides socially\noptimal arm recommendations with proper informational and monetary incentives\nto players according to their time-varying private beliefs. Our CISP mechanism\nkeeps ex-post budget balanced for social planner and ensures truthful reporting\nfrom players, achieving the minimum PoA=1 and same convergence time as social\noptimum.\n","date":"2025-03-26"}
{"id":"2503.20978","title":"ScreenLLM: Stateful Screen Schema for Efficient Action Understanding and\n  Prediction","abstract":"  Graphical User Interface (GUI) agents are autonomous systems that interpret\nand generate actions, enabling intelligent user assistance and automation.\nEffective training of these agent presents unique challenges, such as sparsity\nin supervision signals, scalability for large datasets, and the need for\nnuanced user understanding. We propose stateful screen schema, an efficient\nrepresentation of GUI interactions that captures key user actions and\nintentions over time. Building on this foundation, we introduce ScreenLLM, a\nset of multimodal large language models (MLLMs) tailored for advanced UI\nunderstanding and action prediction. Extensive experiments on both open-source\nand proprietary models show that ScreenLLM accurately models user behavior and\npredicts actions. Our work lays the foundation for scalable, robust, and\nintelligent GUI agents that enhance user interaction in diverse software\nenvironments.\n","date":"2025-03-26"}
{"id":"2503.20981","title":"Patients Speak, AI Listens: LLM-based Analysis of Online Reviews\n  Uncovers Key Drivers for Urgent Care Satisfaction","abstract":"  Investigating the public experience of urgent care facilities is essential\nfor promoting community healthcare development. Traditional survey methods\noften fall short due to limited scope, time, and spatial coverage.\nCrowdsourcing through online reviews or social media offers a valuable approach\nto gaining such insights. With recent advancements in large language models\n(LLMs), extracting nuanced perceptions from reviews has become feasible. This\nstudy collects Google Maps reviews across the DMV and Florida areas and\nconducts prompt engineering with the GPT model to analyze the aspect-based\nsentiment of urgent care. We first analyze the geospatial patterns of various\naspects, including interpersonal factors, operational efficiency, technical\nquality, finances, and facilities. Next, we determine Census Block\nGroup(CBG)-level characteristics underpinning differences in public perception,\nincluding population density, median income, GINI Index, rent-to-income ratio,\nhousehold below poverty rate, no insurance rate, and unemployment rate. Our\nresults show that interpersonal factors and operational efficiency emerge as\nthe strongest determinants of patient satisfaction in urgent care, while\ntechnical quality, finances, and facilities show no significant independent\neffects when adjusted for in multivariate models. Among socioeconomic and\ndemographic factors, only population density demonstrates a significant but\nmodest association with patient ratings, while the remaining factors exhibit no\nsignificant correlations. Overall, this study highlights the potential of\ncrowdsourcing to uncover the key factors that matter to residents and provide\nvaluable insights for stakeholders to improve public satisfaction with urgent\ncare.\n","date":"2025-03-26"}
{"id":"2503.20988","title":"Cross-Modal State-Space Graph Reasoning for Structured Summarization","abstract":"  The ability to extract compact, meaningful summaries from large-scale and\nmultimodal data is critical for numerous applications, ranging from video\nanalytics to medical reports. Prior methods in cross-modal summarization have\noften suffered from high computational overheads and limited interpretability.\nIn this paper, we propose a \\textit{Cross-Modal State-Space Graph Reasoning}\n(\\textbf{CSS-GR}) framework that incorporates a state-space model with\ngraph-based message passing, inspired by prior work on efficient state-space\nmodels. Unlike existing approaches relying on purely sequential models, our\nmethod constructs a graph that captures inter- and intra-modal relationships,\nallowing more holistic reasoning over both textual and visual streams. We\ndemonstrate that our approach significantly improves summarization quality and\ninterpretability while maintaining computational efficiency, as validated on\nstandard multimodal summarization benchmarks. We also provide a thorough\nablation study to highlight the contributions of each component.\n","date":"2025-03-26"}
{"id":"2503.20990","title":"FinAudio: A Benchmark for Audio Large Language Models in Financial\n  Applications","abstract":"  Audio Large Language Models (AudioLLMs) have received widespread attention\nand have significantly improved performance on audio tasks such as\nconversation, audio understanding, and automatic speech recognition (ASR).\nDespite these advancements, there is an absence of a benchmark for assessing\nAudioLLMs in financial scenarios, where audio data, such as earnings conference\ncalls and CEO speeches, are crucial resources for financial analysis and\ninvestment decisions. In this paper, we introduce \\textsc{FinAudio}, the first\nbenchmark designed to evaluate the capacity of AudioLLMs in the financial\ndomain. We first define three tasks based on the unique characteristics of the\nfinancial domain: 1) ASR for short financial audio, 2) ASR for long financial\naudio, and 3) summarization of long financial audio. Then, we curate two short\nand two long audio datasets, respectively, and develop a novel dataset for\nfinancial audio summarization, comprising the \\textsc{FinAudio} benchmark.\nThen, we evaluate seven prevalent AudioLLMs on \\textsc{FinAudio}. Our\nevaluation reveals the limitations of existing AudioLLMs in the financial\ndomain and offers insights for improving AudioLLMs. All datasets and codes will\nbe released.\n","date":"2025-03-26"}
{"id":"2503.20991","title":"MVFNet: Multipurpose Video Forensics Network using Multiple Forms of\n  Forensic Evidence","abstract":"  While videos can be falsified in many different ways, most existing forensic\nnetworks are specialized to detect only a single manipulation type (e.g.\ndeepfake, inpainting). This poses a significant issue as the manipulation used\nto falsify a video is not known a priori. To address this problem, we propose\nMVFNet - a multipurpose video forensics network capable of detecting multiple\ntypes of manipulations including inpainting, deepfakes, splicing, and editing.\nOur network does this by extracting and jointly analyzing a broad set of\nforensic feature modalities that capture both spatial and temporal anomalies in\nfalsified videos. To reliably detect and localize fake content of all shapes\nand sizes, our network employs a novel Multi-Scale Hierarchical Transformer\nmodule to identify forensic inconsistencies across multiple spatial scales.\nExperimental results show that our network obtains state-of-the-art performance\nin general scenarios where multiple different manipulations are possible, and\nrivals specialized detectors in targeted scenarios.\n","date":"2025-03-26"}
{"id":"2503.20992","title":"ReverBERT: A State Space Model for Efficient Text-Driven Speech Style\n  Transfer","abstract":"  Text-driven speech style transfer aims to mold the intonation, pace, and\ntimbre of a spoken utterance to match stylistic cues from text descriptions.\nWhile existing methods leverage large-scale neural architectures or pre-trained\nlanguage models, the computational costs often remain high. In this paper, we\npresent \\emph{ReverBERT}, an efficient framework for text-driven speech style\ntransfer that draws inspiration from a state space model (SSM) paradigm,\nloosely motivated by the image-based method of Wang and\nLiu~\\cite{wang2024stylemamba}. Unlike image domain techniques, our method\noperates in the speech space and integrates a discrete Fourier transform of\nlatent speech features to enable smooth and continuous style modulation. We\nalso propose a novel \\emph{Transformer-based SSM} layer for bridging textual\nstyle descriptors with acoustic attributes, dramatically reducing inference\ntime while preserving high-quality speech characteristics. Extensive\nexperiments on benchmark speech corpora demonstrate that \\emph{ReverBERT}\nsignificantly outperforms baselines in terms of naturalness, expressiveness,\nand computational efficiency. We release our model and code publicly to foster\nfurther research in text-driven speech style transfer.\n","date":"2025-03-26"}
{"id":"2503.20994","title":"Deep Learning for Forensic Identification of Source","abstract":"  We used contrastive neural networks to learn useful similarity scores between\nthe 144 cartridge casings in the NBIDE dataset, under the common-but-unknown\nsource paradigm. The common-but-unknown source problem is a problem archetype\nin forensics where the question is whether two objects share a common source\n(e.g. were two cartridge casings fired from the same firearm). Similarity\nscores are often used to interpret evidence under this paradigm. We directly\ncompared our results to a state-of-the-art algorithm, Congruent Matching Cells\n(CMC). When trained on the E3 dataset of 2967 cartridge casings, contrastive\nlearning achieved an ROC AUC of 0.892. The CMC algorithm achieved 0.867. We\nalso conducted an ablation study where we varied the neural network\narchitecture; specifically, the network's width or depth. The ablation study\nshowed that contrastive network performance results are somewhat robust to the\nnetwork architecture. This work was in part motivated by the use of similarity\nscores attained via contrastive learning for standard evidence interpretation\nmethods such as score-based likelihood ratios.\n","date":"2025-03-26"}
{"id":"2503.20995","title":"Multi-head Reward Aggregation Guided by Entropy","abstract":"  Aligning large language models (LLMs) with safety guidelines typically\ninvolves reinforcement learning from human feedback (RLHF), relying on\nhuman-generated preference annotations. However, assigning consistent overall\nquality ratings is challenging, prompting recent research to shift towards\ndetailed evaluations based on multiple specific safety criteria. This paper\nuncovers a consistent observation: safety rules characterized by high rating\nentropy are generally less reliable in identifying responses preferred by\nhumans. Leveraging this finding, we introduce ENCORE, a straightforward\nentropy-guided approach that composes multi-head rewards by downweighting rules\nexhibiting high rating entropy. Theoretically, we demonstrate that rules with\nelevated entropy naturally receive minimal weighting in the Bradley-Terry\noptimization framework, justifying our entropy-based penalization. Through\nextensive experiments on RewardBench safety tasks, our method significantly\nsurpasses several competitive baselines, including random weighting, uniform\nweighting, single-head Bradley-Terry models, and LLM-based judging methods. Our\nproposed approach is training-free, broadly applicable to various datasets, and\nmaintains interpretability, offering a practical and effective solution for\nmulti-attribute reward modeling.\n","date":"2025-03-26"}
{"id":"2503.20998","title":"CoMapGS: Covisibility Map-based Gaussian Splatting for Sparse Novel View\n  Synthesis","abstract":"  We propose Covisibility Map-based Gaussian Splatting (CoMapGS), designed to\nrecover underrepresented sparse regions in sparse novel view synthesis. CoMapGS\naddresses both high- and low-uncertainty regions by constructing covisibility\nmaps, enhancing initial point clouds, and applying uncertainty-aware weighted\nsupervision using a proximity classifier. Our contributions are threefold: (1)\nCoMapGS reframes novel view synthesis by leveraging covisibility maps as a core\ncomponent to address region-specific uncertainty; (2) Enhanced initial point\nclouds for both low- and high-uncertainty regions compensate for sparse\nCOLMAP-derived point clouds, improving reconstruction quality and benefiting\nfew-shot 3DGS methods; (3) Adaptive supervision with covisibility-score-based\nweighting and proximity classification achieves consistent performance gains\nacross scenes with varying sparsity scores derived from covisibility maps.\nExperimental results demonstrate that CoMapGS outperforms state-of-the-art\nmethods on datasets including Mip-NeRF 360 and LLFF.\n","date":"2025-03-25"}
{"id":"2503.21000","title":"Improving User Behavior Prediction: Leveraging Annotator Metadata in\n  Supervised Machine Learning Models","abstract":"  Supervised machine-learning models often underperform in predicting user\nbehaviors from conversational text, hindered by poor crowdsourced label quality\nand low NLP task accuracy. We introduce the Metadata-Sensitive\nWeighted-Encoding Ensemble Model (MSWEEM), which integrates annotator\nmeta-features like fatigue and speeding. First, our results show MSWEEM\noutperforms standard ensembles by 14\\% on held-out data and 12\\% on an\nalternative dataset. Second, we find that incorporating signals of annotator\nbehavior, such as speed and fatigue, significantly boosts model performance.\nThird, we find that annotators with higher qualifications, such as Master's,\ndeliver more consistent and faster annotations. Given the increasing\nuncertainty over annotation quality, our experiments show that understanding\nannotator patterns is crucial for enhancing model accuracy in user behavior\nprediction.\n","date":"2025-03-26"}
{"id":"2503.21003","title":"Forensic Self-Descriptions Are All You Need for Zero-Shot Detection,\n  Open-Set Source Attribution, and Clustering of AI-generated Images","abstract":"  The emergence of advanced AI-based tools to generate realistic images poses\nsignificant challenges for forensic detection and source attribution,\nespecially as new generative techniques appear rapidly. Traditional methods\noften fail to generalize to unseen generators due to reliance on features\nspecific to known sources during training. To address this problem, we propose\na novel approach that explicitly models forensic microstructures - subtle,\npixel-level patterns unique to the image creation process. Using only real\nimages in a self-supervised manner, we learn a set of diverse predictive\nfilters to extract residuals that capture different aspects of these\nmicrostructures. By jointly modeling these residuals across multiple scales, we\nobtain a compact model whose parameters constitute a unique forensic\nself-description for each image. This self-description enables us to perform\nzero-shot detection of synthetic images, open-set source attribution of images,\nand clustering based on source without prior knowledge. Extensive experiments\ndemonstrate that our method achieves superior accuracy and adaptability\ncompared to competing techniques, advancing the state of the art in synthetic\nmedia forensics.\n","date":"2025-03-26"}
{"id":"2503.21004","title":"Evaluating Large Language Models for Automated Clinical Abstraction in\n  Pulmonary Embolism Registries: Performance Across Model Sizes, Versions, and\n  Parameters","abstract":"  Pulmonary embolism (PE) is a leading cause of cardiovascular mortality, yet\nour understanding of optimal management remains limited due to heterogeneous\nand inaccessible radiology documentation. The PERT Consortium registry\nstandardizes PE management data but depends on resource-intensive manual\nabstraction. Large language models (LLMs) offer a scalable alternative for\nautomating concept extraction from computed tomography PE (CTPE) reports. This\nstudy evaluated the accuracy of LLMs in extracting PE-related concepts compared\nto a human-curated criterion standard. We retrospectively analyzed MIMIC-IV and\nDuke Health CTPE reports using multiple LLaMA models. Larger models (70B)\noutperformed smaller ones (8B), achieving kappa values of 0.98 (PE detection),\n0.65-0.75 (PE location), 0.48-0.51 (right heart strain), and 0.65-0.70 (image\nartifacts). Moderate temperature tuning (0.2-0.5) improved accuracy, while\nexcessive in-context examples reduced performance. A dual-model review\nframework achieved >80-90% precision. LLMs demonstrate strong potential for\nautomating PE registry abstraction, minimizing manual workload while preserving\naccuracy.\n","date":"2025-03-26"}
{"id":"2503.21011","title":"Can Large Language Models Predict Associations Among Human Attitudes?","abstract":"  Prior work has shown that large language models (LLMs) can predict human\nattitudes based on other attitudes, but this work has largely focused on\npredictions from highly similar and interrelated attitudes. In contrast, human\nattitudes are often strongly associated even across disparate and dissimilar\ntopics. Using a novel dataset of human responses toward diverse attitude\nstatements, we found that a frontier language model (GPT-4o) was able to\nrecreate the pairwise correlations among individual attitudes and to predict\nindividuals' attitudes from one another. Crucially, in an advance over prior\nwork, we tested GPT-4o's ability to predict in the absence of\nsurface-similarity between attitudes, finding that while surface similarity\nimproves prediction accuracy, the model was still highly-capable of generating\nmeaningful social inferences between dissimilar attitudes. Altogether, our\nfindings indicate that LLMs capture crucial aspects of the deeper, latent\nstructure of human belief systems.\n","date":"2025-03-26"}
{"id":"2503.21018","title":"Offline Action-Free Learning of Ex-BMDPs by Comparing Diverse Datasets","abstract":"  While sequential decision-making environments often involve high-dimensional\nobservations, not all features of these observations are relevant for control.\nIn particular, the observation space may capture factors of the environment\nwhich are not controllable by the agent, but which add complexity to the\nobservation space. The need to ignore these \"noise\" features in order to\noperate in a tractably-small state space poses a challenge for efficient policy\nlearning. Due to the abundance of video data available in many such\nenvironments, task-independent representation learning from action-free offline\ndata offers an attractive solution. However, recent work has highlighted\ntheoretical limitations in action-free learning under the Exogenous Block MDP\n(Ex-BMDP) model, where temporally-correlated noise features are present in the\nobservations. To address these limitations, we identify a realistic setting\nwhere representation learning in Ex-BMDPs becomes tractable: when action-free\nvideo data from multiple agents with differing policies are available.\nConcretely, this paper introduces CRAFT (Comparison-based Representations from\nAction-Free Trajectories), a sample-efficient algorithm leveraging differences\nin controllable feature dynamics across agents to learn representations. We\nprovide theoretical guarantees for CRAFT's performance and demonstrate its\nfeasibility on a toy example, offering a foundation for practical methods in\nsimilar settings.\n","date":"2025-03-26"}
{"id":"2503.21022","title":"Reconstructing Gridded Data from Higher Autocorrelations","abstract":"  The higher-order autocorrelations of integer-valued or rational-valued\ngridded data sets appear naturally in X-ray crystallography, and have\napplications in computer vision systems, correlation tomography, correlation\nspectroscopy, and pattern recognition. In this paper, we consider the problem\nof reconstructing a gridded data set from its higher-order autocorrelations. We\ndescribe an explicit reconstruction algorithm, and prove that the\nautocorrelations up to order 3r + 3 are always sufficient to determine the data\nup to translation, where r is the dimension of the grid. We also provide\nexamples of rational-valued gridded data sets which are not determined by their\nautocorrelations up to order 3r + 2.\n","date":"2025-03-26"}
{"id":"2503.21023","title":"Data Mixture Optimization: A Multi-fidelity Multi-scale Bayesian\n  Framework","abstract":"  Careful curation of data sources can significantly improve the performance of\nLLM pre-training, but predominant approaches rely heavily on intuition or\ncostly trial-and-error, making them difficult to generalize across different\ndata domains and downstream tasks. Although scaling laws can provide a\nprincipled and general approach for data curation, standard deterministic\nextrapolation from small-scale experiments to larger scales requires strong\nassumptions on the reliability of such extrapolation, whose brittleness has\nbeen highlighted in prior works. In this paper, we introduce a\n$\\textit{probabilistic extrapolation framework}$ for data mixture optimization\nthat avoids rigid assumptions and explicitly models the uncertainty in\nperformance across decision variables. We formulate data curation as a\nsequential decision-making problem$\\unicode{x2013}$multi-fidelity, multi-scale\nBayesian optimization$\\unicode{x2013}$where $\\{$data mixtures, model scale,\ntraining steps$\\}$ are adaptively selected to balance training cost and\npotential information gain. Our framework naturally gives rise to algorithm\nprototypes that leverage noisy information from inexpensive experiments to\nsystematically inform costly training decisions. To accelerate methodological\nprogress, we build a simulator based on 472 language model pre-training runs\nwith varying data compositions from the SlimPajama dataset. We observe that\neven simple kernels and acquisition functions can enable principled decisions\nacross training models from 20M to 1B parameters and achieve $\\textbf{2.6x}$\nand $\\textbf{3.3x}$ speedups compared to multi-fidelity BO and random search\nbaselines. Taken together, our framework underscores potential efficiency gains\nachievable by developing principled and transferable data mixture optimization\nmethods.\n","date":"2025-03-26"}
{"id":"2503.21025","title":"Improving Speech Recognition Accuracy Using Custom Language Models with\n  the Vosk Toolkit","abstract":"  Although speech recognition algorithms have developed quickly in recent\nyears, achieving high transcription accuracy across diverse audio formats and\nacoustic environments remains a major challenge. This work explores how\nincorporating custom language models with the open-source Vosk Toolkit can\nimprove speech-to-text accuracy in varied settings. Unlike many conventional\nsystems limited to specific audio types, this approach supports multiple audio\nformats such as WAV, MP3, FLAC, and OGG by using Python modules for\npreprocessing and format conversion.\n  A Python-based transcription pipeline was developed to process input audio,\nperform speech recognition using Vosk's KaldiRecognizer, and export the output\nto a DOCX file. Results showed that custom models reduced word error rates,\nespecially in domain-specific scenarios involving technical terminology, varied\naccents, or background noise. This work presents a cost-effective, offline\nsolution for high-accuracy transcription and opens up future opportunities for\nautomation and real-time applications.\n","date":"2025-03-26"}
{"id":"2503.21029","title":"Enhancing Korean Dependency Parsing with Morphosyntactic Features","abstract":"  This paper introduces UniDive for Korean, an integrated framework that\nbridges Universal Dependencies (UD) and Universal Morphology (UniMorph) to\nenhance the representation and processing of Korean {morphosyntax}. Korean's\nrich inflectional morphology and flexible word order pose challenges for\nexisting frameworks, which often treat morphology and syntax separately,\nleading to inconsistencies in linguistic analysis. UniDive unifies syntactic\nand morphological annotations by preserving syntactic dependencies while\nincorporating UniMorph-derived features, improving consistency in annotation.\nWe construct an integrated dataset and apply it to dependency parsing,\ndemonstrating that enriched morphosyntactic features enhance parsing accuracy,\nparticularly in distinguishing grammatical relations influenced by morphology.\nOur experiments, conducted with both encoder-only and decoder-only models,\nconfirm that explicit morphological information contributes to more accurate\nsyntactic analysis.\n","date":"2025-03-26"}
{"id":"2503.21036","title":"The Art of Tool Interface Design","abstract":"  We present an agentic framework, Thinker, which achieves state of art\nperformance in challenging reasoning tasks for realistic customer service\nscenarios that involve complex business logic and human interactions via long\nhorizons. On the $\\tau$-bench retail dataset, Thinker achieves 82.6\\% success\nrate with GPT-4o (version 2024-06-01) (baseline: 68.3\\%), and 81.9\\% success\nrate with Llama-3.1 405B (baseline: 49.6\\%), without any fine-tuning. Thinker\neffectively closes the gap in reasoning capabilities between the base models by\nintroducing proper structure.\n  The key features of the Thinker framework are: (1) State-Machine Augmented\nGeneration (SMAG), which represents business logic as state machines and the\nLLM uses state machines as tools. (2) Delegation of tasks from the main\nreasoning loop to LLM-powered tools. (3) Adaptive context management.\n  Our prompting-only solution achieves signficant gains, while still\nmaintaining a standard agentic architecture with a ReAct style reasoning loop.\nThe key is to innovate on the tool interface design, as exemplified by SMAG and\nthe LLM-powered tools.\n","date":"2025-03-26"}
{"id":"2503.21047","title":"World Model Agents with Change-Based Intrinsic Motivation","abstract":"  Sparse reward environments pose a significant challenge for reinforcement\nlearning due to the scarcity of feedback. Intrinsic motivation and transfer\nlearning have emerged as promising strategies to address this issue. Change\nBased Exploration Transfer (CBET), a technique that combines these two\napproaches for model-free algorithms, has shown potential in addressing sparse\nfeedback but its effectiveness with modern algorithms remains understudied.\nThis paper provides an adaptation of CBET for world model algorithms like\nDreamerV3 and compares the performance of DreamerV3 and IMPALA agents, both\nwith and without CBET, in the sparse reward environments of Crafter and\nMinigrid. Our tabula rasa results highlight the possibility of CBET improving\nDreamerV3's returns in Crafter but the algorithm attains a suboptimal policy in\nMinigrid with CBET further reducing returns. In the same vein, our transfer\nlearning experiments show that pre-training DreamerV3 with intrinsic rewards\ndoes not immediately lead to a policy that maximizes extrinsic rewards in\nMinigrid. Overall, our results suggest that CBET provides a positive impact on\nDreamerV3 in more complex environments like Crafter but may be detrimental in\nenvironments like Minigrid. In the latter case, the behaviours promoted by CBET\nin DreamerV3 may not align with the task objectives of the environment, leading\nto reduced returns and suboptimal policies.\n","date":"2025-03-26"}
{"id":"2503.21048","title":"Integrated utilization of equations and small dataset in the Koopman\n  operator: applications to forward and inverse Problems","abstract":"  In recent years, there has been a growing interest in data-driven approaches\nin physics, such as extended dynamic mode decomposition (EDMD). The EDMD\nalgorithm focuses on nonlinear time-evolution systems, and the constructed\nKoopman matrix yields the next-time prediction with only linear matrix-product\noperations. Note that data-driven approaches generally require a large dataset.\nHowever, assume that one has some prior knowledge, even if it may be ambiguous.\nThen, one could achieve sufficient learning from only a small dataset by taking\nadvantage of the prior knowledge. This paper yields methods for incorporating\nambiguous prior knowledge into the EDMD algorithm. The ambiguous prior\nknowledge in this paper corresponds to the underlying time-evolution equations\nwith unknown parameters. First, we apply the proposed method to forward\nproblems, i.e., prediction tasks. Second, we propose a scheme to apply the\nproposed method to inverse problems, i.e., parameter estimation tasks. We\ndemonstrate the learning with only a small dataset using guiding examples,\ni.e., the Duffing and the van der Pol systems.\n","date":"2025-03-26"}
{"id":"2503.21054","title":"Operating Room Workflow Analysis via Reasoning Segmentation over Digital\n  Twins","abstract":"  Analyzing operating room (OR) workflows to derive quantitative insights into\nOR efficiency is important for hospitals to maximize patient care and financial\nsustainability. Prior work on OR-level workflow analysis has relied on\nend-to-end deep neural networks. While these approaches work well in\nconstrained settings, they are limited to the conditions specified at\ndevelopment time and do not offer the flexibility necessary to accommodate the\nOR workflow analysis needs of various OR scenarios (e.g., large academic center\nvs. rural provider) without data collection, annotation, and retraining.\nReasoning segmentation (RS) based on foundation models offers this flexibility\nby enabling automated analysis of OR workflows from OR video feeds given only\nan implicit text query related to the objects of interest. Due to the reliance\non large language model (LLM) fine-tuning, current RS approaches struggle with\nreasoning about semantic\/spatial relationships and show limited generalization\nto OR video due to variations in visual characteristics and domain-specific\nterminology. To address these limitations, we first propose a novel digital\ntwin (DT) representation that preserves both semantic and spatial relationships\nbetween the various OR components. Then, building on this foundation, we\npropose ORDiRS (Operating Room Digital twin representation for Reasoning\nSegmentation), an LLM-tuning-free RS framework that reformulates RS into a\n\"reason-retrieval-synthesize\" paradigm. Finally, we present ORDiRS-Agent, an\nLLM-based agent that decomposes OR workflow analysis queries into manageable RS\nsub-queries and generates responses by combining detailed textual explanations\nwith supporting visual evidence from RS. Experimental results on both an\nin-house and a public OR dataset demonstrate that our ORDiRS achieves a cIoU\nimprovement of 6.12%-9.74% compared to the existing state-of-the-arts.\n","date":"2025-03-26"}
{"id":"2503.21055","title":"What Changed and What Could Have Changed? State-Change Counterfactuals\n  for Procedure-Aware Video Representation Learning","abstract":"  Understanding a procedural activity requires modeling both how action steps\ntransform the scene, and how evolving scene transformations can influence the\nsequence of action steps, even those that are accidental or erroneous. Existing\nwork has studied procedure-aware video representations by proposing novel\napproaches such as modeling the temporal order of actions and has not\nexplicitly learned the state changes (scene transformations). In this work, we\nstudy procedure-aware video representation learning by incorporating\nstate-change descriptions generated by Large Language Models (LLMs) as\nsupervision signals for video encoders. Moreover, we generate state-change\ncounterfactuals that simulate hypothesized failure outcomes, allowing models to\nlearn by imagining the unseen ``What if'' scenarios. This counterfactual\nreasoning facilitates the model's ability to understand the cause and effect of\neach step in an activity. To verify the procedure awareness of our model, we\nconduct extensive experiments on procedure-aware tasks, including temporal\naction segmentation and error detection. Our results demonstrate the\neffectiveness of the proposed state-change descriptions and their\ncounterfactuals and achieve significant improvements on multiple tasks. We will\nmake our source code and data publicly available soon.\n","date":"2025-03-27"}
{"id":"2503.21056","title":"Online Reasoning Video Segmentation with Just-in-Time Digital Twins","abstract":"  Reasoning segmentation (RS) aims to identify and segment objects of interest\nbased on implicit text queries. As such, RS is a catalyst for embodied AI\nagents, enabling them to interpret high-level commands without requiring\nexplicit step-by-step guidance. However, current RS approaches rely heavily on\nthe visual perception capabilities of multimodal large language models (LLMs),\nleading to several major limitations. First, they struggle with queries that\nrequire multiple steps of reasoning or those that involve complex\nspatial\/temporal relationships. Second, they necessitate LLM fine-tuning, which\nmay require frequent updates to maintain compatibility with contemporary LLMs\nand may increase risks of catastrophic forgetting during fine-tuning. Finally,\nbeing primarily designed for static images or offline video processing, they\nscale poorly to online video data. To address these limitations, we propose an\nagent framework that disentangles perception and reasoning for online video RS\nwithout LLM fine-tuning. Our innovation is the introduction of a just-in-time\ndigital twin concept, where -- given an implicit query -- a LLM plans the\nconstruction of a low-level scene representation from high-level video using\nspecialist vision models. We refer to this approach to creating a digital twin\nas \"just-in-time\" because the LLM planner will anticipate the need for specific\ninformation and only request this limited subset instead of always evaluating\nevery specialist model. The LLM then performs reasoning on this digital twin\nrepresentation to identify target objects. To evaluate our approach, we\nintroduce a new comprehensive video reasoning segmentation benchmark comprising\n200 videos with 895 implicit text queries. The benchmark spans three reasoning\ncategories (semantic, spatial, and temporal) with three different reasoning\nchain complexity.\n","date":"2025-03-27"}
{"id":"2503.21059","title":"Uncertainty propagation in feed-forward neural network models","abstract":"  We develop new uncertainty propagation methods for feed-forward neural\nnetwork architectures with leaky ReLU activation functions subject to random\nperturbations in the input vectors. In particular, we derive analytical\nexpressions for the probability density function (PDF) of the neural network\noutput and its statistical moments as a function of the input uncertainty and\nthe parameters of the network, i.e., weights and biases. A key finding is that\nan appropriate linearization of the leaky ReLU activation function yields\naccurate statistical results even for large perturbations in the input vectors.\nThis can be attributed to the way information propagates through the network.\nWe also propose new analytically tractable Gaussian copula surrogate models to\napproximate the full joint PDF of the neural network output. To validate our\ntheoretical results, we conduct Monte Carlo simulations and a thorough error\nanalysis on a multi-layer neural network representing a nonlinear\nintegro-differential operator between two polynomial function spaces. Our\nfindings demonstrate excellent agreement between the theoretical predictions\nand Monte Carlo simulations.\n","date":"2025-03-27"}
{"id":"2503.21061","title":"Neural Architecture Search by Learning a Hierarchical Search Space","abstract":"  Monte-Carlo Tree Search (MCTS) is a powerful tool for many non-differentiable\nsearch related problems such as adversarial games. However, the performance of\nsuch approach highly depends on the order of the nodes that are considered at\neach branching of the tree. If the first branches cannot distinguish between\npromising and deceiving configurations for the final task, the efficiency of\nthe search is exponentially reduced. In Neural Architecture Search (NAS), as\nonly the final architecture matters, the visiting order of the branching can be\noptimized to improve learning. In this paper, we study the application of MCTS\nto NAS for image classification. We analyze several sampling methods and\nbranching alternatives for MCTS and propose to learn the branching by\nhierarchical clustering of architectures based on their similarity. The\nsimilarity is measured by the pairwise distance of output vectors of\narchitectures. Extensive experiments on two challenging benchmarks on CIFAR10\nand ImageNet show that MCTS, if provided with a good branching hierarchy, can\nyield promising solutions more efficiently than other approaches for NAS\nproblems.\n","date":"2025-03-27"}
{"id":"2503.21067","title":"AskSport: Web Application for Sports Question-Answering","abstract":"  This paper introduces AskSport, a question-answering web application about\nsports. It allows users to ask questions using natural language and retrieve\nthe three most relevant answers, including related information and documents.\nThe paper describes the characteristics and functionalities of the application,\nincluding use cases demonstrating its ability to return names and numerical\nvalues. AskSport and its implementation are available for public access on\nHuggingFace.\n","date":"2025-03-27"}
{"id":"2503.21069","title":"Efficient Multi-Instance Generation with Janus-Pro-Dirven Prompt Parsing","abstract":"  Recent advances in text-guided diffusion models have revolutionized\nconditional image generation, yet they struggle to synthesize complex scenes\nwith multiple objects due to imprecise spatial grounding and limited\nscalability. We address these challenges through two key modules: 1)\nJanus-Pro-driven Prompt Parsing, a prompt-layout parsing module that bridges\ntext understanding and layout generation via a compact 1B-parameter\narchitecture, and 2) MIGLoRA, a parameter-efficient plug-in integrating\nLow-Rank Adaptation (LoRA) into UNet (SD1.5) and DiT (SD3) backbones. MIGLoRA\nis capable of preserving the base model's parameters and ensuring plug-and-play\nadaptability, minimizing architectural intrusion while enabling efficient\nfine-tuning. To support a comprehensive evaluation, we create DescripBox and\nDescripBox-1024, benchmarks that span diverse scenes and resolutions. The\nproposed method achieves state-of-the-art performance on COCO and LVIS\nbenchmarks while maintaining parameter efficiency, demonstrating superior\nlayout fidelity and scalability for open-world synthesis.\n","date":"2025-03-27"}
{"id":"2503.21071","title":"Purifying Approximate Differential Privacy with Randomized\n  Post-processing","abstract":"  We propose a framework to convert $(\\varepsilon, \\delta)$-approximate\nDifferential Privacy (DP) mechanisms into $(\\varepsilon, 0)$-pure DP\nmechanisms, a process we call ``purification''. This algorithmic technique\nleverages randomized post-processing with calibrated noise to eliminate the\n$\\delta$ parameter while preserving utility. By combining the tighter utility\nbounds and computational efficiency of approximate DP mechanisms with the\nstronger guarantees of pure DP, our approach achieves the best of both worlds.\nWe illustrate the applicability of this framework in various settings,\nincluding Differentially Private Empirical Risk Minimization (DP-ERM),\ndata-dependent DP mechanisms such as Propose-Test-Release (PTR), and query\nrelease tasks. To the best of our knowledge, this is the first work to provide\na systematic method for transforming approximate DP into pure DP while\nmaintaining competitive accuracy and computational efficiency.\n","date":"2025-03-27"}
{"id":"2503.21072","title":"HSLiNets: Evaluating Band Ordering Strategies in Hyperspectral and LiDAR\n  Fusion","abstract":"  The integration of hyperspectral imaging (HSI) and Light Detection and\nRanging (LiDAR) data provides complementary spectral and spatial information\nfor remote sensing applications. While previous studies have explored the role\nof band selection and grouping in HSI classification, little attention has been\ngiven to how the spectral sequence or band order affects classification\noutcomes when fused with LiDAR. In this work, we systematically investigate the\ninfluence of band order on HSI-LiDAR fusion performance. Through extensive\nexperiments, we demonstrate that band order significantly impacts\nclassification accuracy, revealing a previously overlooked factor in\nfusion-based models. Motivated by this observation, we propose a novel fusion\narchitecture that not only integrates HSI and LiDAR data but also learns from\nmultiple band order configurations. The proposed method enhances feature\nrepresentation by adaptively fusing different spectral sequences, leading to\nimproved classification accuracy. Experimental results on the Houston 2013 and\nTrento datasets show that our approach outperforms state-of-the-art fusion\nmodels. Data and code are available at https:\/\/github.com\/Judyxyang\/HSLiNets.\n","date":"2025-03-27"}
{"id":"2503.21073","title":"Shared Global and Local Geometry of Language Model Embeddings","abstract":"  Researchers have recently suggested that models share common representations.\nIn this work, we find that the token embeddings of language models exhibit\ncommon geometric structure. First, we find ``global'' similarities: token\nembeddings often share similar relative orientations. Next, we characterize\nlocal geometry in two ways: (1) by using Locally Linear Embeddings, and (2) by\ndefining a simple measure for the intrinsic dimension of each token embedding.\nOur intrinsic dimension measure demonstrates that token embeddings lie on a\nlower dimensional manifold. We qualitatively show that tokens with lower\nintrinsic dimensions often have semantically coherent clusters, while those\nwith higher intrinsic dimensions do not. Both characterizations allow us to\nfind similarities in the local geometry of token embeddings. Perhaps most\nsurprisingly, we find that alignment in token embeddings persists through the\nhidden states of language models, allowing us to develop an application for\ninterpretability. Namely, we empirically demonstrate that steering vectors from\none language model can be transferred to another, despite the two models having\ndifferent dimensions.\n","date":"2025-03-27"}
{"id":"2503.21074","title":"Rerouting Connection: Hybrid Computer Vision Analysis Reveals Visual\n  Similarity Between Indus and Tibetan-Yi Corridor Writing Systems","abstract":"  This thesis employs a hybrid CNN-Transformer architecture, in conjunction\nwith a detailed anthropological framework, to investigate potential historical\nconnections between the visual morphology of the Indus Valley script and\npictographic systems of the Tibetan-Yi Corridor. Through an ensemble\nmethodology of three target scripts across 15 independently trained models, we\ndemonstrate that Tibetan-Yi Corridor scripts exhibit approximately six-fold\nhigher visual similarity to the Indus script (61.7%-63.5%) than to the Bronze\nAge Proto-Cuneiform (10.2%-10.9%) or Proto-Elamite (7.6%-8.7%) systems.\nAdditionally and contrarily to our current understanding of the networks of the\nIndus Valley Civilization, the Indus script unexpectedly maps closer to\nTibetan-Yi Corridor scripts, with a mean cosine similarity of 0.629, than to\nthe aforementioned contemporaneous West Asian signaries, both of which recorded\nmean cosine similarities of 0.104 and 0.080 despite their close geographic\nproximity and evident trade relations. Across various dimensionality reduction\npractices and clustering methodologies, the Indus script consistently clusters\nclosest to Tibetan-Yi Corridor scripts. Our computational results align with\nqualitative observations of specific pictorial parallels in numeral systems,\ngender markers, and key iconographic elements; this is further supported by\narchaeological evidence of sustained contact networks along the ancient\nShu-Shendu road in tandem with the Indus Valley Civilization's decline,\nproviding a plausible transmission pathway. While alternative explanations\ncannot be ruled out, the specificity and consistency of observed similarities\nchallenge conventional narratives of isolated script development and suggest\nmore complex ancient cultural transmission networks between South and East Asia\nthan previously recognized.\n","date":"2025-03-27"}
{"id":"2503.21076","title":"KAC: Kolmogorov-Arnold Classifier for Continual Learning","abstract":"  Continual learning requires models to train continuously across consecutive\ntasks without forgetting. Most existing methods utilize linear classifiers,\nwhich struggle to maintain a stable classification space while learning new\ntasks. Inspired by the success of Kolmogorov-Arnold Networks (KAN) in\npreserving learning stability during simple continual regression tasks, we set\nout to explore their potential in more complex continual learning scenarios. In\nthis paper, we introduce the Kolmogorov-Arnold Classifier (KAC), a novel\nclassifier developed for continual learning based on the KAN structure. We\ndelve into the impact of KAN's spline functions and introduce Radial Basis\nFunctions (RBF) for improved compatibility with continual learning. We replace\nlinear classifiers with KAC in several recent approaches and conduct\nexperiments across various continual learning benchmarks, all of which\ndemonstrate performance improvements, highlighting the effectiveness and\nrobustness of KAC in continual learning. The code is available at\nhttps:\/\/github.com\/Ethanhuhuhu\/KAC.\n","date":"2025-03-27"}
{"id":"2503.21080","title":"EQ-Negotiator: An Emotion-Reasoning LLM Agent in Credit Dialogues","abstract":"  While large language model (LLM)-based chatbots have been applied for\neffective engagement in credit dialogues, their capacity for dynamic emotional\nexpression remains limited. Current agents primarily rely on passive empathy\nrather than affective reasoning. For instance, when faced with persistent\nclient negativity, the agent should employ strategic emotional adaptation by\nexpressing measured anger to discourage counterproductive behavior and guide\nthe conversation toward resolution. This context-aware emotional modulation is\nessential for imitating the nuanced decision-making of human negotiators. This\npaper introduces an EQ-negotiator that combines emotion sensing from\npre-trained language models (PLMs) with emotional reasoning based on Game\nTheory and Hidden Markov Models. It takes into account both the current and\nhistorical emotions of the client to better manage and address negative\nemotions during interactions. By fine-tuning pre-trained language models (PLMs)\non public emotion datasets and validating them on the credit dialogue datasets,\nour approach enables LLM-based agents to effectively capture shifts in client\nemotions and dynamically adjust their response tone based on our emotion\ndecision policies in real-world financial negotiations. This EQ-negotiator can\nalso help credit agencies foster positive client relationships, enhancing\nsatisfaction in credit services.\n","date":"2025-03-27"}
{"id":"2503.21082","title":"Can Video Diffusion Model Reconstruct 4D Geometry?","abstract":"  Reconstructing dynamic 3D scenes (i.e., 4D geometry) from monocular video is\nan important yet challenging problem. Conventional multiview geometry-based\napproaches often struggle with dynamic motion, whereas recent learning-based\nmethods either require specialized 4D representation or sophisticated\noptimization. In this paper, we present Sora3R, a novel framework that taps\ninto the rich spatiotemporal priors of large-scale video diffusion models to\ndirectly infer 4D pointmaps from casual videos. Sora3R follows a two-stage\npipeline: (1) we adapt a pointmap VAE from a pretrained video VAE, ensuring\ncompatibility between the geometry and video latent spaces; (2) we finetune a\ndiffusion backbone in combined video and pointmap latent space to generate\ncoherent 4D pointmaps for every frame. Sora3R operates in a fully feedforward\nmanner, requiring no external modules (e.g., depth, optical flow, or\nsegmentation) or iterative global alignment. Extensive experiments demonstrate\nthat Sora3R reliably recovers both camera poses and detailed scene geometry,\nachieving performance on par with state-of-the-art methods for dynamic 4D\nreconstruction across diverse scenarios.\n","date":"2025-03-27"}
{"id":"2503.21084","title":"Geographical hotspot prediction based on point cloud-voxel-community\n  partition clustering","abstract":"  Existing solutions to the hotspot prediction problem in the field of\ngeographic information remain at a relatively preliminary stage. This study\npresents a novel approach for detecting and predicting geographical hotspots,\nutilizing point cloud-voxel-community partition clustering. By analyzing\nhigh-dimensional data, we represent spatial information through point clouds,\nwhich are then subdivided into multiple voxels to enhance analytical\nefficiency. Our method identifies spatial voxels with similar characteristics\nthrough community partitioning, thereby revealing underlying patterns in\nhotspot distributions. Experimental results indicate that when applied to a\ndataset of archaeological sites in Turkey, our approach achieves a 19.31%\nincrease in processing speed, with an accuracy loss of merely 6%, outperforming\ntraditional clustering methods. This method not only provides a fresh\nperspective for hotspot prediction but also serves as an effective tool for\nhigh-dimensional data analysis.\n","date":"2025-03-27"}
{"id":"2503.21088","title":"ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging","abstract":"  This paper presents the ZJUKLAB team's submission for SemEval-2025 Task 4:\nUnlearning Sensitive Content from Large Language Models. This task aims to\nselectively erase sensitive knowledge from large language models, avoiding both\nover-forgetting and under-forgetting issues. We propose an unlearning system\nthat leverages Model Merging (specifically TIES-Merging), combining two\nspecialized models into a more balanced unlearned model. Our system achieves\ncompetitive results, ranking second among 26 teams, with an online score of\n0.944 for Task Aggregate and 0.487 for overall Aggregate. In this paper, we\nalso conduct local experiments and perform a comprehensive analysis of the\nunlearning process, examining performance trajectories, loss dynamics, and\nweight perspectives, along with several supplementary experiments, to\nunderstand the effectiveness of our method. Furthermore, we analyze the\nshortcomings of our method and evaluation metrics, emphasizing that MIA scores\nand ROUGE-based metrics alone are insufficient to fully evaluate successful\nunlearning. Finally, we emphasize the need for more comprehensive evaluation\nmethodologies and rethinking of unlearning objectives in future research. Code\nis available at https:\/\/github.com\/zjunlp\/unlearn\/tree\/main\/semeval25.\n","date":"2025-03-27"}
{"id":"2503.21095","title":"Confidence Adjusted Surprise Measure for Active Resourceful Trials\n  (CA-SMART): A Data-driven Active Learning Framework for Accelerating Material\n  Discovery under Resource Constraints","abstract":"  Accelerating the discovery and manufacturing of advanced materials with\nspecific properties is a critical yet formidable challenge due to vast search\nspace, high costs of experiments, and time-intensive nature of material\ncharacterization. In recent years, active learning, where a surrogate machine\nlearning (ML) model mimics the scientific discovery process of a human\nscientist, has emerged as a promising approach to address these challenges by\nguiding experimentation toward high-value outcomes with a limited budget. Among\nthe diverse active learning philosophies, the concept of surprise (capturing\nthe divergence between expected and observed outcomes) has demonstrated\nsignificant potential to drive experimental trials and refine predictive\nmodels. Scientific discovery often stems from surprise thereby making it a\nnatural driver to guide the search process. Despite its promise, prior studies\nleveraging surprise metrics such as Shannon and Bayesian surprise lack\nmechanisms to account for prior confidence, leading to excessive exploration of\nuncertain regions that may not yield useful information. To address this, we\npropose the Confidence-Adjusted Surprise Measure for Active Resourceful Trials\n(CA-SMART), a novel Bayesian active learning framework tailored for optimizing\ndata-driven experimentation. On a high level, CA-SMART incorporates\nConfidence-Adjusted Surprise (CAS) to dynamically balance exploration and\nexploitation by amplifying surprises in regions where the model is more certain\nwhile discounting them in highly uncertain areas. We evaluated CA-SMART on two\nbenchmark functions (Six-Hump Camelback and Griewank) and in predicting the\nfatigue strength of steel. The results demonstrate superior accuracy and\nefficiency compared to traditional surprise metrics, standard Bayesian\nOptimization (BO) acquisition functions and conventional ML methods.\n","date":"2025-03-27"}
{"id":"2503.21098","title":"Alleviating LLM-based Generative Retrieval Hallucination in Alipay\n  Search","abstract":"  Generative retrieval (GR) has revolutionized document retrieval with the\nadvent of large language models (LLMs), and LLM-based GR is gradually being\nadopted by the industry. Despite its remarkable advantages and potential,\nLLM-based GR suffers from hallucination and generates documents that are\nirrelevant to the query in some instances, severely challenging its credibility\nin practical applications. We thereby propose an optimized GR framework\ndesigned to alleviate retrieval hallucination, which integrates knowledge\ndistillation reasoning in model training and incorporate decision agent to\nfurther improve retrieval precision. Specifically, we employ LLMs to assess and\nreason GR retrieved query-document (q-d) pairs, and then distill the reasoning\ndata as transferred knowledge to the GR model. Moreover, we utilize a decision\nagent as post-processing to extend the GR retrieved documents through retrieval\nmodel and select the most relevant ones from multi perspectives as the final\ngenerative retrieval result. Extensive offline experiments on real-world\ndatasets and online A\/B tests on Fund Search and Insurance Search in Alipay\ndemonstrate our framework's superiority and effectiveness in improving search\nquality and conversion gains.\n","date":"2025-03-27"}
{"id":"2503.21099","title":"Learning Class Prototypes for Unified Sparse Supervised 3D Object\n  Detection","abstract":"  Both indoor and outdoor scene perceptions are essential for embodied\nintelligence. However, current sparse supervised 3D object detection methods\nfocus solely on outdoor scenes without considering indoor settings. To this\nend, we propose a unified sparse supervised 3D object detection method for both\nindoor and outdoor scenes through learning class prototypes to effectively\nutilize unlabeled objects. Specifically, we first propose a prototype-based\nobject mining module that converts the unlabeled object mining into a matching\nproblem between class prototypes and unlabeled features. By using optimal\ntransport matching results, we assign prototype labels to high-confidence\nfeatures, thereby achieving the mining of unlabeled objects. We then present a\nmulti-label cooperative refinement module to effectively recover missed\ndetections through pseudo label quality control and prototype label\ncooperation. Experiments show that our method achieves state-of-the-art\nperformance under the one object per scene sparse supervised setting across\nindoor and outdoor datasets. With only one labeled object per scene, our method\nachieves about 78%, 90%, and 96% performance compared to the fully supervised\ndetector on ScanNet V2, SUN RGB-D, and KITTI, respectively, highlighting the\nscalability of our method. Code is available at\nhttps:\/\/github.com\/zyrant\/CPDet3D.\n","date":"2025-03-27"}
{"id":"2503.21103","title":"Low Stein Discrepancy via Message-Passing Monte Carlo","abstract":"  Message-Passing Monte Carlo (MPMC) was recently introduced as a novel\nlow-discrepancy sampling approach leveraging tools from geometric deep\nlearning. While originally designed for generating uniform point sets, we\nextend this framework to sample from general multivariate probability\ndistributions with known probability density function. Our proposed method,\nStein-Message-Passing Monte Carlo (Stein-MPMC), minimizes a kernelized Stein\ndiscrepancy, ensuring improved sample quality. Finally, we show that Stein-MPMC\noutperforms competing methods, such as Stein Variational Gradient Descent and\n(greedy) Stein Points, by achieving a lower Stein discrepancy.\n","date":"2025-03-27"}
{"id":"2503.21104","title":"StyledStreets: Multi-style Street Simulator with Spatial and Temporal\n  Consistency","abstract":"  Urban scene reconstruction requires modeling both static infrastructure and\ndynamic elements while supporting diverse environmental conditions. We present\n\\textbf{StyledStreets}, a multi-style street simulator that achieves\ninstruction-driven scene editing with guaranteed spatial and temporal\nconsistency. Building on a state-of-the-art Gaussian Splatting framework for\nstreet scenarios enhanced by our proposed pose optimization and multi-view\ntraining, our method enables photorealistic style transfers across seasons,\nweather conditions, and camera setups through three key innovations: First, a\nhybrid embedding scheme disentangles persistent scene geometry from transient\nstyle attributes, allowing realistic environmental edits while preserving\nstructural integrity. Second, uncertainty-aware rendering mitigates supervision\nnoise from diffusion priors, enabling robust training across extreme style\nvariations. Third, a unified parametric model prevents geometric drift through\nregularized updates, maintaining multi-view consistency across seven\nvehicle-mounted cameras.\n  Our framework preserves the original scene's motion patterns and geometric\nrelationships. Qualitative results demonstrate plausible transitions between\ndiverse conditions (snow, sandstorm, night), while quantitative evaluations\nshow state-of-the-art geometric accuracy under style transfers. The approach\nestablishes new capabilities for urban simulation, with applications in\nautonomous vehicle testing and augmented reality systems requiring reliable\nenvironmental consistency. Codes will be publicly available upon publication.\n","date":"2025-03-27"}
{"id":"2503.21105","title":"AugWard: Augmentation-Aware Representation Learning for Accurate Graph\n  Classification","abstract":"  How can we accurately classify graphs? Graph classification is a pivotal task\nin data mining with applications in social network analysis, web analysis, drug\ndiscovery, molecular property prediction, etc. Graph neural networks have\nachieved the state-of-the-art performance in graph classification, but they\nconsistently struggle with overfitting. To mitigate overfitting, researchers\nhave introduced various representation learning methods utilizing graph\naugmentation. However, existing methods rely on simplistic use of graph\naugmentation, which loses augmentation-induced differences and limits the\nexpressiveness of representations.\n  In this paper, we propose AugWard (Augmentation-Aware Training with Graph\nDistance and Consistency Regularization), a novel graph representation learning\nframework that carefully considers the diversity introduced by graph\naugmentation. AugWard applies augmentation-aware training to predict the graph\ndistance between the augmented graph and its original one, aligning the\nrepresentation difference directly with graph distance at both feature and\nstructure levels. Furthermore, AugWard employs consistency regularization to\nencourage the classifier to handle richer representations. Experimental results\nshow that AugWard gives the state-of-the-art performance in supervised,\nsemi-supervised graph classification, and transfer learning.\n","date":"2025-03-27"}
{"id":"2503.21106","title":"Function Alignment: A New Theory of Mind and Intelligence, Part I:\n  Foundations","abstract":"  This paper introduces function alignment, a novel theory of mind and\nintelligence that is both intuitively compelling and structurally grounded. It\nexplicitly models how meaning, interpretation, and analogy emerge from\ninteractions among layered representations, forming a coherent framework\ncapable not only of modeling minds but also of serving as a blueprint for\nbuilding them. One of the key theoretical insights derived from function\nalignment is bounded interpretability, which provides a unified explanation for\npreviously fragmented ideas in cognitive science, such as bounded rationality,\nsymbol grounding, and analogy-making. Beyond modeling, the function alignment\nframework bridges disciplines often kept apart, linking computational\narchitecture, psychological theory, and even contemplative traditions such as\nZen. Rather than building on any philosophical systems, it offers a structural\nfoundation upon which multiple ways of understanding the mind may be\nreconstructed.\n","date":"2025-03-27"}
{"id":"2503.21109","title":"Optimizing Multi-DNN Inference on Mobile Devices through Heterogeneous\n  Processor Co-Execution","abstract":"  Deep Neural Networks (DNNs) are increasingly deployed across diverse\nindustries, driving demand for mobile device support. However, existing mobile\ninference frameworks often rely on a single processor per model, limiting\nhardware utilization and causing suboptimal performance and energy efficiency.\nExpanding DNN accessibility on mobile platforms requires adaptive,\nresource-efficient solutions to meet rising computational needs without\ncompromising functionality. Parallel inference of multiple DNNs on\nheterogeneous processors remains challenging. Some works partition DNN\noperations into subgraphs for parallel execution across processors, but these\noften create excessive subgraphs based only on hardware compatibility,\nincreasing scheduling complexity and memory overhead.\n  To address this, we propose an Advanced Multi-DNN Model Scheduling (ADMS)\nstrategy for optimizing multi-DNN inference on mobile heterogeneous processors.\nADMS constructs an optimal subgraph partitioning strategy offline, balancing\nhardware operation support and scheduling granularity, and uses a\nprocessor-state-aware algorithm to dynamically adjust workloads based on\nreal-time conditions. This ensures efficient workload distribution and\nmaximizes processor utilization. Experiments show ADMS reduces multi-DNN\ninference latency by 4.04 times compared to vanilla frameworks.\n","date":"2025-03-27"}
{"id":"2503.21114","title":"Measuring and Analyzing Subjective Uncertainty in Scientific\n  Communications","abstract":"  Uncertainty of scientific findings are typically reported through statistical\nmetrics such as $p$-values, confidence intervals, etc. The magnitude of this\nobjective uncertainty is reflected in the language used by the authors to\nreport their findings primarily through expressions carrying\nuncertainty-inducing terms or phrases. This language uncertainty is a\nsubjective concept and is highly dependent on the writing style of the authors.\nThere is evidence that such subjective uncertainty influences the impact of\nscience on public audience. In this work, we turned our focus to scientists\nthemselves, and measured\/analyzed the subjective uncertainty and its impact\nwithin scientific communities across different disciplines. We showed that the\nlevel of this type of uncertainty varies significantly across different fields,\nyears of publication and geographical locations. We also studied the\ncorrelation between subjective uncertainty and several bibliographical metrics,\nsuch as number\/gender of authors, centrality of the field's community, citation\ncount, etc. The underlying patterns identified in this work are useful in\nidentification and documentation of linguistic norms in scientific\ncommunication in different communities\/societies.\n","date":"2025-03-27"}
{"id":"2503.21115","title":"Leveraging Large Language Models for Risk Assessment in Hyperconnected\n  Logistic Hub Network Deployment","abstract":"  The growing emphasis on energy efficiency and environmental sustainability in\nglobal supply chains introduces new challenges in the deployment of\nhyperconnected logistic hub networks. In current volatile, uncertain, complex,\nand ambiguous (VUCA) environments, dynamic risk assessment becomes essential to\nensure successful hub deployment. However, traditional methods often struggle\nto effectively capture and analyze unstructured information. In this paper, we\ndesign an Large Language Model (LLM)-driven risk assessment pipeline integrated\nwith multiple analytical tools to evaluate logistic hub deployment. This\nframework enables LLMs to systematically identify potential risks by analyzing\nunstructured data, such as geopolitical instability, financial trends,\nhistorical storm events, traffic conditions, and emerging risks from news\nsources. These data are processed through a suite of analytical tools, which\nare automatically called by LLMs to support a structured and data-driven\ndecision-making process for logistic hub selection. In addition, we design\nprompts that instruct LLMs to leverage these tools for assessing the\nfeasibility of hub selection by evaluating various risk types and levels.\nThrough risk-based similarity analysis, LLMs cluster logistic hubs with\ncomparable risk profiles, enabling a structured approach to risk assessment. In\nconclusion, the framework incorporates scalability with long-term memory and\nenhances decision-making through explanation and interpretation, enabling\ncomprehensive risk assessments for logistic hub deployment in hyperconnected\nsupply chain networks.\n","date":"2025-03-27"}
{"id":"2503.21122","title":"One Snapshot is All You Need: A Generalized Method for mmWave Signal\n  Generation","abstract":"  Wireless sensing systems, particularly those using mmWave technology, offer\ndistinct advantages over traditional vision-based approaches, such as enhanced\nprivacy and effectiveness in poor lighting conditions. These systems,\nleveraging FMCW signals, have shown success in human-centric applications like\nlocalization, gesture recognition, and so on. However, comprehensive mmWave\ndatasets for diverse applications are scarce, often constrained by\npre-processed signatures (e.g., point clouds or RA heatmaps) and inconsistent\nannotation formats. To overcome these limitations, we propose mmGen, a novel\nand generalized framework tailored for full-scene mmWave signal generation. By\nconstructing physical signal transmission models, mmGen synthesizes\nhuman-reflected and environment-reflected mmWave signals from the constructed\n3D meshes. Additionally, we incorporate methods to account for material\nproperties, antenna gains, and multipath reflections, enhancing the realism of\nthe synthesized signals. We conduct extensive experiments using a prototype\nsystem with commercial mmWave devices and Kinect sensors. The results show that\nthe average similarity of Range-Angle and micro-Doppler signatures between the\nsynthesized and real-captured signals across three different environments\nexceeds 0.91 and 0.89, respectively, demonstrating the effectiveness and\npractical applicability of mmGen.\n","date":"2025-03-27"}
{"id":"2503.21124","title":"AdaMHF: Adaptive Multimodal Hierarchical Fusion for Survival Prediction","abstract":"  The integration of pathologic images and genomic data for survival analysis\nhas gained increasing attention with advances in multimodal learning. However,\ncurrent methods often ignore biological characteristics, such as heterogeneity\nand sparsity, both within and across modalities, ultimately limiting their\nadaptability to clinical practice. To address these challenges, we propose\nAdaMHF: Adaptive Multimodal Hierarchical Fusion, a framework designed for\nefficient, comprehensive, and tailored feature extraction and fusion. AdaMHF is\nspecifically adapted to the uniqueness of medical data, enabling accurate\npredictions with minimal resource consumption, even under challenging scenarios\nwith missing modalities. Initially, AdaMHF employs an experts expansion and\nresidual structure to activate specialized experts for extracting heterogeneous\nand sparse features. Extracted tokens undergo refinement via selection and\naggregation, reducing the weight of non-dominant features while preserving\ncomprehensive information. Subsequently, the encoded features are\nhierarchically fused, allowing multi-grained interactions across modalities to\nbe captured. Furthermore, we introduce a survival prediction benchmark designed\nto resolve scenarios with missing modalities, mirroring real-world clinical\nconditions. Extensive experiments on TCGA datasets demonstrate that AdaMHF\nsurpasses current state-of-the-art (SOTA) methods, showcasing exceptional\nperformance in both complete and incomplete modality settings.\n","date":"2025-03-27"}
{"id":"2503.21125","title":"Omni-AD: Learning to Reconstruct Global and Local Features for\n  Multi-class Anomaly Detection","abstract":"  In multi-class unsupervised anomaly detection(MUAD), reconstruction-based\nmethods learn to map input images to normal patterns to identify anomalous\npixels. However, this strategy easily falls into the well-known \"learning\nshortcut\" issue when decoders fail to capture normal patterns and reconstruct\nboth normal and abnormal samples naively. To address that, we propose to learn\nthe input features in global and local manners, forcing the network to memorize\nthe normal patterns more comprehensively. Specifically, we design a two-branch\ndecoder block, named Omni-block. One branch corresponds to global feature\nlearning, where we serialize two self-attention blocks but replace the query\nand (key, value) with learnable tokens, respectively, thus capturing global\nfeatures of normal patterns concisely and thoroughly. The local branch\ncomprises depth-separable convolutions, whose locality enables effective and\nefficient learning of local features for normal patterns. By stacking\nOmni-blocks, we build a framework, Omni-AD, to learn normal patterns of\ndifferent granularity and reconstruct them progressively. Comprehensive\nexperiments on public anomaly detection benchmarks show that our method\noutperforms state-of-the-art approaches in MUAD. Code is available at\nhttps:\/\/github.com\/easyoo\/Omni-AD.git\n","date":"2025-03-27"}
{"id":"2503.21127","title":"Collaborative Evolution: Multi-Round Learning Between Large and Small\n  Language Models for Emergent Fake News Detection","abstract":"  The proliferation of fake news on social media platforms has exerted a\nsubstantial influence on society, leading to discernible impacts and\ndeleterious consequences. Conventional deep learning methodologies employing\nsmall language models (SLMs) suffer from the necessity for extensive supervised\ntraining and the challenge of adapting to rapidly evolving circumstances. Large\nlanguage models (LLMs), despite their robust zero-shot capabilities, have\nfallen short in effectively identifying fake news due to a lack of pertinent\ndemonstrations and the dynamic nature of knowledge. In this paper, a novel\nframework Multi-Round Collaboration Detection (MRCD) is proposed to address\nthese aforementioned limitations. The MRCD framework is capable of enjoying the\nmerits from both LLMs and SLMs by integrating their generalization abilities\nand specialized functionalities, respectively. Our approach features a\ntwo-stage retrieval module that selects relevant and up-to-date demonstrations\nand knowledge, enhancing in-context learning for better detection of emerging\nnews events. We further design a multi-round learning framework to ensure more\nreliable detection results. Our framework MRCD achieves SOTA results on two\nreal-world datasets Pheme and Twitter16, with accuracy improvements of 7.4\\%\nand 12.8\\% compared to using only SLMs, which effectively addresses the\nlimitations of current models and improves the detection of emergent fake news.\n","date":"2025-03-27"}
{"id":"2503.21128","title":"Squared families: Searching beyond regular probability models","abstract":"  We introduce squared families, which are families of probability densities\nobtained by squaring a linear transformation of a statistic. Squared families\nare singular, however their singularity can easily be handled so that they form\nregular models. After handling the singularity, squared families possess many\nconvenient properties. Their Fisher information is a conformal transformation\nof the Hessian metric induced from a Bregman generator. The Bregman generator\nis the normalising constant, and yields a statistical divergence on the family.\nThe normalising constant admits a helpful parameter-integral factorisation,\nmeaning that only one parameter-independent integral needs to be computed for\nall normalising constants in the family, unlike in exponential families.\nFinally, the squared family kernel is the only integral that needs to be\ncomputed for the Fisher information, statistical divergence and normalising\nconstant. We then describe how squared families are special in the broader\nclass of $g$-families, which are obtained by applying a sufficiently regular\nfunction $g$ to a linear transformation of a statistic. After removing special\nsingularities, positively homogeneous families and exponential families are the\nonly $g$-families for which the Fisher information is a conformal\ntransformation of the Hessian metric, where the generator depends on the\nparameter only through the normalising constant. Even-order monomial families\nalso admit parameter-integral factorisations, unlike exponential families. We\nstudy parameter estimation and density estimation in squared families, in the\nwell-specified and misspecified settings. We use a universal approximation\nproperty to show that squared families can learn sufficiently well-behaved\ntarget densities at a rate of $\\mathcal{O}(N^{-1\/2})+C n^{-1\/4}$, where $N$ is\nthe number of datapoints, $n$ is the number of parameters, and $C$ is some\nconstant.\n","date":"2025-03-27"}
{"id":"2503.21130","title":"VideoMix: Aggregating How-To Videos for Task-Oriented Learning","abstract":"  Tutorial videos are a valuable resource for people looking to learn new\ntasks. People often learn these skills by viewing multiple tutorial videos to\nget an overall understanding of a task by looking at different approaches to\nachieve the task. However, navigating through multiple videos can be\ntime-consuming and mentally demanding as these videos are scattered and not\neasy to skim. We propose VideoMix, a system that helps users gain a holistic\nunderstanding of a how-to task by aggregating information from multiple videos\non the task. Insights from our formative study (N=12) reveal that learners\nvalue understanding potential outcomes, required materials, alternative\nmethods, and important details shared by different videos. Powered by a\nVision-Language Model pipeline, VideoMix extracts and organizes this\ninformation, presenting concise textual summaries alongside relevant video\nclips, enabling users to quickly digest and navigate the content. A comparative\nuser study (N=12) demonstrated that VideoMix enabled participants to gain a\nmore comprehensive understanding of tasks with greater efficiency than a\nbaseline video interface, where videos are viewed independently. Our findings\nhighlight the potential of a task-oriented, multi-video approach where videos\nare organized around a shared goal, offering an enhanced alternative to\nconventional video-based learning.\n","date":"2025-03-27"}
{"id":"2503.21135","title":"MoQa: Rethinking MoE Quantization with Multi-stage Data-model\n  Distribution Awareness","abstract":"  With the advances in artificial intelligence, Mix-of-Experts (MoE) has become\nthe main form of Large Language Models (LLMs), and its demand for model\ncompression is increasing. Quantization is an effective method that not only\ncompresses the models but also significantly accelerates their performance.\nExisting quantization methods have gradually shifted the focus from parameter\nscaling to the analysis of data distributions. However, their analysis is\ndesigned for dense LLMs and relies on the simple one-model-all-data mapping,\nwhich is unsuitable for MoEs. This paper proposes a new quantization framework\ncalled MoQa. MoQa decouples the data-model distribution complexity of MoEs in\nmultiple analysis stages, quantitively revealing the dynamics during sparse\ndata activation, data-parameter mapping, and inter-expert correlations. Based\non these, MoQa identifies particular experts' and parameters' significance with\noptimal data-model distribution awareness and proposes a series of fine-grained\nmix-quantization strategies adaptive to various data activation and expert\ncombination scenarios. Moreover, MoQa discusses the limitations of existing\nquantization and analyzes the impact of each stage analysis, showing novel\ninsights for MoE quantization. Experiments show that MoQa achieves a 1.69~2.18\nperplexity decrease in language modeling tasks and a 1.58%~8.91% accuracy\nimprovement in zero-shot inference tasks. We believe MoQa will play a role in\nfuture MoE construction, optimization, and compression.\n","date":"2025-03-27"}
{"id":"2503.21138","title":"A computational theory of evaluation for parameterisable subject","abstract":"  Evaluation is critical to advance decision making across domains, yet\nexisting methodologies often struggle to balance theoretical rigor and\npractical scalability. In order to reduce the cost of experimental evaluation,\nwe introduce a computational theory of evaluation for parameterisable subjects.\nWe prove upper bounds of generalized evaluation error and generalized causal\neffect error of evaluation metric on subject. We also prove efficiency, and\nconsistency to estimated causal effect of subject on metric by prediction. To\noptimize evaluation models, we propose a meta-learner to handle heterogeneous\nevaluation subjects space. Comparing with other computational approaches, our\n(conditional) evaluation model reduced 24.1%-99.0% evaluation errors across 12\nscenes, including individual medicine, scientific simulation, business\nactivities, and quantum trade. The evaluation time is reduced 3-7 order of\nmagnitude comparing with experiments or simulations.\n","date":"2025-03-27"}
{"id":"2503.21140","title":"Recurrent Feature Mining and Keypoint Mixup Padding for\n  Category-Agnostic Pose Estimation","abstract":"  Category-agnostic pose estimation aims to locate keypoints on query images\naccording to a few annotated support images for arbitrary novel classes.\nExisting methods generally extract support features via heatmap pooling, and\nobtain interacted features from support and query via cross-attention. Hence,\nthese works neglect to mine fine-grained and structure-aware (FGSA) features\nfrom both support and query images, which are crucial for pixel-level keypoint\nlocalization. To this end, we propose a novel yet concise framework, which\nrecurrently mines FGSA features from both support and query images.\nSpecifically, we design a FGSA mining module based on deformable attention\nmechanism. On the one hand, we mine fine-grained features by applying\ndeformable attention head over multi-scale feature maps. On the other hand, we\nmine structure-aware features by offsetting the reference points of keypoints\nto their linked keypoints. By means of above module, we recurrently mine FGSA\nfeatures from support and query images, and thus obtain better support features\nand query estimations. In addition, we propose to use mixup keypoints to pad\nvarious classes to a unified keypoint number, which could provide richer\nsupervision than the zero padding used in existing works. We conduct extensive\nexperiments and in-depth studies on large-scale MP-100 dataset, and outperform\nSOTA method dramatically (+3.2\\%PCK@0.05). Code is avaiable at\nhttps:\/\/github.com\/chenbys\/FMMP.\n","date":"2025-03-27"}
{"id":"2503.21141","title":"Safe Human Robot Navigation in Warehouse Scenario","abstract":"  The integration of autonomous mobile robots (AMRs) in industrial\nenvironments, particularly warehouses, has revolutionized logistics and\noperational efficiency. However, ensuring the safety of human workers in\ndynamic, shared spaces remains a critical challenge. This work proposes a novel\nmethodology that leverages control barrier functions (CBFs) to enhance safety\nin warehouse navigation. By integrating learning-based CBFs with the Open\nRobotics Middleware Framework (OpenRMF), the system achieves adaptive and\nsafety-enhanced controls in multi-robot, multi-agent scenarios. Experiments\nconducted using various robot platforms demonstrate the efficacy of the\nproposed approach in avoiding static and dynamic obstacles, including human\npedestrians. Our experiments evaluate different scenarios in which the number\nof robots, robot platforms, speed, and number of obstacles are varied, from\nwhich we achieve promising performance.\n","date":"2025-03-27"}
{"id":"2503.21144","title":"ChatAnyone: Stylized Real-time Portrait Video Generation with\n  Hierarchical Motion Diffusion Model","abstract":"  Real-time interactive video-chat portraits have been increasingly recognized\nas the future trend, particularly due to the remarkable progress made in text\nand voice chat technologies. However, existing methods primarily focus on\nreal-time generation of head movements, but struggle to produce synchronized\nbody motions that match these head actions. Additionally, achieving\nfine-grained control over the speaking style and nuances of facial expressions\nremains a challenge. To address these limitations, we introduce a novel\nframework for stylized real-time portrait video generation, enabling expressive\nand flexible video chat that extends from talking head to upper-body\ninteraction. Our approach consists of the following two stages. The first stage\ninvolves efficient hierarchical motion diffusion models, that take both\nexplicit and implicit motion representations into account based on audio\ninputs, which can generate a diverse range of facial expressions with stylistic\ncontrol and synchronization between head and body movements. The second stage\naims to generate portrait video featuring upper-body movements, including hand\ngestures. We inject explicit hand control signals into the generator to produce\nmore detailed hand movements, and further perform face refinement to enhance\nthe overall realism and expressiveness of the portrait video. Additionally, our\napproach supports efficient and continuous generation of upper-body portrait\nvideo in maximum 512 * 768 resolution at up to 30fps on 4090 GPU, supporting\ninteractive video-chat in real-time. Experimental results demonstrate the\ncapability of our approach to produce portrait videos with rich expressiveness\nand natural upper-body movements.\n","date":"2025-03-27"}
{"id":"2503.21150","title":"The Devil is in Low-Level Features for Cross-Domain Few-Shot\n  Segmentation","abstract":"  Cross-Domain Few-Shot Segmentation (CDFSS) is proposed to transfer the\npixel-level segmentation capabilities learned from large-scale source-domain\ndatasets to downstream target-domain datasets, with only a few annotated images\nper class. In this paper, we focus on a well-observed but unresolved phenomenon\nin CDFSS: for target domains, particularly those distant from the source\ndomain, segmentation performance peaks at the very early epochs, and declines\nsharply as the source-domain training proceeds. We delve into this phenomenon\nfor an interpretation: low-level features are vulnerable to domain shifts,\nleading to sharper loss landscapes during the source-domain training, which is\nthe devil of CDFSS. Based on this phenomenon and interpretation, we further\npropose a method that includes two plug-and-play modules: one to flatten the\nloss landscapes for low-level features during source-domain training as a novel\nsharpness-aware minimization method, and the other to directly supplement\ntarget-domain information to the model during target-domain testing by\nlow-level-based calibration. Extensive experiments on four target datasets\nvalidate our rationale and demonstrate that our method surpasses the\nstate-of-the-art method in CDFSS signifcantly by 3.71% and 5.34% average MIoU\nin 1-shot and 5-shot scenarios, respectively.\n","date":"2025-03-27"}
{"id":"2503.21153","title":"Unveiling the Power of Uncertainty: A Journey into Bayesian Neural\n  Networks for Stellar dating","abstract":"  Context: Astronomy and astrophysics demand rigorous handling of uncertainties\nto ensure the credibility of outcomes. The growing integration of artificial\nintelligence offers a novel avenue to address this necessity. This convergence\npresents an opportunity to create advanced models capable of quantifying\ndiverse sources of uncertainty and automating complex data relationship\nexploration.\n  What: We introduce a hierarchical Bayesian architecture whose probabilistic\nrelationships are modeled by neural networks, designed to forecast stellar\nattributes such as mass, radius, and age (our main target). This architecture\nhandles both observational uncertainties stemming from measurements and\nepistemic uncertainties inherent in the predictive model itself. As a result,\nour system generates distributions that encapsulate the potential range of\nvalues for our predictions, providing a comprehensive understanding of their\nvariability and robustness.\n  Methods: Our focus is on dating main sequence stars using a technique known\nas Chemical Clocks, which serves as both our primary astronomical challenge and\na model prototype. In this work, we use hierarchical architectures to account\nfor correlations between stellar parameters and optimize information extraction\nfrom our dataset. We also employ Bayesian neural networks for their versatility\nand flexibility in capturing complex data relationships.\n  Results: By integrating our machine learning algorithm into a Bayesian\nframework, we have successfully propagated errors consistently and managed\nuncertainty treatment effectively, resulting in predictions characterized by\nbroader uncertainty margins. This approach facilitates more conservative\nestimates in stellar dating. Our architecture achieves age predictions with a\nmean absolute error of less than 1 Ga for the stars in the test dataset.\n","date":"2025-03-27"}
{"id":"2503.21154","title":"Federated Learning with Differential Privacy: An Utility-Enhanced\n  Approach","abstract":"  Federated learning has emerged as an attractive approach to protect data\nprivacy by eliminating the need for sharing clients' data while reducing\ncommunication costs compared with centralized machine learning algorithms.\nHowever, recent studies have shown that federated learning alone does not\nguarantee privacy, as private data may still be inferred from the uploaded\nparameters to the central server. In order to successfully avoid data leakage,\nadopting differential privacy (DP) in the local optimization process or in the\nlocal update aggregation process has emerged as two feasible ways for achieving\nsample-level or user-level privacy guarantees respectively, in federated\nlearning models. However, compared to their non-private equivalents, these\napproaches suffer from a poor utility. To improve the privacy-utility\ntrade-off, we present a modification to these vanilla differentially private\nalgorithms based on a Haar wavelet transformation step and a novel noise\ninjection scheme that significantly lowers the asymptotic bound of the noise\nvariance. We also present a holistic convergence analysis of our proposed\nalgorithm, showing that our method yields better convergence performance than\nthe vanilla DP algorithms. Numerical experiments on real-world datasets\ndemonstrate that our method outperforms existing approaches in model utility\nwhile maintaining the same privacy guarantees.\n","date":"2025-03-27"}
{"id":"2503.21155","title":"Embedding Domain-Specific Knowledge from LLMs into the Feature\n  Engineering Pipeline","abstract":"  Feature engineering is mandatory in the machine learning pipeline to obtain\nrobust models. While evolutionary computation is well-known for its great\nresults both in feature selection and feature construction, its methods are\ncomputationally expensive due to the large number of evaluations required to\ninduce the final model. Part of the reason why these algorithms require a large\nnumber of evaluations is their lack of domain-specific knowledge, resulting in\na lot of random guessing during evolution. In this work, we propose using Large\nLanguage Models (LLMs) as an initial feature construction step to add knowledge\nto the dataset. By doing so, our results show that the evolution can converge\nfaster, saving us computational resources. The proposed approach only provides\nthe names of the features in the dataset and the target objective to the LLM,\nmaking it usable even when working with datasets containing private data. While\nconsistent improvements to test performance were only observed for one-third of\nthe datasets (CSS, PM, and IM10), possibly due to problems being easily\nexplored by LLMs, this approach only decreased the model performance in 1\/77\ntest cases. Additionally, this work introduces the M6GP feature engineering\nalgorithm to symbolic regression, showing it can improve the results of the\nrandom forest regressor and produce competitive results with its predecessor,\nM3GP.\n","date":"2025-03-27"}
{"id":"2503.21156","title":"A Theoretical Analysis of Analogy-Based Evolutionary Transfer\n  Optimization","abstract":"  Evolutionary transfer optimization (ETO) has been gaining popularity in\nresearch over the years due to its outstanding knowledge transfer ability to\naddress various challenges in optimization. However, a pressing issue in this\nfield is that the invention of new ETO algorithms has far outpaced the\ndevelopment of fundamental theories needed to clearly understand the key\nfactors contributing to the success of these algorithms for effective\ngeneralization. In response to this challenge, this study aims to establish\ntheoretical foundations for analogy-based ETO, specifically to support various\nalgorithms that frequently reference a key concept known as similarity. First,\nwe introduce analogical reasoning and link its subprocesses to three key issues\nin ETO. Then, we develop theories for analogy-based knowledge transfer, rooted\nin the principles that underlie the subprocesses. Afterwards, we present two\ntheorems related to the performance gain of analogy-based knowledge transfer,\nnamely unconditionally nonnegative performance gain and conditionally positive\nperformance gain, to theoretically demonstrate the effectiveness of various\nanalogy-based ETO methods. Last but not least, we offer a novel insight into\nanalogy-based ETO that interprets its conditional superiority over traditional\nevolutionary optimization through the lens of the no free lunch theorem for\noptimization.\n","date":"2025-03-27"}
{"id":"2503.21157","title":"Real-Time Evaluation Models for RAG: Who Detects Hallucinations Best?","abstract":"  This article surveys Evaluation models to automatically detect hallucinations\nin Retrieval-Augmented Generation (RAG), and presents a comprehensive benchmark\nof their performance across six RAG applications. Methods included in our study\ninclude: LLM-as-a-Judge, Prometheus, Lynx, the Hughes Hallucination Evaluation\nModel (HHEM), and the Trustworthy Language Model (TLM). These approaches are\nall reference-free, requiring no ground-truth answers\/labels to catch incorrect\nLLM responses. Our study reveals that, across diverse RAG applications, some of\nthese approaches consistently detect incorrect RAG responses with high\nprecision\/recall.\n","date":"2025-03-27"}
{"id":"2503.21158","title":"Integrating Travel Behavior Forecasting and Generative Modeling for\n  Predicting Future Urban Mobility and Spatial Transformations","abstract":"  Transportation planning plays a critical role in shaping urban development,\neconomic mobility, and infrastructure sustainability. However, traditional\nplanning methods often struggle to accurately predict long-term urban growth\nand transportation demands. This may sometimes result in infrastructure\ndemolition to make room for current transportation planning demands. This study\nintegrates a Temporal Fusion Transformer to predict travel patterns from\ndemographic data with a Generative Adversarial Network to predict future urban\nsettings through satellite imagery. The framework achieved a 0.76 R-square\nscore in travel behavior prediction and generated high-fidelity satellite\nimages with a Structural Similarity Index of 0.81. The results demonstrate that\nintegrating predictive analytics and spatial visualization can significantly\nimprove the decision-making process, fostering more sustainable and efficient\nurban development. This research highlights the importance of data-driven\nmethodologies in modern transportation planning and presents a step toward\noptimizing infrastructure placement, capacity, and long-term viability.\n","date":"2025-03-27"}
{"id":"2503.21159","title":"Multi-Objective Optimization for Privacy-Utility Balance in\n  Differentially Private Federated Learning","abstract":"  Federated learning (FL) enables collaborative model training across\ndistributed clients without sharing raw data, making it a promising approach\nfor privacy-preserving machine learning. However, ensuring differential privacy\n(DP) in FL presents challenges due to the trade-off between model utility and\nprivacy protection. Clipping gradients before aggregation is a common strategy\nto limit privacy loss, but selecting an optimal clipping norm is non-trivial,\nas excessively high values compromise privacy, while overly restrictive\nclipping degrades model performance. In this work, we propose an adaptive\nclipping mechanism that dynamically adjusts the clipping norm using a\nmulti-objective optimization framework. By integrating privacy and utility\nconsiderations into the optimization objective, our approach balances privacy\npreservation with model accuracy. We theoretically analyze the convergence\nproperties of our method and demonstrate its effectiveness through extensive\nexperiments on MNIST, Fashion-MNIST, and CIFAR-10 datasets. Our results show\nthat adaptive clipping consistently outperforms fixed-clipping baselines,\nachieving improved accuracy under the same privacy constraints. This work\nhighlights the potential of dynamic clipping strategies to enhance\nprivacy-utility trade-offs in differentially private federated learning.\n","date":"2025-03-27"}
{"id":"2503.21160","title":"A Data Balancing and Ensemble Learning Approach for Credit Card Fraud\n  Detection","abstract":"  This research introduces an innovative method for identifying credit card\nfraud by combining the SMOTE-KMEANS technique with an ensemble machine learning\nmodel. The proposed model was benchmarked against traditional models such as\nlogistic regression, decision trees, random forests, and support vector\nmachines. Performance was evaluated using metrics, including accuracy, recall,\nand area under the curve (AUC). The results demonstrated that the proposed\nmodel achieved superior performance, with an AUC of 0.96 when combined with the\nSMOTE-KMEANS algorithm. This indicates a significant improvement in detecting\nfraudulent transactions while maintaining high precision and recall. The study\nalso explores the application of different oversampling techniques to enhance\nthe performance of various classifiers. The findings suggest that the proposed\nmethod is robust and effective for classification tasks on balanced datasets.\nFuture research directions include further optimization of the SMOTE-KMEANS\napproach and its integration into existing fraud detection systems to enhance\nfinancial security and consumer protection.\n","date":"2025-03-27"}
{"id":"2503.21164","title":"Adversarial Wear and Tear: Exploiting Natural Damage for Generating\n  Physical-World Adversarial Examples","abstract":"  The presence of adversarial examples in the physical world poses significant\nchallenges to the deployment of Deep Neural Networks in safety-critical\napplications such as autonomous driving. Most existing methods for crafting\nphysical-world adversarial examples are ad-hoc, relying on temporary\nmodifications like shadows, laser beams, or stickers that are tailored to\nspecific scenarios. In this paper, we introduce a new class of physical-world\nadversarial examples, AdvWT, which draws inspiration from the naturally\noccurring phenomenon of `wear and tear', an inherent property of physical\nobjects. Unlike manually crafted perturbations, `wear and tear' emerges\norganically over time due to environmental degradation, as seen in the gradual\ndeterioration of outdoor signboards. To achieve this, AdvWT follows a two-step\napproach. First, a GAN-based, unsupervised image-to-image translation network\nis employed to model these naturally occurring damages, particularly in the\ncontext of outdoor signboards. The translation network encodes the\ncharacteristics of damaged signs into a latent `damage style code'. In the\nsecond step, we introduce adversarial perturbations into the style code,\nstrategically optimizing its transformation process. This manipulation subtly\nalters the damage style representation, guiding the network to generate\nadversarial images where the appearance of damages remains perceptually\nrealistic, while simultaneously ensuring their effectiveness in misleading\nneural networks. Through comprehensive experiments on two traffic sign\ndatasets, we show that AdvWT effectively misleads DNNs in both digital and\nphysical domains. AdvWT achieves an effective attack success rate, greater\nrobustness, and a more natural appearance compared to existing physical-world\nadversarial examples. Additionally, integrating AdvWT into training enhances a\nmodel's generalizability to real-world damaged signs.\n","date":"2025-03-27"}
{"id":"2503.21166","title":"Unveiling the Potential of Superexpressive Networks in Implicit Neural\n  Representations","abstract":"  In this study, we examine the potential of one of the ``superexpressive''\nnetworks in the context of learning neural functions for representing complex\nsignals and performing machine learning downstream tasks. Our focus is on\nevaluating their performance on computer vision and scientific machine learning\ntasks including signal representation\/inverse problems and solutions of partial\ndifferential equations. Through an empirical investigation in various benchmark\ntasks, we demonstrate that superexpressive networks, as proposed by [Zhang et\nal. NeurIPS, 2022], which employ a specialized network structure characterized\nby having an additional dimension, namely width, depth, and ``height'', can\nsurpass recent implicit neural representations that use highly-specialized\nnonlinear activation functions.\n","date":"2025-03-27"}
{"id":"2503.21169","title":"VADMamba: Exploring State Space Models for Fast Video Anomaly Detection","abstract":"  Video anomaly detection (VAD) methods are mostly CNN-based or\nTransformer-based, achieving impressive results, but the focus on detection\naccuracy often comes at the expense of inference speed. The emergence of state\nspace models in computer vision, exemplified by the Mamba model, demonstrates\nimproved computational efficiency through selective scans and showcases the\ngreat potential for long-range modeling. Our study pioneers the application of\nMamba to VAD, dubbed VADMamba, which is based on multi-task learning for frame\nprediction and optical flow reconstruction. Specifically, we propose the\nVQ-Mamba Unet (VQ-MaU) framework, which incorporates a Vector Quantization (VQ)\nlayer and Mamba-based Non-negative Visual State Space (NVSS) block.\nFurthermore, two individual VQ-MaU networks separately predict frames and\nreconstruct corresponding optical flows, further boosting accuracy through a\nclip-level fusion evaluation strategy. Experimental results validate the\nefficacy of the proposed VADMamba across three benchmark datasets,\ndemonstrating superior performance in inference speed compared to previous\nwork. Code is available at https:\/\/github.com\/jLooo\/VADMamba.\n","date":"2025-03-27"}
{"id":"2503.21172","title":"Model as a Game: On Numerical and Spatial Consistency for Generative\n  Games","abstract":"  Recent advances in generative models have significantly impacted game\ngeneration. However, despite producing high-quality graphics and adequately\nreceiving player input, existing models often fail to maintain fundamental game\nproperties such as numerical and spatial consistency. Numerical consistency\nensures gameplay mechanics correctly reflect score changes and other\nquantitative elements, while spatial consistency prevents jarring scene\ntransitions, providing seamless player experiences. In this paper, we revisit\nthe paradigm of generative games to explore what truly constitutes a Model as a\nGame (MaaG) with a well-developed mechanism. We begin with an empirical study\non ``Traveler'', a 2D game created by an LLM featuring minimalist rules yet\nchallenging generative models in maintaining consistency. Based on the DiT\narchitecture, we design two specialized modules: (1) a numerical module that\nintegrates a LogicNet to determine event triggers, with calculations processed\nexternally as conditions for image generation; and (2) a spatial module that\nmaintains a map of explored areas, retrieving location-specific information\nduring generation and linking new observations to ensure continuity.\nExperiments across three games demonstrate that our integrated modules\nsignificantly enhance performance on consistency metrics compared to baselines,\nwhile incurring minimal time overhead during inference.\n","date":"2025-03-27"}
{"id":"2503.21178","title":"Integrating Large Language Models For Monte Carlo Simulation of Chemical\n  Reaction Networks","abstract":"  Chemical reaction network is an important method for modeling and exploring\ncomplex biological processes, bio-chemical interactions and the behavior of\ndifferent dynamics in system biology. But, formulating such reaction kinetics\ntakes considerable time. In this paper, we leverage the efficiency of modern\nlarge language models to automate the stochastic monte carlo simulation of\nchemical reaction networks and enable the simulation through the reaction\ndescription provided in the form of natural languages. We also integrate this\nprocess into widely used simulation tool Copasi to further give the edge and\nease to the modelers and researchers. In this work, we show the efficacy and\nlimitations of the modern large language models to parse and create reaction\nkinetics for modelling complex chemical reaction processes.\n","date":"2025-03-27"}
{"id":"2503.21187","title":"DSU-Net:An Improved U-Net Model Based on DINOv2 and SAM2 with\n  Multi-scale Cross-model Feature Enhancement","abstract":"  Despite the significant advancements in general image segmentation achieved\nby large-scale pre-trained foundation models (such as Meta's Segment Any-thing\nModel (SAM) series and DINOv2), their performance in specialized fields remains\nlimited by two critical issues: the excessive training costs due to large model\nparameters, and the insufficient ability to represent specific domain\ncharacteristics. This paper proposes a multi-scale feature collabora-tion\nframework guided by DINOv2 for SAM2, with core innovations in three aspects:\n(1) Establishing a feature collaboration mechanism between DINOv2 and SAM2\nbackbones, where high-dimensional semantic features extracted by the\nself-supervised model guide multi-scale feature fusion; (2) Designing\nlightweight adapter modules and cross-modal, cross-layer feature fusion units\nto inject cross-domain knowledge while freezing the base model parameters; (3)\nConstructing a U-shaped network structure based on U-net, which utilizes\nattention mechanisms to achieve adaptive aggregation decoding of\nmulti-granularity features. This framework surpasses existing state-of-the-art\nmeth-ods in downstream tasks such as camouflage target detection and salient\nob-ject detection, without requiring costly training processes. It provides a\ntech-nical pathway for efficient deployment of visual image segmentation,\ndemon-strating significant application value in a wide range of downstream\ntasks and specialized fields within image segmentation.Project page:\nhttps:\/\/github.com\/CheneyXuYiMin\/SAM2DINO-Seg\n","date":"2025-03-27"}
{"id":"2503.21190","title":"Leveraging LLMs with Iterative Loop Structure for Enhanced Social\n  Intelligence in Video Question Answering","abstract":"  Social intelligence, the ability to interpret emotions, intentions, and\nbehaviors, is essential for effective communication and adaptive responses. As\nrobots and AI systems become more prevalent in caregiving, healthcare, and\neducation, the demand for AI that can interact naturally with humans grows.\nHowever, creating AI that seamlessly integrates multiple modalities, such as\nvision and speech, remains a challenge. Current video-based methods for social\nintelligence rely on general video recognition or emotion recognition\ntechniques, often overlook the unique elements inherent in human interactions.\nTo address this, we propose the Looped Video Debating (LVD) framework, which\nintegrates Large Language Models (LLMs) with visual information, such as facial\nexpressions and body movements, to enhance the transparency and reliability of\nquestion-answering tasks involving human interaction videos. Our results on the\nSocial-IQ 2.0 benchmark show that LVD achieves state-of-the-art performance\nwithout fine-tuning. Furthermore, supplementary human annotations on existing\ndatasets provide insights into the model's accuracy, guiding future\nimprovements in AI-driven social intelligence.\n","date":"2025-03-27"}
{"id":"2503.21193","title":"UGen: Unified Autoregressive Multimodal Model with Progressive\n  Vocabulary Learning","abstract":"  We introduce UGen, a unified autoregressive multimodal model that\ndemonstrates strong performance across text processing, image understanding,\nand image generation tasks simultaneously. UGen converts both texts and images\ninto discrete token sequences and utilizes a single transformer to generate\nthem uniformly in an autoregressive manner. To address the challenges\nassociated with unified multimodal learning, UGen is trained using a novel\nmechanism, namely progressive vocabulary learning. In this process, visual\ntoken IDs are incrementally activated and integrated into the training phase,\nultimately enhancing the effectiveness of unified multimodal learning.\nExperiments on comprehensive text and image tasks show that UGen achieves a\nsignificant overall performance improvement of 13.3% compared to the vanilla\nunified autoregressive method, and it also delivers competitive results across\nall tasks against several task-specific models.\n","date":"2025-03-27"}
{"id":"2503.21197","title":"WVSC: Wireless Video Semantic Communication with Multi-frame\n  Compensation","abstract":"  Existing wireless video transmission schemes directly conduct video coding in\npixel level, while neglecting the inner semantics contained in videos. In this\npaper, we propose a wireless video semantic communication framework,\nabbreviated as WVSC, which integrates the idea of semantic communication into\nwireless video transmission scenarios. WVSC first encodes original video frames\nas semantic frames and then conducts video coding based on such compact\nrepresentations, enabling the video coding in semantic level rather than pixel\nlevel. Moreover, to further reduce the communication overhead, a reference\nsemantic frame is introduced to substitute motion vectors of each frame in\ncommon video coding methods. At the receiver, multi-frame compensation (MFC) is\nproposed to produce compensated current semantic frame with a multi-frame\nfusion attention module. With both the reference frame transmission and MFC,\nthe bandwidth efficiency improves with satisfying video transmission\nperformance. Experimental results verify the performance gain of WVSC over\nother DL-based methods e.g. DVSC about 1 dB and traditional schemes about 2 dB\nin terms of PSNR.\n","date":"2025-03-27"}
{"id":"2503.21200","title":"Learning Generalizable Skills from Offline Multi-Task Data for\n  Multi-Agent Cooperation","abstract":"  Learning cooperative multi-agent policy from offline multi-task data that can\ngeneralize to unseen tasks with varying numbers of agents and targets is an\nattractive problem in many scenarios. Although aggregating general behavior\npatterns among multiple tasks as skills to improve policy transfer is a\npromising approach, two primary challenges hinder the further advancement of\nskill learning in offline multi-task MARL. Firstly, extracting general\ncooperative behaviors from various action sequences as common skills lacks\nbringing cooperative temporal knowledge into them. Secondly, existing works\nonly involve common skills and can not adaptively choose independent knowledge\nas task-specific skills in each task for fine-grained action execution. To\ntackle these challenges, we propose Hierarchical and Separate Skill Discovery\n(HiSSD), a novel approach for generalizable offline multi-task MARL through\nskill learning. HiSSD leverages a hierarchical framework that jointly learns\ncommon and task-specific skills. The common skills learn cooperative temporal\nknowledge and enable in-sample exploitation for offline multi-task MARL. The\ntask-specific skills represent the priors of each task and achieve a\ntask-guided fine-grained action execution. To verify the advancement of our\nmethod, we conduct experiments on multi-agent MuJoCo and SMAC benchmarks. After\ntraining the policy using HiSSD on offline multi-task data, the empirical\nresults show that HiSSD assigns effective cooperative behaviors and obtains\nsuperior performance in unseen tasks.\n","date":"2025-03-27"}
{"id":"2503.21208","title":"An improved EfficientNetV2 for garbage classification","abstract":"  This paper presents an enhanced waste classification framework based on\nEfficientNetV2 to address challenges in data acquisition cost, generalization,\nand real-time performance. We propose a Channel-Efficient Attention\n(CE-Attention) module that mitigates feature loss during global pooling without\nintroducing dimensional scaling, effectively enhancing critical feature\nextraction. Additionally, a lightweight multi-scale spatial feature extraction\nmodule (SAFM) is developed by integrating depthwise separable convolutions,\nsignificantly reducing model complexity. Comprehensive data augmentation\nstrategies are further employed to improve generalization. Experiments on the\nHuawei Cloud waste classification dataset demonstrate that our method achieves\na classification accuracy of 95.4\\%, surpassing the baseline by 3.2\\% and\noutperforming mainstream models. The results validate the effectiveness of our\napproach in balancing accuracy and efficiency for practical waste\nclassification scenarios.\n","date":"2025-03-27"}
{"id":"2503.21210","title":"FakeReasoning: Towards Generalizable Forgery Detection and Reasoning","abstract":"  Accurate and interpretable detection of AI-generated images is essential for\nmitigating risks associated with AI misuse. However, the substantial domain gap\namong generative models makes it challenging to develop a generalizable forgery\ndetection model. Moreover, since every pixel in an AI-generated image is\nsynthesized, traditional saliency-based forgery explanation methods are not\nwell suited for this task. To address these challenges, we propose modeling\nAI-generated image detection and explanation as a Forgery Detection and\nReasoning task (FDR-Task), leveraging vision-language models (VLMs) to provide\naccurate detection through structured and reliable reasoning over forgery\nattributes. To facilitate this task, we introduce the Multi-Modal Forgery\nReasoning dataset (MMFR-Dataset), a large-scale dataset containing 100K images\nacross 10 generative models, with 10 types of forgery reasoning annotations,\nenabling comprehensive evaluation of FDR-Task. Additionally, we propose\nFakeReasoning, a forgery detection and reasoning framework with two key\ncomponents. First, Forgery-Aligned Contrastive Learning enhances VLMs'\nunderstanding of forgery-related semantics through both cross-modal and\nintra-modal contrastive learning between images and forgery attribute\nreasoning. Second, a Classification Probability Mapper bridges the optimization\ngap between forgery detection and language modeling by mapping the output\nlogits of VLMs to calibrated binary classification probabilities. Experiments\nacross multiple generative models demonstrate that FakeReasoning not only\nachieves robust generalization but also outperforms state-of-the-art methods on\nboth detection and reasoning tasks.\n","date":"2025-03-27"}
{"id":"2503.21211","title":"Interpretable Cross-Sphere Multiscale Deep Learning Predicts ENSO\n  Skilfully Beyond 2 Years","abstract":"  El Ni\\~no-Southern Oscillation (ENSO) exerts global climate and societal\nimpacts, but real-time prediction with lead times beyond one year remains\nchallenging. Dynamical models suffer from large biases and uncertainties, while\ndeep learning struggles with interpretability and multi-scale dynamics. Here,\nwe introduce PTSTnet, an interpretable model that unifies dynamical processes\nand cross-scale spatiotemporal learning in an innovative neural-network\nframework with physics-encoding learning. PTSTnet produces interpretable\npredictions significantly outperforming state-of-the-art benchmarks with lead\ntimes beyond 24 months, providing physical insights into error propagation in\nocean-atmosphere interactions. PTSTnet learns feature representations with\nphysical consistency from sparse data to tackle inherent multi-scale and\nmulti-physics challenges underlying ocean-atmosphere processes, thereby\ninherently enhancing long-term prediction skill. Our successful realizations\nmark substantial steps forward in interpretable insights into innovative neural\nocean modelling.\n","date":"2025-03-27"}
{"id":"2503.21213","title":"Resource-Efficient Federated Fine-Tuning Large Language Models for\n  Heterogeneous Data","abstract":"  Fine-tuning large language models (LLMs) via federated learning, i.e.,\nFedLLM, has been proposed to adapt LLMs for various downstream applications in\na privacy-preserving way. To reduce the fine-tuning costs on\nresource-constrained devices, FedLoRA is proposed to fine-tune only a small\nsubset of model parameters by integrating low-rank adaptation (LoRA) into\nFedLLM. However, apart from resource constraints, there is still another\ncritical challenge, i.e., data heterogeneity, severely hindering the\nimplementation of FedLoRA in practical applications. Herein, inspired by the\nprevious group-based federated learning paradigm, we propose a hierarchical\nFedLoRA framework, termed HierFedLoRA, to address these challenges.\nSpecifically, HierFedLoRA partitions all devices into multiple near-IID groups\nand adjusts the intra-group aggregation frequency for each group to eliminate\nthe negative effects of non-IID data. Meanwhile, to reduce the computation and\ncommunication cost, HierFedLoRA dynamically assigns diverse and suitable\nfine-tuning depth (i.e., the number of continuous fine-tuning layers from the\noutput) for each group. HierFedLoRA explores jointly optimizing aggregation\nfrequency and depth upon their coupled relationship to better enhance the\nperformance of FedLoRA. Extensive experiments are conducted on a physical\nplatform with 80 commercial devices. The results show that HierFedLoRA improves\nthe final model accuracy by 1.6% to 4.2%, speeding up the fine-tuning process\nby at least 2.1$\\times$, compared to the strong baselines.\n","date":"2025-03-27"}
{"id":"2503.21214","title":"VoxRep: Enhancing 3D Spatial Understanding in 2D Vision-Language Models\n  via Voxel Representation","abstract":"  Comprehending 3D environments is vital for intelligent systems in domains\nlike robotics and autonomous navigation. Voxel grids offer a structured\nrepresentation of 3D space, but extracting high-level semantic meaning remains\nchallenging. This paper proposes a novel approach utilizing a Vision-Language\nModel (VLM) to extract \"voxel semantics\"-object identity, color, and\nlocation-from voxel data. Critically, instead of employing complex 3D networks,\nour method processes the voxel space by systematically slicing it along a\nprimary axis (e.g., the Z-axis, analogous to CT scan slices). These 2D slices\nare then formatted and sequentially fed into the image encoder of a standard\nVLM. The model learns to aggregate information across slices and correlate\nspatial patterns with semantic concepts provided by the language component.\nThis slice-based strategy aims to leverage the power of pre-trained 2D VLMs for\nefficient 3D semantic understanding directly from voxel representations.\n","date":"2025-03-27"}
{"id":"2503.21219","title":"GenFusion: Closing the Loop between Reconstruction and Generation via\n  Videos","abstract":"  Recently, 3D reconstruction and generation have demonstrated impressive novel\nview synthesis results, achieving high fidelity and efficiency. However, a\nnotable conditioning gap can be observed between these two fields, e.g.,\nscalable 3D scene reconstruction often requires densely captured views, whereas\n3D generation typically relies on a single or no input view, which\nsignificantly limits their applications. We found that the source of this\nphenomenon lies in the misalignment between 3D constraints and generative\npriors. To address this problem, we propose a reconstruction-driven video\ndiffusion model that learns to condition video frames on artifact-prone RGB-D\nrenderings. Moreover, we propose a cyclical fusion pipeline that iteratively\nadds restoration frames from the generative model to the training set, enabling\nprogressive expansion and addressing the viewpoint saturation limitations seen\nin previous reconstruction and generation pipelines. Our evaluation, including\nview synthesis from sparse view and masked input, validates the effectiveness\nof our approach. More details at https:\/\/genfusion.sibowu.com.\n","date":"2025-03-27"}
{"id":"2503.21223","title":"Rethinking Graph Structure Learning in the Era of LLMs","abstract":"  Recently, the emergence of large language models (LLMs) has prompted\nresearchers to explore the integration of language descriptions into graphs,\naiming to enhance model encoding capabilities from a data-centric perspective.\nThis graph representation is called text-attributed graphs (TAGs). A review of\nprior advancements highlights that graph structure learning (GSL) is a pivotal\ntechnique for improving data utility, making it highly relevant to efficient\nTAG learning. However, most GSL methods are tailored for traditional graphs\nwithout textual information, underscoring the necessity of developing a new GSL\nparadigm. Despite clear motivations, it remains challenging: (1) How can we\ndefine a reasonable optimization objective for GSL in the era of LLMs,\nconsidering the massive parameters in LLM? (2) How can we design an efficient\nmodel architecture that enables seamless integration of LLM for this\noptimization objective? For Question 1, we reformulate existing GSL\noptimization objectives as a tree optimization framework, shifting the focus\nfrom obtaining a well-trained edge predictor to a language-aware tree sampler.\nFor Question 2, we propose decoupled and training-free model design principles\nfor LLM integration, shifting the focus from computation-intensive fine-tuning\nto more efficient inference. Based on this, we propose Large Language and Tree\nAssistant (LLaTA), which leverages tree-based LLM in-context learning to\nenhance the understanding of topology and text, enabling reliable inference and\ngenerating improved graph structure. Extensive experiments on 10 TAG datasets\ndemonstrate that LLaTA enjoys flexibility - incorporated with any backbone;\nscalability - outperforms other LLM-based GSL methods in terms of running\nefficiency; effectiveness - achieves SOTA performance.\n","date":"2025-03-27"}
{"id":"2503.21224","title":"Efficient Learning for Entropy-Regularized Markov Decision Processes via\n  Multilevel Monte Carlo","abstract":"  Designing efficient learning algorithms with complexity guarantees for Markov\ndecision processes (MDPs) with large or continuous state and action spaces\nremains a fundamental challenge. We address this challenge for\nentropy-regularized MDPs with Polish state and action spaces, assuming access\nto a generative model of the environment. We propose a novel family of\nmultilevel Monte Carlo (MLMC) algorithms that integrate fixed-point iteration\nwith MLMC techniques and a generic stochastic approximation of the Bellman\noperator. We quantify the precise impact of the chosen approximate Bellman\noperator on the accuracy of the resulting MLMC estimator. Leveraging this error\nanalysis, we show that using a biased plain MC estimate for the Bellman\noperator results in quasi-polynomial sample complexity, whereas an unbiased\nrandomized multilevel approximation of the Bellman operator achieves polynomial\nsample complexity in expectation. Notably, these complexity bounds are\nindependent of the dimensions or cardinalities of the state and action spaces,\ndistinguishing our approach from existing algorithms whose complexities scale\nwith the sizes of these spaces. We validate these theoretical performance\nguarantees through numerical experiments.\n","date":"2025-03-27"}
{"id":"2503.21226","title":"Frequency-Aware Gaussian Splatting Decomposition","abstract":"  3D Gaussian Splatting (3D-GS) has revolutionized novel view synthesis with\nits efficient, explicit representation. However, it lacks frequency\ninterpretability, making it difficult to separate low-frequency structures from\nfine details. We introduce a frequency-decomposed 3D-GS framework that groups\n3D Gaussians that correspond to subbands in the Laplacian Pyrmaids of the input\nimages. Our approach enforces coherence within each subband (i.e., group of 3D\nGaussians) through dedicated regularization, ensuring well-separated frequency\ncomponents. We extend color values to both positive and negative ranges,\nallowing higher-frequency layers to add or subtract residual details. To\nstabilize optimization, we employ a progressive training scheme that refines\ndetails in a coarse-to-fine manner. Beyond interpretability, this\nfrequency-aware design unlocks a range of practical benefits. Explicit\nfrequency separation enables advanced 3D editing and stylization, allowing\nprecise manipulation of specific frequency bands. It also supports dynamic\nlevel-of-detail control for progressive rendering, streaming, foveated\nrendering and fast geometry interaction. Through extensive experiments, we\ndemonstrate that our method provides improved control and flexibility for\nemerging applications in scene editing and interactive rendering. Our code will\nbe made publicly available.\n","date":"2025-03-27"}
{"id":"2503.21227","title":"LLaVA-CMoE: Towards Continual Mixture of Experts for Large\n  Vision-Language Models","abstract":"  Although applying Mixture of Experts to large language models for learning\nnew tasks is widely regarded as an effective strategy for continuous learning,\nthere still remain two major challenges: (1) As the number of tasks grows,\nsimple parameter expansion strategies can lead to excessively large models. (2)\nModifying the parameters of the existing router results in the erosion of\npreviously acquired knowledge. In this paper, we present an innovative\nframework named LLaVA-CMoE, which is a continuous Mixture of Experts (MoE)\narchitecture without any replay data. Specifically, we have developed a method\ncalled Probe-Guided Knowledge Extension (PGKE), which employs probe experts to\nassess whether additional knowledge is required for a specific layer. This\napproach enables the model to adaptively expand its network parameters based on\ntask distribution, thereby significantly improving the efficiency of parameter\nexpansion. Additionally, we introduce a hierarchical routing algorithm called\nProbabilistic Task Locator (PTL), where high-level routing captures inter-task\ninformation and low-level routing focuses on intra-task details, ensuring that\nnew task experts do not interfere with existing ones. Our experiments shows\nthat our efficient architecture has substantially improved model performance on\nthe Coin benchmark while maintaining a reasonable parameter count.\n","date":"2025-03-27"}
{"id":"2503.21232","title":"Knowledge Graphs as World Models for Semantic Material-Aware Obstacle\n  Handling in Autonomous Vehicles","abstract":"  The inability of autonomous vehicles (AVs) to infer the material properties\nof obstacles limits their decision-making capacity. While AVs rely on sensor\nsystems such as cameras, LiDAR, and radar to detect obstacles, this study\nsuggests combining sensors with a knowledge graph (KG)-based world model to\nimprove AVs' comprehension of physical material qualities. Beyond sensor data,\nAVs can infer qualities such as malleability, density, and elasticity using a\nsemantic KG that depicts the relationships between obstacles and their\nattributes. Using the CARLA autonomous driving simulator, we evaluated AV\nperformance with and without KG integration. The findings demonstrate that the\nKG-based method improves obstacle management, which allows AVs to use material\nqualities to make better decisions about when to change lanes or apply\nemergency braking. For example, the KG-integrated AV changed lanes for hard\nimpediments like traffic cones and successfully avoided collisions with\nflexible items such as plastic bags by passing over them. Compared to the\ncontrol system, the KG framework demonstrated improved responsiveness to\nobstacles by resolving conflicting sensor data, causing emergency stops for\n13.3% more cases. In addition, our method exhibits a 6.6% higher success rate\nin lane-changing maneuvers in experimental scenarios, particularly for larger,\nhigh-impact obstacles. While we focus particularly on autonomous driving, our\nwork demonstrates the potential of KG-based world models to improve\ndecision-making in embodied AI systems and scale to other domains, including\nrobotics, healthcare, and environmental simulation.\n","date":"2025-03-27"}
{"id":"2503.21236","title":"Clean Image May be Dangerous: Data Poisoning Attacks Against Deep\n  Hashing","abstract":"  Large-scale image retrieval using deep hashing has become increasingly\npopular due to the exponential growth of image data and the remarkable feature\nextraction capabilities of deep neural networks (DNNs). However, deep hashing\nmethods are vulnerable to malicious attacks, including adversarial and backdoor\nattacks. It is worth noting that these attacks typically involve altering the\nquery images, which is not a practical concern in real-world scenarios. In this\npaper, we point out that even clean query images can be dangerous, inducing\nmalicious target retrieval results, like undesired or illegal images. To the\nbest of our knowledge, we are the first to study data \\textbf{p}oisoning\n\\textbf{a}ttacks against \\textbf{d}eep \\textbf{hash}ing\n\\textbf{(\\textit{PADHASH})}. Specifically, we first train a surrogate model to\nsimulate the behavior of the target deep hashing model. Then, a strict gradient\nmatching strategy is proposed to generate the poisoned images. Extensive\nexperiments on different models, datasets, hash methods, and hash code lengths\ndemonstrate the effectiveness and generality of our attack method.\n","date":"2025-03-27"}
{"id":"2503.21237","title":"Bias-Aware Agent: Enhancing Fairness in AI-Driven Knowledge Retrieval","abstract":"  Advancements in retrieving accessible information have evolved faster in the\nlast few years compared to the decades since the internet's creation. Search\nengines, like Google, have been the number one way to find relevant data. They\nhave always relied on the user's abilities to find the best information in its\nbillions of links and sources at everybody's fingertips. The advent of large\nlanguage models (LLMs) has completely transformed the field of information\nretrieval. The LLMs excel not only at retrieving relevant knowledge but also at\nsummarizing it effectively, making information more accessible and consumable\nfor users. On top of it, the rise of AI Agents has introduced another aspect to\ninformation retrieval i.e. dynamic information retrieval which enables the\nintegration of real-time data such as weather forecasts, and financial data\nwith the knowledge base to curate context-aware knowledge. However, despite\nthese advancements the agents remain susceptible to issues of bias and\nfairness, challenges deeply rooted within the knowledge base and training of\nLLMs. This study introduces a novel approach to bias-aware knowledge retrieval\nby leveraging agentic framework and the innovative use of bias detectors as\ntools to identify and highlight inherent biases in the retrieved content. By\nempowering users with transparency and awareness, this approach aims to foster\nmore equitable information systems and promote the development of responsible\nAI.\n","date":"2025-03-27"}
{"id":"2503.21241","title":"Feature-Enhanced Machine Learning for All-Cause Mortality Prediction in\n  Healthcare Data","abstract":"  Accurate patient mortality prediction enables effective risk stratification,\nleading to personalized treatment plans and improved patient outcomes. However,\npredicting mortality in healthcare remains a significant challenge, with\nexisting studies often focusing on specific diseases or limited predictor sets.\nThis study evaluates machine learning models for all-cause in-hospital\nmortality prediction using the MIMIC-III database, employing a comprehensive\nfeature engineering approach. Guided by clinical expertise and literature, we\nextracted key features such as vital signs (e.g., heart rate, blood pressure),\nlaboratory results (e.g., creatinine, glucose), and demographic information.\nThe Random Forest model achieved the highest performance with an AUC of 0.94,\nsignificantly outperforming other machine learning and deep learning\napproaches. This demonstrates Random Forest's robustness in handling\nhigh-dimensional, noisy clinical data and its potential for developing\neffective clinical decision support tools. Our findings highlight the\nimportance of careful feature engineering for accurate mortality prediction. We\nconclude by discussing implications for clinical adoption and propose future\ndirections, including enhancing model robustness and tailoring prediction\nmodels for specific diseases.\n","date":"2025-03-27"}
{"id":"2503.21242","title":"PLAIN: Scalable Estimation Architecture for Integrated Sensing and\n  Communication","abstract":"  Integrated sensing and communication (ISAC) is envisioned be to one of the\nparadigms upon which next-generation mobile networks will be built, extending\nlocalization and tracking capabilities, as well as giving birth to\nenvironment-aware wireless access. A key aspect of sensing integration is\nparameter estimation, which involves extracting information about the\nsurrounding environment, such as the direction, distance, and velocity of\nvarious objects within. This is typically of a high-dimensional nature, which\nleads to significant computational complexity, if performed jointly across\nmultiple sensing dimensions, such as space, frequency, and time. Additionally,\ndue to the incorporation of sensing on top of the data transmission, the time\nwindow available for sensing is likely to be short, resulting in an estimation\nproblem where only a single snapshot is accessible. In this work, we propose\nPLAIN, a tensor-based estimation architecture that flexibly scales with\nmultiple sensing dimensions and can handle high dimensionality, limited\nmeasurement time, and super-resolution requirements. It consists of three\nstages: a compression stage, where the high dimensional input is converted into\nlower dimensionality, without sacrificing resolution; a decoupled estimation\nstage, where the parameters across the different dimensions are estimated in\nparallel with low complexity; an input-based fusion stage, where the decoupled\nparameters are fused together to form a paired multidimensional estimate. We\ninvestigate the performance of the architecture for different configurations\nand compare it against practical sequential and joint estimation baselines, as\nwell as theoretical bounds. Our results show that PLAIN, using tools from\ntensor algebra, subspace-based processing, and compressed sensing, can scale\nflexibly with dimensionality, while operating with low complexity and\nmaintaining super-resolution.\n","date":"2025-03-27"}
{"id":"2503.21244","title":"Improving $(\\alpha, f)$-Byzantine Resilience in Federated Learning via\n  layerwise aggregation and cosine distance","abstract":"  The rapid development of artificial intelligence systems has amplified\nsocietal concerns regarding their usage, necessitating regulatory frameworks\nthat encompass data privacy. Federated Learning (FL) is posed as potential\nsolution to data privacy challenges in distributed machine learning by enabling\ncollaborative model training {without data sharing}. However, FL systems remain\nvulnerable to Byzantine attacks, where malicious nodes contribute corrupted\nmodel updates. While Byzantine Resilient operators have emerged as a widely\nadopted robust aggregation algorithm to mitigate these attacks, its efficacy\ndiminishes significantly in high-dimensional parameter spaces, sometimes\nleading to poor performing models. This paper introduces Layerwise Cosine\nAggregation, a novel aggregation scheme designed to enhance robustness of these\nrules in such high-dimensional settings while preserving computational\nefficiency. A theoretical analysis is presented, demonstrating the superior\nrobustness of the proposed Layerwise Cosine Aggregation compared to original\nrobust aggregation operators. Empirical evaluation across diverse image\nclassification datasets, under varying data distributions and Byzantine attack\nscenarios, consistently demonstrates the improved performance of Layerwise\nCosine Aggregation, achieving up to a 16% increase in model accuracy.\n","date":"2025-03-27"}
{"id":"2503.21246","title":"DynamiCtrl: Rethinking the Basic Structure and the Role of Text for\n  High-quality Human Image Animation","abstract":"  Human image animation has recently gained significant attention due to\nadvancements in generative models. However, existing methods still face two\nmajor challenges: (1) architectural limitations, most models rely on U-Net,\nwhich underperforms compared to the MM-DiT; and (2) the neglect of textual\ninformation, which can enhance controllability. In this work, we introduce\nDynamiCtrl, a novel framework that not only explores different pose-guided\ncontrol structures in MM-DiT, but also reemphasizes the crucial role of text in\nthis task. Specifically, we employ a Shared VAE encoder for both reference\nimages and driving pose videos, eliminating the need for an additional pose\nencoder and simplifying the overall framework. To incorporate pose features\ninto the full attention blocks, we propose Pose-adaptive Layer Norm (PadaLN),\nwhich utilizes adaptive layer normalization to encode sparse pose features. The\nencoded features are directly added to the visual input, preserving the\nspatiotemporal consistency of the backbone while effectively introducing pose\ncontrol into MM-DiT. Furthermore, within the full attention mechanism, we align\ntextual and visual features to enhance controllability. By leveraging text, we\nnot only enable fine-grained control over the generated content, but also, for\nthe first time, achieve simultaneous control over both background and motion.\nExperimental results verify the superiority of DynamiCtrl on benchmark\ndatasets, demonstrating its strong identity preservation, heterogeneous\ncharacter driving, background controllability, and high-quality synthesis. The\nproject page is available at https:\/\/gulucaptain.github.io\/DynamiCtrl\/.\n","date":"2025-03-27"}
{"id":"2503.21248","title":"ResearchBench: Benchmarking LLMs in Scientific Discovery via\n  Inspiration-Based Task Decomposition","abstract":"  Large language models (LLMs) have demonstrated potential in assisting\nscientific research, yet their ability to discover high-quality research\nhypotheses remains unexamined due to the lack of a dedicated benchmark. To\naddress this gap, we introduce the first large-scale benchmark for evaluating\nLLMs with a near-sufficient set of sub-tasks of scientific discovery:\ninspiration retrieval, hypothesis composition, and hypothesis ranking. We\ndevelop an automated framework that extracts critical components - research\nquestions, background surveys, inspirations, and hypotheses - from scientific\npapers across 12 disciplines, with expert validation confirming its accuracy.\nTo prevent data contamination, we focus exclusively on papers published in\n2024, ensuring minimal overlap with LLM pretraining data. Our evaluation\nreveals that LLMs perform well in retrieving inspirations, an\nout-of-distribution task, suggesting their ability to surface novel knowledge\nassociations. This positions LLMs as \"research hypothesis mines\", capable of\nfacilitating automated scientific discovery by generating innovative hypotheses\nat scale with minimal human intervention.\n","date":"2025-03-27"}
{"id":"2503.21250","title":"Orange Quality Grading with Deep Learning","abstract":"  Orange grading is a crucial step in the fruit industry, as it helps to sort\noranges according to different criteria such as size, quality, ripeness, and\nhealth condition, ensuring safety for human consumption and better price\nallocation and client satisfaction. Automated grading enables faster\nprocessing, precision, and reduced human labor. In this paper, we implement a\ndeep learning-based solution for orange grading via machine vision. Unlike\ntypical grading systems that analyze fruits from a single view, we capture\nmultiview images of each single orange in order to enable a richer\nrepresentation. Afterwards, we compose the acquired images into one collage.\nThis enables the analysis of the whole orange skin. We train a convolutional\nneural network (CNN) on the composed images to grade the oranges into three\nclasses, namely good, bad, and undefined. We also evaluate the performance with\ntwo different CNNs (ResNet-18 and SqueezeNet). We show experimentally that\nmulti-view grading is superior to single view grading.\n","date":"2025-03-27"}
{"id":"2503.21251","title":"Dual-Splitting Conformal Prediction for Multi-Step Time Series\n  Forecasting","abstract":"  Time series forecasting is crucial for applications like resource scheduling\nand risk management, where multi-step predictions provide a comprehensive view\nof future trends. Uncertainty Quantification (UQ) is a mainstream approach for\naddressing forecasting uncertainties, with Conformal Prediction (CP) gaining\nattention due to its model-agnostic nature and statistical guarantees. However,\nmost variants of CP are designed for single-step predictions and face\nchallenges in multi-step scenarios, such as reliance on real-time data and\nlimited scalability. This highlights the need for CP methods specifically\ntailored to multi-step forecasting. We propose the Dual-Splitting Conformal\nPrediction (DSCP) method, a novel CP approach designed to capture inherent\ndependencies within time-series data for multi-step forecasting. Experimental\nresults on real-world datasets from four different domains demonstrate that the\nproposed DSCP significantly outperforms existing CP variants in terms of the\nWinkler Score, achieving a performance improvement of up to 23.59% compared to\nstate-of-the-art methods. Furthermore, we deployed the DSCP approach for\nrenewable energy generation and IT load forecasting in power management of a\nreal-world trajectory-based application, achieving an 11.25% reduction in\ncarbon emissions through predictive optimization of data center operations and\ncontrols.\n","date":"2025-03-27"}
{"id":"2503.21254","title":"Vision-to-Music Generation: A Survey","abstract":"  Vision-to-music Generation, including video-to-music and image-to-music\ntasks, is a significant branch of multimodal artificial intelligence\ndemonstrating vast application prospects in fields such as film scoring, short\nvideo creation, and dance music synthesis. However, compared to the rapid\ndevelopment of modalities like text and images, research in vision-to-music is\nstill in its preliminary stage due to its complex internal structure and the\ndifficulty of modeling dynamic relationships with video. Existing surveys focus\non general music generation without comprehensive discussion on\nvision-to-music. In this paper, we systematically review the research progress\nin the field of vision-to-music generation. We first analyze the technical\ncharacteristics and core challenges for three input types: general videos,\nhuman movement videos, and images, as well as two output types of symbolic\nmusic and audio music. We then summarize the existing methodologies on\nvision-to-music generation from the architecture perspective. A detailed review\nof common datasets and evaluation metrics is provided. Finally, we discuss\ncurrent challenges and promising directions for future research. We hope our\nsurvey can inspire further innovation in vision-to-music generation and the\nbroader field of multimodal generation in academic research and industrial\napplications. To follow latest works and foster further innovation in this\nfield, we are continuously maintaining a GitHub repository at\nhttps:\/\/github.com\/wzk1015\/Awesome-Vision-to-Music-Generation.\n","date":"2025-03-27"}
{"id":"2503.21257","title":"OminiAdapt: Learning Cross-Task Invariance for Robust and\n  Environment-Aware Robotic Manipulation","abstract":"  With the rapid development of embodied intelligence, leveraging large-scale\nhuman data for high-level imitation learning on humanoid robots has become a\nfocal point of interest in both academia and industry. However, applying\nhumanoid robots to precision operation domains remains challenging due to the\ncomplexities they face in perception and control processes, the long-standing\nphysical differences in morphology and actuation mechanisms between humanoid\nrobots and humans, and the lack of task-relevant features obtained from\negocentric vision. To address the issue of covariate shift in imitation\nlearning, this paper proposes an imitation learning algorithm tailored for\nhumanoid robots. By focusing on the primary task objectives, filtering out\nbackground information, and incorporating channel feature fusion with spatial\nattention mechanisms, the proposed algorithm suppresses environmental\ndisturbances and utilizes a dynamic weight update strategy to significantly\nimprove the success rate of humanoid robots in accomplishing target tasks.\nExperimental results demonstrate that the proposed method exhibits robustness\nand scalability across various typical task scenarios, providing new ideas and\napproaches for autonomous learning and control in humanoid robots. The project\nwill be open-sourced on GitHub.\n","date":"2025-03-27"}
{"id":"2503.21258","title":"Learn by Reasoning: Analogical Weight Generation for Few-Shot\n  Class-Incremental Learning","abstract":"  Few-shot class-incremental Learning (FSCIL) enables models to learn new\nclasses from limited data while retaining performance on previously learned\nclasses. Traditional FSCIL methods often require fine-tuning parameters with\nlimited new class data and suffer from a separation between learning new\nclasses and utilizing old knowledge. Inspired by the analogical learning\nmechanisms of the human brain, we propose a novel analogical generative method.\nOur approach includes the Brain-Inspired Analogical Generator (BiAG), which\nderives new class weights from existing classes without parameter fine-tuning\nduring incremental stages. BiAG consists of three components: Weight\nSelf-Attention Module (WSA), Weight & Prototype Analogical Attention Module\n(WPAA), and Semantic Conversion Module (SCM). SCM uses Neural Collapse theory\nfor semantic conversion, WSA supplements new class weights, and WPAA computes\nanalogies to generate new class weights. Experiments on miniImageNet, CUB-200,\nand CIFAR-100 datasets demonstrate that our method achieves higher final and\naverage accuracy compared to SOTA methods.\n","date":"2025-03-27"}
{"id":"2503.21259","title":"Reducing CT Metal Artifacts by Learning Latent Space Alignment with\n  Gemstone Spectral Imaging Data","abstract":"  Metal artifacts in CT slices have long posed challenges in medical\ndiagnostics. These artifacts degrade image quality, resulting in suboptimal\nvisualization and complicating the accurate interpretation of tissues adjacent\nto metal implants. To address these issues, we introduce the Latent Gemstone\nSpectral Imaging (GSI) Alignment Framework, which effectively reduces metal\nartifacts while avoiding the introduction of noise information. Our work is\nbased on a key finding that even artifact-affected ordinary CT sequences\ncontain sufficient information to discern detailed structures. The challenge\nlies in the inability to clearly represent this information. To address this\nissue, we developed an Alignment Framework that adjusts the representation of\nordinary CT images to match GSI CT sequences. GSI is an advanced imaging\ntechnique using multiple energy levels to mitigate artifacts caused by metal\nimplants. By aligning the representation to GSI data, we can effectively\nsuppress metal artifacts while clearly revealing detailed structure, without\nintroducing extraneous information into CT sequences. To facilitate the\napplication, we propose a new dataset, Artifacts-GSI, captured from real\npatients with metal implants, and establish a new benchmark based on this\ndataset. Experimental results show that our method significantly reduces metal\nartifacts and greatly enhances the readability of CT slices. All our code and\ndata are available at: https:\/\/um-lab.github.io\/GSI-MAR\/\n","date":"2025-03-27"}
{"id":"2503.21261","title":"HOT: Hadamard-based Optimized Training","abstract":"  It has become increasingly important to optimize backpropagation to reduce\nmemory usage and computational overhead. Achieving this goal is highly\nchallenging, as multiple objectives must be considered jointly while\nmaintaining training quality. In this paper, we focus on matrix multiplication,\nwhich accounts for the largest portion of training costs, and analyze its\nbackpropagation in detail to identify lightweight techniques that offer the\nbest benefits. Based on this analysis, we introduce a novel method,\nHadamard-based Optimized Training (HOT). In this approach, we apply\nHadamard-based optimizations, such as Hadamard quantization and Hadamard\nlow-rank approximation, selectively and with awareness of the suitability of\neach optimization for different backward paths. Additionally, we introduce two\nenhancements: activation buffer compression and layer-wise quantizer selection.\nOur extensive analysis shows that HOT achieves up to 75% memory savings and a\n2.6 times acceleration on real GPUs, with negligible accuracy loss compared to\nFP32 precision.\n","date":"2025-03-27"}
{"id":"2503.21262","title":"vGamba: Attentive State Space Bottleneck for efficient Long-range\n  Dependencies in Visual Recognition","abstract":"  Capturing long-range dependencies efficiently is essential for visual\nrecognition tasks, yet existing methods face limitations. Convolutional neural\nnetworks (CNNs) struggle with restricted receptive fields, while Vision\nTransformers (ViTs) achieve global context and long-range modeling at a high\ncomputational cost. State-space models (SSMs) offer an alternative, but their\napplication in vision remains underexplored. This work introduces vGamba, a\nhybrid vision backbone that integrates SSMs with attention mechanisms to\nenhance efficiency and expressiveness. At its core, the Gamba bottleneck block\nthat includes, Gamba Cell, an adaptation of Mamba for 2D spatial structures,\nalongside a Multi-Head Self-Attention (MHSA) mechanism and a Gated Fusion\nModule for effective feature representation. The interplay of these components\nensures that vGamba leverages the low computational demands of SSMs while\nmaintaining the accuracy of attention mechanisms for modeling long-range\ndependencies in vision tasks. Additionally, the Fusion module enables seamless\ninteraction between these components. Extensive experiments on classification,\ndetection, and segmentation tasks demonstrate that vGamba achieves a superior\ntrade-off between accuracy and computational efficiency, outperforming several\nexisting models.\n","date":"2025-03-27"}
{"id":"2503.21263","title":"Cultivating Game Sense for Yourself: Making VLMs Gaming Experts","abstract":"  Developing agents capable of fluid gameplay in first\/third-person games\nwithout API access remains a critical challenge in Artificial General\nIntelligence (AGI). Recent efforts leverage Vision Language Models (VLMs) as\ndirect controllers, frequently pausing the game to analyze screens and plan\naction through language reasoning. However, this inefficient paradigm\nfundamentally restricts agents to basic and non-fluent interactions: relying on\nisolated VLM reasoning for each action makes it impossible to handle tasks\nrequiring high reactivity (e.g., FPS shooting) or dynamic adaptability (e.g.,\nACT combat). To handle this, we propose a paradigm shift in gameplay agent\ndesign: instead of directly controlling gameplay, VLM develops specialized\nexecution modules tailored for tasks like shooting and combat. These modules\nhandle real-time game interactions, elevating VLM to a high-level developer.\nBuilding upon this paradigm, we introduce GameSense, a gameplay agent framework\nwhere VLM develops task-specific game sense modules by observing task execution\nand leveraging vision tools and neural network training pipelines. These\nmodules encapsulate action-feedback logic, ranging from direct action rules to\nneural network-based decisions. Experiments demonstrate that our framework is\nthe first to achieve fluent gameplay in diverse genres, including ACT, FPS, and\nFlappy Bird, setting a new benchmark for game-playing agents.\n","date":"2025-03-27"}
{"id":"2503.21268","title":"ClimbingCap: Multi-Modal Dataset and Method for Rock Climbing in World\n  Coordinate","abstract":"  Human Motion Recovery (HMR) research mainly focuses on ground-based motions\nsuch as running. The study on capturing climbing motion, an off-ground motion,\nis sparse. This is partly due to the limited availability of climbing motion\ndatasets, especially large-scale and challenging 3D labeled datasets. To\naddress the insufficiency of climbing motion datasets, we collect AscendMotion,\na large-scale well-annotated, and challenging climbing motion dataset. It\nconsists of 412k RGB, LiDAR frames, and IMU measurements, including the\nchallenging climbing motions of 22 skilled climbing coaches across 12 different\nrock walls. Capturing the climbing motions is challenging as it requires\nprecise recovery of not only the complex pose but also the global position of\nclimbers. Although multiple global HMR methods have been proposed, they cannot\nfaithfully capture climbing motions. To address the limitations of HMR methods\nfor climbing, we propose ClimbingCap, a motion recovery method that\nreconstructs continuous 3D human climbing motion in a global coordinate system.\nOne key insight is to use the RGB and LiDAR modalities to separately\nreconstruct motions in camera coordinates and global coordinates and to\noptimize them jointly. We demonstrate the quality of the AscendMotion dataset\nand present promising results from ClimbingCap. The AscendMotion dataset and\nsource code release publicly at \\href{this\nlink}{http:\/\/www.lidarhumanmotion.net\/climbingcap\/}\n","date":"2025-03-27"}
{"id":"2503.21269","title":"Delving Deep into Semantic Relation Distillation","abstract":"  Knowledge distillation has become a cornerstone technique in deep learning,\nfacilitating the transfer of knowledge from complex models to lightweight\ncounterparts. Traditional distillation approaches focus on transferring\nknowledge at the instance level, but fail to capture nuanced semantic\nrelationships within the data. In response, this paper introduces a novel\nmethodology, Semantics-based Relation Knowledge Distillation (SeRKD), which\nreimagines knowledge distillation through a semantics-relation lens among each\nsample. By leveraging semantic components, \\ie, superpixels, SeRKD enables a\nmore comprehensive and context-aware transfer of knowledge, which skillfully\nintegrates superpixel-based semantic extraction with relation-based knowledge\ndistillation for a sophisticated model compression and distillation.\nParticularly, the proposed method is naturally relevant in the domain of Vision\nTransformers (ViTs), where visual tokens serve as fundamental units of\nrepresentation. Experimental evaluations on benchmark datasets demonstrate the\nsuperiority of SeRKD over existing methods, underscoring its efficacy in\nenhancing model performance and generalization capabilities.\n","date":"2025-03-27"}
{"id":"2503.21272","title":"Reinforced Model Merging","abstract":"  The success of large language models has garnered widespread attention for\nmodel merging techniques, especially training-free methods which combine model\ncapabilities within the parameter space. However, two challenges remain: (1)\nuniform treatment of all parameters leads to performance degradation; (2)\nsearch-based algorithms are often inefficient. In this paper, we present an\ninnovative framework termed Reinforced Model Merging (RMM), which encompasses\nan environment and agent tailored for merging tasks. These components interact\nto execute layer-wise merging actions, aiming to search the optimal merging\narchitecture. Notably, RMM operates without any gradient computations on the\noriginal models, rendering it feasible for edge devices. Furthermore, by\nutilizing data subsets during the evaluation process, we addressed the\nbottleneck in the reward feedback phase, thereby accelerating RMM by up to 100\ntimes. Extensive experiments demonstrate that RMM achieves state-of-the-art\nperformance across various vision and NLP datasets and effectively overcomes\nthe limitations of the existing baseline methods. Our code is available at\nhttps:\/\/github.com\/WuDiHJQ\/Reinforced-Model-Merging.\n","date":"2025-03-27"}
{"id":"2503.21277","title":"Zero-Shot Visual Concept Blending Without Text Guidance","abstract":"  We propose a novel, zero-shot image generation technique called \"Visual\nConcept Blending\" that provides fine-grained control over which features from\nmultiple reference images are transferred to a source image. If only a single\nreference image is available, it is difficult to isolate which specific\nelements should be transferred. However, using multiple reference images, the\nproposed approach distinguishes between common and unique features by\nselectively incorporating them into a generated output. By operating within a\npartially disentangled Contrastive Language-Image Pre-training (CLIP) embedding\nspace (from IP-Adapter), our method enables the flexible transfer of texture,\nshape, motion, style, and more abstract conceptual transformations without\nrequiring additional training or text prompts. We demonstrate its effectiveness\nacross a diverse range of tasks, including style transfer, form metamorphosis,\nand conceptual transformations, showing how subtle or abstract attributes\n(e.g., brushstroke style, aerodynamic lines, and dynamism) can be seamlessly\ncombined into a new image. In a user study, participants accurately recognized\nwhich features were intended to be transferred. Its simplicity, flexibility,\nand high-level control make Visual Concept Blending valuable for creative\nfields such as art, design, and content creation, where combining specific\nvisual qualities from multiple inspirations is crucial.\n","date":"2025-03-27"}
{"id":"2503.21284","title":"Multi-Scale Invertible Neural Network for Wide-Range Variable-Rate\n  Learned Image Compression","abstract":"  Autoencoder-based structures have dominated recent learned image compression\nmethods. However, the inherent information loss associated with autoencoders\nlimits their rate-distortion performance at high bit rates and restricts their\nflexibility of rate adaptation. In this paper, we present a variable-rate image\ncompression model based on invertible transform to overcome these limitations.\nSpecifically, we design a lightweight multi-scale invertible neural network,\nwhich bijectively maps the input image into multi-scale latent representations.\nTo improve the compression efficiency, a multi-scale spatial-channel context\nmodel with extended gain units is devised to estimate the entropy of the latent\nrepresentation from high to low levels. Experimental results demonstrate that\nthe proposed method achieves state-of-the-art performance compared to existing\nvariable-rate methods, and remains competitive with recent multi-model\napproaches. Notably, our method is the first learned image compression solution\nthat outperforms VVC across a very wide range of bit rates using a single\nmodel, especially at high bit rates. The source code is available at\nhttps:\/\/github.com\/hytu99\/MSINN-VRLIC.\n","date":"2025-03-27"}
{"id":"2503.21295","title":"R-PRM: Reasoning-Driven Process Reward Modeling","abstract":"  Large language models (LLMs) inevitably make mistakes when performing\nstep-by-step mathematical reasoning. Process Reward Models (PRMs) have emerged\nas a promising solution by evaluating each reasoning step. However, existing\nPRMs typically output evaluation scores directly, limiting both learning\nefficiency and evaluation accuracy, which is further exacerbated by the\nscarcity of annotated data. To address these issues, we propose\nReasoning-Driven Process Reward Modeling (R-PRM). First, we leverage stronger\nLLMs to generate seed data from limited annotations, effectively bootstrapping\nour model's reasoning capabilities and enabling comprehensive step-by-step\nevaluation. Second, we further enhance performance through preference\noptimization, without requiring additional annotated data. Third, we introduce\ninference-time scaling to fully harness the model's reasoning potential.\nExtensive experiments demonstrate R-PRM's effectiveness: on ProcessBench and\nPRMBench, it surpasses strong baselines by 11.9 and 8.5 points in F1 scores,\nrespectively. When applied to guide mathematical reasoning, R-PRM achieves\nconsistent accuracy improvements of over 8.5 points across six challenging\ndatasets. Further analysis reveals that R-PRM exhibits more comprehensive\nevaluation and stronger generalization capabilities, thereby highlighting its\nsignificant potential.\n","date":"2025-03-27"}
{"id":"2503.21303","title":"Simulation-informed deep learning for enhanced SWOT observations of\n  fine-scale ocean dynamics","abstract":"  Oceanic processes at fine scales are crucial yet difficult to observe\naccurately due to limitations in satellite and in-situ measurements. The\nSurface Water and Ocean Topography (SWOT) mission provides high-resolution Sea\nSurface Height (SSH) data, though noise patterns often obscure fine scale\nstructures. Current methods struggle with noisy data or require extensive\nsupervised training, limiting their effectiveness on real-world observations.\nWe introduce SIMPGEN (Simulation-Informed Metric and Prior for Generative\nEnsemble Networks), an unsupervised adversarial learning framework combining\nreal SWOT observations with simulated reference data. SIMPGEN leverages\nwavelet-informed neural metrics to distinguish noisy from clean fields, guiding\nrealistic SSH reconstructions. Applied to SWOT data, SIMPGEN effectively\nremoves noise, preserving fine-scale features better than existing neural\nmethods. This robust, unsupervised approach not only improves SWOT SSH data\ninterpretation but also demonstrates strong potential for broader oceanographic\napplications, including data assimilation and super-resolution.\n","date":"2025-03-27"}
{"id":"2503.21305","title":"DeBackdoor: A Deductive Framework for Detecting Backdoor Attacks on Deep\n  Models with Limited Data","abstract":"  Backdoor attacks are among the most effective, practical, and stealthy\nattacks in deep learning. In this paper, we consider a practical scenario where\na developer obtains a deep model from a third party and uses it as part of a\nsafety-critical system. The developer wants to inspect the model for potential\nbackdoors prior to system deployment. We find that most existing detection\ntechniques make assumptions that are not applicable to this scenario. In this\npaper, we present a novel framework for detecting backdoors under realistic\nrestrictions. We generate candidate triggers by deductively searching over the\nspace of possible triggers. We construct and optimize a smoothed version of\nAttack Success Rate as our search objective. Starting from a broad class of\ntemplate attacks and just using the forward pass of a deep model, we reverse\nengineer the backdoor attack. We conduct extensive evaluation on a wide range\nof attacks, models, and datasets, with our technique performing almost\nperfectly across these settings.\n","date":"2025-03-27"}
{"id":"2503.21307","title":"InternVL-X: Advancing and Accelerating InternVL Series with Efficient\n  Visual Token Compression","abstract":"  Most multimodal large language models (MLLMs) treat visual tokens as \"a\nsequence of text\", integrating them with text tokens into a large language\nmodel (LLM). However, a great quantity of visual tokens significantly increases\nthe demand for computational resources and time. In this paper, we propose\nInternVL-X, which outperforms the InternVL model in both performance and\nefficiency by incorporating three visual token compression methods. First, we\npropose a novel vision-language projector, PVTC. This component integrates\nadjacent visual embeddings to form a local query and utilizes the transformed\nCLS token as a global query, then performs point-to-region cross-attention\nthrough these local and global queries to more effectively convert visual\nfeatures. Second, we present a layer-wise visual token compression module,\nLVTC, which compresses tokens in the LLM shallow layers and then expands them\nthrough upsampling and residual connections in the deeper layers. This\nsignificantly enhances the model computational efficiency. Futhermore, we\npropose an efficient high resolution slicing method, RVTC, which dynamically\nadjusts the number of visual tokens based on image area or length filtering.\nRVTC greatly enhances training efficiency with only a slight reduction in\nperformance. By utilizing 20% or fewer visual tokens, InternVL-X achieves\nstate-of-the-art performance on 7 public MLLM benchmarks, and improves the\naverage metric by 2.34% across 12 tasks.\n","date":"2025-03-27"}
{"id":"2503.21309","title":"FineCIR: Explicit Parsing of Fine-Grained Modification Semantics for\n  Composed Image Retrieval","abstract":"  Composed Image Retrieval (CIR) facilitates image retrieval through a\nmultimodal query consisting of a reference image and modification text. The\nreference image defines the retrieval context, while the modification text\nspecifies desired alterations. However, existing CIR datasets predominantly\nemploy coarse-grained modification text (CoarseMT), which inadequately captures\nfine-grained retrieval intents. This limitation introduces two key challenges:\n(1) ignoring detailed differences leads to imprecise positive samples, and (2)\ngreater ambiguity arises when retrieving visually similar images. These issues\ndegrade retrieval accuracy, necessitating manual result filtering or repeated\nqueries. To address these limitations, we develop a robust fine-grained CIR\ndata annotation pipeline that minimizes imprecise positive samples and enhances\nCIR systems' ability to discern modification intents accurately. Using this\npipeline, we refine the FashionIQ and CIRR datasets to create two fine-grained\nCIR datasets: Fine-FashionIQ and Fine-CIRR. Furthermore, we introduce FineCIR,\nthe first CIR framework explicitly designed to parse the modification text.\nFineCIR effectively captures fine-grained modification semantics and aligns\nthem with ambiguous visual entities, enhancing retrieval precision. Extensive\nexperiments demonstrate that FineCIR consistently outperforms state-of-the-art\nCIR baselines on both fine-grained and traditional CIR benchmark datasets. Our\nFineCIR code and fine-grained CIR datasets are available at\nhttps:\/\/github.com\/SDU-L\/FineCIR.git.\n","date":"2025-03-27"}
{"id":"2503.21313","title":"HORT: Monocular Hand-held Objects Reconstruction with Transformers","abstract":"  Reconstructing hand-held objects in 3D from monocular images remains a\nsignificant challenge in computer vision. Most existing approaches rely on\nimplicit 3D representations, which produce overly smooth reconstructions and\nare time-consuming to generate explicit 3D shapes. While more recent methods\ndirectly reconstruct point clouds with diffusion models, the multi-step\ndenoising makes high-resolution reconstruction inefficient. To address these\nlimitations, we propose a transformer-based model to efficiently reconstruct\ndense 3D point clouds of hand-held objects. Our method follows a coarse-to-fine\nstrategy, first generating a sparse point cloud from the image and\nprogressively refining it into a dense representation using pixel-aligned image\nfeatures. To enhance reconstruction accuracy, we integrate image features with\n3D hand geometry to jointly predict the object point cloud and its pose\nrelative to the hand. Our model is trained end-to-end for optimal performance.\nExperimental results on both synthetic and real datasets demonstrate that our\nmethod achieves state-of-the-art accuracy with much faster inference speed,\nwhile generalizing well to in-the-wild images.\n","date":"2025-03-27"}
{"id":"2503.21315","title":"Tricking Retrievers with Influential Tokens: An Efficient Black-Box\n  Corpus Poisoning Attack","abstract":"  Retrieval-augmented generation (RAG) systems enhance large language models by\nincorporating external knowledge, addressing issues like outdated internal\nknowledge and hallucination. However, their reliance on external knowledge\nbases makes them vulnerable to corpus poisoning attacks, where adversarial\npassages can be injected to manipulate retrieval results. Existing methods for\ncrafting such passages, such as random token replacement or training inversion\nmodels, are often slow and computationally expensive, requiring either access\nto retriever's gradients or large computational resources. To address these\nlimitations, we propose Dynamic Importance-Guided Genetic Algorithm (DIGA), an\nefficient black-box method that leverages two key properties of retrievers:\ninsensitivity to token order and bias towards influential tokens. By focusing\non these characteristics, DIGA dynamically adjusts its genetic operations to\ngenerate effective adversarial passages with significantly reduced time and\nmemory usage. Our experimental evaluation shows that DIGA achieves superior\nefficiency and scalability compared to existing methods, while maintaining\ncomparable or better attack success rates across multiple datasets.\n","date":"2025-03-27"}
{"id":"2503.21321","title":"Explainable Boosting Machine for Predicting Claim Severity and Frequency\n  in Car Insurance","abstract":"  In a context of constant increase in competition and heightened regulatory\npressure, accuracy, actuarial precision, as well as transparency and\nunderstanding of the tariff, are key issues in non-life insurance.\nTraditionally used generalized linear models (GLM) result in a multiplicative\ntariff that favors interpretability. With the rapid development of machine\nlearning and deep learning techniques, actuaries and the rest of the insurance\nindustry have adopted these techniques widely. However, there is a need to\nassociate them with interpretability techniques. In this paper, our study\nfocuses on introducing an Explainable Boosting Machine (EBM) model that\ncombines intrinsically interpretable characteristics and high prediction\nperformance. This approach is described as a glass-box model and relies on the\nuse of a Generalized Additive Model (GAM) and a cyclic gradient boosting\nalgorithm. It accounts for univariate and pairwise interaction effects between\nfeatures and provides naturally explanations on them. We implement this\napproach on car insurance frequency and severity data and extensively compare\nthe performance of this approach with classical competitors: a GLM, a GAM, a\nCART model and an Extreme Gradient Boosting (XGB) algorithm. Finally, we\nexamine the interpretability of these models to capture the main determinants\nof claim costs.\n","date":"2025-03-27"}
{"id":"2503.21322","title":"HyperGraphRAG: Retrieval-Augmented Generation with Hypergraph-Structured\n  Knowledge Representation","abstract":"  While standard Retrieval-Augmented Generation (RAG) based on chunks, GraphRAG\nstructures knowledge as graphs to leverage the relations among entities.\nHowever, previous GraphRAG methods are limited by binary relations: one edge in\nthe graph only connects two entities, which cannot well model the n-ary\nrelations among more than two entities that widely exist in reality. To address\nthis limitation, we propose HyperGraphRAG, a novel hypergraph-based RAG method\nthat represents n-ary relational facts via hyperedges, modeling the complicated\nn-ary relations in the real world. To retrieve and generate over hypergraphs,\nwe introduce a complete pipeline with a hypergraph construction method, a\nhypergraph retrieval strategy, and a hypergraph-guided generation mechanism.\nExperiments across medicine, agriculture, computer science, and law demonstrate\nthat HyperGraphRAG outperforms standard RAG and GraphRAG in accuracy and\ngeneration quality.\n","date":"2025-03-27"}
{"id":"2503.21323","title":"DuckSegmentation: A segmentation model based on the AnYue Hemp Duck\n  Dataset","abstract":"  The modernization of smart farming is a way to improve agricultural\nproduction efficiency, and improve the agricultural production environment.\nAlthough many large models have achieved high accuracy in the task of object\nrecognition and segmentation, they cannot really be put into use in the farming\nindustry due to their own poor interpretability and limitations in\ncomputational volume. In this paper, we built AnYue Shelduck Dateset, which\ncontains a total of 1951 Shelduck datasets, and performed target detection and\nsegmentation annotation with the help of professional annotators. Based on\nAnYue ShelduckDateset, this paper describes DuckProcessing, an efficient and\npowerful module for duck identification based on real shelduckfarms. First of\nall, using the YOLOv8 module designed to divide the mahjong between them,\nPrecision reached 98.10%, Recall reached 96.53% and F1 score reached 0.95 on\nthe test set. Again using the DuckSegmentation segmentation model,\nDuckSegmentation reached 96.43% mIoU. Finally, the excellent DuckSegmentation\nwas used as the teacher model, and through knowledge distillation, Deeplabv3\nr50 was used as the student model, and the final student model achieved 94.49%\nmIoU on the test set. The method provides a new way of thinking in practical\nsisal duck smart farming.\n","date":"2025-03-27"}
{"id":"2503.21332","title":"ReFeed: Multi-dimensional Summarization Refinement with Reflective\n  Reasoning on Feedback","abstract":"  Summarization refinement faces challenges when extending to multi-dimension.\nIn this paper, we introduce ReFeed, a powerful summarization refinement\npipeline that enhances multiple dimensions through reflective reasoning on\nfeedback. To achieve this, we release SumFeed-CoT, a large-scale Long-CoT-based\ndataset optimized for training a lightweight model with reflective reasoning.\nOur experiments reveal how the number of dimensions, feedback exposure, and\nreasoning policy influence refinement performance, highlighting reflective\nreasoning and simultaneously addressing multiple feedback is crucial to\nmitigate trade-off between dimensions. Furthermore, ReFeed is robust to noisy\nfeedback and feedback order. Lastly, our finding emphasizes that creating data\nwith a proper goal and guideline constitutes a fundamental pillar of effective\nreasoning. The dataset and model will be released.\n","date":"2025-03-27"}
{"id":"2503.21335","title":"A Low-Power Streaming Speech Enhancement Accelerator For Edge Devices","abstract":"  Transformer-based speech enhancement models yield impressive results.\nHowever, their heterogeneous and complex structure restricts model compression\npotential, resulting in greater complexity and reduced hardware efficiency.\nAdditionally, these models are not tailored for streaming and low-power\napplications. Addressing these challenges, this paper proposes a low-power\nstreaming speech enhancement accelerator through model and hardware\noptimization. The proposed high performance model is optimized for hardware\nexecution with the co-design of model compression and target application, which\nreduces 93.9\\% of model size by the proposed domain-aware and streaming-aware\npruning techniques. The required latency is further reduced with batch\nnormalization-based transformers. Additionally, we employed softmax-free\nattention, complemented by an extra batch normalization, facilitating simpler\nhardware design. The tailored hardware accommodates these diverse computing\npatterns by breaking them down into element-wise multiplication and\naccumulation (MAC). This is achieved through a 1-D processing array, utilizing\nconfigurable SRAM addressing, thereby minimizing hardware complexities and\nsimplifying zero skipping. Using the TSMC 40nm CMOS process, the final\nimplementation requires merely 207.8K gates and 53.75KB SRAM. It consumes only\n8.08 mW for real-time inference at a 62.5MHz frequency.\n","date":"2025-03-27"}
{"id":"2503.21337","title":"A 71.2-$\\mu$W Speech Recognition Accelerator with Recurrent Spiking\n  Neural Network","abstract":"  This paper introduces a 71.2-$\\mu$W speech recognition accelerator designed\nfor edge devices' real-time applications, emphasizing an ultra low power\ndesign. Achieved through algorithm and hardware co-optimizations, we propose a\ncompact recurrent spiking neural network with two recurrent layers, one fully\nconnected layer, and a low time step (1 or 2). The 2.79-MB model undergoes\npruning and 4-bit fixed-point quantization, shrinking it by 96.42\\% to 0.1 MB.\nOn the hardware front, we take advantage of \\textit{mixed-level pruning},\n\\textit{zero-skipping} and \\textit{merged spike} techniques, reducing\ncomplexity by 90.49\\% to 13.86 MMAC\/S. The \\textit{parallel time-step\nexecution} addresses inter-time-step data dependencies and enables weight\nbuffer power savings through weight sharing. Capitalizing on the sparse spike\nactivity, an input broadcasting scheme eliminates zero computations, further\nsaving power. Implemented on the TSMC 28-nm process, the design operates in\nreal time at 100 kHz, consuming 71.2 $\\mu$W, surpassing state-of-the-art\ndesigns. At 500 MHz, it has 28.41 TOPS\/W and 1903.11 GOPS\/mm$^2$ in energy and\narea efficiency, respectively.\n","date":"2025-03-27"}
{"id":"2503.21338","title":"UGNA-VPR: A Novel Training Paradigm for Visual Place Recognition Based\n  on Uncertainty-Guided NeRF Augmentation","abstract":"  Visual place recognition (VPR) is crucial for robots to identify previously\nvisited locations, playing an important role in autonomous navigation in both\nindoor and outdoor environments. However, most existing VPR datasets are\nlimited to single-viewpoint scenarios, leading to reduced recognition accuracy,\nparticularly in multi-directional driving or feature-sparse scenes. Moreover,\nobtaining additional data to mitigate these limitations is often expensive.\nThis paper introduces a novel training paradigm to improve the performance of\nexisting VPR networks by enhancing multi-view diversity within current datasets\nthrough uncertainty estimation and NeRF-based data augmentation. Specifically,\nwe initially train NeRF using the existing VPR dataset. Then, our devised\nself-supervised uncertainty estimation network identifies places with high\nuncertainty. The poses of these uncertain places are input into NeRF to\ngenerate new synthetic observations for further training of VPR networks.\nAdditionally, we propose an improved storage method for efficient organization\nof augmented and original training data. We conducted extensive experiments on\nthree datasets and tested three different VPR backbone networks. The results\ndemonstrate that our proposed training paradigm significantly improves VPR\nperformance by fully utilizing existing data, outperforming other training\napproaches. We further validated the effectiveness of our approach on\nself-recorded indoor and outdoor datasets, consistently demonstrating superior\nresults. Our dataset and code have been released at\n\\href{https:\/\/github.com\/nubot-nudt\/UGNA-VPR}{https:\/\/github.com\/nubot-nudt\/UGNA-VPR}.\n","date":"2025-03-27"}
{"id":"2503.21346","title":"Scalable Expectation Estimation with Subtractive Mixture Models","abstract":"  Many Monte Carlo (MC) and importance sampling (IS) methods use mixture models\n(MMs) for their simplicity and ability to capture multimodal distributions.\nRecently, subtractive mixture models (SMMs), i.e. MMs with negative\ncoefficients, have shown greater expressiveness and success in generative\nmodeling. However, their negative parameters complicate sampling, requiring\ncostly auto-regressive techniques or accept-reject algorithms that do not scale\nin high dimensions. In this work, we use the difference representation of SMMs\nto construct an unbiased IS estimator ($\\Delta\\text{Ex}$) that removes the need\nto sample from the SMM, enabling high-dimensional expectation estimation with\nSMMs. In our experiments, we show that $\\Delta\\text{Ex}$ can achieve comparable\nestimation quality to auto-regressive sampling while being considerably faster\nin MC estimation. Moreover, we conduct initial experiments with\n$\\Delta\\text{Ex}$ using hand-crafted proposals, gaining first insights into how\nto construct safe proposals for $\\Delta\\text{Ex}$.\n","date":"2025-03-27"}
{"id":"2503.21347","title":"Residual Learning Inspired Crossover Operator and Strategy Enhancements\n  for Evolutionary Multitasking","abstract":"  In evolutionary multitasking, strategies such as crossover operators and\nskill factor assignment are critical for effective knowledge transfer. Existing\nimprovements to crossover operators primarily focus on low-dimensional variable\ncombinations, such as arithmetic crossover or partially mapped crossover, which\nare insufficient for modeling complex high-dimensional interactions.Moreover,\nstatic or semi-dynamic crossover strategies fail to adapt to the dynamic\ndependencies among tasks. In addition, current Multifactorial Evolutionary\nAlgorithm frameworks often rely on fixed skill factor assignment strategies,\nlacking flexibility. To address these limitations, this paper proposes the\nMultifactorial Evolutionary Algorithm-Residual Learning (MFEA-RL) method based\non residual learning. The method employs a Very Deep Super-Resolution (VDSR)\nmodel to generate high-dimensional residual representations of individuals,\nenhancing the modeling of complex relationships within dimensions. A\nResNet-based mechanism dynamically assigns skill factors to improve task\nadaptability, while a random mapping mechanism efficiently performs crossover\noperations and mitigates the risk of negative transfer. Theoretical analysis\nand experimental results show that MFEA-RL outperforms state-of-the-art\nmultitasking algorithms. It excels in both convergence and adaptability on\nstandard evolutionary multitasking benchmarks, including CEC2017-MTSO and\nWCCI2020-MTSO. Additionally, its effectiveness is validated through a\nreal-world application scenario.\n","date":"2025-03-27"}
{"id":"2503.21349","title":"Fine-Tuning LLMs on Small Medical Datasets: Text Classification and\n  Normalization Effectiveness on Cardiology reports and Discharge records","abstract":"  We investigate the effectiveness of fine-tuning large language models (LLMs)\non small medical datasets for text classification and named entity recognition\ntasks. Using a German cardiology report dataset and the i2b2 Smoking Challenge\ndataset, we demonstrate that fine-tuning small LLMs locally on limited training\ndata can improve performance achieving comparable results to larger models. Our\nexperiments show that fine-tuning improves performance on both tasks, with\nnotable gains observed with as few as 200-300 training examples. Overall, the\nstudy highlights the potential of task-specific fine-tuning of LLMs for\nautomating clinical workflows and efficiently extracting structured data from\nunstructured medical text.\n","date":"2025-03-27"}
{"id":"2503.21352","title":"Using large language models to produce literature reviews: Usages and\n  systematic biases of microphysics parametrizations in 2699 publications","abstract":"  Large language models afford opportunities for using computers for intensive\ntasks, realizing research opportunities that have not been considered before.\nOne such opportunity could be a systematic interrogation of the scientific\nliterature. Here, we show how a large language model can be used to construct a\nliterature review of 2699 publications associated with microphysics\nparametrizations in the Weather and Research Forecasting (WRF) model, with the\ngoal of learning how they were used and their systematic biases, when\nsimulating precipitation. The database was constructed of publications\nidentified from Web of Science and Scopus searches. The large language model\nGPT-4 Turbo was used to extract information about model configurations and\nperformance from the text of 2699 publications. Our results reveal the\nlandscape of how nine of the most popular microphysics parameterizations have\nbeen used around the world: Lin, Ferrier, WRF Single-Moment, Goddard Cumulus\nEnsemble, Morrison, Thompson, and WRF Double-Moment. More studies used\none-moment parameterizations before 2020 and two-moment parameterizations after\n2020. Seven out of nine parameterizations tended to overestimate precipitation.\nHowever, systematic biases of parameterizations differed in various regions.\nExcept simulations using the Lin, Ferrier, and Goddard parameterizations that\ntended to underestimate precipitation over almost all locations, the remaining\nsix parameterizations tended to overestimate, particularly over China,\nsoutheast Asia, western United States, and central Africa. This method could be\nused by other researchers to help understand how the increasingly massive body\nof scientific literature can be harnessed through the power of artificial\nintelligence to solve their research problems.\n","date":"2025-03-27"}
{"id":"2503.21356","title":"Investigating the Duality of Interpretability and Explainability in\n  Machine Learning","abstract":"  The rapid evolution of machine learning (ML) has led to the widespread\nadoption of complex \"black box\" models, such as deep neural networks and\nensemble methods. These models exhibit exceptional predictive performance,\nmaking them invaluable for critical decision-making across diverse domains\nwithin society. However, their inherently opaque nature raises concerns about\ntransparency and interpretability, making them untrustworthy decision support\nsystems. To alleviate such a barrier to high-stakes adoption, research\ncommunity focus has been on developing methods to explain black box models as a\nmeans to address the challenges they pose. Efforts are focused on explaining\nthese models instead of developing ones that are inherently interpretable.\nDesigning inherently interpretable models from the outset, however, can pave\nthe path towards responsible and beneficial applications in the field of ML. In\nthis position paper, we clarify the chasm between explaining black boxes and\nadopting inherently interpretable models. We emphasize the imperative need for\nmodel interpretability and, following the purpose of attaining better (i.e.,\nmore effective or efficient w.r.t. predictive performance) and trustworthy\npredictors, provide an experimental evaluation of latest hybrid learning\nmethods that integrates symbolic knowledge into neural network predictors. We\ndemonstrate how interpretable hybrid models could potentially supplant black\nbox ones in different domains.\n","date":"2025-03-27"}
{"id":"2503.21360","title":"From User Preferences to Optimization Constraints Using Large Language\n  Models","abstract":"  This work explores using Large Language Models (LLMs) to translate user\npreferences into energy optimization constraints for home appliances. We\ndescribe a task where natural language user utterances are converted into\nformal constraints for smart appliances, within the broader context of a\nrenewable energy community (REC) and in the Italian scenario. We evaluate the\neffectiveness of various LLMs currently available for Italian in translating\nthese preferences resorting to classical zero-shot, one-shot, and few-shot\nlearning settings, using a pilot dataset of Italian user requests paired with\ncorresponding formal constraint representation. Our contributions include\nestablishing a baseline performance for this task, publicly releasing the\ndataset and code for further research, and providing insights on observed best\npractices and limitations of LLMs in this particular domain\n","date":"2025-03-27"}
{"id":"2503.21364","title":"LandMarkSystem Technical Report","abstract":"  3D reconstruction is vital for applications in autonomous driving, virtual\nreality, augmented reality, and the metaverse. Recent advancements such as\nNeural Radiance Fields(NeRF) and 3D Gaussian Splatting (3DGS) have transformed\nthe field, yet traditional deep learning frameworks struggle to meet the\nincreasing demands for scene quality and scale. This paper introduces\nLandMarkSystem, a novel computing framework designed to enhance multi-scale\nscene reconstruction and rendering. By leveraging a componentized model\nadaptation layer, LandMarkSystem supports various NeRF and 3DGS structures\nwhile optimizing computational efficiency through distributed parallel\ncomputing and model parameter offloading. Our system addresses the limitations\nof existing frameworks, providing dedicated operators for complex 3D sparse\ncomputations, thus facilitating efficient training and rapid inference over\nextensive scenes. Key contributions include a modular architecture, a dynamic\nloading strategy for limited resources, and proven capabilities across multiple\nrepresentative algorithms.This comprehensive solution aims to advance the\nefficiency and effectiveness of 3D reconstruction tasks.To facilitate further\nresearch and collaboration, the source code and documentation for the\nLandMarkSystem project are publicly available in an open-source repository,\naccessing the repository at: https:\/\/github.com\/InternLandMark\/LandMarkSystem.\n","date":"2025-03-27"}
{"id":"2503.21367","title":"Multimodal surface defect detection from wooden logs for sawing\n  optimization","abstract":"  We propose a novel, good-quality, and less demanding method for detecting\nknots on the surface of wooden logs using multimodal data fusion. Knots are a\nprimary factor affecting the quality of sawn timber, making their detection\nfundamental to any timber grading or cutting optimization system. While X-ray\ncomputed tomography provides accurate knot locations and internal structures,\nit is often too slow or expensive for practical use. An attractive alternative\nis to use fast and cost-effective log surface measurements, such as laser\nscanners or RGB cameras, to detect surface knots and estimate the internal\nstructure of wood. However, due to the small size of knots and noise caused by\nfactors, such as bark and other natural variations, detection accuracy often\nremains low when only one measurement modality is used. In this paper, we\ndemonstrate that by using a data fusion pipeline consisting of separate streams\nfor RGB and point cloud data, combined by a late fusion module, higher knot\ndetection accuracy can be achieved compared to using either modality alone. We\nfurther propose a simple yet efficient sawing angle optimization method that\nutilizes surface knot detections and cross-correlation to minimize the amount\nof unwanted arris knots, demonstrating its benefits over randomized sawing\nangles.\n","date":"2025-03-27"}
{"id":"2503.21377","title":"Unsupervised Real-World Denoising: Sparsity is All You Need","abstract":"  Supervised training for real-world denoising presents challenges due to the\ndifficulty of collecting large datasets of paired noisy and clean images.\nRecent methods have attempted to address this by utilizing unpaired datasets of\nclean and noisy images. Some approaches leverage such unpaired data to train\ndenoisers in a supervised manner by generating synthetic clean-noisy pairs.\nHowever, these methods often fall short due to the distribution gap between\nsynthetic and real noisy images. To mitigate this issue, we propose a solution\nbased on input sparsification, specifically using random input masking. Our\nmethod, which we refer to as Mask, Inpaint and Denoise (MID), trains a denoiser\nto simultaneously denoise and inpaint synthetic clean-noisy pairs. On one hand,\ninput sparsification reduces the gap between synthetic and real noisy images.\nOn the other hand, an inpainter trained in a supervised manner can still\naccurately reconstruct sparse inputs by predicting missing clean pixels using\nthe remaining unmasked pixels. Our approach begins with a synthetic Gaussian\nnoise sampler and iteratively refines it using a noise dataset derived from the\ndenoiser's predictions. The noise dataset is created by subtracting predicted\npseudo-clean images from real noisy images at each iteration. The core\nintuition is that improving the denoiser results in a more accurate noise\ndataset and, consequently, a better noise sampler. We validate our method\nthrough extensive experiments on real-world noisy image datasets, demonstrating\ncompetitive performance compared to existing unsupervised denoising methods.\n","date":"2025-03-27"}
{"id":"2503.21378","title":"Retrieving Time-Series Differences Using Natural Language Queries","abstract":"  Effectively searching time-series data is essential for system analysis;\nhowever, traditional methods often require domain expertise to define search\ncriteria. Recent advancements have enabled natural language-based search, but\nthese methods struggle to handle differences between time-series data. To\naddress this limitation, we propose a natural language query-based approach for\nretrieving pairs of time-series data based on differences specified in the\nquery. Specifically, we define six key characteristics of differences,\nconstruct a corresponding dataset, and develop a contrastive learning-based\nmodel to align differences between time-series data with query texts.\nExperimental results demonstrate that our model achieves an overall mAP score\nof 0.994 in retrieving time-series pairs.\n","date":"2025-03-27"}
{"id":"2503.21380","title":"Challenging the Boundaries of Reasoning: An Olympiad-Level Math\n  Benchmark for Large Language Models","abstract":"  In recent years, the rapid development of large reasoning models has resulted\nin the saturation of existing benchmarks for evaluating mathematical reasoning,\nhighlighting the urgent need for more challenging and rigorous evaluation\nframeworks. To address this gap, we introduce OlymMATH, a novel Olympiad-level\nmathematical benchmark, designed to rigorously test the complex reasoning\ncapabilities of LLMs. OlymMATH features 200 meticulously curated problems, each\nmanually verified and available in parallel English and Chinese versions. The\nproblems are systematically organized into two distinct difficulty tiers: (1)\nAIME-level problems (easy) that establish a baseline for mathematical reasoning\nassessment, and (2) significantly more challenging problems (hard) designed to\npush the boundaries of current state-of-the-art models. In our benchmark, these\nproblems span four core mathematical fields, each including a verifiable\nnumerical solution to enable objective, rule-based evaluation. Empirical\nresults underscore the significant challenge presented by OlymMATH, with\nstate-of-the-art models including DeepSeek-R1 and OpenAI's o3-mini\ndemonstrating notably limited accuracy on the hard subset. Furthermore, the\nbenchmark facilitates comprehensive bilingual assessment of mathematical\nreasoning abilities-a critical dimension that remains largely unaddressed in\nmainstream mathematical reasoning benchmarks. We release the OlymMATH benchmark\nat the STILL project: https:\/\/github.com\/RUCAIBox\/Slow_Thinking_with_LLMs.\n","date":"2025-03-27"}
{"id":"2503.21383","title":"Controlling Large Language Model with Latent Actions","abstract":"  Adapting Large Language Models (LLMs) to downstream tasks using Reinforcement\nLearning (RL) has proven to be an effective approach. However, LLMs do not\ninherently define the structure of an agent for RL training, particularly in\nterms of defining the action space. This paper studies learning a compact\nlatent action space to enhance the controllability and exploration of RL for\nLLMs. We propose Controlling Large Language Models with Latent Actions (CoLA),\na framework that integrates a latent action space into pre-trained LLMs. We\napply CoLA to the Llama-3.1-8B model. Our experiments demonstrate that,\ncompared to RL with token-level actions, CoLA's latent action enables greater\nsemantic diversity in text generation. For enhancing downstream tasks, we show\nthat CoLA with RL achieves a score of 42.4 on the math500 benchmark, surpassing\nthe baseline score of 38.2, and reaches 68.2 when augmented with a Monte Carlo\nTree Search variant. Furthermore, CoLA with RL consistently improves\nperformance on agent-based tasks without degrading the pre-trained LLM's\ncapabilities, unlike the baseline. Finally, CoLA reduces computation time by\nhalf in tasks involving enhanced thinking prompts for LLMs by RL. These results\nhighlight CoLA's potential to advance RL-based adaptation of LLMs for\ndownstream applications.\n","date":"2025-03-27"}
{"id":"2503.21392","title":"HybridoNet-Adapt: A Domain-Adapted Framework for Accurate Lithium-Ion\n  Battery RUL Prediction","abstract":"  Accurate prediction of the remaining useful life (RUL) in Lithium-ion battery\n(LIB) health management systems is crucial for ensuring reliability and safety.\nCurrent methods typically assume that training and testing data share the same\ndistribution, overlooking the benefits of incorporating diverse data sources to\nenhance model performance. To address this limitation, we introduce a\ndata-independent RUL prediction framework along with its domain adaptation (DA)\napproach, which leverages heterogeneous data sources for improved target\npredictions. Our approach integrates comprehensive data preprocessing,\nincluding feature extraction, denoising, and normalization, with a\ndata-independent prediction model that combines Long Short-Term Memory (LSTM),\nMultihead Attention, and a Neural Ordinary Differential Equation (NODE) block,\ntermed HybridoNet. The domain-adapted version, HybridoNet Adapt, is trained\nusing a novel technique inspired by the Domain-Adversarial Neural Network\n(DANN) framework, a regression ensemble method, and Maximum Mean Discrepancy\n(MMD) to learn domain-invariant features from labeled cycling data in the\nsource and target domains. Experimental results demonstrate that our approach\noutperforms state-of-the-art techniques, providing reliable RUL predictions for\nreal-world applications.\n","date":"2025-03-27"}
{"id":"2503.21393","title":"An evaluation of LLMs and Google Translate for translation of selected\n  Indian languages via sentiment and semantic analyses","abstract":"  Large Language models (LLMs) have been prominent for language translation,\nincluding low-resource languages. There has been limited study about the\nassessment of the quality of translations generated by LLMs, including Gemini,\nGPT and Google Translate. In this study, we address this limitation by using\nsemantic and sentiment analysis of selected LLMs for Indian languages,\nincluding Sanskrit, Telugu and Hindi. We select prominent texts that have been\nwell translated by experts and use LLMs to generate their translations to\nEnglish, and then we provide a comparison with selected expert (human)\ntranslations. Our findings suggest that while LLMs have made significant\nprogress in translation accuracy, challenges remain in preserving sentiment and\nsemantic integrity, especially in figurative and philosophical contexts. The\nsentiment analysis revealed that GPT-4o and GPT-3.5 are better at preserving\nthe sentiments for the Bhagavad Gita (Sanskrit-English) translations when\ncompared to Google Translate. We observed a similar trend for the case of Tamas\n(Hindi-English) and Maha P (Telugu-English) translations. GPT-4o performs\nsimilarly to GPT-3.5 in the translation in terms of sentiments for the three\nlanguages. We found that LLMs are generally better at translation for capturing\nsentiments when compared to Google Translate.\n","date":"2025-03-27"}
{"id":"2503.21394","title":"Composable Prompting Workspaces for Creative Writing: Exploration and\n  Iteration Using Dynamic Widgets","abstract":"  Generative AI models offer many possibilities for text creation and\ntransformation. Current graphical user interfaces (GUIs) for prompting them\nlack support for iterative exploration, as they do not represent prompts as\nactionable interface objects. We propose the concept of a composable prompting\ncanvas for text exploration and iteration using dynamic widgets. Users generate\nwidgets through system suggestions, prompting, or manually to capture\ntask-relevant facets that affect the generated text. In a comparative study\nwith a baseline (conversational UI), 18 participants worked on two writing\ntasks, creating diverse prompting environments with custom widgets and spatial\nlayouts. They reported having more control over the generated text and\npreferred our system over the baseline. Our design significantly outperformed\nthe baseline on the Creativity Support Index, and participants felt the results\nwere worth the effort. This work highlights the need for GUIs that support\nuser-driven customization and (re-)structuring to increase both the flexibility\nand efficiency of prompting.\n","date":"2025-03-27"}
{"id":"2503.21397","title":"ProHOC: Probabilistic Hierarchical Out-of-Distribution Classification\n  via Multi-Depth Networks","abstract":"  Out-of-distribution (OOD) detection in deep learning has traditionally been\nframed as a binary task, where samples are either classified as belonging to\nthe known classes or marked as OOD, with little attention given to the semantic\nrelationships between OOD samples and the in-distribution (ID) classes. We\npropose a framework for detecting and classifying OOD samples in a given class\nhierarchy. Specifically, we aim to predict OOD data to their correct internal\nnodes of the class hierarchy, whereas the known ID classes should be predicted\nas their corresponding leaf nodes. Our approach leverages the class hierarchy\nto create a probabilistic model and we implement this model by using networks\ntrained for ID classification at multiple hierarchy depths. We conduct\nexperiments on three datasets with predefined class hierarchies and show the\neffectiveness of our method. Our code is available at\nhttps:\/\/github.com\/walline\/prohoc.\n","date":"2025-03-27"}
{"id":"2503.21401","title":"AcL: Action Learner for Fault-Tolerant Quadruped Locomotion Control","abstract":"  Quadrupedal robots can learn versatile locomotion skills but remain\nvulnerable when one or more joints lose power. In contrast, dogs and cats can\nadopt limping gaits when injured, demonstrating their remarkable ability to\nadapt to physical conditions. Inspired by such adaptability, this paper\npresents Action Learner (AcL), a novel teacher-student reinforcement learning\nframework that enables quadrupeds to autonomously adapt their gait for stable\nwalking under multiple joint faults. Unlike conventional teacher-student\napproaches that enforce strict imitation, AcL leverages teacher policies to\ngenerate style rewards, guiding the student policy without requiring precise\nreplication. We train multiple teacher policies, each corresponding to a\ndifferent fault condition, and subsequently distill them into a single student\npolicy with an encoder-decoder architecture. While prior works primarily\naddress single-joint faults, AcL enables quadrupeds to walk with up to four\nfaulty joints across one or two legs, autonomously switching between different\nlimping gaits when faults occur. We validate AcL on a real Go2 quadruped robot\nunder single- and double-joint faults, demonstrating fault-tolerant, stable\nwalking, smooth gait transitions between normal and lamb gaits, and robustness\nagainst external disturbances.\n","date":"2025-03-27"}
{"id":"2503.21406","title":"Neuro-Symbolic Imitation Learning: Discovering Symbolic Abstractions for\n  Skill Learning","abstract":"  Imitation learning is a popular method for teaching robots new behaviors.\nHowever, most existing methods focus on teaching short, isolated skills rather\nthan long, multi-step tasks. To bridge this gap, imitation learning algorithms\nmust not only learn individual skills but also an abstract understanding of how\nto sequence these skills to perform extended tasks effectively. This paper\naddresses this challenge by proposing a neuro-symbolic imitation learning\nframework. Using task demonstrations, the system first learns a symbolic\nrepresentation that abstracts the low-level state-action space. The learned\nrepresentation decomposes a task into easier subtasks and allows the system to\nleverage symbolic planning to generate abstract plans. Subsequently, the system\nutilizes this task decomposition to learn a set of neural skills capable of\nrefining abstract plans into actionable robot commands. Experimental results in\nthree simulated robotic environments demonstrate that, compared to baselines,\nour neuro-symbolic approach increases data efficiency, improves generalization\ncapabilities, and facilitates interpretability.\n","date":"2025-03-27"}
{"id":"2503.21408","title":"VALLR: Visual ASR Language Model for Lip Reading","abstract":"  Lip Reading, or Visual Automatic Speech Recognition (V-ASR), is a complex\ntask requiring the interpretation of spoken language exclusively from visual\ncues, primarily lip movements and facial expressions. This task is especially\nchallenging due to the absence of auditory information and the inherent\nambiguity when visually distinguishing phonemes that have overlapping visemes\nwhere different phonemes appear identical on the lips. Current methods\ntypically attempt to predict words or characters directly from these visual\ncues, but this approach frequently encounters high error rates due to\ncoarticulation effects and viseme ambiguity. We propose a novel two-stage,\nphoneme-centric framework for Visual Automatic Speech Recognition (V-ASR) that\naddresses these longstanding challenges. First, our model predicts a compact\nsequence of phonemes from visual inputs using a Video Transformer with a CTC\nhead, thereby reducing the task complexity and achieving robust speaker\ninvariance. This phoneme output then serves as the input to a fine-tuned Large\nLanguage Model (LLM), which reconstructs coherent words and sentences by\nleveraging broader linguistic context. Unlike existing methods that either\npredict words directly-often faltering on visually similar phonemes-or rely on\nlarge-scale multimodal pre-training, our approach explicitly encodes\nintermediate linguistic structure while remaining highly data efficient. We\ndemonstrate state-of-the-art performance on two challenging datasets, LRS2 and\nLRS3, where our method achieves significant reductions in Word Error Rate (WER)\nachieving a SOTA WER of 18.7 on LRS3 despite using 99.4% less labelled data\nthan the next best approach.\n","date":"2025-03-27"}
{"id":"2503.21410","title":"Diffusion Image Prior","abstract":"  Zero-shot image restoration (IR) methods based on pretrained diffusion models\nhave recently achieved significant success. These methods typically require at\nleast a parametric form of the degradation model. However, in real-world\nscenarios, the degradation may be too complex to define explicitly. To handle\nthis general case, we introduce the Diffusion Image Prior (DIIP). We take\ninspiration from the Deep Image Prior (DIP)[16], since it can be used to remove\nartifacts without the need for an explicit degradation model. However, in\ncontrast to DIP, we find that pretrained diffusion models offer a much stronger\nprior, despite being trained without knowledge from corrupted data. We show\nthat, the optimization process in DIIP first reconstructs a clean version of\nthe image before eventually overfitting to the degraded input, but it does so\nfor a broader range of degradations than DIP. In light of this result, we\npropose a blind image restoration (IR) method based on early stopping, which\ndoes not require prior knowledge of the degradation model. We validate DIIP on\nvarious degradation-blind IR tasks, including JPEG artifact removal, waterdrop\nremoval, denoising and super-resolution with state-of-the-art results.\n","date":"2025-03-27"}
{"id":"2503.21411","title":"Exploring the Roles of Large Language Models in Reshaping Transportation\n  Systems: A Survey, Framework, and Roadmap","abstract":"  Modern transportation systems face pressing challenges due to increasing\ndemand, dynamic environments, and heterogeneous information integration. The\nrapid evolution of Large Language Models (LLMs) offers transformative potential\nto address these challenges. Extensive knowledge and high-level capabilities\nderived from pretraining evolve the default role of LLMs as text generators to\nbecome versatile, knowledge-driven task solvers for intelligent transportation\nsystems. This survey first presents LLM4TR, a novel conceptual framework that\nsystematically categorizes the roles of LLMs in transportation into four\nsynergetic dimensions: information processors, knowledge encoders, component\ngenerators, and decision facilitators. Through a unified taxonomy, we\nsystematically elucidate how LLMs bridge fragmented data pipelines, enhance\npredictive analytics, simulate human-like reasoning, and enable closed-loop\ninteractions across sensing, learning, modeling, and managing tasks in\ntransportation systems. For each role, our review spans diverse applications,\nfrom traffic prediction and autonomous driving to safety analytics and urban\nmobility optimization, highlighting how emergent capabilities of LLMs such as\nin-context learning and step-by-step reasoning can enhance the operation and\nmanagement of transportation systems. We further curate practical guidance,\nincluding available resources and computational guidelines, to support\nreal-world deployment. By identifying challenges in existing LLM-based\nsolutions, this survey charts a roadmap for advancing LLM-driven transportation\nresearch, positioning LLMs as central actors in the next generation of\ncyber-physical-social mobility ecosystems. Online resources can be found in the\nproject page: https:\/\/github.com\/tongnie\/awesome-llm4tr.\n","date":"2025-03-27"}
{"id":"2503.21412","title":"Federated Intelligence: When Large AI Models Meet Federated Fine-Tuning\n  and Collaborative Reasoning at the Network Edge","abstract":"  Large artificial intelligence (AI) models exhibit remarkable capabilities in\nvarious application scenarios, but deploying them at the network edge poses\nsignificant challenges due to issues such as data privacy, computational\nresources, and latency. In this paper, we explore federated fine-tuning and\ncollaborative reasoning techniques to facilitate the implementation of large AI\nmodels in resource-constrained wireless networks. Firstly, promising\napplications of large AI models within specific domains are discussed.\nSubsequently, federated fine-tuning methods are proposed to adapt large AI\nmodels to specific tasks or environments at the network edge, effectively\naddressing the challenges associated with communication overhead and enhancing\ncommunication efficiency. These methodologies follow clustered, hierarchical,\nand asynchronous paradigms to effectively tackle privacy issues and eliminate\ndata silos. Furthermore, to enhance operational efficiency and reduce latency,\nefficient frameworks for model collaborative reasoning are developed, which\ninclude decentralized horizontal collaboration, cloud-edge-end vertical\ncollaboration, and multi-access collaboration. Next, simulation results\ndemonstrate the effectiveness of our proposed methods in reducing the\nfine-tuning loss of large AI models across various downstream tasks. Finally,\nseveral open challenges and research opportunities are outlined.\n","date":"2025-03-27"}
{"id":"2503.21415","title":"Workshop Scientific HPC in the pre-Exascale era (part of ITADATA 2024)\n  Proceedings","abstract":"  The proceedings of Workshop Scientific HPC in the pre-Exascale era (SHPC),\nheld in Pisa, Italy, September 18, 2024, are part of 3rd Italian Conference on\nBig Data and Data Science (ITADATA2024) proceedings (arXiv: 2503.14937).\n  The main objective of SHPC workshop was to discuss how the current most\ncritical questions in HPC emerge in astrophysics, cosmology, and other\nscientific contexts and experiments. In particular, SHPC workshop focused on:\n  $\\bullet$ Scientific (mainly in astrophysical and medical fields)\napplications toward (pre-)Exascale computing\n  $\\bullet$ Performance portability\n  $\\bullet$ Green computing\n  $\\bullet$ Machine learning\n  $\\bullet$ Big Data management\n  $\\bullet$ Programming on heterogeneous architectures\n  $\\bullet$ Programming on accelerators\n  $\\bullet$ I\/O techniques\n","date":"2025-03-26"}
{"id":"2503.21419","title":"Neuroplasticity in Artificial Intelligence -- An Overview and\n  Inspirations on Drop In & Out Learning","abstract":"  Artificial Intelligence (AI) has achieved new levels of performance and\nspread in public usage with the rise of deep neural networks (DNNs). Initially\ninspired by human neurons and their connections, NNs have become the foundation\nof AI models for many advanced architectures. However, some of the most\nintegral processes in the human brain, particularly neurogenesis and\nneuroplasticity in addition to the more spread neuroapoptosis have largely been\nignored in DNN architecture design. Instead, contemporary AI development\npredominantly focuses on constructing advanced frameworks, such as large\nlanguage models, which retain a static structure of neural connections during\ntraining and inference. In this light, we explore how neurogenesis,\nneuroapoptosis, and neuroplasticity can inspire future AI advances.\nSpecifically, we examine analogous activities in artificial NNs, introducing\nthe concepts of ``dropin'' for neurogenesis and revisiting ``dropout'' and\nstructural pruning for neuroapoptosis. We additionally suggest neuroplasticity\ncombining the two for future large NNs in ``life-long learning'' settings\nfollowing the biological inspiration. We conclude by advocating for greater\nresearch efforts in this interdisciplinary domain and identifying promising\ndirections for future exploration.\n","date":"2025-03-27"}
{"id":"2503.21422","title":"From Deep Learning to LLMs: A survey of AI in Quantitative Investment","abstract":"  Quantitative investment (quant) is an emerging, technology-driven approach in\nasset management, increasingy shaped by advancements in artificial\nintelligence. Recent advances in deep learning and large language models (LLMs)\nfor quant finance have improved predictive modeling and enabled agent-based\nautomation, suggesting a potential paradigm shift in this field. In this\nsurvey, taking alpha strategy as a representative example, we explore how AI\ncontributes to the quantitative investment pipeline. We first examine the early\nstage of quant research, centered on human-crafted features and traditional\nstatistical models with an established alpha pipeline. We then discuss the rise\nof deep learning, which enabled scalable modeling across the entire pipeline\nfrom data processing to order execution. Building on this, we highlight the\nemerging role of LLMs in extending AI beyond prediction, empowering autonomous\nagents to process unstructured data, generate alphas, and support\nself-iterative workflows.\n","date":"2025-03-27"}
{"id":"2503.21425","title":"STAMICS: Splat, Track And Map with Integrated Consistency and Semantics\n  for Dense RGB-D SLAM","abstract":"  Simultaneous Localization and Mapping (SLAM) is a critical task in robotics,\nenabling systems to autonomously navigate and understand complex environments.\nCurrent SLAM approaches predominantly rely on geometric cues for mapping and\nlocalization, but they often fail to ensure semantic consistency, particularly\nin dynamic or densely populated scenes. To address this limitation, we\nintroduce STAMICS, a novel method that integrates semantic information with 3D\nGaussian representations to enhance both localization and mapping accuracy.\nSTAMICS consists of three key components: a 3D Gaussian-based scene\nrepresentation for high-fidelity reconstruction, a graph-based clustering\ntechnique that enforces temporal semantic consistency, and an open-vocabulary\nsystem that allows for the classification of unseen objects. Extensive\nexperiments show that STAMICS significantly improves camera pose estimation and\nmap quality, outperforming state-of-the-art methods while reducing\nreconstruction errors. Code will be public available.\n","date":"2025-03-27"}
{"id":"2503.21426","title":"AdvSGM: Differentially Private Graph Learning via Adversarial Skip-gram\n  Model","abstract":"  The skip-gram model (SGM), which employs a neural network to generate node\nvectors, serves as the basis for numerous popular graph embedding techniques.\nHowever, since the training datasets contain sensitive linkage information, the\nparameters of a released SGM may encode private information and pose\nsignificant privacy risks. Differential privacy (DP) is a rigorous standard for\nprotecting individual privacy in data analysis. Nevertheless, when applying\ndifferential privacy to skip-gram in graphs, it becomes highly challenging due\nto the complex link relationships, which potentially result in high sensitivity\nand necessitate substantial noise injection. To tackle this challenge, we\npresent AdvSGM, a differentially private skip-gram for graphs via adversarial\ntraining. Our core idea is to leverage adversarial training to privatize\nskip-gram while improving its utility. Towards this end, we develop a novel\nadversarial training module by devising two optimizable noise terms that\ncorrespond to the parameters of a skip-gram. By fine-tuning the weights between\nmodules within AdvSGM, we can achieve differentially private gradient updates\nwithout additional noise injection. Extensive experimental results on six\nreal-world graph datasets show that AdvSGM preserves high data utility across\ndifferent downstream tasks.\n","date":"2025-03-27"}
{"id":"2503.21431","title":"Nearest Neighbour Equilibrium Clustering","abstract":"  A novel and intuitive nearest neighbours based clustering algorithm is\nintroduced, in which a cluster is defined in terms of an equilibrium condition\nwhich balances its size and cohesiveness. The formulation of the equilibrium\ncondition allows for a quantification of the strength of alignment of each\npoint to a cluster, with these cluster alignment strengths leading naturally to\na model selection criterion which renders the proposed approach fully\nautomatable. The algorithm is simple to implement and computationally\nefficient, and produces clustering solutions of extremely high quality in\ncomparison with relevant benchmarks from the literature. R code to implement\nthe approach is available from https:\/\/github.com\/DavidHofmeyr\/NNEC.\n","date":"2025-03-27"}
{"id":"2503.21432","title":"Exploring the flavor structure of leptons via diffusion models","abstract":"  We propose a method to explore the flavor structure of leptons using\ndiffusion models, which are known as one of generative artificial intelligence\n(generative AI). We consider a simple extension of the Standard Model with the\ntype I seesaw mechanism and train a neural network to generate the neutrino\nmass matrix. By utilizing transfer learning, the diffusion model generates 104\nsolutions that are consistent with the neutrino mass squared differences and\nthe leptonic mixing angles. The distributions of the CP phases and the sums of\nneutrino masses, which are not included in the conditional labels but are\ncalculated from the solutions, exhibit non-trivial tendencies. In addition, the\neffective mass in neutrinoless double beta decay is concentrated near the\nboundaries of the existing confidence intervals, allowing us to verify the\nobtained solutions through future experiments. An inverse approach using the\ndiffusion model is expected to facilitate the experimental verification of\nflavor models from a perspective distinct from conventional analytical methods.\n","date":"2025-03-27"}
{"id":"2503.21435","title":"Graph-to-Vision: Multi-graph Understanding and Reasoning using\n  Vision-Language Models","abstract":"  Graph Neural Networks (GNNs), as the dominant paradigm for graph-structured\nlearning, have long faced dual challenges of exponentially escalating\ncomputational complexity and inadequate cross-scenario generalization\ncapability. With the rapid advancement of multimodal learning, Vision-Language\nModels (VLMs) have demonstrated exceptional cross-modal relational reasoning\ncapabilities and generalization capacities, thereby opening up novel pathways\nfor overcoming the inherent limitations of conventional graph learning\nparadigms. However, current research predominantly concentrates on\ninvestigating the single-graph reasoning capabilities of VLMs, which\nfundamentally fails to address the critical requirement for coordinated\nreasoning across multiple heterogeneous graph data in real-world application\nscenarios. To address these limitations, we propose the first multi-graph joint\nreasoning benchmark for VLMs. Our benchmark encompasses four graph categories:\nknowledge graphs, flowcharts, mind maps, and route maps,with each graph group\naccompanied by three progressively challenging instruction-response pairs.\nLeveraging this benchmark, we conducted comprehensive capability assessments of\nstate-of-the-art VLMs and performed fine-tuning on open-source models. This\nstudy not only addresses the underexplored evaluation gap in multi-graph\nreasoning for VLMs but also empirically validates their generalization\nsuperiority in graph-structured learning.\n","date":"2025-03-27"}
{"id":"2503.21436","title":"Stochastic Engrams for Efficient Continual Learning with Binarized\n  Neural Networks","abstract":"  The ability to learn continuously in artificial neural networks (ANNs) is\noften limited by catastrophic forgetting, a phenomenon in which new knowledge\nbecomes dominant. By taking mechanisms of memory encoding in neuroscience (aka.\nengrams) as inspiration, we propose a novel approach that integrates\nstochastically-activated engrams as a gating mechanism for metaplastic\nbinarized neural networks (mBNNs). This method leverages the computational\nefficiency of mBNNs combined with the robustness of probabilistic memory traces\nto mitigate forgetting and maintain the model's reliability. Previously\nvalidated metaplastic optimization techniques have been incorporated to enhance\nsynaptic stability further. Compared to baseline binarized models and benchmark\nfully connected continual learning approaches, our method is the only strategy\ncapable of reaching average accuracies over 20% in class-incremental scenarios\nand achieving comparable domain-incremental results to full precision\nstate-of-the-art methods. Furthermore, we achieve a significant reduction in\npeak GPU and RAM usage, under 5% and 20%, respectively. Our findings\ndemonstrate (A) an improved stability vs. plasticity trade-off, (B) a reduced\nmemory intensiveness, and (C) an enhanced performance in binarized\narchitectures. By uniting principles of neuroscience and efficient computing,\nwe offer new insights into the design of scalable and robust deep learning\nsystems.\n","date":"2025-03-27"}
{"id":"2503.21438","title":"Dual-Task Learning for Dead Tree Detection and Segmentation with Hybrid\n  Self-Attention U-Nets in Aerial Imagery","abstract":"  Mapping standing dead trees is critical for assessing forest health,\nmonitoring biodiversity, and mitigating wildfire risks, for which aerial\nimagery has proven useful. However, dense canopy structures, spectral overlaps\nbetween living and dead vegetation, and over-segmentation errors limit the\nreliability of existing methods. This study introduces a hybrid postprocessing\nframework that refines deep learning-based tree segmentation by integrating\nwatershed algorithms with adaptive filtering, enhancing boundary delineation,\nand reducing false positives in complex forest environments. Tested on\nhigh-resolution aerial imagery from boreal forests, the framework improved\ninstance-level segmentation accuracy by 41.5% and reduced positional errors by\n57%, demonstrating robust performance in densely vegetated regions. By\nbalancing detection accuracy and over-segmentation artifacts, the method\nenabled the precise identification of individual dead trees, which is critical\nfor ecological monitoring. The framework's computational efficiency supports\nscalable applications, such as wall-to-wall tree mortality mapping over large\ngeographic regions using aerial or satellite imagery. These capabilities\ndirectly benefit wildfire risk assessment (identifying fuel accumulations),\ncarbon stock estimation (tracking emissions from decaying biomass), and\nprecision forestry (targeting salvage loggings). By bridging advanced remote\nsensing techniques with practical forest management needs, this work advances\ntools for large-scale ecological conservation and climate resilience planning.\n","date":"2025-03-27"}
{"id":"2503.21439","title":"Improved Runtime Analysis of a Multi-Valued Compact Genetic Algorithm on\n  Two Generalized OneMax Problems","abstract":"  Recent research in the runtime analysis of estimation of distribution\nalgorithms (EDAs) has focused on univariate EDAs for multi-valued decision\nvariables. In particular, the runtime of the multi-valued cGA (r-cGA) and UMDA\non multi-valued functions has been a significant area of study. Adak and Witt\n(PPSN 2024) and Hamano et al. (ECJ 2024) independently performed a first\nruntime analysis of the r-cGA on the r-valued OneMax function (r-OneMax). Adak\nand Witt also introduced a different r-valued OneMax function called G-OneMax.\nHowever, for that function, only empirical results were provided so far due to\nthe increased complexity of its runtime analysis, since r-OneMax involves\ncategorical values of two types only, while G-OneMax encompasses all possible\nvalues.\n  In this paper, we present the first theoretical runtime analysis of the r-cGA\non the G-OneMax function. We demonstrate that the runtime is O(nr^3 log^2 n log\nr) with high probability. Additionally, we refine the previously established\nruntime analysis of the r-cGA on r-OneMax, improving the previous bound to O(nr\nlog n log r), which improves the state of the art by an asymptotic factor of\nlog n and is tight for the binary case. Moreover, we for the first time include\nthe case of frequency borders.\n","date":"2025-03-27"}
{"id":"2503.21442","title":"RainyGS: Efficient Rain Synthesis with Physically-Based Gaussian\n  Splatting","abstract":"  We consider the problem of adding dynamic rain effects to in-the-wild scenes\nin a physically-correct manner. Recent advances in scene modeling have made\nsignificant progress, with NeRF and 3DGS techniques emerging as powerful tools\nfor reconstructing complex scenes. However, while effective for novel view\nsynthesis, these methods typically struggle with challenging scene editing\ntasks, such as physics-based rain simulation. In contrast, traditional\nphysics-based simulations can generate realistic rain effects, such as\nraindrops and splashes, but they often rely on skilled artists to carefully set\nup high-fidelity scenes. This process lacks flexibility and scalability,\nlimiting its applicability to broader, open-world environments. In this work,\nwe introduce RainyGS, a novel approach that leverages the strengths of both\nphysics-based modeling and 3DGS to generate photorealistic, dynamic rain\neffects in open-world scenes with physical accuracy. At the core of our method\nis the integration of physically-based raindrop and shallow water simulation\ntechniques within the fast 3DGS rendering framework, enabling realistic and\nefficient simulations of raindrop behavior, splashes, and reflections. Our\nmethod supports synthesizing rain effects at over 30 fps, offering users\nflexible control over rain intensity -- from light drizzles to heavy downpours.\nWe demonstrate that RainyGS performs effectively for both real-world outdoor\nscenes and large-scale driving scenarios, delivering more photorealistic and\nphysically-accurate rain effects compared to state-of-the-art methods. Project\npage can be found at https:\/\/pku-vcl-geometry.github.io\/RainyGS\/\n","date":"2025-03-27"}
{"id":"2503.21443","title":"Sparse Bayesian Learning for Label Efficiency in Cardiac Real-Time MRI","abstract":"  Cardiac real-time magnetic resonance imaging (MRI) is an emerging technology\nthat images the heart at up to 50 frames per second, offering insight into the\nrespiratory effects on the heartbeat. However, this method significantly\nincreases the number of images that must be segmented to derive critical health\nindicators. Although neural networks perform well on inner slices, predictions\non outer slices are often unreliable.\n  This work proposes sparse Bayesian learning (SBL) to predict the ventricular\nvolume on outer slices with minimal manual labeling to address this challenge.\nThe ventricular volume over time is assumed to be dominated by sparse\nfrequencies corresponding to the heart and respiratory rates. Moreover, SBL\nidentifies these sparse frequencies on well-segmented inner slices by\noptimizing hyperparameters via type -II likelihood, automatically pruning\nirrelevant components. The identified sparse frequencies guide the selection of\nouter slice images for labeling, minimizing posterior variance.\n  This work provides performance guarantees for the greedy algorithm. Testing\non patient data demonstrates that only a few labeled images are necessary for\naccurate volume prediction. The labeling procedure effectively avoids selecting\ninefficient images. Furthermore, the Bayesian approach provides uncertainty\nestimates, highlighting unreliable predictions (e.g., when choosing suboptimal\nlabels).\n","date":"2025-03-27"}
{"id":"2503.21449","title":"Towards Generating Realistic 3D Semantic Training Data for Autonomous\n  Driving","abstract":"  Semantic scene understanding is crucial for robotics and computer vision\napplications. In autonomous driving, 3D semantic segmentation plays an\nimportant role for enabling safe navigation. Despite significant advances in\nthe field, the complexity of collecting and annotating 3D data is a bottleneck\nin this developments. To overcome that data annotation limitation, synthetic\nsimulated data has been used to generate annotated data on demand. There is\nstill however a domain gap between real and simulated data. More recently,\ndiffusion models have been in the spotlight, enabling close-to-real data\nsynthesis. Those generative models have been recently applied to the 3D data\ndomain for generating scene-scale data with semantic annotations. Still, those\nmethods either rely on image projection or decoupled models trained with\ndifferent resolutions in a coarse-to-fine manner. Such intermediary\nrepresentations impact the generated data quality due to errors added in those\ntransformations. In this work, we propose a novel approach able to generate 3D\nsemantic scene-scale data without relying on any projection or decoupled\ntrained multi-resolution models, achieving more realistic semantic scene data\ngeneration compared to previous state-of-the-art methods. Besides improving 3D\nsemantic scene-scale data synthesis, we thoroughly evaluate the use of the\nsynthetic scene samples as labeled data to train a semantic segmentation\nnetwork. In our experiments, we show that using the synthetic annotated data\ngenerated by our method as training data together with the real semantic\nsegmentation labels, leads to an improvement in the semantic segmentation model\nperformance. Our results show the potential of generated scene-scale point\nclouds to generate more training data to extend existing datasets, reducing the\ndata annotation effort. Our code is available at\nhttps:\/\/github.com\/PRBonn\/3DiSS.\n","date":"2025-03-27"}
{"id":"2503.21457","title":"FaceBench: A Multi-View Multi-Level Facial Attribute VQA Dataset for\n  Benchmarking Face Perception MLLMs","abstract":"  Multimodal large language models (MLLMs) have demonstrated remarkable\ncapabilities in various tasks. However, effectively evaluating these MLLMs on\nface perception remains largely unexplored. To address this gap, we introduce\nFaceBench, a dataset featuring hierarchical multi-view and multi-level\nattributes specifically designed to assess the comprehensive face perception\nabilities of MLLMs. Initially, we construct a hierarchical facial attribute\nstructure, which encompasses five views with up to three levels of attributes,\ntotaling over 210 attributes and 700 attribute values. Based on the structure,\nthe proposed FaceBench consists of 49,919 visual question-answering (VQA) pairs\nfor evaluation and 23,841 pairs for fine-tuning. Moreover, we further develop a\nrobust face perception MLLM baseline, Face-LLaVA, by training with our proposed\nface VQA data. Extensive experiments on various mainstream MLLMs and Face-LLaVA\nare conducted to test their face perception ability, with results also compared\nagainst human performance. The results reveal that, the existing MLLMs are far\nfrom satisfactory in understanding the fine-grained facial attributes, while\nour Face-LLaVA significantly outperforms existing open-source models with a\nsmall amount of training data and is comparable to commercial ones like GPT-4o\nand Gemini. The dataset will be released at\nhttps:\/\/github.com\/CVI-SZU\/FaceBench.\n","date":"2025-03-27"}
{"id":"2503.21458","title":"DATA-WA: Demand-based Adaptive Task Assignment with Dynamic Worker\n  Availability Windows","abstract":"  With the rapid advancement of mobile networks and the widespread use of\nmobile devices, spatial crowdsourcing, which involves assigning location-based\ntasks to mobile workers, has gained significant attention. However, most\nexisting research focuses on task assignment at the current moment, overlooking\nthe fluctuating demand and supply between tasks and workers over time. To\naddress this issue, we introduce an adaptive task assignment problem, which\naims to maximize the number of assigned tasks by dynamically adjusting task\nassignments in response to changing demand and supply. We develop a spatial\ncrowdsourcing framework, namely demand-based adaptive task assignment with\ndynamic worker availability windows, which consists of two components including\ntask demand prediction and task assignment. In the first component, we\nconstruct a graph adjacency matrix representing the demand dependency\nrelationships in different regions and employ a multivariate time series\nlearning approach to predict future task demands. In the task assignment\ncomponent, we adjust tasks to workers based on these predictions, worker\navailability windows, and the current task assignments, where each worker has\nan availability window that indicates the time periods they are available for\ntask assignments. To reduce the search space of task assignments and be\nefficient, we propose a worker dependency separation approach based on graph\npartition and a task value function with reinforcement learning. Experiments on\nreal data demonstrate that our proposals are both effective and efficient.\n","date":"2025-03-27"}
{"id":"2503.21459","title":"RoadSocial: A Diverse VideoQA Dataset and Benchmark for Road Event\n  Understanding from Social Video Narratives","abstract":"  We introduce RoadSocial, a large-scale, diverse VideoQA dataset tailored for\ngeneric road event understanding from social media narratives. Unlike existing\ndatasets limited by regional bias, viewpoint bias and expert-driven\nannotations, RoadSocial captures the global complexity of road events with\nvaried geographies, camera viewpoints (CCTV, handheld, drones) and rich social\ndiscourse. Our scalable semi-automatic annotation framework leverages Text LLMs\nand Video LLMs to generate comprehensive question-answer pairs across 12\nchallenging QA tasks, pushing the boundaries of road event understanding.\nRoadSocial is derived from social media videos spanning 14M frames and 414K\nsocial comments, resulting in a dataset with 13.2K videos, 674 tags and 260K\nhigh-quality QA pairs. We evaluate 18 Video LLMs (open-source and proprietary,\ndriving-specific and general-purpose) on our road event understanding\nbenchmark. We also demonstrate RoadSocial's utility in improving road event\nunderstanding capabilities of general-purpose Video LLMs.\n","date":"2025-03-27"}
{"id":"2503.21460","title":"Large Language Model Agent: A Survey on Methodology, Applications and\n  Challenges","abstract":"  The era of intelligent agents is upon us, driven by revolutionary\nadvancements in large language models. Large Language Model (LLM) agents, with\ngoal-driven behaviors and dynamic adaptation capabilities, potentially\nrepresent a critical pathway toward artificial general intelligence. This\nsurvey systematically deconstructs LLM agent systems through a\nmethodology-centered taxonomy, linking architectural foundations, collaboration\nmechanisms, and evolutionary pathways. We unify fragmented research threads by\nrevealing fundamental connections between agent design principles and their\nemergent behaviors in complex environments. Our work provides a unified\narchitectural perspective, examining how agents are constructed, how they\ncollaborate, and how they evolve over time, while also addressing evaluation\nmethodologies, tool applications, practical challenges, and diverse application\ndomains. By surveying the latest developments in this rapidly evolving field,\nwe offer researchers a structured taxonomy for understanding LLM agents and\nidentify promising directions for future research. The collection is available\nat https:\/\/github.com\/luo-junyu\/Awesome-Agent-Papers.\n","date":"2025-03-27"}
{"id":"2503.21463","title":"Unveiling Latent Information in Transaction Hashes: Hypergraph Learning\n  for Ethereum Ponzi Scheme Detection","abstract":"  With the widespread adoption of Ethereum, financial frauds such as Ponzi\nschemes have become increasingly rampant in the blockchain ecosystem, posing\nsignificant threats to the security of account assets. Existing Ethereum fraud\ndetection methods typically model account transactions as graphs, but this\napproach primarily focuses on binary transactional relationships between\naccounts, failing to adequately capture the complex multi-party interaction\npatterns inherent in Ethereum. To address this, we propose a hypergraph\nmodeling method for the Ponzi scheme detection method in Ethereum, called\nHyperDet. Specifically, we treat transaction hashes as hyperedges that connect\nall the relevant accounts involved in a transaction. Additionally, we design a\ntwo-step hypergraph sampling strategy to significantly reduce computational\ncomplexity. Furthermore, we introduce a dual-channel detection module,\nincluding the hypergraph detection channel and the hyper-homo graph detection\nchannel, to be compatible with existing detection methods. Experimental results\nshow that, compared to traditional homogeneous graph-based methods, the\nhyper-homo graph detection channel achieves significant performance\nimprovements, demonstrating the superiority of hypergraph in Ponzi scheme\ndetection. This research offers innovations for modeling complex relationships\nin blockchain data.\n","date":"2025-03-27"}
{"id":"2503.21464","title":"Harnessing Chain-of-Thought Metadata for Task Routing and Adversarial\n  Prompt Detection","abstract":"  In this work, we propose a metric called Number of Thoughts (NofT) to\ndetermine the difficulty of tasks pre-prompting and support Large Language\nModels (LLMs) in production contexts. By setting thresholds based on the number\nof thoughts, this metric can discern the difficulty of prompts and support more\neffective prompt routing. A 2% decrease in latency is achieved when routing\nprompts from the MathInstruct dataset through quantized, distilled versions of\nDeepseek with 1.7 billion, 7 billion, and 14 billion parameters. Moreover, this\nmetric can be used to detect adversarial prompts used in prompt injection\nattacks with high efficacy. The Number of Thoughts can inform a classifier that\nachieves 95% accuracy in adversarial prompt detection. Our experiments ad\ndatasets used are available on our GitHub page:\nhttps:\/\/github.com\/rymarinelli\/Number_Of_Thoughts\/tree\/main.\n","date":"2025-03-27"}
{"id":"2503.21465","title":"Retinal Fundus Multi-Disease Image Classification using Hybrid\n  CNN-Transformer-Ensemble Architectures","abstract":"  Our research is motivated by the urgent global issue of a large population\naffected by retinal diseases, which are evenly distributed but underserved by\nspecialized medical expertise, particularly in non-urban areas. Our primary\nobjective is to bridge this healthcare gap by developing a comprehensive\ndiagnostic system capable of accurately predicting retinal diseases solely from\nfundus images. However, we faced significant challenges due to limited, diverse\ndatasets and imbalanced class distributions. To overcome these issues, we have\ndevised innovative strategies. Our research introduces novel approaches,\nutilizing hybrid models combining deeper Convolutional Neural Networks (CNNs),\nTransformer encoders, and ensemble architectures sequentially and in parallel\nto classify retinal fundus images into 20 disease labels. Our overarching goal\nis to assess these advanced models' potential in practical applications, with a\nstrong focus on enhancing retinal disease diagnosis accuracy across a broader\nspectrum of conditions. Importantly, our efforts have surpassed baseline model\nresults, with the C-Tran ensemble model emerging as the leader, achieving a\nremarkable model score of 0.9166, surpassing the baseline score of 0.9.\nAdditionally, experiments with the IEViT model showcased equally promising\noutcomes with improved computational efficiency. We've also demonstrated the\neffectiveness of dynamic patch extraction and the integration of domain\nknowledge in computer vision tasks. In summary, our research strives to\ncontribute significantly to retinal disease diagnosis, addressing the critical\nneed for accessible healthcare solutions in underserved regions while aiming\nfor comprehensive and accurate disease prediction.\n","date":"2025-03-27"}
{"id":"2503.21469","title":"Embedding Compression Distortion in Video Coding for Machines","abstract":"  Currently, video transmission serves not only the Human Visual System (HVS)\nfor viewing but also machine perception for analysis. However, existing codecs\nare primarily optimized for pixel-domain and HVS-perception metrics rather than\nthe needs of machine vision tasks. To address this issue, we propose a\nCompression Distortion Representation Embedding (CDRE) framework, which\nextracts machine-perception-related distortion representation and embeds it\ninto downstream models, addressing the information lost during compression and\nimproving task performance. Specifically, to better analyze the\nmachine-perception-related distortion, we design a compression-sensitive\nextractor that identifies compression degradation in the feature domain. For\nefficient transmission, a lightweight distortion codec is introduced to\ncompress the distortion information into a compact representation.\nSubsequently, the representation is progressively embedded into the downstream\nmodel, enabling it to be better informed about compression degradation and\nenhancing performance. Experiments across various codecs and downstream tasks\ndemonstrate that our framework can effectively boost the rate-task performance\nof existing codecs with minimal overhead in terms of bitrate, execution time,\nand number of parameters. Our codes and supplementary materials are released in\nhttps:\/\/github.com\/Ws-Syx\/CDRE\/.\n","date":"2025-03-27"}
{"id":"2503.21473","title":"DeepRV: pre-trained spatial priors for accelerated disease mapping","abstract":"  Recently introduced prior-encoding deep generative models (e.g., PriorVAE,\n$\\pi$VAE, and PriorCVAE) have emerged as powerful tools for scalable Bayesian\ninference by emulating complex stochastic processes like Gaussian processes\n(GPs). However, these methods remain largely a proof-of-concept and\ninaccessible to practitioners. We propose DeepRV, a lightweight, decoder-only\napproach that accelerates training, and enhances real-world applicability in\ncomparison to current VAE-based prior encoding approaches. Leveraging\nprobabilistic programming frameworks (e.g., NumPyro) for inference, DeepRV\nachieves significant speedups while also improving the quality of parameter\ninference, closely matching full MCMC sampling. We showcase its effectiveness\nin process emulation and spatial analysis of the UK using simulated data,\ngender-wise cancer mortality rates for individuals under 50, and HIV prevalence\nin Zimbabwe. To bridge the gap between theory and practice, we provide a\nuser-friendly API, enabling scalable and efficient Bayesian inference.\n","date":"2025-03-27"}
{"id":"2503.21474","title":"The Procedural Content Generation Benchmark: An Open-source Testbed for\n  Generative Challenges in Games","abstract":"  This paper introduces the Procedural Content Generation Benchmark for\nevaluating generative algorithms on different game content creation tasks. The\nbenchmark comes with 12 game-related problems with multiple variants on each\nproblem. Problems vary from creating levels of different kinds to creating rule\nsets for simple arcade games. Each problem has its own content representation,\ncontrol parameters, and evaluation metrics for quality, diversity, and\ncontrollability. This benchmark is intended as a first step towards a\nstandardized way of comparing generative algorithms. We use the benchmark to\nscore three baseline algorithms: a random generator, an evolution strategy, and\na genetic algorithm. Results show that some problems are easier to solve than\nothers, as well as the impact the chosen objective has on quality, diversity,\nand controllability of the generated artifacts.\n","date":"2025-03-27"}
{"id":"2503.21476","title":"Robust DNN Partitioning and Resource Allocation Under Uncertain\n  Inference Time","abstract":"  In edge intelligence systems, deep neural network (DNN) partitioning and data\noffloading can provide real-time task inference for resource-constrained mobile\ndevices. However, the inference time of DNNs is typically uncertain and cannot\nbe precisely determined in advance, presenting significant challenges in\nensuring timely task processing within deadlines. To address the uncertain\ninference time, we propose a robust optimization scheme to minimize the total\nenergy consumption of mobile devices while meeting task probabilistic\ndeadlines. The scheme only requires the mean and variance information of the\ninference time, without any prediction methods or distribution functions. The\nproblem is formulated as a mixed-integer nonlinear programming (MINLP) that\ninvolves jointly optimizing the DNN model partitioning and the allocation of\nlocal CPU\/GPU frequencies and uplink bandwidth. To tackle the problem, we first\ndecompose the original problem into two subproblems: resource allocation and\nDNN model partitioning. Subsequently, the two subproblems with probability\nconstraints are equivalently transformed into deterministic optimization\nproblems using the chance-constrained programming (CCP) method. Finally, the\nconvex optimization technique and the penalty convex-concave procedure (PCCP)\ntechnique are employed to obtain the optimal solution of the resource\nallocation subproblem and a stationary point of the DNN model partitioning\nsubproblem, respectively. The proposed algorithm leverages real-world data from\npopular hardware platforms and is evaluated on widely used DNN models.\nExtensive simulations show that our proposed algorithm effectively addresses\nthe inference time uncertainty with probabilistic deadline guarantees while\nminimizing the energy consumption of mobile devices.\n","date":"2025-03-27"}
{"id":"2503.21477","title":"Fine-Grained Behavior and Lane Constraints Guided Trajectory Prediction\n  Method","abstract":"  Trajectory prediction, as a critical component of autonomous driving systems,\nhas attracted the attention of many researchers. Existing prediction algorithms\nfocus on extracting more detailed scene features or selecting more reasonable\ntrajectory destinations. However, in the face of dynamic and evolving future\nmovements of the target vehicle, these algorithms cannot provide a fine-grained\nand continuous description of future behaviors and lane constraints, which\ndegrades the prediction accuracy. To address this challenge, we present BLNet,\na novel dualstream architecture that synergistically integrates behavioral\nintention recognition and lane constraint modeling through parallel attention\nmechanisms. The framework generates fine-grained behavior state queries\n(capturing spatial-temporal movement patterns) and lane queries (encoding lane\ntopology constraints), supervised by two auxiliary losses, respectively.\nSubsequently, a two-stage decoder first produces trajectory proposals, then\nperforms point-level refinement by jointly incorporating both the continuity of\npassed lanes and future motion features. Extensive experiments on two large\ndatasets, nuScenes and Argoverse, show that our network exhibits significant\nperformance gains over existing direct regression and goal-based algorithms.\n","date":"2025-03-27"}
{"id":"2503.21480","title":"OmniVox: Zero-Shot Emotion Recognition with Omni-LLMs","abstract":"  The use of omni-LLMs (large language models that accept any modality as\ninput), particularly for multimodal cognitive state tasks involving speech, is\nunderstudied. We present OmniVox, the first systematic evaluation of four\nomni-LLMs on the zero-shot emotion recognition task. We evaluate on two widely\nused multimodal emotion benchmarks: IEMOCAP and MELD, and find zero-shot\nomni-LLMs outperform or are competitive with fine-tuned audio models. Alongside\nour audio-only evaluation, we also evaluate omni-LLMs on text only and text and\naudio. We present acoustic prompting, an audio-specific prompting strategy for\nomni-LLMs which focuses on acoustic feature analysis, conversation context\nanalysis, and step-by-step reasoning. We compare our acoustic prompting to\nminimal prompting and full chain-of-thought prompting techniques. We perform a\ncontext window analysis on IEMOCAP and MELD, and find that using context helps,\nespecially on IEMOCAP. We conclude with an error analysis on the generated\nacoustic reasoning outputs from the omni-LLMs.\n","date":"2025-03-27"}
{"id":"2503.21483","title":"BOLT: Boost Large Vision-Language Model Without Training for Long-form\n  Video Understanding","abstract":"  Large video-language models (VLMs) have demonstrated promising progress in\nvarious video understanding tasks. However, their effectiveness in long-form\nvideo analysis is constrained by limited context windows. Traditional\napproaches, such as uniform frame sampling, often inevitably allocate resources\nto irrelevant content, diminishing their effectiveness in real-world scenarios.\nIn this paper, we introduce BOLT, a method to BOost Large VLMs without\nadditional Training through a comprehensive study of frame selection\nstrategies. First, to enable a more realistic evaluation of VLMs in long-form\nvideo understanding, we propose a multi-source retrieval evaluation setting.\nOur findings reveal that uniform sampling performs poorly in noisy contexts,\nunderscoring the importance of selecting the right frames. Second, we explore\nseveral frame selection strategies based on query-frame similarity and analyze\ntheir effectiveness at inference time. Our results show that inverse transform\nsampling yields the most significant performance improvement, increasing\naccuracy on the Video-MME benchmark from 53.8% to 56.1% and MLVU benchmark from\n58.9% to 63.4%. Our code is available at https:\/\/github.com\/sming256\/BOLT.\n","date":"2025-03-27"}
{"id":"2503.21486","title":"Invert2Restore: Zero-Shot Degradation-Blind Image Restoration","abstract":"  Two of the main challenges of image restoration in real-world scenarios are\nthe accurate characterization of an image prior and the precise modeling of the\nimage degradation operator. Pre-trained diffusion models have been very\nsuccessfully used as image priors in zero-shot image restoration methods.\nHowever, how to best handle the degradation operator is still an open problem.\nIn real-world data, methods that rely on specific parametric assumptions about\nthe degradation model often face limitations in their applicability. To address\nthis, we introduce Invert2Restore, a zero-shot, training-free method that\noperates in both fully blind and partially blind settings -- requiring no prior\nknowledge of the degradation model or only partial knowledge of its parametric\nform without known parameters. Despite this, Invert2Restore achieves\nhigh-fidelity results and generalizes well across various types of image\ndegradation. It leverages a pre-trained diffusion model as a deterministic\nmapping between normal samples and undistorted image samples. The key insight\nis that the input noise mapped by a diffusion model to a degraded image lies in\na low-probability density region of the standard normal distribution. Thus, we\ncan restore the degraded image by carefully guiding its input noise toward a\nhigher-density region. We experimentally validate Invert2Restore across several\nimage restoration tasks, demonstrating that it achieves state-of-the-art\nperformance in scenarios where the degradation operator is either unknown or\npartially known.\n","date":"2025-03-27"}
{"id":"2503.21489","title":"Shape Modeling of Longitudinal Medical Images: From Diffeomorphic Metric\n  Mapping to Deep Learning","abstract":"  Living biological tissue is a complex system, constantly growing and changing\nin response to external and internal stimuli. These processes lead to\nremarkable and intricate changes in shape. Modeling and understanding both\nnatural and pathological (or abnormal) changes in the shape of anatomical\nstructures is highly relevant, with applications in diagnostic, prognostic, and\ntherapeutic healthcare. Nevertheless, modeling the longitudinal shape change of\nbiological tissue is a non-trivial task due to its inherent nonlinear nature.\nIn this review, we highlight several existing methodologies and tools for\nmodeling longitudinal shape change (i.e., spatiotemporal shape modeling). These\nmethods range from diffeomorphic metric mapping to deep-learning based\napproaches (e.g., autoencoders, generative networks, recurrent neural networks,\netc.). We discuss the synergistic combinations of existing technologies and\npotential directions for future research, underscoring key deficiencies in the\ncurrent research landscape.\n","date":"2025-03-27"}
{"id":"2503.21495","title":"Adaptive Resampling with Bootstrap for Noisy Multi-Objective\n  Optimization Problems","abstract":"  The challenge of noisy multi-objective optimization lies in the constant\ntrade-off between exploring new decision points and improving the precision of\nknown points through resampling. This decision should take into account both\nthe variability of the objective functions and the current estimate of a point\nin relation to the Pareto front. Since the amount and distribution of noise are\ngenerally unknown, it is desirable for a decision function to be highly\nadaptive to the properties of the optimization problem. This paper presents a\nresampling decision function that incorporates the stochastic nature of the\noptimization problem by using bootstrapping and the probability of dominance.\nThe distribution-free estimation of the probability of dominance is achieved\nusing bootstrap estimates of the means. To make the procedure applicable even\nwith very few observations, we transfer the distribution observed at other\ndecision points. The efficiency of this resampling approach is demonstrated by\napplying it in the NSGA-II algorithm with a sequential resampling procedure\nunder multiple noise variations.\n","date":"2025-03-27"}
{"id":"2503.21500","title":"OpenHuEval: Evaluating Large Language Model on Hungarian Specifics","abstract":"  We introduce OpenHuEval, the first benchmark for LLMs focusing on the\nHungarian language and specifics. OpenHuEval is constructed from a vast\ncollection of Hungarian-specific materials sourced from multiple origins. In\nthe construction, we incorporated the latest design principles for evaluating\nLLMs, such as using real user queries from the internet, emphasizing the\nassessment of LLMs' generative capabilities, and employing LLM-as-judge to\nenhance the multidimensionality and accuracy of evaluations. Ultimately,\nOpenHuEval encompasses eight Hungarian-specific dimensions, featuring five\ntasks and 3953 questions. Consequently, OpenHuEval provides the comprehensive,\nin-depth, and scientifically accurate assessment of LLM performance in the\ncontext of the Hungarian language and its specifics. We evaluated current\nmainstream LLMs, including both traditional LLMs and recently developed Large\nReasoning Models. The results demonstrate the significant necessity for\nevaluation and model optimization tailored to the Hungarian language and\nspecifics. We also established the framework for analyzing the thinking\nprocesses of LRMs with OpenHuEval, revealing intrinsic patterns and mechanisms\nof these models in non-English languages, with Hungarian serving as a\nrepresentative example. We will release OpenHuEval at\nhttps:\/\/github.com\/opendatalab\/OpenHuEval .\n","date":"2025-03-27"}
{"id":"2503.21501","title":"Double Blind Imaging with Generative Modeling","abstract":"  Blind inverse problems in imaging arise from uncertainties in the system used\nto collect (noisy) measurements of images. Recovering clean images from these\nmeasurements typically requires identifying the imaging system, either\nimplicitly or explicitly. A common solution leverages generative models as\npriors for both the images and the imaging system parameters (e.g., a class of\npoint spread functions). To learn these priors in a straightforward manner\nrequires access to a dataset of clean images as well as samples of the imaging\nsystem. We propose an AmbientGAN-based generative technique to identify the\ndistribution of parameters in unknown imaging systems, using only unpaired\nclean images and corrupted measurements. This learned distribution can then be\nused in model-based recovery algorithms to solve blind inverse problems such as\nblind deconvolution. We successfully demonstrate our technique for learning\nGaussian blur and motion blur priors from noisy measurements and show their\nutility in solving blind deconvolution with diffusion posterior sampling.\n","date":"2025-03-27"}
{"id":"2503.21504","title":"Keyword-Oriented Multimodal Modeling for Euphemism Identification","abstract":"  Euphemism identification deciphers the true meaning of euphemisms, such as\nlinking \"weed\" (euphemism) to \"marijuana\" (target keyword) in illicit texts,\naiding content moderation and combating underground markets. While existing\nmethods are primarily text-based, the rise of social media highlights the need\nfor multimodal analysis, incorporating text, images, and audio. However, the\nlack of multimodal datasets for euphemisms limits further research. To address\nthis, we regard euphemisms and their corresponding target keywords as keywords\nand first introduce a keyword-oriented multimodal corpus of euphemisms\n(KOM-Euph), involving three datasets (Drug, Weapon, and Sexuality), including\ntext, images, and speech. We further propose a keyword-oriented multimodal\neuphemism identification method (KOM-EI), which uses cross-modal feature\nalignment and dynamic fusion modules to explicitly utilize the visual and audio\nfeatures of the keywords for efficient euphemism identification. Extensive\nexperiments demonstrate that KOM-EI outperforms state-of-the-art models and\nlarge language models, and show the importance of our multimodal datasets.\n","date":"2025-03-27"}
{"id":"2503.21505","title":"Fine-Grained Evaluation of Large Vision-Language Models in Autonomous\n  Driving","abstract":"  Existing benchmarks for Vision-Language Model (VLM) on autonomous driving\n(AD) primarily assess interpretability through open-form visual question\nanswering (QA) within coarse-grained tasks, which remain insufficient to assess\ncapabilities in complex driving scenarios. To this end, we introduce\n$\\textbf{VLADBench}$, a challenging and fine-grained dataset featuring\nclose-form QAs that progress from static foundational knowledge and elements to\nadvanced reasoning for dynamic on-road situations. The elaborate\n$\\textbf{VLADBench}$ spans 5 key domains: Traffic Knowledge Understanding,\nGeneral Element Recognition, Traffic Graph Generation, Target Attribute\nComprehension, and Ego Decision-Making and Planning. These domains are further\nbroken down into 11 secondary aspects and 29 tertiary tasks for a granular\nevaluation. A thorough assessment of general and domain-specific (DS) VLMs on\nthis benchmark reveals both their strengths and critical limitations in AD\ncontexts. To further exploit the cognitive and reasoning interactions among the\n5 domains for AD understanding, we start from a small-scale VLM and train the\nDS models on individual domain datasets (collected from 1.4M DS QAs across\npublic sources). The experimental results demonstrate that the proposed\nbenchmark provides a crucial step toward a more comprehensive assessment of\nVLMs in AD, paving the way for the development of more cognitively\nsophisticated and reasoning-capable AD systems.\n","date":"2025-03-27"}
{"id":"2503.21507","title":"F-INR: Functional Tensor Decomposition for Implicit Neural\n  Representations","abstract":"  Implicit Neural Representation (INR) has emerged as a powerful tool for\nencoding discrete signals into continuous, differentiable functions using\nneural networks. However, these models often have an unfortunate reliance on\nmonolithic architectures to represent high-dimensional data, leading to\nprohibitive computational costs as dimensionality grows. We propose F-INR, a\nframework that reformulates INR learning through functional tensor\ndecomposition, breaking down high-dimensional tasks into lightweight,\naxis-specific sub-networks. Each sub-network learns a low-dimensional data\ncomponent (e.g., spatial or temporal). Then, we combine these components via\ntensor operations, reducing forward pass complexity while improving accuracy\nthrough specialized learning. F-INR is modular and, therefore,\narchitecture-agnostic, compatible with MLPs, SIREN, WIRE, or other\nstate-of-the-art INR architecture. It is also decomposition-agnostic,\nsupporting CP, TT, and Tucker modes with user-defined rank for speed-accuracy\ncontrol. In our experiments, F-INR trains $100\\times$ faster than existing\napproaches on video tasks while achieving higher fidelity (+3.4 dB PSNR).\nSimilar gains hold for image compression, physics simulations, and 3D geometry\nreconstruction. Through this, F-INR offers a new scalable, flexible solution\nfor high-dimensional signal modeling.\n","date":"2025-03-27"}
{"id":"2503.21510","title":"Uncertainty-aware Bayesian machine learning modelling of land cover\n  classification","abstract":"  Land cover classification involves the production of land cover maps, which\ndetermine the type of land through remote sensing imagery. Over recent years,\nsuch classification is being performed by machine learning classification\nmodels, which can give highly accurate predictions on land cover per pixel\nusing large quantities of input training data. However, such models do not\ncurrently take account of input measurement uncertainty, which is vital for\ntraceability in metrology. In this work we propose a Bayesian classification\nframework using generative modelling to take account of input measurement\nuncertainty. We take the specific case of Bayesian quadratic discriminant\nanalysis, and apply it to land cover datasets from Copernicus Sentinel-2 in\n2020 and 2021. We benchmark the performance of the model against more popular\nclassification models used in land cover maps such as random forests and neural\nnetworks. We find that such Bayesian models are more trustworthy, in the sense\nthat they are more interpretable, explicitly model the input measurement\nuncertainty, and maintain predictive performance of class probability outputs\nacross datasets of different years and sizes, whilst also being computationally\nefficient.\n","date":"2025-03-27"}
{"id":"2503.21513","title":"Datasets for Depression Modeling in Social Media: An Overview","abstract":"  Depression is the most common mental health disorder, and its prevalence\nincreased during the COVID-19 pandemic. As one of the most extensively\nresearched psychological conditions, recent research has increasingly focused\non leveraging social media data to enhance traditional methods of depression\nscreening. This paper addresses the growing interest in interdisciplinary\nresearch on depression, and aims to support early-career researchers by\nproviding a comprehensive and up-to-date list of datasets for analyzing and\npredicting depression through social media data. We present an overview of\ndatasets published between 2019 and 2024. We also make the comprehensive list\nof datasets available online as a continuously updated resource, with the hope\nthat it will facilitate further interdisciplinary research into the linguistic\nexpressions of depression on social media.\n","date":"2025-03-27"}
{"id":"2503.21514","title":"Quantitative Evaluation of Quantum\/Classical Neural Network Using a Game\n  Solver Metric","abstract":"  To evaluate the performance of quantum computing systems relative to\nclassical counterparts and explore the potential for quantum advantage, we\npropose a game-solving benchmark based on Elo ratings in the game of\ntic-tac-toe. We compare classical convolutional neural networks (CNNs), quantum\nconvolutional neural networks (QCNNs), and hybrid classical-quantum models by\nassessing their performance against a random-move agent in automated matches.\nAdditionally, we implement a QCNN integrated with quantum communication and\nevaluate its performance to quantify the overhead introduced by noisy quantum\nchannels. Our results show that the classical-quantum hybrid model achieves Elo\nratings comparable to those of classical CNNs, while the standalone QCNN\nunderperforms under current hardware constraints. The communication overhead\nwas found to be modest. These findings demonstrate the viability of using\ngame-based benchmarks for evaluating quantum computing systems and suggest that\nquantum communication can be incorporated with limited impact on performance,\nproviding a foundation for future hybrid quantum applications.\n","date":"2025-03-27"}
{"id":"2503.21522","title":"MONO2REST: Identifying and Exposing Microservices: a Reusable\n  RESTification Approach","abstract":"  The microservices architectural style has become the de facto standard for\nlarge-scale cloud applications, offering numerous benefits in scalability,\nmaintainability, and deployment flexibility. Many organizations are pursuing\nthe migration of legacy monolithic systems to a microservices architecture.\nHowever, this process is challenging, risky, time-intensive, and\nprone-to-failure while several organizations lack necessary financial\nresources, time, or expertise to set up this migration process. So, rather than\ntrying to migrate a legacy system where migration is risky or not feasible, we\nsuggest exposing it as a microservice application without without having to\nmigrate it. In this paper, we present a reusable, automated, two-phase approach\nthat combines evolutionary algorithms with machine learning techniques. In the\nfirst phase, we identify microservices at the method level using a\nmulti-objective genetic algorithm that considers both structural and semantic\ndependencies between methods. In the second phase, we generate REST APIs for\neach identified microservice using a classification algorithm to assign HTTP\nmethods and endpoints. We evaluated our approach with a case study on the\nSpring PetClinic application, which has both monolithic and microservices\nimplementations that serve as ground truth for comparison. Results demonstrate\nthat our approach successfully aligns identified microservices with those in\nthe reference microservices implementation, highlighting its effectiveness in\nservice identification and API generation.\n","date":"2025-03-27"}
{"id":"2503.21525","title":"ICG-MVSNet: Learning Intra-view and Cross-view Relationships for\n  Guidance in Multi-View Stereo","abstract":"  Multi-view Stereo (MVS) aims to estimate depth and reconstruct 3D point\nclouds from a series of overlapping images. Recent learning-based MVS\nframeworks overlook the geometric information embedded in features and\ncorrelations, leading to weak cost matching. In this paper, we propose\nICG-MVSNet, which explicitly integrates intra-view and cross-view relationships\nfor depth estimation. Specifically, we develop an intra-view feature fusion\nmodule that leverages the feature coordinate correlations within a single image\nto enhance robust cost matching. Additionally, we introduce a lightweight\ncross-view aggregation module that efficiently utilizes the contextual\ninformation from volume correlations to guide regularization. Our method is\nevaluated on the DTU dataset and Tanks and Temples benchmark, consistently\nachieving competitive performance against state-of-the-art works, while\nrequiring lower computational resources.\n","date":"2025-03-27"}
{"id":"2503.21526","title":"Constraint-based causal discovery with tiered background knowledge and\n  latent variables in single or overlapping datasets","abstract":"  In this paper we consider the use of tiered background knowledge within\nconstraint based causal discovery. Our focus is on settings relaxing causal\nsufficiency, i.e. allowing for latent variables which may arise because\nrelevant information could not be measured at all, or not jointly, as in the\ncase of multiple overlapping datasets. We first present novel insights into the\nproperties of the 'tiered FCI' (tFCI) algorithm. Building on this, we introduce\na new extension of the IOD (integrating overlapping datasets) algorithm\nincorporating tiered background knowledge, the 'tiered IOD' (tIOD) algorithm.\nWe show that under full usage of the tiered background knowledge tFCI and tIOD\nare sound, while simple versions of the tIOD and tFCI are sound and complete.\nWe further show that the tIOD algorithm can often be expected to be\nconsiderably more efficient and informative than the IOD algorithm even beyond\nthe obvious restriction of the Markov equivalence classes. We provide a formal\nresult on the conditions for this gain in efficiency and informativeness. Our\nresults are accompanied by a series of examples illustrating the exact role and\nusefulness of tiered background knowledge.\n","date":"2025-03-27"}
{"id":"2503.21528","title":"Bayesian Pseudo Posterior Mechanism for Differentially Private Machine\n  Learning","abstract":"  Differential privacy (DP) is becoming increasingly important for deployed\nmachine learning applications because it provides strong guarantees for\nprotecting the privacy of individuals whose data is used to train models.\nHowever, DP mechanisms commonly used in machine learning tend to struggle on\nmany real world distributions, including highly imbalanced or small labeled\ntraining sets. In this work, we propose a new scalable DP mechanism for deep\nlearning models, SWAG-PPM, by using a pseudo posterior distribution that\ndownweights by-record likelihood contributions proportionally to their\ndisclosure risks as the randomized mechanism. As a motivating example from\nofficial statistics, we demonstrate SWAG-PPM on a workplace injury text\nclassification task using a highly imbalanced public dataset published by the\nU.S. Occupational Safety and Health Administration (OSHA). We find that\nSWAG-PPM exhibits only modest utility degradation against a non-private\ncomparator while greatly outperforming the industry standard DP-SGD for a\nsimilar privacy budget.\n","date":"2025-03-27"}
{"id":"2503.21530","title":"Low-Resource Transliteration for Roman-Urdu and Urdu Using\n  Transformer-Based Models","abstract":"  As the Information Retrieval (IR) field increasingly recognizes the\nimportance of inclusivity, addressing the needs of low-resource languages\nremains a significant challenge. Transliteration between Urdu and its Romanized\nform, Roman Urdu, remains underexplored despite the widespread use of both\nscripts in South Asia. Prior work using RNNs on the Roman-Urdu-Parl dataset\nshowed promising results but suffered from poor domain adaptability and limited\nevaluation. We propose a transformer-based approach using the m2m100\nmultilingual translation model, enhanced with masked language modeling (MLM)\npretraining and fine-tuning on both Roman-Urdu-Parl and the domain-diverse\nDakshina dataset. To address previous evaluation flaws, we introduce rigorous\ndataset splits and assess performance using BLEU, character-level BLEU, and\nCHRF. Our model achieves strong transliteration performance, with Char-BLEU\nscores of 96.37 for Urdu->Roman-Urdu and 97.44 for Roman-Urdu->Urdu. These\nresults outperform both RNN baselines and GPT-4o Mini and demonstrate the\neffectiveness of multilingual transfer learning for low-resource\ntransliteration tasks.\n","date":"2025-03-27"}
{"id":"2503.21536","title":"Exploring the Energy Landscape of RBMs: Reciprocal Space Insights into\n  Bosons, Hierarchical Learning and Symmetry Breaking","abstract":"  Deep generative models have become ubiquitous due to their ability to learn\nand sample from complex distributions. Despite the proliferation of various\nframeworks, the relationships among these models remain largely unexplored, a\ngap that hinders the development of a unified theory of AI learning. We address\ntwo central challenges: clarifying the connections between different deep\ngenerative models and deepening our understanding of their learning mechanisms.\nWe focus on Restricted Boltzmann Machines (RBMs), known for their universal\napproximation capabilities for discrete distributions. By introducing a\nreciprocal space formulation, we reveal a connection between RBMs, diffusion\nprocesses, and coupled Bosons. We show that at initialization, the RBM operates\nat a saddle point, where the local curvature is determined by the singular\nvalues, whose distribution follows the Marcenko-Pastur law and exhibits\nrotational symmetry. During training, this rotational symmetry is broken due to\nhierarchical learning, where different degrees of freedom progressively capture\nfeatures at multiple levels of abstraction. This leads to a symmetry breaking\nin the energy landscape, reminiscent of Landau theory. This symmetry breaking\nin the energy landscape is characterized by the singular values and the weight\nmatrix eigenvector matrix. We derive the corresponding free energy in a\nmean-field approximation. We show that in the limit of infinite size RBM, the\nreciprocal variables are Gaussian distributed. Our findings indicate that in\nthis regime, there will be some modes for which the diffusion process will not\nconverge to the Boltzmann distribution. To illustrate our results, we trained\nreplicas of RBMs with different hidden layer sizes using the MNIST dataset. Our\nfindings bridge the gap between disparate generative frameworks and also shed\nlight on the processes underpinning learning in generative models.\n","date":"2025-03-27"}
{"id":"2503.21538","title":"Formation Shape Control using the Gromov-Wasserstein Metric","abstract":"  This article introduces a formation shape control algorithm, in the optimal\ncontrol framework, for steering an initial population of agents to a desired\nconfiguration via employing the Gromov-Wasserstein distance. The underlying\ndynamical system is assumed to be a constrained linear system and the objective\nfunction is a sum of quadratic control-dependent stage cost and a\nGromov-Wasserstein terminal cost. The inclusion of the Gromov-Wasserstein cost\ntransforms the resulting optimal control problem into a well-known NP-hard\nproblem, making it both numerically demanding and difficult to solve with high\naccuracy. Towards that end, we employ a recent semi-definite relaxation-driven\ntechnique to tackle the Gromov-Wasserstein distance. A numerical example is\nprovided to illustrate our results.\n","date":"2025-03-27"}
{"id":"2503.21541","title":"LOCATEdit: Graph Laplacian Optimized Cross Attention for Localized\n  Text-Guided Image Editing","abstract":"  Text-guided image editing aims to modify specific regions of an image\naccording to natural language instructions while maintaining the general\nstructure and the background fidelity. Existing methods utilize masks derived\nfrom cross-attention maps generated from diffusion models to identify the\ntarget regions for modification. However, since cross-attention mechanisms\nfocus on semantic relevance, they struggle to maintain the image integrity. As\na result, these methods often lack spatial consistency, leading to editing\nartifacts and distortions. In this work, we address these limitations and\nintroduce LOCATEdit, which enhances cross-attention maps through a graph-based\napproach utilizing self-attention-derived patch relationships to maintain\nsmooth, coherent attention across image regions, ensuring that alterations are\nlimited to the designated items while retaining the surrounding structure.\nLOCATEdit consistently and substantially outperforms existing baselines on\nPIE-Bench, demonstrating its state-of-the-art performance and effectiveness on\nvarious editing tasks. Code can be found on\nhttps:\/\/github.com\/LOCATEdit\/LOCATEdit\/\n","date":"2025-03-27"}
{"id":"2503.21544","title":"SWI: Speaking with Intent in Large Language Models","abstract":"  Intent, typically clearly formulated and planned, functions as a cognitive\nframework for reasoning and problem-solving. This paper introduces the concept\nof Speaking with Intent (SWI) in large language models (LLMs), where the\nexplicitly generated intent encapsulates the model's underlying intention and\nprovides high-level planning to guide subsequent analysis and communication. By\nemulating deliberate and purposeful thoughts in the human mind, SWI is\nhypothesized to enhance the reasoning capabilities and generation quality of\nLLMs. Extensive experiments on mathematical reasoning benchmarks consistently\ndemonstrate the superiority of Speaking with Intent over Baseline (i.e.,\ngeneration without explicit intent). Moreover, SWI outperforms answer-trigger\nprompting methods Chain-of-Thought and Plan-and-Solve and maintains competitive\nperformance with the strong method ARR (Analyzing, Retrieving, and Reasoning).\nAdditionally, the effectiveness and generalizability of SWI are solidified on\nreasoning-intensive question answering (QA) and text summarization benchmarks,\nwhere SWI brings consistent improvement to the Baseline generation. In text\nsummarization, SWI-generated summaries exhibit greater accuracy, conciseness,\nand factual correctness, with fewer hallucinations. Furthermore, human\nevaluations verify the coherence, effectiveness, and interpretability of the\nintent produced by SWI. This proof-of-concept study creates a novel avenue for\nenhancing LLMs' reasoning abilities with cognitive notions.\n","date":"2025-03-27"}
{"id":"2503.21555","title":"SyncSDE: A Probabilistic Framework for Diffusion Synchronization","abstract":"  There have been many attempts to leverage multiple diffusion models for\ncollaborative generation, extending beyond the original domain. A prominent\napproach involves synchronizing multiple diffusion trajectories by mixing the\nestimated scores to artificially correlate the generation processes. However,\nexisting methods rely on naive heuristics, such as averaging, without\nconsidering task specificity. These approaches do not clarify why such methods\nwork and often fail when a heuristic suitable for one task is blindly applied\nto others. In this paper, we present a probabilistic framework for analyzing\nwhy diffusion synchronization works and reveal where heuristics should be\nfocused - modeling correlations between multiple trajectories and adapting them\nto each specific task. We further identify optimal correlation models per task,\nachieving better results than previous approaches that apply a single heuristic\nacross all tasks without justification.\n","date":"2025-03-27"}
{"id":"2503.21557","title":"debug-gym: A Text-Based Environment for Interactive Debugging","abstract":"  Large Language Models (LLMs) are increasingly relied upon for coding tasks,\nyet in most scenarios it is assumed that all relevant information can be either\naccessed in context or matches their training data. We posit that LLMs can\nbenefit from the ability to interactively explore a codebase to gather the\ninformation relevant to their task. To achieve this, we present a textual\nenvironment, namely debug-gym, for developing LLM-based agents in an\ninteractive coding setting. Our environment is lightweight and provides a\npreset of useful tools, such as a Python debugger (pdb), designed to facilitate\nan LLM-based agent's interactive debugging. Beyond coding and debugging tasks,\nthis approach can be generalized to other tasks that would benefit from\ninformation-seeking behavior by an LLM agent.\n","date":"2025-03-27"}
{"id":"2503.21558","title":"A Local Perspective-based Model for Overlapping Community Detection","abstract":"  Community detection, which identifies densely connected node clusters with\nsparse between-group links, is vital for analyzing network structure and\nfunction in real-world systems. Most existing community detection methods based\non GCNs primarily focus on node-level information while overlooking\ncommunity-level features, leading to performance limitations on large-scale\nnetworks. To address this issue, we propose LQ-GCN, an overlapping community\ndetection model from a local community perspective. LQ-GCN employs a\nBernoulli-Poisson model to construct a community affiliation matrix and form an\nend-to-end detection framework. By adopting local modularity as the objective\nfunction, the model incorporates local community information to enhance the\nquality and accuracy of clustering results. Additionally, the conventional GCNs\narchitecture is optimized to improve the model capability in identifying\noverlapping communities in large-scale networks. Experimental results\ndemonstrate that LQ-GCN achieves up to a 33% improvement in Normalized Mutual\nInformation (NMI) and a 26.3% improvement in Recall compared to baseline models\nacross multiple real-world benchmark datasets.\n","date":"2025-03-27"}
{"id":"2503.21560","title":"Statistical learning of structure-property relationships for transport\n  in porous media, using hybrid AI modeling","abstract":"  The 3D microstructure of porous media, such as electrodes in lithium-ion\nbatteries or fiber-based materials, significantly impacts the resulting\nmacroscopic properties, including effective diffusivity or permeability.\nConsequently, quantitative structure-property relationships, which link\nstructural descriptors of 3D microstructures such as porosity or geodesic\ntortuosity to effective transport properties, are crucial for further\noptimizing the performance of porous media. To overcome the limitations of 3D\nimaging, parametric stochastic 3D microstructure modeling is a powerful tool to\ngenerate many virtual but realistic structures at the cost of computer\nsimulations. The present paper uses 90,000 virtually generated 3D\nmicrostructures of porous media derived from literature by systematically\nvarying parameters of stochastic 3D microstructure models. Previously, this\ndata set has been used to establish quantitative microstructure-property\nrelationships. The present paper extends these findings by applying a hybrid AI\nframework to this data set. More precisely, symbolic regression, powered by\ndeep neural networks, genetic algorithms, and graph attention networks, is used\nto derive precise and robust analytical equations. These equations model the\nrelationships between structural descriptors and effective transport properties\nwithout requiring manual specification of the underlying functional\nrelationship. By integrating AI with traditional computational methods, the\nhybrid AI framework not only generates predictive equations but also enhances\nconventional modeling approaches by capturing relationships influenced by\nspecific microstructural features traditionally underrepresented. Thus, this\npaper significantly advances the predictive modeling capabilities in materials\nscience, offering vital insights for designing and optimizing new materials\nwith tailored transport properties.\n","date":"2025-03-27"}
{"id":"2503.21562","title":"uLayout: Unified Room Layout Estimation for Perspective and Panoramic\n  Images","abstract":"  We present uLayout, a unified model for estimating room layout geometries\nfrom both perspective and panoramic images, whereas traditional solutions\nrequire different model designs for each image type. The key idea of our\nsolution is to unify both domains into the equirectangular projection,\nparticularly, allocating perspective images into the most suitable latitude\ncoordinate to effectively exploit both domains seamlessly. To address the\nField-of-View (FoV) difference between the input domains, we design uLayout\nwith a shared feature extractor with an extra 1D-Convolution layer to condition\neach domain input differently. This conditioning allows us to efficiently\nformulate a column-wise feature regression problem regardless of the FoV input.\nThis simple yet effective approach achieves competitive performance with\ncurrent state-of-the-art solutions and shows for the first time a single\nend-to-end model for both domains. Extensive experiments in the real-world\ndatasets, LSUN, Matterport3D, PanoContext, and Stanford 2D-3D evidence the\ncontribution of our approach. Code is available at\nhttps:\/\/github.com\/JonathanLee112\/uLayout.\n","date":"2025-03-27"}
{"id":"2503.21563","title":"Consistent Multigroup Low-Rank Approximation","abstract":"  We consider the problem of consistent low-rank approximation for multigroup\ndata: we ask for a sequence of $k$ basis vectors such that projecting the data\nonto their spanned subspace treats all groups as equally as possible, by\nminimizing the maximum error among the groups. Additionally, we require that\nthe sequence of basis vectors satisfies the natural consistency property: when\nlooking for the best $k$ vectors, the first $d<k$ vectors are the best possible\nsolution to the problem of finding $d$ basis vectors. Thus, this multigroup\nlow-rank approximation method naturally generalizes \\svd and reduces to \\svd\nfor data with a single group. We give an iterative algorithm for this task that\nsequentially adds to the basis the vector that gives the best rank$-1$\nprojection according to the min-max criterion, and then projects the data onto\nthe orthogonal complement of that vector. For finding the best rank$-1$\nprojection, we use primal-dual approaches or semidefinite programming. We\nanalyze the theoretical properties of the algorithms and demonstrate\nempirically that the proposed methods compare favorably to existing methods for\nmultigroup (or fair) PCA.\n","date":"2025-03-27"}
{"id":"2503.21566","title":"Bearing fault diagnosis based on multi-scale spectral images and\n  convolutional neural network","abstract":"  To address the challenges of low diagnostic accuracy in traditional bearing\nfault diagnosis methods, this paper proposes a novel fault diagnosis approach\nbased on multi-scale spectrum feature images and deep learning. Firstly, the\nvibration signal are preprocessed through mean removal and then converted to\nmulti-length spectrum with fast Fourier transforms (FFT). Secondly, a novel\nfeature called multi-scale spectral image (MSSI) is constructed by multi-length\nspectrum paving scheme. Finally, a deep learning framework, convolutional\nneural network (CNN), is formulated to diagnose the bearing faults. Two\nexperimental cases are utilized to verify the effectiveness of the proposed\nmethod. Experimental results demonstrate that the proposed method significantly\nimproves the accuracy of fault diagnosis.\n","date":"2025-03-27"}
{"id":"2503.21571","title":"Magnitude-Phase Dual-Path Speech Enhancement Network based on\n  Self-Supervised Embedding and Perceptual Contrast Stretch Boosting","abstract":"  Speech self-supervised learning (SSL) has made great progress in various\nspeech processing tasks, but there is still room for improvement in speech\nenhancement (SE). This paper presents BSP-MPNet, a dual-path framework that\ncombines self-supervised features with magnitude-phase information for SE. The\napproach starts by applying the perceptual contrast stretching (PCS) algorithm\nto enhance the magnitude-phase spectrum. A magnitude-phase 2D coarse (MP-2DC)\nencoder then extracts coarse features from the enhanced spectrum. Next, a\nfeature-separating self-supervised learning (FS-SSL) model generates\nself-supervised embeddings for the magnitude and phase components separately.\nThese embeddings are fused to create cross-domain feature representations.\nFinally, two parallel RNN-enhanced multi-attention (REMA) mask decoders refine\nthe features, apply them to the mask, and reconstruct the speech signal. We\nevaluate BSP-MPNet on the VoiceBank+DEMAND and WHAMR! datasets. Experimental\nresults show that BSP-MPNet outperforms existing methods under various noise\nconditions, providing new directions for self-supervised speech enhancement\nresearch. The implementation of the BSP-MPNet code is available\nonline\\footnote[2]{https:\/\/github.com\/AlimMat\/BSP-MPNet. \\label{s1}}\n","date":"2025-03-27"}
{"id":"2503.21579","title":"Fusion of Graph Neural Networks via Optimal Transport","abstract":"  In this paper, we explore the idea of combining GCNs into one model. To that\nend, we align the weights of different models layer-wise using optimal\ntransport (OT). We present and evaluate three types of transportation costs and\nshow that the studied fusion method consistently outperforms the performance of\nvanilla averaging. Finally, we present results suggesting that model fusion\nusing OT is harder in the case of GCNs than MLPs and that incorporating the\ngraph structure into the process does not improve the performance of the\nmethod.\n","date":"2025-03-27"}
{"id":"2503.21581","title":"AlignDiff: Learning Physically-Grounded Camera Alignment via Diffusion","abstract":"  Accurate camera calibration is a fundamental task for 3D perception,\nespecially when dealing with real-world, in-the-wild environments where complex\noptical distortions are common. Existing methods often rely on pre-rectified\nimages or calibration patterns, which limits their applicability and\nflexibility. In this work, we introduce a novel framework that addresses these\nchallenges by jointly modeling camera intrinsic and extrinsic parameters using\na generic ray camera model. Unlike previous approaches, AlignDiff shifts focus\nfrom semantic to geometric features, enabling more accurate modeling of local\ndistortions. We propose AlignDiff, a diffusion model conditioned on geometric\npriors, enabling the simultaneous estimation of camera distortions and scene\ngeometry. To enhance distortion prediction, we incorporate edge-aware\nattention, focusing the model on geometric features around image edges, rather\nthan semantic content. Furthermore, to enhance generalizability to real-world\ncaptures, we incorporate a large database of ray-traced lenses containing over\nthree thousand samples. This database characterizes the distortion inherent in\na diverse variety of lens forms. Our experiments demonstrate that the proposed\nmethod significantly reduces the angular error of estimated ray bundles by ~8.2\ndegrees and overall calibration accuracy, outperforming existing approaches on\nchallenging, real-world datasets.\n","date":"2025-03-27"}
{"id":"2503.21585","title":"Probabilistic Functional Neural Networks","abstract":"  High-dimensional functional time series (HDFTS) are often characterized by\nnonlinear trends and high spatial dimensions. Such data poses unique challenges\nfor modeling and forecasting due to the nonlinearity, nonstationarity, and high\ndimensionality. We propose a novel probabilistic functional neural network\n(ProFnet) to address these challenges. ProFnet integrates the strengths of\nfeedforward and deep neural networks with probabilistic modeling. The model\ngenerates probabilistic forecasts using Monte Carlo sampling and also enables\nthe quantification of uncertainty in predictions. While capturing both temporal\nand spatial dependencies across multiple regions, ProFnet offers a scalable and\nunified solution for large datasets. Applications to Japan's mortality rates\ndemonstrate superior performance. This approach enhances predictive accuracy\nand provides interpretable uncertainty estimates, making it a valuable tool for\nforecasting complex high-dimensional functional data and HDFTS.\n","date":"2025-03-27"}
{"id":"2503.21588","title":"Generalizable Implicit Neural Representations via Parameterized Latent\n  Dynamics for Baroclinic Ocean Forecasting","abstract":"  Mesoscale ocean dynamics play a critical role in climate systems, governing\nheat transport, hurricane genesis, and drought patterns. However, simulating\nthese processes at high resolution remains computationally prohibitive due to\ntheir nonlinear, multiscale nature and vast spatiotemporal domains. Implicit\nneural representations (INRs) reduce the computational costs as\nresolution-independent surrogates but fail in many-query scenarios (inverse\nmodeling) requiring rapid evaluations across diverse parameters. We present\nPINROD, a novel framework combining dynamics-aware implicit neural\nrepresentations with parameterized neural ordinary differential equations to\naddress these limitations. By integrating parametric dependencies into latent\ndynamics, our method efficiently captures nonlinear oceanic behavior across\nvarying boundary conditions and physical parameters. Experiments on ocean\nmesoscale activity data show superior accuracy over existing baselines and\nimproved computational efficiency compared to standard numerical simulations.\n","date":"2025-03-27"}
{"id":"2503.21592","title":"Critical Iterative Denoising: A Discrete Generative Model Applied to\n  Graphs","abstract":"  Discrete Diffusion and Flow Matching models have significantly advanced\ngenerative modeling for discrete structures, including graphs. However, the\ntime dependencies in the noising process of these models lead to error\naccumulation and propagation during the backward process. This issue,\nparticularly pronounced in mask diffusion, is a known limitation in sequence\nmodeling and, as we demonstrate, also impacts discrete diffusion models for\ngraphs.\n  To address this problem, we propose a novel framework called Iterative\nDenoising, which simplifies discrete diffusion and circumvents the issue by\nassuming conditional independence across time. Additionally, we enhance our\nmodel by incorporating a Critic, which during generation selectively retains or\ncorrupts elements in an instance based on their likelihood under the data\ndistribution. Our empirical evaluations demonstrate that the proposed method\nsignificantly outperforms existing discrete diffusion baselines in graph\ngeneration tasks.\n","date":"2025-03-27"}
{"id":"2503.21595","title":"FusionSegReID: Advancing Person Re-Identification with Multimodal\n  Retrieval and Precise Segmentation","abstract":"  Person re-identification (ReID) plays a critical role in applications like\nsecurity surveillance and criminal investigations by matching individuals\nacross large image galleries captured by non-overlapping cameras. Traditional\nReID methods rely on unimodal inputs, typically images, but face limitations\ndue to challenges like occlusions, lighting changes, and pose variations. While\nadvancements in image-based and text-based ReID systems have been made, the\nintegration of both modalities has remained under-explored. This paper presents\nFusionSegReID, a multimodal model that combines both image and text inputs for\nenhanced ReID performance. By leveraging the complementary strengths of these\nmodalities, our model improves matching accuracy and robustness, particularly\nin complex, real-world scenarios where one modality may struggle. Our\nexperiments show significant improvements in Top-1 accuracy and mean Average\nPrecision (mAP) for ReID, as well as better segmentation results in challenging\nscenarios like occlusion and low-quality images. Ablation studies further\nconfirm that multimodal fusion and segmentation modules contribute to enhanced\nre-identification and mask accuracy. The results show that FusionSegReID\noutperforms traditional unimodal models, offering a more robust and flexible\nsolution for real-world person ReID tasks.\n","date":"2025-03-27"}
{"id":"2503.21598","title":"Prompt, Divide, and Conquer: Bypassing Large Language Model Safety\n  Filters via Segmented and Distributed Prompt Processing","abstract":"  Large Language Models (LLMs) have transformed task automation and content\ngeneration across various domains while incorporating safety filters to prevent\nmisuse. We introduce a novel jailbreaking framework that employs distributed\nprompt processing combined with iterative refinements to bypass these safety\nmeasures, particularly in generating malicious code. Our architecture consists\nof four key modules: prompt segmentation, parallel processing, response\naggregation, and LLM-based jury evaluation. Tested on 500 malicious prompts\nacross 10 cybersecurity categories, the framework achieves a 73.2% Success Rate\n(SR) in generating malicious code. Notably, our comparative analysis reveals\nthat traditional single-LLM judge evaluation overestimates SRs (93.8%) compared\nto our LLM jury system (73.2%), with manual verification confirming that\nsingle-judge assessments often accept incomplete implementations. Moreover, we\ndemonstrate that our distributed architecture improves SRs by 12% over the\nnon-distributed approach in an ablation study, highlighting both the\neffectiveness of distributed prompt processing and the importance of robust\nevaluation methodologies in assessing jailbreak attempts.\n","date":"2025-03-27"}
{"id":"2503.21602","title":"GenEdit: Compounding Operators and Continuous Improvement to Tackle\n  Text-to-SQL in the Enterprise","abstract":"  Recent advancements in Text-to-SQL, driven by large language models, are\ndemocratizing data access. Despite these advancements, enterprise deployments\nremain challenging due to the need to capture business-specific knowledge,\nhandle complex queries, and meet expectations of continuous improvements. To\naddress these issues, we designed and implemented GenEdit: our Text-to-SQL\ngeneration system that improves with user feedback. GenEdit builds and\nmaintains a company-specific knowledge set, employs a pipeline of operators\ndecomposing SQL generation, and uses feedback to update its knowledge set to\nimprove future SQL generations.\n  We describe GenEdit's architecture made of two core modules: (i) decomposed\nSQL generation; and (ii) knowledge set edits based on user feedback. For\ngeneration, GenEdit leverages compounding operators to improve knowledge\nretrieval and to create a plan as chain-of-thought steps that guides\ngeneration. GenEdit first retrieves relevant examples in an initial retrieval\nstage where original SQL queries are decomposed into sub-statements, clauses or\nsub-queries. It then also retrieves instructions and schema elements. Using the\nretrieved contextual information, GenEdit then generates step-by-step plan in\nnatural language on how to produce the query. Finally, GenEdit uses the plan to\ngenerate SQL, minimizing the need for model reasoning, which enhances complex\nSQL generation. If necessary, GenEdit regenerates the query based on syntactic\nand semantic errors. The knowledge set edits are recommended through an\ninteractive copilot, allowing users to iterate on their feedback and to\nregenerate SQL queries as needed. Each generation uses staged edits which\nupdate the generation prompt. Once the feedback is submitted, it gets merged\nafter passing regression testing and obtaining an approval, improving future\ngenerations.\n","date":"2025-03-27"}
{"id":"2503.21608","title":"Nonlinear Multiple Response Regression and Learning of Latent Spaces","abstract":"  Identifying low-dimensional latent structures within high-dimensional data\nhas long been a central topic in the machine learning community, driven by the\nneed for data compression, storage, transmission, and deeper data\nunderstanding. Traditional methods, such as principal component analysis (PCA)\nand autoencoders (AE), operate in an unsupervised manner, ignoring label\ninformation even when it is available. In this work, we introduce a unified\nmethod capable of learning latent spaces in both unsupervised and supervised\nsettings. We formulate the problem as a nonlinear multiple-response regression\nwithin an index model context. By applying the generalized Stein's lemma, the\nlatent space can be estimated without knowing the nonlinear link functions. Our\nmethod can be viewed as a nonlinear generalization of PCA. Moreover, unlike AE\nand other neural network methods that operate as \"black boxes\", our approach\nnot only offers better interpretability but also reduces computational\ncomplexity while providing strong theoretical guarantees. Comprehensive\nnumerical experiments and real data analyses demonstrate the superior\nperformance of our method.\n","date":"2025-03-27"}
{"id":"2503.21613","title":"Evaluating book summaries from internal knowledge in Large Language\n  Models: a cross-model and semantic consistency approach","abstract":"  We study the ability of large language models (LLMs) to generate\ncomprehensive and accurate book summaries solely from their internal knowledge,\nwithout recourse to the original text. Employing a diverse set of books and\nmultiple LLM architectures, we examine whether these models can synthesize\nmeaningful narratives that align with established human interpretations.\nEvaluation is performed with a LLM-as-a-judge paradigm: each AI-generated\nsummary is compared against a high-quality, human-written summary via a\ncross-model assessment, where all participating LLMs evaluate not only their\nown outputs but also those produced by others. This methodology enables the\nidentification of potential biases, such as the proclivity for models to favor\ntheir own summarization style over others. In addition, alignment between the\nhuman-crafted and LLM-generated summaries is quantified using ROUGE and\nBERTScore metrics, assessing the depth of grammatical and semantic\ncorrespondence. The results reveal nuanced variations in content representation\nand stylistic preferences among the models, highlighting both strengths and\nlimitations inherent in relying on internal knowledge for summarization tasks.\nThese findings contribute to a deeper understanding of LLM internal encodings\nof factual information and the dynamics of cross-model evaluation, with\nimplications for the development of more robust natural language generative\nsystems.\n","date":"2025-03-27"}
{"id":"2503.21614","title":"A Survey of Efficient Reasoning for Large Reasoning Models: Language,\n  Multimodality, and Beyond","abstract":"  Recent Large Reasoning Models (LRMs), such as DeepSeek-R1 and OpenAI o1, have\ndemonstrated strong performance gains by scaling up the length of\nChain-of-Thought (CoT) reasoning during inference. However, a growing concern\nlies in their tendency to produce excessively long reasoning traces, which are\noften filled with redundant content (e.g., repeated definitions), over-analysis\nof simple problems, and superficial exploration of multiple reasoning paths for\nharder tasks. This inefficiency introduces significant challenges for training,\ninference, and real-world deployment (e.g., in agent-based systems), where\ntoken economy is critical. In this survey, we provide a comprehensive overview\nof recent efforts aimed at improving reasoning efficiency in LRMs, with a\nparticular focus on the unique challenges that arise in this new paradigm. We\nidentify common patterns of inefficiency, examine methods proposed across the\nLRM lifecycle, i.e., from pretraining to inference, and discuss promising\nfuture directions for research. To support ongoing development, we also\nmaintain a real-time GitHub repository tracking recent progress in the field.\nWe hope this survey serves as a foundation for further exploration and inspires\ninnovation in this rapidly evolving area.\n","date":"2025-03-27"}
{"id":"2503.21615","title":"A Measure Based Generalizable Approach to Understandability","abstract":"  Successful agent-human partnerships require that any agent generated\ninformation is understandable to the human, and that the human can easily steer\nthe agent towards a goal. Such effective communication requires the agent to\ndevelop a finer-level notion of what is understandable to the human.\nState-of-the-art agents, including LLMs, lack this detailed notion of\nunderstandability because they only capture average human sensibilities from\nthe training data, and therefore afford limited steerability (e.g., requiring\nnon-trivial prompt engineering).\n  In this paper, instead of only relying on data, we argue for developing\ngeneralizable, domain-agnostic measures of understandability that can be used\nas directives for these agents. Existing research on understandability measures\nis fragmented, we survey various such efforts across domains, and lay a\ncognitive-science-rooted groundwork for more coherent and domain-agnostic\nresearch investigations in future.\n","date":"2025-03-27"}
{"id":"2503.21616","title":"Audio-driven Gesture Generation via Deviation Feature in the Latent\n  Space","abstract":"  Gestures are essential for enhancing co-speech communication, offering visual\nemphasis and complementing verbal interactions. While prior work has\nconcentrated on point-level motion or fully supervised data-driven methods, we\nfocus on co-speech gestures, advocating for weakly supervised learning and\npixel-level motion deviations. We introduce a weakly supervised framework that\nlearns latent representation deviations, tailored for co-speech gesture video\ngeneration. Our approach employs a diffusion model to integrate latent motion\nfeatures, enabling more precise and nuanced gesture representation. By\nleveraging weakly supervised deviations in latent space, we effectively\ngenerate hand gestures and mouth movements, crucial for realistic video\nproduction. Experiments show our method significantly improves video quality,\nsurpassing current state-of-the-art techniques.\n","date":"2025-03-27"}
{"id":"2503.21617","title":"Leveraging Language Models for Analyzing Longitudinal Experiential Data\n  in Education","abstract":"  We propose a novel approach to leveraging pre-trained language models (LMs)\nfor early forecasting of academic trajectories in STEM students using\nhigh-dimensional longitudinal experiential data. This data, which captures\nstudents' study-related activities, behaviors, and psychological states, offers\nvaluable insights for forecasting-based interventions. Key challenges in\nhandling such data include high rates of missing values, limited dataset size\ndue to costly data collection, and complex temporal variability across\nmodalities. Our approach addresses these issues through a comprehensive data\nenrichment process, integrating strategies for managing missing values,\naugmenting data, and embedding task-specific instructions and contextual cues\nto enhance the models' capacity for learning temporal patterns. Through\nextensive experiments on a curated student learning dataset, we evaluate both\nencoder-decoder and decoder-only LMs. While our findings show that LMs\neffectively integrate data across modalities and exhibit resilience to missing\ndata, they primarily rely on high-level statistical patterns rather than\ndemonstrating a deeper understanding of temporal dynamics. Furthermore, their\nability to interpret explicit temporal information remains limited. This work\nadvances educational data science by highlighting both the potential and\nlimitations of LMs in modeling student trajectories for early intervention\nbased on longitudinal experiential data.\n","date":"2025-03-27"}
{"id":"2503.21620","title":"UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement\n  Learning","abstract":"  The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities\nin LLMs through reinforcement learning (RL) with rule-based rewards. Building\non this idea, we are the first to explore how rule-based RL can enhance the\nreasoning capabilities of multimodal large language models (MLLMs) for graphic\nuser interface (GUI) action prediction tasks. To this end, we curate a small\nyet high-quality dataset of 136 challenging tasks, encompassing five common\naction types on mobile devices. We also introduce a unified rule-based action\nreward, enabling model optimization via policy-based algorithms such as Group\nRelative Policy Optimization (GRPO). Experimental results demonstrate that our\nproposed data-efficient model, UI-R1-3B, achieves substantial improvements on\nboth in-domain (ID) and out-of-domain (OOD) tasks. Specifically, on the ID\nbenchmark AndroidControl, the action type accuracy improves by 15%, while\ngrounding accuracy increases by 10.3%, compared with the base model (i.e.\nQwen2.5-VL-3B). On the OOD GUI grounding benchmark ScreenSpot-Pro, our model\nsurpasses the base model by 6.0% and achieves competitive performance with\nlarger models (e.g., OS-Atlas-7B), which are trained via supervised fine-tuning\n(SFT) on 76K data. These results underscore the potential of rule-based\nreinforcement learning to advance GUI understanding and control, paving the way\nfor future research in this domain.\n","date":"2025-03-27"}
{"id":"2503.21622","title":"The MVTec AD 2 Dataset: Advanced Scenarios for Unsupervised Anomaly\n  Detection","abstract":"  In recent years, performance on existing anomaly detection benchmarks like\nMVTec AD and VisA has started to saturate in terms of segmentation AU-PRO, with\nstate-of-the-art models often competing in the range of less than one\npercentage point. This lack of discriminatory power prevents a meaningful\ncomparison of models and thus hinders progress of the field, especially when\nconsidering the inherent stochastic nature of machine learning results. We\npresent MVTec AD 2, a collection of eight anomaly detection scenarios with more\nthan 8000 high-resolution images. It comprises challenging and highly relevant\nindustrial inspection use cases that have not been considered in previous\ndatasets, including transparent and overlapping objects, dark-field and back\nlight illumination, objects with high variance in the normal data, and\nextremely small defects. We provide comprehensive evaluations of\nstate-of-the-art methods and show that their performance remains below 60%\naverage AU-PRO. Additionally, our dataset provides test scenarios with lighting\ncondition changes to assess the robustness of methods under real-world\ndistribution shifts. We host a publicly accessible evaluation server that holds\nthe pixel-precise ground truth of the test set (https:\/\/benchmark.mvtec.com\/).\nAll image data is available at\nhttps:\/\/www.mvtec.com\/company\/research\/datasets\/mvtec-ad-2.\n","date":"2025-03-27"}
{"id":"2503.21627","title":"Provable Reduction in Communication Rounds for Non-Smooth Convex\n  Federated Learning","abstract":"  Multiple local steps are key to communication-efficient federated learning.\nHowever, theoretical guarantees for such algorithms, without data\nheterogeneity-bounding assumptions, have been lacking in general non-smooth\nconvex problems. Leveraging projection-efficient optimization methods, we\npropose FedMLS, a federated learning algorithm with provable improvements from\nmultiple local steps. FedMLS attains an $\\epsilon$-suboptimal solution in\n$\\mathcal{O}(1\/\\epsilon)$ communication rounds, requiring a total of\n$\\mathcal{O}(1\/\\epsilon^2)$ stochastic subgradient oracle calls.\n","date":"2025-03-27"}
{"id":"2503.21629","title":"ClusterSC: Advancing Synthetic Control with Donor Selection","abstract":"  In causal inference with observational studies, synthetic control (SC) has\nemerged as a prominent tool. SC has traditionally been applied to\naggregate-level datasets, but more recent work has extended its use to\nindividual-level data. As they contain a greater number of observed units, this\nshift introduces the curse of dimensionality to SC. To address this, we propose\nCluster Synthetic Control (ClusterSC), based on the idea that groups of\nindividuals may exist where behavior aligns internally but diverges between\ngroups. ClusterSC incorporates a clustering step to select only the relevant\ndonors for the target. We provide theoretical guarantees on the improvements\ninduced by ClusterSC, supported by empirical demonstrations on synthetic and\nreal-world datasets. The results indicate that ClusterSC consistently\noutperforms classical SC approaches.\n","date":"2025-03-27"}
{"id":"2503.21634","title":"When Astronomy Meets AI: Manazel For Crescent Visibility Prediction in\n  Morocco","abstract":"  The accurate determination of the beginning of each Hijri month is essential\nfor religious, cultural, and administrative purposes. Manazel (The code and\ndatasets are available at https:\/\/github.com\/lairgiyassir\/manazel) addresses\nthis challenge in Morocco by leveraging 13 years of crescent visibility data to\nrefine the ODEH criterion, a widely used standard for lunar crescent visibility\nprediction. The study integrates two key features, the Arc of Vision (ARCV) and\nthe total width of the crescent (W), to enhance the accuracy of lunar\nvisibility assessments. A machine learning approach utilizing the Logistic\nRegression algorithm is employed to classify crescent visibility conditions,\nachieving a predictive accuracy of 98.83%. This data-driven methodology offers\na robust and reliable framework for determining the start of the Hijri month,\ncomparing different data classification tools, and improving the consistency of\nlunar calendar calculations in Morocco. The findings demonstrate the\neffectiveness of machine learning in astronomical applications and highlight\nthe potential for further enhancements in the modeling of crescent visibility.\n","date":"2025-03-27"}
{"id":"2503.21638","title":"Data-Driven Extreme Response Estimation","abstract":"  A method to rapidly estimate extreme ship response events is developed in\nthis paper. The method involves training by a Long Short-Term Memory (LSTM)\nneural network to correct a lower-fidelity hydrodynamic model to the level of a\nhigher-fidelity simulation. More focus is placed on larger responses by\nisolating the time-series near peak events identified in the lower-fidelity\nsimulations and training on only the shorter time-series around the large\nevent. The method is tested on the estimation of pitch time-series maxima in\nSea State 5 (significant wave height of 4.0 meters and modal period of 15.0\nseconds,) generated by a lower-fidelity hydrodynamic solver known as SimpleCode\nand a higher-fidelity tool known as the Large Amplitude Motion Program (LAMP).\nThe results are also compared with an LSTM trained without special\nconsiderations for large events.\n","date":"2025-03-27"}
{"id":"2503.21639","title":"Locally minimax optimal and dimension-agnostic discrete argmin inference","abstract":"  We revisit the discrete argmin inference problem in high-dimensional\nsettings. Given $n$ observations from a $d$ dimensional vector, the goal is to\ntest whether the $r$th component of the mean vector is the smallest among all\ncomponents. We propose dimension-agnostic tests that maintain validity\nregardless of how $d$ scales with $n$, and regardless of arbitrary ties in the\nmean vector. Notably, our validity holds under mild moment conditions,\nrequiring little more than finiteness of a second moment, and permitting\npossibly strong dependence between coordinates. In addition, we establish the\nlocal minimax separation rate for this problem, which adapts to the cardinality\nof a confusion set, and show that the proposed tests attain this rate. Our\nmethod uses the sample splitting and self-normalization approach of Kim and\nRamdas (2024). Our tests can be easily inverted to yield confidence sets for\nthe argmin index. Empirical results illustrate the strong performance of our\napproach in terms of type I error control and power compared to existing\nmethods.\n","date":"2025-03-27"}
{"id":"2503.21640","title":"Towards Fully Automated Decision-Making Systems for Greenhouse Control:\n  Challenges and Opportunities","abstract":"  Machine learning has been successful in building control policies to drive a\ncomplex system to desired states in various applications (e.g. games, robotics,\netc.). To be specific, a number of parameters of policy can be automatically\noptimized from the observations of environment to be able to generate a\nsequence of decisions leading to the best performance. In this survey paper, we\nparticularly explore such policy-learning techniques for another unique,\npractical use-case scenario--farming, in which critical decisions (e.g., water\nsupply, heating, etc.) must be made in a timely manner to minimize risks (e.g.,\ndamage to plants) while maximizing the revenue (e.g., healthy crops) in the\nend. We first provide a broad overview of latest studies on it to identify not\nonly domain-specific challenges but opportunities with potential solutions,\nsome of which are suggested as promising directions for future research. Also,\nwe then introduce our successful approach to being ranked second among 46 teams\nat the ''3rd Autonomous Greenhouse Challenge'' to use this specific example to\ndiscuss the lessons learned about important considerations for design to create\nautonomous farm-management systems.\n","date":"2025-03-27"}
{"id":"2503.21646","title":"Unlocking the Potential of Past Research: Using Generative AI to\n  Reconstruct Healthcare Simulation Models","abstract":"  Discrete-event simulation (DES) is widely used in healthcare Operations\nResearch, but the models themselves are rarely shared. This limits their\npotential for reuse and long-term impact in the modelling and healthcare\ncommunities. This study explores the feasibility of using generative artificial\nintelligence (AI) to recreate published models using Free and Open Source\nSoftware (FOSS), based on the descriptions provided in an academic journal.\nUsing a structured methodology, we successfully generated, tested and\ninternally reproduced two DES models, including user interfaces. The reported\nresults were replicated for one model, but not the other, likely due to missing\ninformation on distributions. These models are substantially more complex than\nAI-generated DES models published to date. Given the challenges we faced in\nprompt engineering, code generation, and model testing, we conclude that our\niterative approach to model development, systematic comparison and testing, and\nthe expertise of our team were necessary to the success of our recreated\nsimulation models.\n","date":"2025-03-27"}
{"id":"2503.21657","title":"Model Assembly Learning with Heterogeneous Layer Weight Merging","abstract":"  Model merging acquires general capabilities without extra data or training by\ncombining multiple models' parameters. Previous approaches achieve linear mode\nconnectivity by aligning parameters into the same loss basin using permutation\ninvariance. In this paper, we introduce Model Assembly Learning (MAL), a novel\nparadigm for model merging that iteratively integrates parameters from diverse\nmodels in an open-ended model zoo to enhance the base model's capabilities.\nUnlike previous works that require identical architectures, MAL allows the\nmerging of heterogeneous architectures and selective parameters across layers.\nSpecifically, the base model can incorporate parameters from different layers\nof multiple pre-trained models. We systematically investigate the conditions\nand fundamental settings of heterogeneous parameter merging, addressing all\npossible mismatches in layer widths between the base and target models.\nFurthermore, we establish key laws and provide practical guidelines for\neffectively implementing MAL.\n","date":"2025-03-27"}
{"id":"2503.21659","title":"InteractionMap: Improving Online Vectorized HDMap Construction with\n  Interaction","abstract":"  Vectorized high-definition (HD) maps are essential for an autonomous driving\nsystem. Recently, state-of-the-art map vectorization methods are mainly based\non DETR-like framework to generate HD maps in an end-to-end manner. In this\npaper, we propose InteractionMap, which improves previous map vectorization\nmethods by fully leveraging local-to-global information interaction in both\ntime and space. Firstly, we explore enhancing DETR-like detectors by explicit\nposition relation prior from point-level to instance-level, since map elements\ncontain strong shape priors. Secondly, we propose a key-frame-based\nhierarchical temporal fusion module, which interacts temporal information from\nlocal to global. Lastly, the separate classification branch and regression\nbranch lead to the problem of misalignment in the output distribution. We\ninteract semantic information with geometric information by introducing a novel\ngeometric-aware classification loss in optimization and a geometric-aware\nmatching cost in label assignment. InteractionMap achieves state-of-the-art\nperformance on both nuScenes and Argoverse2 benchmarks.\n","date":"2025-03-27"}
{"id":"2503.21668","title":"Cognitive Science-Inspired Evaluation of Core Capabilities for Object\n  Understanding in AI","abstract":"  One of the core components of our world models is 'intuitive physics' - an\nunderstanding of objects, space, and causality. This capability enables us to\npredict events, plan action and navigate environments, all of which rely on a\ncomposite sense of objecthood. Despite its importance, there is no single,\nunified account of objecthood, though multiple theoretical frameworks provide\ninsights. In the first part of this paper, we present a comprehensive overview\nof the main theoretical frameworks in objecthood research - Gestalt psychology,\nenactive cognition, and developmental psychology - and identify the core\ncapabilities each framework attributes to object understanding, as well as what\nfunctional roles they play in shaping world models in biological agents. Given\nthe foundational role of objecthood in world modelling, understanding\nobjecthood is also essential in AI. In the second part of the paper, we\nevaluate how current AI paradigms approach and test objecthood capabilities\ncompared to those in cognitive science. We define an AI paradigm as a\ncombination of how objecthood is conceptualised, the methods used for studying\nobjecthood, the data utilised, and the evaluation techniques. We find that,\nwhilst benchmarks can detect that AI systems model isolated aspects of\nobjecthood, the benchmarks cannot detect when AI systems lack functional\nintegration across these capabilities, not solving the objecthood challenge\nfully. Finally, we explore novel evaluation approaches that align with the\nintegrated vision of objecthood outlined in this paper. These methods are\npromising candidates for advancing from isolated object capabilities toward\ngeneral-purpose AI with genuine object understanding in real-world contexts.\n","date":"2025-03-27"}
{"id":"2503.21670","title":"COMI-LINGUA: Expert Annotated Large-Scale Dataset for Multitask NLP in\n  Hindi-English Code-Mixing","abstract":"  The rapid growth of digital communication has driven the widespread use of\ncode-mixing, particularly Hindi-English, in multilingual communities. Existing\ndatasets often focus on romanized text, have limited scope, or rely on\nsynthetic data, which fails to capture realworld language nuances. Human\nannotations are crucial for assessing the naturalness and acceptability of\ncode-mixed text. To address these challenges, We introduce COMI-LINGUA, the\nlargest manually annotated dataset for code-mixed text, comprising 100,970\ninstances evaluated by three expert annotators in both Devanagari and Roman\nscripts. The dataset supports five fundamental NLP tasks: Language\nIdentification, Matrix Language Identification, Part-of-Speech Tagging, Named\nEntity Recognition, and Translation. We evaluate LLMs on these tasks using\nCOMILINGUA, revealing limitations in current multilingual modeling strategies\nand emphasizing the need for improved code-mixed text processing capabilities.\nCOMI-LINGUA is publically availabe at:\nhttps:\/\/huggingface.co\/datasets\/LingoIITGN\/COMI-LINGUA.\n","date":"2025-03-27"}
{"id":"2503.21673","title":"A friendly introduction to triangular transport","abstract":"  Decision making under uncertainty is a cross-cutting challenge in science and\nengineering. Most approaches to this challenge employ probabilistic\nrepresentations of uncertainty. In complicated systems accessible only via data\nor black-box models, however, these representations are rarely known. We\ndiscuss how to characterize and manipulate such representations using\ntriangular transport maps, which approximate any complex probability\ndistribution as a transformation of a simple, well-understood distribution. The\nparticular structure of triangular transport guarantees many desirable\nmathematical and computational properties that translate well into solving\npractical problems. Triangular maps are actively used for density estimation,\n(conditional) generative modelling, Bayesian inference, data assimilation,\noptimal experimental design, and related tasks. While there is ample literature\non the development and theory of triangular transport methods, this manuscript\nprovides a detailed introduction for scientists interested in employing measure\ntransport without assuming a formal mathematical background. We build intuition\nfor the key foundations of triangular transport, discuss many aspects of its\npractical implementation, and outline the frontiers of this field.\n","date":"2025-03-27"}
{"id":"2503.21674","title":"Intelligent IoT Attack Detection Design via ODLLM with Feature\n  Ranking-based Knowledge Base","abstract":"  The widespread adoption of Internet of Things (IoT) devices has introduced\nsignificant cybersecurity challenges, particularly with the increasing\nfrequency and sophistication of Distributed Denial of Service (DDoS) attacks.\nTraditional machine learning (ML) techniques often fall short in detecting such\nattacks due to the complexity of blended and evolving patterns. To address\nthis, we propose a novel framework leveraging On-Device Large Language Models\n(ODLLMs) augmented with fine-tuning and knowledge base (KB) integration for\nintelligent IoT network attack detection. By implementing feature ranking\ntechniques and constructing both long and short KBs tailored to model\ncapacities, the proposed framework ensures efficient and accurate detection of\nDDoS attacks while overcoming computational and privacy limitations. Simulation\nresults demonstrate that the optimized framework achieves superior accuracy\nacross diverse attack types, especially when using compact models in edge\ncomputing environments. This work provides a scalable and secure solution for\nreal-time IoT security, advancing the applicability of edge intelligence in\ncybersecurity.\n","date":"2025-03-27"}
{"id":"2503.21676","title":"How do language models learn facts? Dynamics, curricula and\n  hallucinations","abstract":"  Large language models accumulate vast knowledge during pre-training, yet the\ndynamics governing this acquisition remain poorly understood. This work\ninvestigates the learning dynamics of language models on a synthetic factual\nrecall task, uncovering three key findings: First, language models learn in\nthree phases, exhibiting a performance plateau before acquiring precise factual\nknowledge. Mechanistically, this plateau coincides with the formation of\nattention-based circuits that support recall. Second, the training data\ndistribution significantly impacts learning dynamics, as imbalanced\ndistributions lead to shorter plateaus. Finally, hallucinations emerge\nsimultaneously with knowledge, and integrating new knowledge into the model\nthrough fine-tuning is challenging, as it quickly corrupts its existing\nparametric memories. Our results emphasize the importance of data distribution\nin knowledge acquisition and suggest novel data scheduling strategies to\naccelerate neural network training.\n","date":"2025-03-27"}
{"id":"2503.21677","title":"A tale of two goals: leveraging sequentiality in multi-goal scenarios","abstract":"  Several hierarchical reinforcement learning methods leverage planning to\ncreate a graph or sequences of intermediate goals, guiding a lower-level\ngoal-conditioned (GC) policy to reach some final goals. The low-level policy is\ntypically conditioned on the current goal, with the aim of reaching it as\nquickly as possible. However, this approach can fail when an intermediate goal\ncan be reached in multiple ways, some of which may make it impossible to\ncontinue toward subsequent goals. To address this issue, we introduce two\ninstances of Markov Decision Process (MDP) where the optimization objective\nfavors policies that not only reach the current goal but also subsequent ones.\nIn the first, the agent is conditioned on both the current and final goals,\nwhile in the second, it is conditioned on the next two goals in the sequence.\nWe conduct a series of experiments on navigation and pole-balancing tasks in\nwhich sequences of intermediate goals are given. By evaluating policies trained\nwith TD3+HER on both the standard GC-MDP and our proposed MDPs, we show that,\nin most cases, conditioning on the next two goals improves stability and sample\nefficiency over other approaches.\n","date":"2025-03-27"}
{"id":"2503.21679","title":"JiraiBench: A Bilingual Benchmark for Evaluating Large Language Models'\n  Detection of Human Self-Destructive Behavior Content in Jirai Community","abstract":"  This paper introduces JiraiBench, the first bilingual benchmark for\nevaluating large language models' effectiveness in detecting self-destructive\ncontent across Chinese and Japanese social media communities. Focusing on the\ntransnational \"Jirai\" (landmine) online subculture that encompasses multiple\nforms of self-destructive behaviors including drug overdose, eating disorders,\nand self-harm, we present a comprehensive evaluation framework incorporating\nboth linguistic and cultural dimensions. Our dataset comprises 10,419 Chinese\nposts and 5,000 Japanese posts with multidimensional annotation along three\nbehavioral categories, achieving substantial inter-annotator agreement.\nExperimental evaluations across four state-of-the-art models reveal significant\nperformance variations based on instructional language, with Japanese prompts\nunexpectedly outperforming Chinese prompts when processing Chinese content.\nThis emergent cross-cultural transfer suggests that cultural proximity can\nsometimes outweigh linguistic similarity in detection tasks. Cross-lingual\ntransfer experiments with fine-tuned models further demonstrate the potential\nfor knowledge transfer between these language systems without explicit target\nlanguage training. These findings highlight the need for culturally-informed\napproaches to multilingual content moderation and provide empirical evidence\nfor the importance of cultural context in developing more effective detection\nsystems for vulnerable online communities.\n","date":"2025-03-27"}
{"id":"2503.21681","title":"A Comprehensive Benchmark for RNA 3D Structure-Function Modeling","abstract":"  The RNA structure-function relationship has recently garnered significant\nattention within the deep learning community, promising to grow in importance\nas nucleic acid structure models advance. However, the absence of standardized\nand accessible benchmarks for deep learning on RNA 3D structures has impeded\nthe development of models for RNA functional characteristics.\n  In this work, we introduce a set of seven benchmarking datasets for RNA\nstructure-function prediction, designed to address this gap. Our library builds\non the established Python library rnaglib, and offers easy data distribution\nand encoding, splitters and evaluation methods, providing a convenient\nall-in-one framework for comparing models. Datasets are implemented in a fully\nmodular and reproducible manner, facilitating for community contributions and\ncustomization. Finally, we provide initial baseline results for all tasks using\na graph neural network.\n  Source code: https:\/\/github.com\/cgoliver\/rnaglib\n  Documentation: https:\/\/rnaglib.org\n","date":"2025-03-27"}
{"id":"2503.21683","title":"LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku\n  with Self-Play and Reinforcement Learning","abstract":"  In recent years, large language models (LLMs) have shown significant\nadvancements in natural language processing (NLP), with strong capa-bilities in\ngeneration, comprehension, and rea-soning. These models have found applications\nin education, intelligent decision-making, and gaming. However, effectively\nutilizing LLMs for strategic planning and decision-making in the game of Gomoku\nremains a challenge. This study aims to develop a Gomoku AI system based on\nLLMs, simulating the human learning process of playing chess. The system is\nde-signed to understand and apply Gomoku strat-egies and logic to make rational\ndecisions. The research methods include enabling the model to \"read the board,\"\n\"understand the rules,\" \"select strategies,\" and \"evaluate positions,\" while\nen-hancing its abilities through self-play and rein-forcement learning. The\nresults demonstrate that this approach significantly improves the se-lection of\nmove positions, resolves the issue of generating illegal positions, and reduces\npro-cess time through parallel position evaluation. After extensive self-play\ntraining, the model's Gomoku-playing capabilities have been notably enhanced.\n","date":"2025-03-27"}
{"id":"2503.21686","title":"Molecular Quantum Transformer","abstract":"  The Transformer model, renowned for its powerful attention mechanism, has\nachieved state-of-the-art performance in various artificial intelligence tasks\nbut faces challenges such as high computational cost and memory usage.\nResearchers are exploring quantum computing to enhance the Transformer's\ndesign, though it still shows limited success with classical data. With a\ngrowing focus on leveraging quantum machine learning for quantum data,\nparticularly in quantum chemistry, we propose the Molecular Quantum Transformer\n(MQT) for modeling interactions in molecular quantum systems. By utilizing\nquantum circuits to implement the attention mechanism on the molecular\nconfigurations, MQT can efficiently calculate ground-state energies for all\nconfigurations. Numerical demonstrations show that in calculating ground-state\nenergies for H_2, LiH, BeH_2, and H_4, MQT outperforms the classical\nTransformer, highlighting the promise of quantum effects in Transformer\nstructures. Furthermore, its pretraining capability on diverse molecular data\nfacilitates the efficient learning of new molecules, extending its\napplicability to complex molecular systems with minimal additional effort. Our\nmethod offers an alternative to existing quantum algorithms for estimating\nground-state energies, opening new avenues in quantum chemistry and materials\nscience.\n","date":"2025-03-27"}
{"id":"2503.21690","title":"CMED: A Child Micro-Expression Dataset","abstract":"  Micro-expressions are short bursts of emotion that are difficult to hide.\nTheir detection in children is an important cue to assist psychotherapists in\nconducting better therapy. However, existing research on the detection of\nmicro-expressions has focused on adults, whose expressions differ in their\ncharacteristics from those of children. The lack of research is a direct\nconsequence of the lack of a child-based micro-expressions dataset as it is\nmuch more challenging to capture children's facial expressions due to the lack\nof predictability and controllability. This study compiles a dataset of\nspontaneous child micro-expression videos, the first of its kind, to the best\nof the authors knowledge. The dataset is captured in the wild using video\nconferencing software. This dataset enables us to then explore key features and\ndifferences between adult and child micro-expressions. This study also\nestablishes a baseline for the automated spotting and recognition of\nmicro-expressions in children using three approaches comprising of hand-created\nand learning-based approaches.\n","date":"2025-03-27"}
{"id":"2503.21692","title":"RapidPoseTriangulation: Multi-view Multi-person Whole-body Human Pose\n  Triangulation in a Millisecond","abstract":"  The integration of multi-view imaging and pose estimation represents a\nsignificant advance in computer vision applications, offering new possibilities\nfor understanding human movement and interactions. This work presents a new\nalgorithm that improves multi-view multi-person pose estimation, focusing on\nfast triangulation speeds and good generalization capabilities. The approach\nextends to whole-body pose estimation, capturing details from facial\nexpressions to finger movements across multiple individuals and viewpoints.\nAdaptability to different settings is demonstrated through strong performance\nacross unseen datasets and configurations. To support further progress in this\nfield, all of this work is publicly accessible.\n","date":"2025-03-27"}
{"id":"2503.21694","title":"Progressive Rendering Distillation: Adapting Stable Diffusion for\n  Instant Text-to-Mesh Generation without 3D Data","abstract":"  It is highly desirable to obtain a model that can generate high-quality 3D\nmeshes from text prompts in just seconds. While recent attempts have adapted\npre-trained text-to-image diffusion models, such as Stable Diffusion (SD), into\ngenerators of 3D representations (e.g., Triplane), they often suffer from poor\nquality due to the lack of sufficient high-quality 3D training data. Aiming at\novercoming the data shortage, we propose a novel training scheme, termed as\nProgressive Rendering Distillation (PRD), eliminating the need for 3D\nground-truths by distilling multi-view diffusion models and adapting SD into a\nnative 3D generator. In each iteration of training, PRD uses the U-Net to\nprogressively denoise the latent from random noise for a few steps, and in each\nstep it decodes the denoised latent into 3D output. Multi-view diffusion\nmodels, including MVDream and RichDreamer, are used in joint with SD to distill\ntext-consistent textures and geometries into the 3D outputs through score\ndistillation. Since PRD supports training without 3D ground-truths, we can\neasily scale up the training data and improve generation quality for\nchallenging text prompts with creative concepts. Meanwhile, PRD can accelerate\nthe inference speed of the generation model in just a few steps. With PRD, we\ntrain a Triplane generator, namely TriplaneTurbo, which adds only $2.5\\%$\ntrainable parameters to adapt SD for Triplane generation. TriplaneTurbo\noutperforms previous text-to-3D generators in both efficiency and quality.\nSpecifically, it can produce high-quality 3D meshes in 1.2 seconds and\ngeneralize well for challenging text input. The code is available at\nhttps:\/\/github.com\/theEricMa\/TriplaneTurbo.\n","date":"2025-03-27"}
{"id":"2503.21695","title":"AMA-SAM: Adversarial Multi-Domain Alignment of Segment Anything Model\n  for High-Fidelity Histology Nuclei Segmentation","abstract":"  Accurate segmentation of cell nuclei in histopathology images is essential\nfor numerous biomedical research and clinical applications. However, existing\ncell nucleus segmentation methods only consider a single dataset (i.e., primary\ndomain), while neglecting to leverage supplementary data from diverse sources\n(i.e., auxiliary domains) to reduce overfitting and enhance the performance.\nAlthough incorporating multiple datasets could alleviate overfitting, it often\nexacerbates performance drops caused by domain shifts. In this work, we\nintroduce Adversarial Multi-domain Alignment of Segment Anything Model\n(AMA-SAM) that extends the Segment Anything Model (SAM) to overcome these\nobstacles through two key innovations. First, we propose a Conditional Gradient\nReversal Layer (CGRL), a multi-domain alignment module that harmonizes features\nfrom diverse domains to promote domain-invariant representation learning while\npreserving crucial discriminative features for the primary dataset. Second, we\naddress SAM's inherent low-resolution output by designing a High-Resolution\nDecoder (HR-Decoder), which directly produces fine-grained segmentation maps in\norder to capture intricate nuclei boundaries in high-resolution histology\nimages. To the best of our knowledge, this is the first attempt to adapt SAM\nfor multi-dataset learning with application to histology nuclei segmentation.\nWe validate our method on several publicly available datasets, demonstrating\nconsistent and significant improvements over state-of-the-art approaches.\n","date":"2025-03-27"}
{"id":"2503.21696","title":"Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for\n  Embodied Interactive Tasks","abstract":"  Recent advances in deep thinking models have demonstrated remarkable\nreasoning capabilities on mathematical and coding tasks. However, their\neffectiveness in embodied domains which require continuous interaction with\nenvironments through image action interleaved trajectories remains largely\n-unexplored. We present Embodied Reasoner, a model that extends o1 style\nreasoning to interactive embodied search tasks. Unlike mathematical reasoning\nthat relies primarily on logical deduction, embodied scenarios demand spatial\nunderstanding, temporal reasoning, and ongoing self-reflection based on\ninteraction history. To address these challenges, we synthesize 9.3k coherent\nObservation-Thought-Action trajectories containing 64k interactive images and\n90k diverse thinking processes (analysis, spatial reasoning, reflection,\nplanning, and verification). We develop a three-stage training pipeline that\nprogressively enhances the model's capabilities through imitation learning,\nself-exploration via rejection sampling, and self-correction through reflection\ntuning. The evaluation shows that our model significantly outperforms those\nadvanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and\nClaude-3.7 by +9\\%, 24\\%, and +13\\%. Analysis reveals our model exhibits fewer\nrepeated searches and logical inconsistencies, with particular advantages in\ncomplex long-horizon tasks. Real-world environments also show our superiority\nwhile exhibiting fewer repeated searches and logical inconsistency cases.\n","date":"2025-03-27"}
{"id":"2503.21699","title":"MAVERIX: Multimodal Audio-Visual Evaluation Reasoning IndeX","abstract":"  Frontier models have either been language-only or have primarily focused on\nvision and language modalities. Although recent advancements in models with\nvision and audio understanding capabilities have shown substantial progress,\nthe field lacks a standardized evaluation framework for thoroughly assessing\ntheir cross-modality perception performance. We introduce MAVERIX~(Multimodal\nAudio-Visual Evaluation Reasoning IndeX), a novel benchmark with 700 videos and\n2,556 questions explicitly designed to evaluate multimodal models through tasks\nthat necessitate close integration of video and audio information. MAVERIX\nuniquely provides models with audiovisual tasks, closely mimicking the\nmultimodal perceptual experiences available to humans during inference and\ndecision-making processes. To our knowledge, MAVERIX is the first benchmark\naimed explicitly at assessing comprehensive audiovisual integration.\nExperiments with state-of-the-art models, including Gemini 1.5 Pro and o1, show\nperformance approaching human levels (around 70% accuracy), while human experts\nreach near-ceiling performance (95.1%). With standardized evaluation protocols,\na rigorously annotated pipeline, and a public toolkit, MAVERIX establishes a\nchallenging testbed for advancing audiovisual multimodal intelligence.\n","date":"2025-03-27"}
{"id":"2503.21704","title":"Learning to Represent Individual Differences for Choice Decision Making","abstract":"  Human decision making can be challenging to predict because decisions are\naffected by a number of complex factors. Adding to this complexity,\ndecision-making processes can differ considerably between individuals, and\nmethods aimed at predicting human decisions need to take individual differences\ninto account. Behavioral science offers methods by which to measure individual\ndifferences (e.g., questionnaires, behavioral models), but these are often\nnarrowed down to low dimensions and not tailored to specific prediction tasks.\nThis paper investigates the use of representation learning to measure\nindividual differences from behavioral experiment data. Representation learning\noffers a flexible approach to create individual embeddings from data that are\nboth structured (e.g., demographic information) and unstructured (e.g., free\ntext), where the flexibility provides more options for individual difference\nmeasures for personalization, e.g., free text responses may allow for\nopen-ended questions that are less privacy-sensitive. In the current paper we\nuse representation learning to characterize individual differences in human\nperformance on an economic decision-making task. We demonstrate that models\nusing representation learning to capture individual differences consistently\nimprove decision predictions over models without representation learning, and\neven outperform well-known theory-based behavioral models used in these\nenvironments. Our results propose that representation learning offers a useful\nand flexible tool to capture individual differences.\n","date":"2025-03-27"}
{"id":"2503.21708","title":"The Mathematical Relationship Between Layer Normalization and Dynamic\n  Activation Functions","abstract":"  A recent paper proposes Dynamic Tanh (DyT) as a drop-in replacement for layer\nnormalization (LN). Although the method is empirically well-motivated and\nappealing from a practical point of view, it lacks a theoretical foundation. In\nthis work, we shed light on the mathematical relationship between layer\nnormalization and dynamic activation functions. In particular, we derive DyT\nfrom LN and show that a well-defined approximation is needed to do so. By\ndropping said approximation, an alternative activation function is obtained,\nwhich we call Dynamic Inverse Square Root Unit (DyISRU). DyISRU is the exact\ncounterpart of layer normalization, and we demonstrate numerically that it\nindeed resembles LN more accurately than DyT does.\n","date":"2025-03-27"}
{"id":"2503.21714","title":"As easy as PIE: understanding when pruning causes language models to\n  disagree","abstract":"  Language Model (LM) pruning compresses the model by removing weights, nodes,\nor other parts of its architecture. Typically, pruning focuses on the resulting\nefficiency gains at the cost of effectiveness. However, when looking at how\nindividual data points are affected by pruning, it turns out that a particular\nsubset of data points always bears most of the brunt (in terms of reduced\naccuracy) when pruning, but this effect goes unnoticed when reporting the mean\naccuracy of all data points. These data points are called PIEs and have been\nstudied in image processing, but not in NLP. In a study of various NLP\ndatasets, pruning methods, and levels of compression, we find that PIEs impact\ninference quality considerably, regardless of class frequency, and that BERT is\nmore prone to this than BiLSTM. We also find that PIEs contain a high amount of\ndata points that have the largest influence on how well the model generalises\nto unseen data. This means that when pruning, with seemingly moderate loss to\naccuracy across all data points, we in fact hurt tremendously those data points\nthat matter the most. We trace what makes PIEs both hard and impactful to\ninference to their overall longer and more semantically complex text. These\nfindings are novel and contribute to understanding how LMs are affected by\npruning. The code is available at: https:\/\/github.com\/pietrotrope\/AsEasyAsPIE\n","date":"2025-03-27"}
{"id":"2503.21717","title":"CLAIMCHECK: How Grounded are LLM Critiques of Scientific Papers?","abstract":"  A core part of scientific peer review involves providing expert critiques\nthat directly assess the scientific claims a paper makes. While it is now\npossible to automatically generate plausible (if generic) reviews, ensuring\nthat these reviews are sound and grounded in the papers' claims remains\nchallenging. To facilitate LLM benchmarking on these challenges, we introduce\nCLAIMCHECK, an annotated dataset of NeurIPS 2023 and 2024 submissions and\nreviews mined from OpenReview. CLAIMCHECK is richly annotated by ML experts for\nweakness statements in the reviews and the paper claims that they dispute, as\nwell as fine-grained labels of the validity, objectivity, and type of the\nidentified weaknesses. We benchmark several LLMs on three claim-centric tasks\nsupported by CLAIMCHECK, requiring models to (1) associate weaknesses with the\nclaims they dispute, (2) predict fine-grained labels for weaknesses and rewrite\nthe weaknesses to enhance their specificity, and (3) verify a paper's claims\nwith grounded reasoning. Our experiments reveal that cutting-edge LLMs, while\ncapable of predicting weakness labels in (2), continue to underperform relative\nto human experts on all other tasks.\n","date":"2025-03-27"}
{"id":"2503.21718","title":"Outlier dimensions favor frequent tokens in language models","abstract":"  We study last-layer outlier dimensions, i.e. dimensions that display extreme\nactivations for the majority of inputs. We show that outlier dimensions arise\nin many different modern language models, and trace their function back to the\nheuristic of constantly predicting frequent words. We further show how a model\ncan block this heuristic when it is not contextually appropriate, by assigning\na counterbalancing weight mass to the remaining dimensions, and we investigate\nwhich model parameters boost outlier dimensions and when they arise during\ntraining. We conclude that outlier dimensions are a specialized mechanism\ndiscovered by many distinct models to implement a useful token prediction\nheuristic.\n","date":"2025-03-27"}
{"id":"2503.21720","title":"Collab: Controlled Decoding using Mixture of Agents for LLM Alignment","abstract":"  Alignment of Large Language models (LLMs) is crucial for safe and trustworthy\ndeployment in applications. Reinforcement learning from human feedback (RLHF)\nhas emerged as an effective technique to align LLMs to human preferences and\nbroader utilities, but it requires updating billions of model parameters, which\nis computationally expensive. Controlled Decoding, by contrast, provides a\nmechanism for aligning a model at inference time without retraining. However,\nsingle-agent decoding approaches often struggle to adapt to diverse tasks due\nto the complexity and variability inherent in these tasks. To strengthen the\ntest-time performance w.r.t the target task, we propose a mixture of\nagent-based decoding strategies leveraging the existing off-the-shelf aligned\nLLM policies. Treating each prior policy as an agent in the spirit of mixture\nof agent collaboration, we develop a decoding method that allows for\ninference-time alignment through a token-level selection strategy among\nmultiple agents. For each token, the most suitable LLM is dynamically chosen\nfrom a pool of models based on a long-term utility metric. This\npolicy-switching mechanism ensures optimal model selection at each step,\nenabling efficient collaboration and alignment among LLMs during decoding.\nTheoretical analysis of our proposed algorithm establishes optimal performance\nwith respect to the target task represented via a target reward for the given\noff-the-shelf models. We conduct comprehensive empirical evaluations with\nopen-source aligned models on diverse tasks and preferences, which demonstrates\nthe merits of this approach over single-agent decoding baselines. Notably,\nCollab surpasses the current SoTA decoding strategy, achieving an improvement\nof up to 1.56x in average reward and 71.89% in GPT-4 based win-tie rate.\n","date":"2025-03-27"}
{"id":"2503.21721","title":"Evaluating Text-to-Image Synthesis with a Conditional Fr\\'{e}chet\n  Distance","abstract":"  Evaluating text-to-image synthesis is challenging due to misalignment between\nestablished metrics and human preferences. We propose cFreD, a metric based on\nthe notion of Conditional Fr\\'echet Distance that explicitly accounts for both\nvisual fidelity and text-prompt alignment. Existing metrics such as Inception\nScore (IS), Fr\\'echet Inception Distance (FID) and CLIPScore assess either\nimage quality or image-text alignment but not both which limits their\ncorrelation with human preferences. Scoring models explicitly trained to\nreplicate human preferences require constant updates and may not generalize to\nnovel generation techniques or out-of-domain inputs. Through extensive\nexperiments across multiple recently proposed text-to-image models and diverse\nprompt datasets, we demonstrate that cFreD exhibits a higher correlation with\nhuman judgments compared to statistical metrics, including metrics trained with\nhuman preferences. Our findings validate cFreD as a robust, future-proof metric\nfor the systematic evaluation of text-to-image models, standardizing\nbenchmarking in this rapidly evolving field. We release our evaluation toolkit\nand benchmark in the appendix.\n","date":"2025-03-27"}
{"id":"2503.21722","title":"Energy Minimization for Participatory Federated Learning in IoT Analyzed\n  via Game Theory","abstract":"  The Internet of Things requires intelligent decision making in many\nscenarios. To this end, resources available at the individual nodes for sensing\nor computing, or both, can be leveraged. This results in approaches known as\nparticipatory sensing and federated learning, respectively. We investigate the\nsimultaneous implementation of both, through a distributed approach based on\nempowering local nodes with game theoretic decision making. A global objective\nof energy minimization is combined with the individual node's optimization of\nlocal expenditure for sensing and transmitting data over multiple learning\nrounds. We present extensive evaluations of this technique, based on both a\ntheoretical framework and experiments in a simulated network scenario with real\ndata. Such a distributed approach can reach a desired level of accuracy for\nfederated learning without a centralized supervision of the data collector.\nHowever, depending on the weight attributed to the local costs of the single\nnode, it may also result in a significantly high Price of Anarchy (from 1.28\nonwards). Thus, we argue for the need of incentive mechanisms, possibly based\non Age of Information of the single nodes.\n","date":"2025-03-27"}
{"id":"2503.21723","title":"OccRobNet : Occlusion Robust Network for Accurate 3D Interacting\n  Hand-Object Pose Estimation","abstract":"  Occlusion is one of the challenging issues when estimating 3D hand pose. This\nproblem becomes more prominent when hand interacts with an object or two hands\nare involved. In the past works, much attention has not been given to these\noccluded regions. But these regions contain important and beneficial\ninformation that is vital for 3D hand pose estimation. Thus, in this paper, we\npropose an occlusion robust and accurate method for the estimation of 3D\nhand-object pose from the input RGB image. Our method includes first localising\nthe hand joints using a CNN based model and then refining them by extracting\ncontextual information. The self attention transformer then identifies the\nspecific joints along with the hand identity. This helps the model to identify\nthe hand belongingness of a particular joint which helps to detect the joint\neven in the occluded region. Further, these joints with hand identity are then\nused to estimate the pose using cross attention mechanism. Thus, by identifying\nthe joints in the occluded region, the obtained network becomes robust to\nocclusion. Hence, this network achieves state-of-the-art results when evaluated\non the InterHand2.6M, HO3D and H$_2$O3D datasets.\n","date":"2025-03-27"}
{"id":"2503.21729","title":"ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large\n  Reasoning Models with Iterative Retrieval Augmented Generation","abstract":"  Large Reasoning Models (LRMs) exhibit remarkable reasoning abilities but rely\nprimarily on parametric knowledge, limiting factual accuracy. While recent\nworks equip reinforcement learning (RL)-based LRMs with retrieval capabilities,\nthey suffer from overthinking and lack robustness in reasoning, reducing their\neffectiveness in question answering (QA) tasks. To address this, we propose\nReaRAG, a factuality-enhanced reasoning model that explores diverse queries\nwithout excessive iterations. Our solution includes a novel data construction\nframework with an upper bound on the reasoning chain length. Specifically, we\nfirst leverage an LRM to generate deliberate thinking, then select an action\nfrom a predefined action space (Search and Finish). For Search action, a query\nis executed against the RAG engine, where the result is returned as observation\nto guide reasoning steps later. This process iterates until a Finish action is\nchosen. Benefiting from ReaRAG's strong reasoning capabilities, our approach\noutperforms existing baselines on multi-hop QA. Further analysis highlights its\nstrong reflective ability to recognize errors and refine its reasoning\ntrajectory. Our study enhances LRMs' factuality while effectively integrating\nrobust reasoning for Retrieval-Augmented Generation (RAG).\n","date":"2025-03-27"}
{"id":"2503.21730","title":"Effective Skill Unlearning through Intervention and Abstention","abstract":"  Large language Models (LLMs) have demonstrated remarkable skills across\nvarious domains. Understanding the mechanisms behind their abilities and\nimplementing controls over them is becoming increasingly important for\ndeveloping better models. In this paper, we focus on skill unlearning in LLMs,\nspecifically unlearning a particular skill while retaining their overall\ncapabilities. We introduce two lightweight, training-free machine skill\nunlearning techniques for LLMs. First, we observe that the pre-activation\ndistribution of neurons in each Feed-Forward Layer (FFL) differs when the model\ndemonstrates different skills. Additionally, we find that queries triggering\nthe same skill cluster within the FFL key space and can be separated from other\nqueries using a hypercube. Based on these observations, we propose two\nlightweight, training-free skill unlearning methods via \\textit{intervention}\nand \\textit{abstention} respectively: \\texttt{Neuron Adjust} and \\texttt{Key\nSpace Detection}. We evaluate our methods on unlearning math-solving,\nPython-coding, and comprehension skills across seven different languages. The\nresults demonstrate their strong unlearning capabilities for the designated\nskills. Specifically, \\texttt{Key Space Detection} achieves over 80\\% relative\nperformance drop on the forgetting skill and less than 10\\% relative\nperformance drop on other skills and the model's general knowledge (MMLU) for\nmost unlearning tasks. Our code is available at\nhttps:\/\/github.com\/Trustworthy-ML-Lab\/effective_skill_unlearning\n","date":"2025-03-27"}
{"id":"2503.21732","title":"SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling","abstract":"  Creating high-fidelity 3D meshes with arbitrary topology, including open\nsurfaces and complex interiors, remains a significant challenge. Existing\nimplicit field methods often require costly and detail-degrading watertight\nconversion, while other approaches struggle with high resolutions. This paper\nintroduces SparseFlex, a novel sparse-structured isosurface representation that\nenables differentiable mesh reconstruction at resolutions up to $1024^3$\ndirectly from rendering losses. SparseFlex combines the accuracy of Flexicubes\nwith a sparse voxel structure, focusing computation on surface-adjacent regions\nand efficiently handling open surfaces. Crucially, we introduce a frustum-aware\nsectional voxel training strategy that activates only relevant voxels during\nrendering, dramatically reducing memory consumption and enabling\nhigh-resolution training. This also allows, for the first time, the\nreconstruction of mesh interiors using only rendering supervision. Building\nupon this, we demonstrate a complete shape modeling pipeline by training a\nvariational autoencoder (VAE) and a rectified flow transformer for high-quality\n3D shape generation. Our experiments show state-of-the-art reconstruction\naccuracy, with a ~82% reduction in Chamfer Distance and a ~88% increase in\nF-score compared to previous methods, and demonstrate the generation of\nhigh-resolution, detailed 3D shapes with arbitrary topology. By enabling\nhigh-resolution, differentiable mesh reconstruction and generation with\nrendering losses, SparseFlex significantly advances the state-of-the-art in 3D\nshape representation and modeling.\n","date":"2025-03-27"}
{"id":"2503.21735","title":"GateLens: A Reasoning-Enhanced LLM Agent for Automotive Software Release\n  Analytics","abstract":"  Ensuring the reliability and effectiveness of software release decisions is\ncritical, particularly in safety-critical domains like automotive systems.\nPrecise analysis of release validation data, often presented in tabular form,\nplays a pivotal role in this process. However, traditional methods that rely on\nmanual analysis of extensive test datasets and validation metrics are prone to\ndelays and high costs. Large Language Models (LLMs) offer a promising\nalternative but face challenges in analytical reasoning, contextual\nunderstanding, handling out-of-scope queries, and processing structured test\ndata consistently; limitations that hinder their direct application in\nsafety-critical scenarios. This paper introduces GateLens, an LLM-based tool\nfor analyzing tabular data in the automotive domain. GateLens translates\nnatural language queries into Relational Algebra (RA) expressions and then\ngenerates optimized Python code. It outperforms the baseline system on\nbenchmarking datasets, achieving higher F1 scores and handling complex and\nambiguous queries with greater robustness. Ablation studies confirm the\ncritical role of the RA module, with performance dropping sharply when omitted.\nIndustrial evaluations reveal that GateLens reduces analysis time by over 80%\nwhile maintaining high accuracy and reliability. As demonstrated by presented\nresults, GateLens achieved high performance without relying on few-shot\nexamples, showcasing strong generalization across various query types from\ndiverse company roles. Insights from deploying GateLens with a partner\nautomotive company offer practical guidance for integrating AI into critical\nworkflows such as release validation. Results show that by automating test\nresult analysis, GateLens enables faster, more informed, and dependable release\ndecisions, and can thus advance software scalability and reliability in\nautomotive systems.\n","date":"2025-03-27"}
{"id":"2503.21745","title":"3DGen-Bench: Comprehensive Benchmark Suite for 3D Generative Models","abstract":"  3D generation is experiencing rapid advancements, while the development of 3D\nevaluation has not kept pace. How to keep automatic evaluation equitably\naligned with human perception has become a well-recognized challenge. Recent\nadvances in the field of language and image generation have explored human\npreferences and showcased respectable fitting ability. However, the 3D domain\nstill lacks such a comprehensive preference dataset over generative models. To\nmitigate this absence, we develop 3DGen-Arena, an integrated platform in a\nbattle manner. Then, we carefully design diverse text and image prompts and\nleverage the arena platform to gather human preferences from both public users\nand expert annotators, resulting in a large-scale multi-dimension human\npreference dataset 3DGen-Bench. Using this dataset, we further train a\nCLIP-based scoring model, 3DGen-Score, and a MLLM-based automatic evaluator,\n3DGen-Eval. These two models innovatively unify the quality evaluation of\ntext-to-3D and image-to-3D generation, and jointly form our automated\nevaluation system with their respective strengths. Extensive experiments\ndemonstrate the efficacy of our scoring model in predicting human preferences,\nexhibiting a superior correlation with human ranks compared to existing\nmetrics. We believe that our 3DGen-Bench dataset and automated evaluation\nsystem will foster a more equitable evaluation in the field of 3D generation,\nfurther promoting the development of 3D generative models and their downstream\napplications.\n","date":"2025-03-27"}
{"id":"2503.21747","title":"CTRL-O: Language-Controllable Object-Centric Visual Representation\n  Learning","abstract":"  Object-centric representation learning aims to decompose visual scenes into\nfixed-size vectors called \"slots\" or \"object files\", where each slot captures a\ndistinct object. Current state-of-the-art object-centric models have shown\nremarkable success in object discovery in diverse domains, including complex\nreal-world scenes. However, these models suffer from a key limitation: they\nlack controllability. Specifically, current object-centric models learn\nrepresentations based on their preconceived understanding of objects, without\nallowing user input to guide which objects are represented. Introducing\ncontrollability into object-centric models could unlock a range of useful\ncapabilities, such as the ability to extract instance-specific representations\nfrom a scene. In this work, we propose a novel approach for user-directed\ncontrol over slot representations by conditioning slots on language\ndescriptions. The proposed ConTRoLlable Object-centric representation learning\napproach, which we term CTRL-O, achieves targeted object-language binding in\ncomplex real-world scenes without requiring mask supervision. Next, we apply\nthese controllable slot representations on two downstream vision language\ntasks: text-to-image generation and visual question answering. The proposed\napproach enables instance-specific text-to-image generation and also achieves\nstrong performance on visual question answering.\n","date":"2025-03-27"}
{"id":"2503.21749","title":"LeX-Art: Rethinking Text Generation via Scalable High-Quality Data\n  Synthesis","abstract":"  We introduce LeX-Art, a comprehensive suite for high-quality text-image\nsynthesis that systematically bridges the gap between prompt expressiveness and\ntext rendering fidelity. Our approach follows a data-centric paradigm,\nconstructing a high-quality data synthesis pipeline based on Deepseek-R1 to\ncurate LeX-10K, a dataset of 10K high-resolution, aesthetically refined\n1024$\\times$1024 images. Beyond dataset construction, we develop LeX-Enhancer,\na robust prompt enrichment model, and train two text-to-image models, LeX-FLUX\nand LeX-Lumina, achieving state-of-the-art text rendering performance. To\nsystematically evaluate visual text generation, we introduce LeX-Bench, a\nbenchmark that assesses fidelity, aesthetics, and alignment, complemented by\nPairwise Normalized Edit Distance (PNED), a novel metric for robust text\naccuracy evaluation. Experiments demonstrate significant improvements, with\nLeX-Lumina achieving a 79.81% PNED gain on CreateBench, and LeX-FLUX\noutperforming baselines in color (+3.18%), positional (+4.45%), and font\naccuracy (+3.81%). Our codes, models, datasets, and demo are publicly\navailable.\n","date":"2025-03-27"}
{"id":"2503.21751","title":"Reconstructing Humans with a Biomechanically Accurate Skeleton","abstract":"  In this paper, we introduce a method for reconstructing 3D humans from a\nsingle image using a biomechanically accurate skeleton model. To achieve this,\nwe train a transformer that takes an image as input and estimates the\nparameters of the model. Due to the lack of training data for this task, we\nbuild a pipeline to produce pseudo ground truth model parameters for single\nimages and implement a training procedure that iteratively refines these pseudo\nlabels. Compared to state-of-the-art methods for 3D human mesh recovery, our\nmodel achieves competitive performance on standard benchmarks, while it\nsignificantly outperforms them in settings with extreme 3D poses and\nviewpoints. Additionally, we show that previous reconstruction methods\nfrequently violate joint angle limits, leading to unnatural rotations. In\ncontrast, our approach leverages the biomechanically plausible degrees of\nfreedom making more realistic joint rotation estimates. We validate our\napproach across multiple human pose estimation benchmarks. We make the code,\nmodels and data available at: https:\/\/isshikihugh.github.io\/HSMR\/\n","date":"2025-03-27"}
{"id":"2503.21755","title":"VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic\n  Faithfulness","abstract":"  Video generation has advanced significantly, evolving from producing\nunrealistic outputs to generating videos that appear visually convincing and\ntemporally coherent. To evaluate these video generative models, benchmarks such\nas VBench have been developed to assess their faithfulness, measuring factors\nlike per-frame aesthetics, temporal consistency, and basic prompt adherence.\nHowever, these aspects mainly represent superficial faithfulness, which focus\non whether the video appears visually convincing rather than whether it adheres\nto real-world principles. While recent models perform increasingly well on\nthese metrics, they still struggle to generate videos that are not just\nvisually plausible but fundamentally realistic. To achieve real \"world models\"\nthrough video generation, the next frontier lies in intrinsic faithfulness to\nensure that generated videos adhere to physical laws, commonsense reasoning,\nanatomical correctness, and compositional integrity. Achieving this level of\nrealism is essential for applications such as AI-assisted filmmaking and\nsimulated world modeling. To bridge this gap, we introduce VBench-2.0, a\nnext-generation benchmark designed to automatically evaluate video generative\nmodels for their intrinsic faithfulness. VBench-2.0 assesses five key\ndimensions: Human Fidelity, Controllability, Creativity, Physics, and\nCommonsense, each further broken down into fine-grained capabilities. Tailored\nfor individual dimensions, our evaluation framework integrates generalists such\nas state-of-the-art VLMs and LLMs, and specialists, including anomaly detection\nmethods proposed for video generation. We conduct extensive annotations to\nensure alignment with human judgment. By pushing beyond superficial\nfaithfulness toward intrinsic faithfulness, VBench-2.0 aims to set a new\nstandard for the next generation of video generative models in pursuit of\nintrinsic faithfulness.\n","date":"2025-03-27"}
{"id":"2503.21756","title":"A Unified Framework for Diffusion Bridge Problems: Flow Matching and\n  Schr\\\"{o}dinger Matching into One","abstract":"  The bridge problem is to find an SDE (or sometimes an ODE) that bridges two\ngiven distributions. The application areas of the bridge problem are enormous,\namong which the recent generative modeling (e.g., conditional or unconditional\nimage generation) is the most popular. Also the famous Schr\\\"{o}dinger bridge\nproblem, a widely known problem for a century, is a special instance of the\nbridge problem. Two most popular algorithms to tackle the bridge problems in\nthe deep learning era are: (conditional) flow matching and iterative fitting\nalgorithms, where the former confined to ODE solutions, and the latter\nspecifically for the Schr\\\"{o}dinger bridge problem. The main contribution of\nthis article is in two folds: i) We provide concise reviews of these algorithms\nwith technical details to some extent; ii) We propose a novel unified\nperspective and framework that subsumes these seemingly unrelated algorithms\n(and their variants) into one. In particular, we show that our unified\nframework can instantiate the Flow Matching (FM) algorithm, the (mini-batch)\noptimal transport FM algorithm, the (mini-batch) Schr\\\"{o}dinger bridge FM\nalgorithm, and the deep Schr\\\"{o}dinger bridge matching (DSBM) algorithm as its\nspecial cases. We believe that this unified framework will be useful for\nviewing the bridge problems in a more general and flexible perspective, and in\nturn can help researchers and practitioners to develop new bridge algorithms in\ntheir fields.\n","date":"2025-03-27"}
{"id":"2503.21757","title":"Fwd2Bot: LVLM Visual Token Compression with Double Forward Bottleneck","abstract":"  In this work, we aim to compress the vision tokens of a Large Vision Language\nModel (LVLM) into a representation that is simultaneously suitable for (a)\ngenerative and (b) discriminative tasks, (c) is nearly lossless, and (d) is\nstorage-efficient. We propose a novel compression approach, called Fwd2Bot,\nthat uses the LVLM itself to compress the visual information in a task-agnostic\nmanner. At the core of Fwd2bot there exists a \"double-forward pass\" training\nstrategy, whereby, during the first forward pass, the LLM (of the LVLM) creates\na bottleneck by condensing the visual information into a small number of\nsummary tokens. Then, using the same LLM, the second forward pass processes the\nlanguage instruction(s) alongside the summary tokens, used as a direct\nreplacement for the image ones. The training signal is provided by two losses:\nan autoregressive one applied after the second pass that provides a direct\noptimization objective for compression, and a contrastive loss, applied after\nthe first pass, that further boosts the representation strength, especially for\ndiscriminative tasks. The training is further enhanced by stage-specific\nadapters. We accompany the proposed method by an in-depth ablation study.\nOverall, Fwd2Bot results in highly-informative compressed representations\nsuitable for both generative and discriminative tasks. For generative tasks, we\noffer a 2x higher compression rate without compromising the generative\ncapabilities, setting a new state-of-the-art result. For discriminative tasks,\nwe set a new state-of-the-art on image retrieval and compositionality.\n","date":"2025-03-27"}
{"id":"2503.21758","title":"Lumina-Image 2.0: A Unified and Efficient Image Generative Framework","abstract":"  We introduce Lumina-Image 2.0, an advanced text-to-image generation framework\nthat achieves significant progress compared to previous work, Lumina-Next.\nLumina-Image 2.0 is built upon two key principles: (1) Unification - it adopts\na unified architecture (Unified Next-DiT) that treats text and image tokens as\na joint sequence, enabling natural cross-modal interactions and allowing\nseamless task expansion. Besides, since high-quality captioners can provide\nsemantically well-aligned text-image training pairs, we introduce a unified\ncaptioning system, Unified Captioner (UniCap), specifically designed for T2I\ngeneration tasks. UniCap excels at generating comprehensive and accurate\ncaptions, accelerating convergence and enhancing prompt adherence. (2)\nEfficiency - to improve the efficiency of our proposed model, we develop\nmulti-stage progressive training strategies and introduce inference\nacceleration techniques without compromising image quality. Extensive\nevaluations on academic benchmarks and public text-to-image arenas show that\nLumina-Image 2.0 delivers strong performances even with only 2.6B parameters,\nhighlighting its scalability and design efficiency. We have released our\ntraining details, code, and models at\nhttps:\/\/github.com\/Alpha-VLLM\/Lumina-Image-2.0.\n","date":"2025-03-27"}
{"id":"2503.21760","title":"MemInsight: Autonomous Memory Augmentation for LLM Agents","abstract":"  Large language model (LLM) agents have evolved to intelligently process\ninformation, make decisions, and interact with users or tools. A key capability\nis the integration of long-term memory capabilities, enabling these agents to\ndraw upon historical interactions and knowledge. However, the growing memory\nsize and need for semantic structuring pose significant challenges. In this\nwork, we propose an autonomous memory augmentation approach, MemInsight, to\nenhance semantic data representation and retrieval mechanisms. By leveraging\nautonomous augmentation to historical interactions, LLM agents are shown to\ndeliver more accurate and contextualized responses. We empirically validate the\nefficacy of our proposed approach in three task scenarios; conversational\nrecommendation, question answering and event summarization. On the LLM-REDIAL\ndataset, MemInsight boosts persuasiveness of recommendations by up to 14%.\nMoreover, it outperforms a RAG baseline by 34% in recall for LoCoMo retrieval.\nOur empirical results show the potential of MemInsight to enhance the\ncontextual performance of LLM agents across multiple tasks.\n","date":"2025-03-27"}
{"id":"2503.21761","title":"Uni4D: Unifying Visual Foundation Models for 4D Modeling from a Single\n  Video","abstract":"  This paper presents a unified approach to understanding dynamic scenes from\ncasual videos. Large pretrained vision foundation models, such as\nvision-language, video depth prediction, motion tracking, and segmentation\nmodels, offer promising capabilities. However, training a single model for\ncomprehensive 4D understanding remains challenging. We introduce Uni4D, a\nmulti-stage optimization framework that harnesses multiple pretrained models to\nadvance dynamic 3D modeling, including static\/dynamic reconstruction, camera\npose estimation, and dense 3D motion tracking. Our results show\nstate-of-the-art performance in dynamic 4D modeling with superior visual\nquality. Notably, Uni4D requires no retraining or fine-tuning, highlighting the\neffectiveness of repurposing visual foundation models for 4D understanding.\n","date":"2025-03-27"}
{"id":"2503.21765","title":"Exploring the Evolution of Physics Cognition in Video Generation: A\n  Survey","abstract":"  Recent advancements in video generation have witnessed significant progress,\nespecially with the rapid advancement of diffusion models. Despite this, their\ndeficiencies in physical cognition have gradually received widespread attention\n- generated content often violates the fundamental laws of physics, falling\ninto the dilemma of ''visual realism but physical absurdity\". Researchers began\nto increasingly recognize the importance of physical fidelity in video\ngeneration and attempted to integrate heuristic physical cognition such as\nmotion representations and physical knowledge into generative systems to\nsimulate real-world dynamic scenarios. Considering the lack of a systematic\noverview in this field, this survey aims to provide a comprehensive summary of\narchitecture designs and their applications to fill this gap. Specifically, we\ndiscuss and organize the evolutionary process of physical cognition in video\ngeneration from a cognitive science perspective, while proposing a three-tier\ntaxonomy: 1) basic schema perception for generation, 2) passive cognition of\nphysical knowledge for generation, and 3) active cognition for world\nsimulation, encompassing state-of-the-art methods, classical paradigms, and\nbenchmarks. Subsequently, we emphasize the inherent key challenges in this\ndomain and delineate potential pathways for future research, contributing to\nadvancing the frontiers of discussion in both academia and industry. Through\nstructured review and interdisciplinary analysis, this survey aims to provide\ndirectional guidance for developing interpretable, controllable, and physically\nconsistent video generation paradigms, thereby propelling generative models\nfrom the stage of ''visual mimicry'' towards a new phase of ''human-like\nphysical comprehension''.\n","date":"2025-03-27"}
{"id":"2503.21766","title":"Stable-SCore: A Stable Registration-based Framework for 3D Shape\n  Correspondence","abstract":"  Establishing character shape correspondence is a critical and fundamental\ntask in computer vision and graphics, with diverse applications including\nre-topology, attribute transfer, and shape interpolation. Current dominant\nfunctional map methods, while effective in controlled scenarios, struggle in\nreal situations with more complex challenges such as non-isometric shape\ndiscrepancies. In response, we revisit registration-for-correspondence methods\nand tap their potential for more stable shape correspondence estimation. To\novercome their common issues including unstable deformations and the necessity\nfor careful pre-alignment or high-quality initial 3D correspondences, we\nintroduce Stable-SCore: A Stable Registration-based Framework for 3D Shape\nCorrespondence. We first re-purpose a foundation model for 2D character\ncorrespondence that ensures reliable and stable 2D mappings. Crucially, we\npropose a novel Semantic Flow Guided Registration approach that leverages 2D\ncorrespondence to guide mesh deformations. Our framework significantly\nsurpasses existing methods in challenging scenarios, and brings possibilities\nfor a wide array of real applications, as demonstrated in our results.\n","date":"2025-03-27"}
{"id":"2503.21767","title":"Semantic Consistent Language Gaussian Splatting for Point-Level\n  Open-vocabulary Querying","abstract":"  Open-vocabulary querying in 3D Gaussian Splatting aims to identify\nsemantically relevant regions within a 3D Gaussian representation based on a\ngiven text query. Prior work, such as LangSplat, addressed this task by\nretrieving these regions in the form of segmentation masks on 2D renderings.\nMore recently, OpenGaussian introduced point-level querying, which directly\nselects a subset of 3D Gaussians. In this work, we propose a point-level\nquerying method that builds upon LangSplat's framework. Our approach improves\nthe framework in two key ways: (a) we leverage masklets from the Segment\nAnything Model 2 (SAM2) to establish semantic consistent ground-truth for\ndistilling the language Gaussians; (b) we introduces a novel two-step querying\napproach that first retrieves the distilled ground-truth and subsequently uses\nthe ground-truth to query the individual Gaussians. Experimental evaluations on\nthree benchmark datasets demonstrate that the proposed method achieves better\nperformance compared to state-of-the-art approaches. For instance, our method\nachieves an mIoU improvement of +20.42 on the 3D-OVS dataset.\n","date":"2025-03-27"}
{"id":"2503.21770","title":"Visual Jenga: Discovering Object Dependencies via Counterfactual\n  Inpainting","abstract":"  This paper proposes a novel scene understanding task called Visual Jenga.\nDrawing inspiration from the game Jenga, the proposed task involves\nprogressively removing objects from a single image until only the background\nremains. Just as Jenga players must understand structural dependencies to\nmaintain tower stability, our task reveals the intrinsic relationships between\nscene elements by systematically exploring which objects can be removed while\npreserving scene coherence in both physical and geometric sense. As a starting\npoint for tackling the Visual Jenga task, we propose a simple, data-driven,\ntraining-free approach that is surprisingly effective on a range of real-world\nimages. The principle behind our approach is to utilize the asymmetry in the\npairwise relationships between objects within a scene and employ a large\ninpainting model to generate a set of counterfactuals to quantify the\nasymmetry.\n","date":"2025-03-27"}
{"id":"2503.21771","title":"A Unified Image-Dense Annotation Generation Model for Underwater Scenes","abstract":"  Underwater dense prediction, especially depth estimation and semantic\nsegmentation, is crucial for gaining a comprehensive understanding of\nunderwater scenes. Nevertheless, high-quality and large-scale underwater\ndatasets with dense annotations remain scarce because of the complex\nenvironment and the exorbitant data collection costs. This paper proposes a\nunified Text-to-Image and DEnse annotation generation method (TIDE) for\nunderwater scenes. It relies solely on text as input to simultaneously generate\nrealistic underwater images and multiple highly consistent dense annotations.\nSpecifically, we unify the generation of text-to-image and text-to-dense\nannotations within a single model. The Implicit Layout Sharing mechanism (ILS)\nand cross-modal interaction method called Time Adaptive Normalization (TAN) are\nintroduced to jointly optimize the consistency between image and dense\nannotations. We synthesize a large-scale underwater dataset using TIDE to\nvalidate the effectiveness of our method in underwater dense prediction tasks.\nThe results demonstrate that our method effectively improves the performance of\nexisting underwater dense prediction models and mitigates the scarcity of\nunderwater data with dense annotations. We hope our method can offer new\nperspectives on alleviating data scarcity issues in other fields. The code is\navailable at https: \/\/github.com\/HongkLin\/TIDE.\n","date":"2025-03-27"}
{"id":"2503.21772","title":"LOCORE: Image Re-ranking with Long-Context Sequence Modeling","abstract":"  We introduce LOCORE, Long-Context Re-ranker, a model that takes as input\nlocal descriptors corresponding to an image query and a list of gallery images\nand outputs similarity scores between the query and each gallery image. This\nmodel is used for image retrieval, where typically a first ranking is performed\nwith an efficient similarity measure, and then a shortlist of top-ranked images\nis re-ranked based on a more fine-grained similarity measure. Compared to\nexisting methods that perform pair-wise similarity estimation with local\ndescriptors or list-wise re-ranking with global descriptors, LOCORE is the\nfirst method to perform list-wise re-ranking with local descriptors. To achieve\nthis, we leverage efficient long-context sequence models to effectively capture\nthe dependencies between query and gallery images at the local-descriptor\nlevel. During testing, we process long shortlists with a sliding window\nstrategy that is tailored to overcome the context size limitations of sequence\nmodels. Our approach achieves superior performance compared with other\nre-rankers on established image retrieval benchmarks of landmarks (ROxf and\nRPar), products (SOP), fashion items (In-Shop), and bird species (CUB-200)\nwhile having comparable latency to the pair-wise local descriptor re-rankers.\n","date":"2025-03-27"}
{"id":"2503.21774","title":"Optimal Stepsize for Diffusion Sampling","abstract":"  Diffusion models achieve remarkable generation quality but suffer from\ncomputational intensive sampling due to suboptimal step discretization. While\nexisting works focus on optimizing denoising directions, we address the\nprincipled design of stepsize schedules. This paper proposes Optimal Stepsize\nDistillation, a dynamic programming framework that extracts theoretically\noptimal schedules by distilling knowledge from reference trajectories. By\nreformulating stepsize optimization as recursive error minimization, our method\nguarantees global discretization bounds through optimal substructure\nexploitation. Crucially, the distilled schedules demonstrate strong robustness\nacross architectures, ODE solvers, and noise schedules. Experiments show 10x\naccelerated text-to-image generation while preserving 99.4% performance on\nGenEval. Our code is available at https:\/\/github.com\/bebebe666\/OptimalSteps.\n","date":"2025-03-27"}
{"id":"2503.21775","title":"StyleMotif: Multi-Modal Motion Stylization using Style-Content Cross\n  Fusion","abstract":"  We present StyleMotif, a novel Stylized Motion Latent Diffusion model,\ngenerating motion conditioned on both content and style from multiple\nmodalities. Unlike existing approaches that either focus on generating diverse\nmotion content or transferring style from sequences, StyleMotif seamlessly\nsynthesizes motion across a wide range of content while incorporating stylistic\ncues from multi-modal inputs, including motion, text, image, video, and audio.\nTo achieve this, we introduce a style-content cross fusion mechanism and align\na style encoder with a pre-trained multi-modal model, ensuring that the\ngenerated motion accurately captures the reference style while preserving\nrealism. Extensive experiments demonstrate that our framework surpasses\nexisting methods in stylized motion generation and exhibits emergent\ncapabilities for multi-modal motion stylization, enabling more nuanced motion\nsynthesis. Source code and pre-trained models will be released upon acceptance.\nProject Page: https:\/\/stylemotif.github.io\n","date":"2025-03-27"}
{"id":"2503.21776","title":"Video-R1: Reinforcing Video Reasoning in MLLMs","abstract":"  Inspired by DeepSeek-R1's success in eliciting reasoning abilities through\nrule-based reinforcement learning (RL), we introduce Video-R1 as the first\nattempt to systematically explore the R1 paradigm for eliciting video reasoning\nwithin multimodal large language models (MLLMs). However, directly applying RL\ntraining with the GRPO algorithm to video reasoning presents two primary\nchallenges: (i) a lack of temporal modeling for video reasoning, and (ii) the\nscarcity of high-quality video-reasoning data. To address these issues, we\nfirst propose the T-GRPO algorithm, which encourages models to utilize temporal\ninformation in videos for reasoning. Additionally, instead of relying solely on\nvideo data, we incorporate high-quality image-reasoning data into the training\nprocess. We have constructed two datasets: Video-R1-COT-165k for SFT cold start\nand Video-R1-260k for RL training, both comprising image and video data.\nExperimental results demonstrate that Video-R1 achieves significant\nimprovements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as\nwell as on general video benchmarks including MVBench and TempCompass, etc.\nNotably, Video-R1-7B attains a 35.8% accuracy on video spatial reasoning\nbenchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All\ncodes, models, data are released.\n","date":"2025-03-27"}
{"id":"2503.21777","title":"Test-Time Visual In-Context Tuning","abstract":"  Visual in-context learning (VICL), as a new paradigm in computer vision,\nallows the model to rapidly adapt to various tasks with only a handful of\nprompts and examples. While effective, the existing VICL paradigm exhibits poor\ngeneralizability under distribution shifts. In this work, we propose test-time\nVisual In-Context Tuning (VICT), a method that can adapt VICL models on the fly\nwith a single test sample. Specifically, we flip the role between the task\nprompts and the test sample and use a cycle consistency loss to reconstruct the\noriginal task prompt output. Our key insight is that a model should be aware of\na new test distribution if it can successfully recover the original task\nprompts. Extensive experiments on six representative vision tasks ranging from\nhigh-level visual understanding to low-level image processing, with 15 common\ncorruptions, demonstrate that our VICT can improve the generalizability of VICL\nto unseen new domains. In addition, we show the potential of applying VICT for\nunseen tasks at test time. Code: https:\/\/github.com\/Jiahao000\/VICT.\n","date":"2025-03-27"}
{"id":"2503.21778","title":"HS-SLAM: Hybrid Representation with Structural Supervision for Improved\n  Dense SLAM","abstract":"  NeRF-based SLAM has recently achieved promising results in tracking and\nreconstruction. However, existing methods face challenges in providing\nsufficient scene representation, capturing structural information, and\nmaintaining global consistency in scenes emerging significant movement or being\nforgotten. To this end, we present HS-SLAM to tackle these problems. To enhance\nscene representation capacity, we propose a hybrid encoding network that\ncombines the complementary strengths of hash-grid, tri-planes, and one-blob,\nimproving the completeness and smoothness of reconstruction. Additionally, we\nintroduce structural supervision by sampling patches of non-local pixels rather\nthan individual rays to better capture the scene structure. To ensure global\nconsistency, we implement an active global bundle adjustment (BA) to eliminate\ncamera drifts and mitigate accumulative errors. Experimental results\ndemonstrate that HS-SLAM outperforms the baselines in tracking and\nreconstruction accuracy while maintaining the efficiency required for robotics.\n","date":"2025-03-27"}
{"id":"2503.21779","title":"X$^{2}$-Gaussian: 4D Radiative Gaussian Splatting for Continuous-time\n  Tomographic Reconstruction","abstract":"  Four-dimensional computed tomography (4D CT) reconstruction is crucial for\ncapturing dynamic anatomical changes but faces inherent limitations from\nconventional phase-binning workflows. Current methods discretize temporal\nresolution into fixed phases with respiratory gating devices, introducing\nmotion misalignment and restricting clinical practicality. In this paper, We\npropose X$^2$-Gaussian, a novel framework that enables continuous-time 4D-CT\nreconstruction by integrating dynamic radiative Gaussian splatting with\nself-supervised respiratory motion learning. Our approach models anatomical\ndynamics through a spatiotemporal encoder-decoder architecture that predicts\ntime-varying Gaussian deformations, eliminating phase discretization. To remove\ndependency on external gating devices, we introduce a physiology-driven\nperiodic consistency loss that learns patient-specific breathing cycles\ndirectly from projections via differentiable optimization. Extensive\nexperiments demonstrate state-of-the-art performance, achieving a 9.93 dB PSNR\ngain over traditional methods and 2.25 dB improvement against prior Gaussian\nsplatting techniques. By unifying continuous motion modeling with hardware-free\nperiod learning, X$^2$-Gaussian advances high-fidelity 4D CT reconstruction for\ndynamic clinical imaging. Project website at: https:\/\/x2-gaussian.github.io\/.\n","date":"2025-03-27"}
{"id":"2503.21780","title":"Semantic Library Adaptation: LoRA Retrieval and Fusion for\n  Open-Vocabulary Semantic Segmentation","abstract":"  Open-vocabulary semantic segmentation models associate vision and text to\nlabel pixels from an undefined set of classes using textual queries, providing\nversatile performance on novel datasets. However, large shifts between training\nand test domains degrade their performance, requiring fine-tuning for effective\nreal-world applications. We introduce Semantic Library Adaptation (SemLA), a\nnovel framework for training-free, test-time domain adaptation. SemLA leverages\na library of LoRA-based adapters indexed with CLIP embeddings, dynamically\nmerging the most relevant adapters based on proximity to the target domain in\nthe embedding space. This approach constructs an ad-hoc model tailored to each\nspecific input without additional training. Our method scales efficiently,\nenhances explainability by tracking adapter contributions, and inherently\nprotects data privacy, making it ideal for sensitive applications.\nComprehensive experiments on a 20-domain benchmark built over 10 standard\ndatasets demonstrate SemLA's superior adaptability and performance across\ndiverse settings, establishing a new standard in domain adaptation for\nopen-vocabulary semantic segmentation.\n","date":"2025-03-27"}
{"id":"2503.21781","title":"VideoMage: Multi-Subject and Motion Customization of Text-to-Video\n  Diffusion Models","abstract":"  Customized text-to-video generation aims to produce high-quality videos that\nincorporate user-specified subject identities or motion patterns. However,\nexisting methods mainly focus on personalizing a single concept, either subject\nidentity or motion pattern, limiting their effectiveness for multiple subjects\nwith the desired motion patterns. To tackle this challenge, we propose a\nunified framework VideoMage for video customization over both multiple subjects\nand their interactive motions. VideoMage employs subject and motion LoRAs to\ncapture personalized content from user-provided images and videos, along with\nan appearance-agnostic motion learning approach to disentangle motion patterns\nfrom visual appearance. Furthermore, we develop a spatial-temporal composition\nscheme to guide interactions among subjects within the desired motion patterns.\nExtensive experiments demonstrate that VideoMage outperforms existing methods,\ngenerating coherent, user-controlled videos with consistent subject identities\nand interactions.\n","date":"2025-03-27"}
{"id":"2503.21782","title":"Mobile-VideoGPT: Fast and Accurate Video Understanding Language Model","abstract":"  Video understanding models often struggle with high computational\nrequirements, extensive parameter counts, and slow inference speed, making them\ninefficient for practical use. To tackle these challenges, we propose\nMobile-VideoGPT, an efficient multimodal framework designed to operate with\nfewer than a billion parameters. Unlike traditional video large multimodal\nmodels (LMMs), Mobile-VideoGPT consists of lightweight dual visual encoders,\nefficient projectors, and a small language model (SLM), enabling real-time\nthroughput. To further improve efficiency, we present an Attention-Based Frame\nScoring mechanism to select the key-frames, along with an efficient token\nprojector that prunes redundant visual tokens and preserves essential\ncontextual cues. We evaluate our model across well-established six video\nunderstanding benchmarks (e.g., MVBench, EgoSchema, NextQA, and PercepTest).\nOur results show that Mobile-VideoGPT-0.5B can generate up to 46 tokens per\nsecond while outperforming existing state-of-the-art 0.5B-parameter models by 6\npoints on average with 40% fewer parameters and more than 2x higher throughput.\nOur code and models are publicly available at:\nhttps:\/\/github.com\/Amshaker\/Mobile-VideoGPT.\n","date":"2025-03-27"}
{"id":"2503.21797","title":"A Novel Two-Phase Cooperative Co-evolution Framework for Large-Scale\n  Global Optimization with Complex Overlapping","abstract":"  Cooperative Co-evolution, through the decomposition of the problem space, is\na primary approach for solving large-scale global optimization problems.\nTypically, when the subspaces are disjoint, the algorithms demonstrate\nsignificantly both effectiveness and efficiency compared to non-decomposition\nalgorithms. However, the presence of overlapping variables complicates the\ndecomposition process and adversely affects the performance of cooperative\nco-evolution. In this study, we propose a novel two-phase cooperative\nco-evolution framework to address large-scale global optimization problems with\ncomplex overlapping. An effective method for decomposing overlapping problems,\ngrounded in their mathematical properties, is embedded within the framework.\nAdditionally, a customizable benchmark for overlapping problems is introduced\nto extend existing benchmarks and facilitate experimentation. Extensive\nexperiments demonstrate that the algorithm instantiated within our framework\nsignificantly outperforms existing algorithms. The results reveal the\ncharacteristics of overlapping problems and highlight the differing strengths\nof cooperative co-evolution and non-decomposition algorithms. Our work is\nopen-source and accessible at: https:\/\/github.com\/GMC-DRL\/HCC.\n","date":"2025-03-23"}
{"id":"2503.21798","title":"Leveraging Large Language Models for Automated Causal Loop Diagram\n  Generation: Enhancing System Dynamics Modeling through Curated Prompting\n  Techniques","abstract":"  Transforming a dynamic hypothesis into a causal loop diagram (CLD) is crucial\nfor System Dynamics Modelling. Extracting key variables and causal\nrelationships from text to build a CLD is often challenging and time-consuming\nfor novice modelers, limiting SD tool adoption. This paper introduces and tests\na method for automating the translation of dynamic hypotheses into CLDs using\nlarge language models (LLMs) with curated prompting techniques. We first\ndescribe how LLMs work and how they can make the inferences needed to build\nCLDs using a standard digraph structure. Next, we develop a set of simple\ndynamic hypotheses and corresponding CLDs from leading SD textbooks. We then\ncompare the four different combinations of prompting techniques, evaluating\ntheir performance against CLDs labeled by expert modelers. Results show that\nfor simple model structures and using curated prompting techniques, LLMs can\ngenerate CLDs of a similar quality to expert-built ones, accelerating CLD\ncreation.\n","date":"2025-03-23"}
{"id":"2503.21800","title":"ELM: Ensemble of Language Models for Predicting Tumor Group from\n  Pathology Reports","abstract":"  Population-based cancer registries (PBCRs) face a significant bottleneck in\nmanually extracting data from unstructured pathology reports, a process crucial\nfor tasks like tumor group assignment, which can consume 900 person-hours for\napproximately 100,000 reports. To address this, we introduce ELM (Ensemble of\nLanguage Models), a novel ensemble-based approach leveraging both small\nlanguage models (SLMs) and large language models (LLMs). ELM utilizes six\nfine-tuned SLMs, where three SLMs use the top part of the pathology report and\nthree SLMs use the bottom part. This is done to maximize report coverage. ELM\nrequires five-out-of-six agreement for a tumor group classification.\nDisagreements are arbitrated by an LLM with a carefully curated prompt. Our\nevaluation across nineteen tumor groups demonstrates ELM achieves an average\nprecision and recall of 0.94, outperforming single-model and\nensemble-without-LLM approaches. Deployed at the British Columbia Cancer\nRegistry, ELM demonstrates how LLMs can be successfully applied in a PBCR\nsetting to achieve state-of-the-art results and significantly enhance\noperational efficiencies, saving hundreds of person-hours annually.\n","date":"2025-03-24"}
{"id":"2503.21801","title":"Efficient Joint Prediction of Multiple Future Tokens","abstract":"  In this short report, we introduce joint multi-token prediction (JTP), a\nlightweight modification of standard next-token prediction designed to enrich\nhidden state representations by jointly predicting multiple future tokens.\nUnlike previous multi-token prediction approaches, JTP strategically employs\nteacher forcing of future-tokens through a carefully designed representation\nbottleneck, allowing the model to encode rich predictive information with\nminimal computational overhead during training. We show that the JTP approach\nachieves a short-horizon belief state representation, while popular\nalternatives for multi-token prediction fail to do so. We demonstrate the\neffectiveness of our method on the synthetic star graph navigation task from\nfrom Bachmann and Nagarajan [2024], highlighting a significant performance\nimprovement over existing methods. This manuscript presents promising\npreliminary results intended to stimulate further research.\n","date":"2025-03-24"}
{"id":"2503.21802","title":"Structured and sparse partial least squares coherence for multivariate\n  cortico-muscular analysis","abstract":"  Multivariate cortico-muscular analysis has recently emerged as a promising\napproach for evaluating the corticospinal neural pathway. However, current\nmultivariate approaches encounter challenges such as high dimensionality and\nlimited sample sizes, thus restricting their further applications. In this\npaper, we propose a structured and sparse partial least squares coherence\nalgorithm (ssPLSC) to extract shared latent space representations related to\ncortico-muscular interactions. Our approach leverages an embedded optimization\nframework by integrating a partial least squares (PLS)-based objective\nfunction, a sparsity constraint and a connectivity-based structured constraint,\naddressing the generalizability, interpretability and spatial structure. To\nsolve the optimization problem, we develop an efficient alternating iterative\nalgorithm within a unified framework and prove its convergence experimentally.\nExtensive experimental results from one synthetic and several real-world\ndatasets have demonstrated that ssPLSC can achieve competitive or better\nperformance over some representative multivariate cortico-muscular fusion\nmethods, particularly in scenarios characterized by limited sample sizes and\nhigh noise levels. This study provides a novel multivariate fusion method for\ncortico-muscular analysis, offering a transformative tool for the evaluation of\ncorticospinal pathway integrity in neurological disorders.\n","date":"2025-03-25"}
{"id":"2503.21803","title":"Forecasting Volcanic Radiative Power (VPR) at Fuego Volcano Using\n  Bayesian Regularized Neural Network","abstract":"  Forecasting volcanic activity is critical for hazard assessment and risk\nmitigation. Volcanic Radiative Power (VPR), derived from thermal remote sensing\ndata, serves as an essential indicator of volcanic activity. In this study, we\nemploy Bayesian Regularized Neural Networks (BRNN) to predict future VPR values\nbased on historical data from Fuego Volcano, comparing its performance against\nScaled Conjugate Gradient (SCG) and Levenberg-Marquardt (LM) models. The\nresults indicate that BRNN outperforms SCG and LM, achieving the lowest mean\nsquared error (1.77E+16) and the highest R-squared value (0.50), demonstrating\nits superior ability to capture VPR variability while minimizing overfitting.\nDespite these promising results, challenges remain in improving the model's\npredictive accuracy. Future research should focus on integrating additional\ngeophysical parameters, such as seismic and gas emission data, to enhance\nforecasting precision. The findings highlight the potential of machine learning\nmodels, particularly BRNN, in advancing volcanic activity forecasting,\ncontributing to more effective early warning systems for volcanic hazards.\n","date":"2025-03-25"}
{"id":"2503.21804","title":"Comparison of Metadata Representation Models for Knowledge Graph\n  Embeddings","abstract":"  Hyper-relational Knowledge Graphs (HRKGs) extend traditional KGs beyond\nbinary relations, enabling the representation of contextual, provenance, and\ntemporal information in domains, such as historical events, sensor data, video\ncontent, and narratives. HRKGs can be structured using several Metadata\nRepresentation Models (MRMs), including Reification (REF), Singleton Property\n(SGP), and RDF-star (RDR). However, the effects of different MRMs on KG\nEmbedding (KGE) and Link Prediction (LP) models remain unclear. This study\nevaluates MRMs in the context of LP tasks, identifies the limitations of\nexisting evaluation frameworks, and introduces a new task that ensures fair\ncomparisons across MRMs. Furthermore, we propose a framework that effectively\nreflects the knowledge representations of the three MRMs in latent space.\nExperiments on two types of datasets reveal that REF performs well in simple\nHRKGs, whereas SGP is less effective. However, in complex HRKGs, the\ndifferences among MRMs in the LP tasks are minimal. Our findings contribute to\nan optimal knowledge representation strategy for HRKGs in LP tasks.\n","date":"2025-03-25"}
{"id":"2503.21805","title":"ImF: Implicit Fingerprint for Large Language Models","abstract":"  Training large language models (LLMs) is resource-intensive and expensive,\nmaking intellectual property (IP) protection essential. Most existing model\nfingerprint methods inject fingerprints into LLMs to protect model ownership.\nThese methods create fingerprint pairs with weak semantic correlations, lacking\nthe contextual coherence and semantic relatedness founded in normal\nquestion-answer (QA) pairs in LLMs. In this paper, we propose a Generation\nRevision Intervention (GRI) attack that can effectively exploit this flaw to\nerase fingerprints, highlighting the need for more secure model fingerprint\nmethods. Thus, we propose a novel injected fingerprint paradigm called Implicit\nFingerprints (ImF). ImF constructs fingerprint pairs with strong semantic\ncorrelations, disguising them as natural QA pairs within LLMs. This ensures the\nfingerprints are consistent with normal model behavior, making them\nindistinguishable and robust against detection and removal. Our experiment on\nmultiple LLMs demonstrates that ImF retains high verification success rates\nunder adversarial conditions, offering a reliable solution for protecting LLM\nownership.\n","date":"2025-03-25"}
{"id":"2503.21806","title":"Large Language Models Meet Contrastive Learning: Zero-Shot Emotion\n  Recognition Across Languages","abstract":"  Multilingual speech emotion recognition aims to estimate a speaker's\nemotional state using a contactless method across different languages. However,\nvariability in voice characteristics and linguistic diversity poses significant\nchallenges for zero-shot speech emotion recognition, especially with\nmultilingual datasets. In this paper, we propose leveraging contrastive\nlearning to refine multilingual speech features and extend large language\nmodels for zero-shot multilingual speech emotion estimation. Specifically, we\nemploy a novel two-stage training framework to align speech signals with\nlinguistic features in the emotional space, capturing both emotion-aware and\nlanguage-agnostic speech representations. To advance research in this field, we\nintroduce a large-scale synthetic multilingual speech emotion dataset, M5SER.\nOur experiments demonstrate the effectiveness of the proposed method in both\nspeech emotion recognition and zero-shot multilingual speech emotion\nrecognition, including previously unseen datasets and languages.\n","date":"2025-03-25"}
{"id":"2503.21807","title":"LERO: LLM-driven Evolutionary framework with Hybrid Rewards and Enhanced\n  Observation for Multi-Agent Reinforcement Learning","abstract":"  Multi-agent reinforcement learning (MARL) faces two critical bottlenecks\ndistinct from single-agent RL: credit assignment in cooperative tasks and\npartial observability of environmental states. We propose LERO, a framework\nintegrating Large language models (LLMs) with evolutionary optimization to\naddress these MARL-specific challenges. The solution centers on two\nLLM-generated components: a hybrid reward function that dynamically allocates\nindividual credit through reward decomposition, and an observation enhancement\nfunction that augments partial observations with inferred environmental\ncontext. An evolutionary algorithm optimizes these components through iterative\nMARL training cycles, where top-performing candidates guide subsequent LLM\ngenerations. Evaluations in Multi-Agent Particle Environments (MPE) demonstrate\nLERO's superiority over baseline methods, with improved task performance and\ntraining efficiency.\n","date":"2025-03-25"}
{"id":"2503.21809","title":"Enhancing Predictive Accuracy in Tennis: Integrating Fuzzy Logic and\n  CV-GRNN for Dynamic Match Outcome and Player Momentum Analysis","abstract":"  The predictive analysis of match outcomes and player momentum in professional\ntennis has long been a subject of scholarly debate. In this paper, we introduce\na novel approach to game prediction by combining a multi-level fuzzy evaluation\nmodel with a CV-GRNN model. We first identify critical statistical indicators\nvia Principal Component Analysis and then develop a two-tier fuzzy model based\non the Wimbledon data. In addition, the results of Pearson Correlation\nCoefficient indicate that the momentum indicators, such as Player Win Streak\nand Score Difference, have a strong correlation among them, revealing\ninsightful trends among players transitioning between losing and winning\nstreaks. Subsequently, we refine the CV-GRNN model by incorporating 15\nstatistically significant indicators, resulting in an increase in accuracy to\n86.64% and a decrease in MSE by 49.21%. This consequently strengthens the\nmethodological framework for predicting tennis match outcomes, emphasizing its\npractical utility and potential for adaptation in various athletic contexts.\n","date":"2025-03-25"}
{"id":"2503.21810","title":"Taxonomy Inference for Tabular Data Using Large Language Models","abstract":"  Taxonomy inference for tabular data is a critical task of schema inference,\naiming at discovering entity types (i.e., concepts) of the tables and building\ntheir hierarchy. It can play an important role in data management, data\nexploration, ontology learning, and many data-centric applications. Existing\nschema inference systems focus more on XML, JSON or RDF data, and often rely on\nlexical formats and structures of the data for calculating similarities, with\nlimited exploitation of the semantics of the text across a table. Motivated by\nrecent works on taxonomy completion and construction using Large Language\nModels (LLMs), this paper presents two LLM-based methods for taxonomy inference\nfor tables: (i) EmTT which embeds columns by fine-tuning with contrastive\nlearning encoder-alone LLMs like BERT and utilises clustering for hierarchy\nconstruction, and (ii) GeTT which generates table entity types and their\nhierarchy by iterative prompting using a decoder-alone LLM like GPT-4.\nExtensive evaluation on three real-world datasets with six metrics covering\ndifferent aspects of the output taxonomies has demonstrated that EmTT and GeTT\ncan both produce taxonomies with strong consistency relative to the Ground\nTruth.\n","date":"2025-03-25"}
{"id":"2503.21812","title":"IPGO: Indirect Prompt Gradient Optimization on Text-to-Image Generative\n  Models with High Data Efficiency","abstract":"  Text-to-Image Diffusion models excel at generating images from text prompts\nbut often lack optimal alignment with content semantics, aesthetics, and human\npreferences. To address these issues, in this study we introduce a novel\nframework, Indirect Prompt Gradient Optimization (IPGO), for prompt-level\nfine-tuning. IPGO enhances prompt embeddings by injecting continuously\ndifferentiable tokens at the beginning and end of the prompt embeddings, while\nexploiting low-rank benefits and flexibility from rotations. It allows for\ngradient-based optimization of injected tokens while enforcing value,\northonormality, and conformity constraints, facilitating continuous updates and\nempowering computational efficiency. To evaluate the performance of IPGO, we\nconduct prompt-wise and prompt-batch training with three reward models\ntargeting image aesthetics, image-text alignment, and human preferences under\nthree datasets of different complexity. The results show that IPGO consistently\nmatches or outperforms cutting-edge benchmarks, including stable diffusion v1.5\nwith raw prompts, training-based approaches (DRaFT and DDPO), and training-free\nmethods (DPO-Diffusion, Promptist, and ChatGPT-4o). Furthermore, we demonstrate\nIPGO's effectiveness in enhancing image generation quality while requiring\nminimal training data and limited computational resources.\n","date":"2025-03-25"}
{"id":"2503.21813","title":"OAEI-LLM-T: A TBox Benchmark Dataset for Understanding LLM\n  Hallucinations in Ontology Matching Systems","abstract":"  Hallucinations are inevitable in downstream tasks using large language models\n(LLMs). While addressing hallucinations becomes a substantial challenge for\nLLM-based ontology matching (OM) systems, we introduce a new benchmark dataset\ncalled OAEI-LLM-T. The dataset evolves from the TBox (i.e. schema-matching)\ndatasets in the Ontology Alignment Evaluation Initiative (OAEI), capturing\nhallucinations of different LLMs performing OM tasks. These OM-specific\nhallucinations are carefully classified into two primary categories and six\nsub-categories. We showcase the usefulness of the dataset in constructing the\nLLM leaderboard and fine-tuning foundational LLMs for LLM-based OM systems.\n","date":"2025-03-25"}
{"id":"2503.21814","title":"Unsupervised Ordering for Maximum Clique","abstract":"  We propose an unsupervised approach for learning vertex orderings for the\nmaximum clique problem by framing it within a permutation-based framework. We\ntransform the combinatorial constraints into geometric relationships such that\nthe ordering of vertices aligns with the clique structures. By integrating this\nclique-oriented ordering into branch-and-bound search, we improve search\nefficiency and reduce the number of computational steps. Our results\ndemonstrate how unsupervised learning of vertex ordering can enhance search\nefficiency across diverse graph instances. We further study the generalization\nacross different sizes.\n","date":"2025-03-25"}
{"id":"2503.21815","title":"ATP: Adaptive Threshold Pruning for Efficient Data Encoding in Quantum\n  Neural Networks","abstract":"  Quantum Neural Networks (QNNs) offer promising capabilities for complex data\ntasks, but are often constrained by limited qubit resources and high\nentanglement, which can hinder scalability and efficiency. In this paper, we\nintroduce Adaptive Threshold Pruning (ATP), an encoding method that reduces\nentanglement and optimizes data complexity for efficient computations in QNNs.\nATP dynamically prunes non-essential features in the data based on adaptive\nthresholds, effectively reducing quantum circuit requirements while preserving\nhigh performance. Extensive experiments across multiple datasets demonstrate\nthat ATP reduces entanglement entropy and improves adversarial robustness when\ncombined with adversarial training methods like FGSM. Our results highlight\nATPs ability to balance computational efficiency and model resilience,\nachieving significant performance improvements with fewer resources, which will\nhelp make QNNs more feasible in practical, resource-constrained settings.\n","date":"2025-03-26"}
{"id":"2503.21817","title":"Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping","abstract":"  Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency.\n","date":"2025-03-26"}
{"id":"2503.21818","title":"Deep Learning-Based Quantitative Assessment of Renal Chronicity Indices\n  in Lupus Nephritis","abstract":"  Background: Renal chronicity indices (CI) have been identified as strong\npredictors of long-term outcomes in lupus nephritis (LN) patients. However,\nassessment by pathologists is hindered by challenges such as substantial time\nrequirements, high interobserver variation, and susceptibility to fatigue. This\nstudy aims to develop an effective deep learning (DL) pipeline that automates\nthe assessment of CI and provides valuable prognostic insights from a\ndisease-specific perspective. Methods: We curated a dataset comprising 282\nslides obtained from 141 patients across two independent cohorts with a\ncomplete 10-years follow-up. Our DL pipeline was developed on 60 slides (22,410\npatch images) from 30 patients in the training cohort and evaluated on both an\ninternal testing set (148 slides, 77,605 patch images) and an external testing\nset (74 slides, 27,522 patch images). Results: The study included two cohorts\nwith slight demographic differences, particularly in age and hemoglobin levels.\nThe DL pipeline showed high segmentation performance across tissue compartments\nand histopathologic lesions, outperforming state-of-the-art methods. The DL\npipeline also demonstrated a strong correlation with pathologists in assessing\nCI, significantly improving interobserver agreement. Additionally, the DL\npipeline enhanced prognostic accuracy, particularly in outcome prediction, when\ncombined with clinical parameters and pathologist-assessed CIs Conclusions: The\nDL pipeline demonstrated accuracy and efficiency in assessing CI in LN, showing\npromise in improving interobserver agreement among pathologists. It also\nexhibited significant value in prognostic analysis and enhancing outcome\nprediction in LN patients, offering a valuable tool for clinical\ndecision-making.\n","date":"2025-03-26"}
{"id":"2503.21819","title":"Optimizing Safe and Aligned Language Generation: A Multi-Objective GRPO\n  Approach","abstract":"  Aligning large language models (LLMs) with human values and safety\nconstraints is challenging, especially when objectives like helpfulness,\ntruthfulness, and avoidance of harm conflict. Reinforcement Learning from Human\nFeedback (RLHF) has achieved notable success in steering models, but is complex\nand can be unstable. Recent approaches such as Direct Preference Optimization\n(DPO) simplify preference-based fine-tuning but may introduce bias or trade-off\ncertain objectives~\\cite{dpo}. In this work, we propose a Group Relative Policy\nOptimization (GRPO) framework with a multi-label reward regression model to\nachieve safe and aligned language generation. The GRPO algorithm optimizes a\npolicy by comparing groups of sampled responses, eliminating the need for a\nseparate value critic and improving training efficiency~\\cite{grpo}. We train a\nreward model to predict multiple alignment scores (e.g., safety, helpfulness,\netc.), which are combined into a single reward signal. We provide a theoretical\nderivation for using this learned multi-aspect reward within GRPO and discuss\nits advantages and limitations. Empirically, our approach improves all the\nsafety and quality metrics evaluated in language generation tasks on model\nscales (0.5B, 7B, and 14B parameters), demonstrating a robust balance of\nobjectives. We compare GRPO to PPO-based RLHF and DPO, highlighting that GRPO\nachieves alignment with significantly lower computational cost and explicit\nmulti-objective handling. \\textbf{We will open-source all trained models at\nhttps:\/\/huggingface.co\/hydroxai.\n","date":"2025-03-26"}
{"id":"2503.21820","title":"UFM: Unified Feature Matching Pre-training with Multi-Modal Image\n  Assistants","abstract":"  Image feature matching, a foundational task in computer vision, remains\nchallenging for multimodal image applications, often necessitating intricate\ntraining on specific datasets. In this paper, we introduce a Unified Feature\nMatching pre-trained model (UFM) designed to address feature matching\nchallenges across a wide spectrum of modal images. We present Multimodal Image\nAssistant (MIA) transformers, finely tunable structures adept at handling\ndiverse feature matching problems. UFM exhibits versatility in addressing both\nfeature matching tasks within the same modal and those across different modals.\nAdditionally, we propose a data augmentation algorithm and a staged\npre-training strategy to effectively tackle challenges arising from sparse data\nin specific modals and imbalanced modal datasets. Experimental results\ndemonstrate that UFM excels in generalization and performance across various\nfeature matching tasks. The code will be released\nat:https:\/\/github.com\/LiaoYun0x0\/UFM.\n","date":"2025-03-26"}
{"id":"2503.21823","title":"Low-Rank Adaptation of Pre-Trained Stable Diffusion for Rigid-Body\n  Target ISAR Imaging","abstract":"  Traditional range-instantaneous Doppler (RID) methods for rigid-body target\nimaging often suffer from low resolution due to the limitations of\ntime-frequency analysis (TFA). To address this challenge, our primary focus is\non obtaining high resolution time-frequency representations (TFRs) from their\nlow resolution counterparts. Recognizing that the curve features of TFRs are a\nspecific type of texture feature, we argue that pre trained generative models\nsuch as Stable Diffusion (SD) are well suited for enhancing TFRs, thanks to\ntheir powerful capability in capturing texture representations. Building on\nthis insight, we propose a novel inverse synthetic aperture radar (ISAR)\nimaging method for rigid-body targets, leveraging the low-rank adaptation\n(LoRA) of a pre-trained SD model. Our approach adopts the basic structure and\npre-trained parameters of SD Turbo while incorporating additional linear\noperations for LoRA and adversarial training to achieve super-resolution and\nnoise suppression. Then we integrate LoRA-SD into the RID-based ISAR imaging,\nenabling sharply focused and denoised imaging with super-resolution\ncapabilities. We evaluate our method using both simulated and real radar data.\nThe experimental results demonstrate the superiority of our approach in\nfrequency es timation and ISAR imaging compared to traditional methods.\nNotably, the generalization capability is verified by training on simulated\nradar data and testing on measured radar data.\n","date":"2025-03-26"}
{"id":"2503.21824","title":"Protecting Your Video Content: Disrupting Automated Video-based LLM\n  Annotations","abstract":"  Recently, video-based large language models (video-based LLMs) have achieved\nimpressive performance across various video comprehension tasks. However, this\nrapid advancement raises significant privacy and security concerns,\nparticularly regarding the unauthorized use of personal video data in automated\nannotation by video-based LLMs. These unauthorized annotated video-text pairs\ncan then be used to improve the performance of downstream tasks, such as\ntext-to-video generation. To safeguard personal videos from unauthorized use,\nwe propose two series of protective video watermarks with imperceptible\nadversarial perturbations, named Ramblings and Mutes. Concretely, Ramblings aim\nto mislead video-based LLMs into generating inaccurate captions for the videos,\nthereby degrading the quality of video annotations through inconsistencies\nbetween video content and captions. Mutes, on the other hand, are designed to\nprompt video-based LLMs to produce exceptionally brief captions, lacking\ndescriptive detail. Extensive experiments demonstrate that our video\nwatermarking methods effectively protect video data by significantly reducing\nvideo annotation performance across various video-based LLMs, showcasing both\nstealthiness and robustness in protecting personal video content. Our code is\navailable at https:\/\/github.com\/ttthhl\/Protecting_Your_Video_Content.\n","date":"2025-03-26"}
{"id":"2503.21825","title":"Implicit neural representations for end-to-end PET reconstruction","abstract":"  Implicit neural representations (INRs) have demonstrated strong capabilities\nin various medical imaging tasks, such as denoising, registration, and\nsegmentation, by representing images as continuous functions, allowing complex\ndetails to be captured. For image reconstruction problems, INRs can also reduce\nartifacts typically introduced by conventional reconstruction algorithms.\nHowever, to the best of our knowledge, INRs have not been studied in the\ncontext of PET reconstruction. In this paper, we propose an unsupervised PET\nimage reconstruction method based on the implicit SIREN neural network\narchitecture using sinusoidal activation functions. Our method incorporates a\nforward projection model and a loss function adapted to perform PET image\nreconstruction directly from sinograms, without the need for large training\ndatasets. The performance of the proposed approach was compared with that of\nconventional penalized likelihood methods and deep image prior (DIP) based\nreconstruction using brain phantom data and realistically simulated sinograms.\nThe results show that the INR-based approach can reconstruct high-quality\nimages with a simpler, more efficient model, offering improvements in PET image\nreconstruction, particularly in terms of contrast, activity recovery, and\nrelative bias.\n","date":"2025-03-26"}
{"id":"2503.21826","title":"Hierarchical Label Propagation: A Model-Size-Dependent Performance\n  Booster for AudioSet Tagging","abstract":"  AudioSet is one of the most used and largest datasets in audio tagging,\ncontaining about 2 million audio samples that are manually labeled with 527\nevent categories organized into an ontology. However, the annotations contain\ninconsistencies, particularly where categories that should be labeled as\npositive according to the ontology are frequently mislabeled as negative. To\naddress this issue, we apply Hierarchical Label Propagation (HLP), which\npropagates labels up the ontology hierarchy, resulting in a mean increase in\npositive labels per audio clip from 1.98 to 2.39 and affecting 109 out of the\n527 classes. Our results demonstrate that HLP provides performance benefits\nacross various model architectures, including convolutional neural networks\n(PANN's CNN6 and ConvNeXT) and transformers (PaSST), with smaller models\nshowing more improvements. Finally, on FSD50K, another widely used dataset,\nmodels trained on AudioSet with HLP consistently outperformed those trained\nwithout HLP. Our source code will be made available on GitHub.\n","date":"2025-03-26"}
{"id":"2503.21827","title":"Hybrid Multi-Stage Learning Framework for Edge Detection: A Survey","abstract":"  Edge detection remains a fundamental yet challenging task in computer vision,\nespecially under varying illumination, noise, and complex scene conditions.\nThis paper introduces a Hybrid Multi-Stage Learning Framework that integrates\nConvolutional Neural Network (CNN) feature extraction with a Support Vector\nMachine (SVM) classifier to improve edge localization and structural accuracy.\nUnlike conventional end-to-end deep learning models, our approach decouples\nfeature representation and classification stages, enhancing robustness and\ninterpretability. Extensive experiments conducted on benchmark datasets such as\nBSDS500 and NYUDv2 demonstrate that the proposed framework outperforms\ntraditional edge detectors and even recent learning-based methods in terms of\nOptimal Dataset Scale (ODS) and Optimal Image Scale (OIS), while maintaining\ncompetitive Average Precision (AP). Both qualitative and quantitative results\nhighlight enhanced performance on edge continuity, noise suppression, and\nperceptual clarity achieved by our method. This work not only bridges classical\nand deep learning paradigms but also sets a new direction for scalable,\ninterpretable, and high-quality edge detection solutions.\n","date":"2025-03-26"}
{"id":"2503.21829","title":"Learning from spatially inhomogenous data: resolution-adaptive\n  convolutions for multiple sclerosis lesion segmentation","abstract":"  In the setting of clinical imaging, differences in between vendors, hospitals\nand sequences can yield highly inhomogeneous imaging data. In MRI in\nparticular, voxel dimension, slice spacing and acquisition plane can vary\nsubstantially. For clinical applications, therefore, algorithms must be trained\nto handle data with various voxel resolutions. The usual strategy to deal with\nheterogeneity of resolution is harmonization: resampling imaging data to a\ncommon (usually isovoxel) resolution. This can lead to loss of fidelity arising\nfrom interpolation artifacts out-of-plane and downsampling in-plane. We present\nin this paper a network architecture designed to be able to learn directly from\nspatially heterogeneous data, without resampling: a segmentation network based\non the e3nn framework that leverages a spherical harmonic, rather than\nvoxel-grid, parameterization of convolutional kernels, with a fixed physical\nradius. Networks based on these kernels can be resampled to their input voxel\ndimensions. We trained and tested our network on a publicly available dataset\nassembled from three centres, and on an in-house dataset of Multiple Sclerosis\ncases with a high degree of spatial inhomogeneity. We compared our approach to\na standard U-Net with two strategies for handling inhomogeneous data: training\ndirectly on the data without resampling, and resampling to a common resolution\nof 1mm isovoxels. We show that our network is able to learn from various\ncombinations of voxel sizes and outperforms classical U-Nets on 2D testing\ncases and most 3D testing cases. This shows an ability to generalize well when\ntested on image resolutions not seen during training. Our code can be found at:\nhttp:\/\/github.com\/SCAN-NRAD\/e3nn\\_U-Net.\n","date":"2025-03-26"}
{"id":"2503.21830","title":"Shape Generation via Weight Space Learning","abstract":"  Foundation models for 3D shape generation have recently shown a remarkable\ncapacity to encode rich geometric priors across both global and local\ndimensions. However, leveraging these priors for downstream tasks can be\nchallenging as real-world data are often scarce or noisy, and traditional\nfine-tuning can lead to catastrophic forgetting. In this work, we treat the\nweight space of a large 3D shape-generative model as a data modality that can\nbe explored directly. We hypothesize that submanifolds within this\nhigh-dimensional weight space can modulate topological properties or\nfine-grained part features separately, demonstrating early-stage evidence via\ntwo experiments. First, we observe a sharp phase transition in global\nconnectivity when interpolating in conditioning space, suggesting that small\nchanges in weight space can drastically alter topology. Second, we show that\nlow-dimensional reparameterizations yield controlled local geometry changes\neven with very limited data. These results highlight the potential of weight\nspace learning to unlock new approaches for 3D shape generation and specialized\nfine-tuning.\n","date":"2025-03-26"}
{"id":"2503.21833","title":"Refining Time Series Anomaly Detectors using Large Language Models","abstract":"  Time series anomaly detection (TSAD) is of widespread interest across many\nindustries, including finance, healthcare, and manufacturing. Despite the\ndevelopment of numerous automatic methods for detecting anomalies, human\noversight remains necessary to review and act upon detected anomalies, as well\nas verify their accuracy. We study the use of multimodal large language models\n(LLMs) to partially automate this process. We find that LLMs can effectively\nidentify false alarms by integrating visual inspection of time series plots\nwith text descriptions of the data-generating process. By leveraging the\ncapabilities of LLMs, we aim to reduce the reliance on human effort required to\nmaintain a TSAD system\n","date":"2025-03-26"}
{"id":"2503.21834","title":"A Multi-Modal Knowledge-Enhanced Framework for Vessel Trajectory\n  Prediction","abstract":"  Accurate vessel trajectory prediction facilitates improved navigational\nsafety, routing, and environmental protection. However, existing prediction\nmethods are challenged by the irregular sampling time intervals of the vessel\ntracking data from the global AIS system and the complexity of vessel movement.\nThese aspects render model learning and generalization difficult. To address\nthese challenges and improve vessel trajectory prediction, we propose the\nmulti-modal knowledge-enhanced framework (MAKER) for vessel trajectory\nprediction. To contend better with the irregular sampling time intervals, MAKER\nfeatures a Large language model-guided Knowledge Transfer (LKT) module that\nleverages pre-trained language models to transfer trajectory-specific\ncontextual knowledge effectively. To enhance the ability to learn complex\ntrajectory patterns, MAKER incorporates a Knowledge-based Self-paced Learning\n(KSL) module. This module employs kinematic knowledge to progressively\nintegrate complex patterns during training, allowing for adaptive learning and\nenhanced generalization. Experimental results on two vessel trajectory datasets\nshow that MAKER can improve the prediction accuracy of state-of-the-art methods\nby 12.08%-17.86%.\n","date":"2025-03-27"}
{"id":"2503.21836","title":"iMedImage Technical Report","abstract":"  Background: Chromosome karyotype analysis is crucial for diagnosing\nhereditary diseases, yet detecting structural abnormalities remains\nchallenging. While AI has shown promise in medical imaging, its effectiveness\nvaries across modalities. Leveraging advances in Foundation Models that\nintegrate multimodal medical imaging for robust feature extraction and accurate\ndiagnosis, we developed iMedImage, an end-to-end model for general medical\nimage recognition, demonstrating strong performance across multiple imaging\ntasks, including chromosome abnormality detection. Materials and Methods: We\nconstructed a comprehensive medical image dataset encompassing multiple\nmodalities from common medical domains, including chromosome, cell, pathology,\nultrasound, X-ray, CT, and MRI images. Based on this dataset, we developed the\niMedImage model, which incorporates the following key features: (1) a unified\nrepresentation method for diverse modality inputs and medical imaging tasks;\n(2) multi-level (case-level, image-level, patch-level) image recognition\ncapabilities enhanced by Chain of Thought (CoT) embedding and Mixture of\nExperts (MoE) strategies. Results: The test set comprised data from 12\ninstitutions across six regions in China, covering three mainstream scanning\ndevices, and included naturally distributed, unscreened abnormal cases. On this\ndiverse dataset, the model achieved a fully automated chromosome analysis\nworkflow, including segmentation, karyotyping, and abnormality detection,\nreaching a sensitivity of 92.75% and a specificity of 91.5%. Conclusion: We\npropose iMedImage, an end-to-end foundation model for medical image analysis,\ndemonstrating its superior performance across various medical imaging tasks.\niMedImage provides clinicians with a precise imaging analysis tool and\ncontributes to improving diagnostic accuracy and disease screening.\n","date":"2025-03-27"}
{"id":"2503.21838","title":"MSPLoRA: A Multi-Scale Pyramid Low-Rank Adaptation for Efficient Model\n  Fine-Tuning","abstract":"  Parameter-Efficient Fine-Tuning (PEFT) has become an essential approach for\nadapting large-scale pre-trained models while reducing computational costs.\nAmong PEFT methods, LoRA significantly reduces trainable parameters by\ndecomposing weight updates into low-rank matrices. However, traditional LoRA\napplies a fixed rank across all layers, failing to account for the varying\ncomplexity of hierarchical information, which leads to inefficient adaptation\nand redundancy. To address this, we propose MSPLoRA (Multi-Scale Pyramid LoRA),\nwhich introduces Global Shared LoRA, Mid-Level Shared LoRA, and Layer-Specific\nLoRA to capture global patterns, mid-level features, and fine-grained\ninformation, respectively. This hierarchical structure reduces inter-layer\nredundancy while maintaining strong adaptation capability. Experiments on\nvarious NLP tasks demonstrate that MSPLoRA achieves more efficient adaptation\nand better performance while significantly reducing the number of trainable\nparameters. Furthermore, additional analyses based on Singular Value\nDecomposition validate its information decoupling ability, highlighting MSPLoRA\nas a scalable and effective optimization strategy for parameter-efficient\nfine-tuning in large language models. Our code is available at\nhttps:\/\/github.com\/Oblivioniss\/MSPLoRA.\n","date":"2025-03-27"}
{"id":"2503.21839","title":"M-DocSum: Do LVLMs Genuinely Comprehend Interleaved Image-Text in\n  Document Summarization?","abstract":"  We investigate a critical yet under-explored question in Large\nVision-Language Models (LVLMs): Do LVLMs genuinely comprehend interleaved\nimage-text in the document? Existing document understanding benchmarks often\nassess LVLMs using question-answer formats, which are information-sparse and\ndifficult to guarantee the coverage of long-range dependencies. To address this\nissue, we introduce a novel and challenging Multimodal Document Summarization\nBenchmark (M-DocSum-Bench), which comprises 500 high-quality arXiv papers,\nalong with interleaved multimodal summaries aligned with human preferences.\nM-DocSum-Bench is a reference-based generation task and necessitates the\ngeneration of interleaved image-text summaries using provided reference images,\nthereby simultaneously evaluating capabilities in understanding, reasoning,\nlocalization, and summarization within complex multimodal document scenarios.\nTo facilitate this benchmark, we develop an automated framework to construct\nsummaries and propose a fine-grained evaluation method called M-DocEval.\nMoreover, we further develop a robust summarization baseline, i.e.,\nM-DocSum-7B, by progressive two-stage training with diverse instruction and\npreference data. The extensive results on our M-DocSum-Bench reveal that the\nleading LVLMs struggle to maintain coherence and accurately integrate\ninformation within long and interleaved contexts, often exhibiting confusion\nbetween similar images and a lack of robustness. Notably, M-DocSum-7B achieves\nstate-of-the-art performance compared to larger and closed-source models\n(including GPT-4o, Gemini Pro, Claude-3.5-Sonnet and Qwen2.5-VL-72B, etc.),\ndemonstrating the potential of LVLMs for improved interleaved image-text\nunderstanding. The code, data, and models are available at\nhttps:\/\/github.com\/stepfun-ai\/M-DocSum-Bench.\n","date":"2025-03-27"}
{"id":"2503.21840","title":"Vision Language Models versus Machine Learning Models Performance on\n  Polyp Detection and Classification in Colonoscopy Images","abstract":"  Introduction: This study provides a comprehensive performance assessment of\nvision-language models (VLMs) against established convolutional neural networks\n(CNNs) and classic machine learning models (CMLs) for computer-aided detection\n(CADe) and computer-aided diagnosis (CADx) of colonoscopy polyp images. Method:\nWe analyzed 2,258 colonoscopy images with corresponding pathology reports from\n428 patients. We preprocessed all images using standardized techniques\n(resizing, normalization, and augmentation) and implemented a rigorous\ncomparative framework evaluating 11 distinct models: ResNet50, 4 CMLs (random\nforest, support vector machine, logistic regression, decision tree), two\nspecialized contrastive vision language encoders (CLIP, BiomedCLIP), and three\ngeneral-purpose VLMs ( GPT-4 Gemini-1.5-Pro, Claude-3-Opus). Our performance\nassessment focused on two clinical tasks: polyp detection (CADe) and\nclassification (CADx). Result: In polyp detection, ResNet50 achieved the best\nperformance (F1: 91.35%, AUROC: 0.98), followed by BiomedCLIP (F1: 88.68%,\nAUROC: [AS1] ). GPT-4 demonstrated comparable effectiveness to traditional\nmachine learning approaches (F1: 81.02%, AUROC: [AS2] ), outperforming other\ngeneral-purpose VLMs. For polyp classification, performance rankings remained\nconsistent but with lower overall metrics. ResNet50 maintained the highest\nefficacy (weighted F1: 74.94%), while GPT-4 demonstrated moderate capability\n(weighted F1: 41.18%), significantly exceeding other VLMs (Claude-3-Opus\nweighted F1: 25.54%, Gemini 1.5 Pro weighted F1: 6.17%). Conclusion: CNNs\nremain superior for both CADx and CADe tasks. However, VLMs like BioMedCLIP and\nGPT-4 may be useful for polyp detection tasks where training CNNs is not\nfeasible.\n","date":"2025-03-27"}
{"id":"2503.21841","title":"HyperFree: A Channel-adaptive and Tuning-free Foundation Model for\n  Hyperspectral Remote Sensing Imagery","abstract":"  Advanced interpretation of hyperspectral remote sensing images benefits many\nprecise Earth observation tasks. Recently, visual foundation models have\npromoted the remote sensing interpretation but concentrating on RGB and\nmultispectral images. Due to the varied hyperspectral channels,existing\nfoundation models would face image-by-image tuning situation, imposing great\npressure on hardware and time resources. In this paper, we propose a\ntuning-free hyperspectral foundation model called HyperFree, by adapting the\nexisting visual prompt engineering. To process varied channel numbers, we\ndesign a learned weight dictionary covering full-spectrum from $0.4 \\sim 2.5 \\,\n\\mu\\text{m}$, supporting to build the embedding layer dynamically. To make the\nprompt design more tractable, HyperFree can generate multiple semantic-aware\nmasks for one prompt by treating feature distance as semantic-similarity. After\npre-training HyperFree on constructed large-scale high-resolution hyperspectral\nimages, HyperFree (1 prompt) has shown comparable results with specialized\nmodels (5 shots) on 5 tasks and 11 datasets.Code and dataset are accessible at\nhttps:\/\/rsidea.whu.edu.cn\/hyperfree.htm.\n","date":"2025-03-27"}
{"id":"2503.21843","title":"CMD-HAR: Cross-Modal Disentanglement for Wearable Human Activity\n  Recognition","abstract":"  Human Activity Recognition (HAR) is a fundamental technology for numerous\nhuman - centered intelligent applications. Although deep learning methods have\nbeen utilized to accelerate feature extraction, issues such as multimodal data\nmixing, activity heterogeneity, and complex model deployment remain largely\nunresolved. The aim of this paper is to address issues such as multimodal data\nmixing, activity heterogeneity, and complex model deployment in sensor-based\nhuman activity recognition. We propose a spatiotemporal attention modal\ndecomposition alignment fusion strategy to tackle the problem of the mixed\ndistribution of sensor data. Key discriminative features of activities are\ncaptured through cross-modal spatio-temporal disentangled representation, and\ngradient modulation is combined to alleviate data heterogeneity. In addition, a\nwearable deployment simulation system is constructed. We conducted experiments\non a large number of public datasets, demonstrating the effectiveness of the\nmodel.\n","date":"2025-03-27"}
{"id":"2503.21846","title":"LightSNN: Lightweight Architecture Search for Sparse and Accurate\n  Spiking Neural Networks","abstract":"  Spiking Neural Networks (SNNs) are highly regarded for their energy\nefficiency, inherent activation sparsity, and suitability for real-time\nprocessing in edge devices. However, most current SNN methods adopt\narchitectures resembling traditional artificial neural networks (ANNs), leading\nto suboptimal performance when applied to SNNs. While SNNs excel in energy\nefficiency, they have been associated with lower accuracy levels than\ntraditional ANNs when utilizing conventional architectures. In response, in\nthis work we present LightSNN, a rapid and efficient Neural Network\nArchitecture Search (NAS) technique specifically tailored for SNNs that\nautonomously leverages the most suitable architecture, striking a good balance\nbetween accuracy and efficiency by enforcing sparsity. Based on the spiking NAS\nnetwork (SNASNet) framework, a cell-based search space including backward\nconnections is utilized to build our training-free pruning-based NAS mechanism.\nOur technique assesses diverse spike activation patterns across different data\nsamples using a sparsity-aware Hamming distance fitness evaluation. Thorough\nexperiments are conducted on both static (CIFAR10 and CIFAR100) and\nneuromorphic datasets (DVS128-Gesture). Our LightSNN model achieves\nstate-of-the-art results on CIFAR10 and CIFAR100, improves performance on\nDVS128Gesture by 4.49%, and significantly reduces search time, most notably\noffering a 98x speedup over SNASNet and running 30% faster than the best\nexisting method on DVS128Gesture.\n","date":"2025-03-27"}
{"id":"2503.21847","title":"ReCoM: Realistic Co-Speech Motion Generation with Recurrent Embedded\n  Transformer","abstract":"  We present ReCoM, an efficient framework for generating high-fidelity and\ngeneralizable human body motions synchronized with speech. The core innovation\nlies in the Recurrent Embedded Transformer (RET), which integrates Dynamic\nEmbedding Regularization (DER) into a Vision Transformer (ViT) core\narchitecture to explicitly model co-speech motion dynamics. This architecture\nenables joint spatial-temporal dependency modeling, thereby enhancing gesture\nnaturalness and fidelity through coherent motion synthesis. To enhance model\nrobustness, we incorporate the proposed DER strategy, which equips the model\nwith dual capabilities of noise resistance and cross-domain generalization,\nthereby improving the naturalness and fluency of zero-shot motion generation\nfor unseen speech inputs. To mitigate inherent limitations of autoregressive\ninference, including error accumulation and limited self-correction, we propose\nan iterative reconstruction inference (IRI) strategy. IRI refines motion\nsequences via cyclic pose reconstruction, driven by two key components: (1)\nclassifier-free guidance improves distribution alignment between generated and\nreal gestures without auxiliary supervision, and (2) a temporal smoothing\nprocess eliminates abrupt inter-frame transitions while ensuring kinematic\ncontinuity. Extensive experiments on benchmark datasets validate ReCoM's\neffectiveness, achieving state-of-the-art performance across metrics. Notably,\nit reduces the Fr\\'echet Gesture Distance (FGD) from 18.70 to 2.48,\ndemonstrating an 86.7% improvement in motion realism. Our project page is\nhttps:\/\/yong-xie-xy.github.io\/ReCoM\/.\n","date":"2025-03-27"}
{"id":"2503.21848","title":"Comparative Analysis of Image, Video, and Audio Classifiers for\n  Automated News Video Segmentation","abstract":"  News videos require efficient content organisation and retrieval systems, but\ntheir unstructured nature poses significant challenges for automated\nprocessing. This paper presents a comprehensive comparative analysis of image,\nvideo, and audio classifiers for automated news video segmentation. This work\npresents the development and evaluation of multiple deep learning approaches,\nincluding ResNet, ViViT, AST, and multimodal architectures, to classify five\ndistinct segment types: advertisements, stories, studio scenes, transitions,\nand visualisations. Using a custom-annotated dataset of 41 news videos\ncomprising 1,832 scene clips, our experiments demonstrate that image-based\nclassifiers achieve superior performance (84.34\\% accuracy) compared to more\ncomplex temporal models. Notably, the ResNet architecture outperformed\nstate-of-the-art video classifiers while requiring significantly fewer\ncomputational resources. Binary classification models achieved high accuracy\nfor transitions (94.23\\%) and advertisements (92.74\\%). These findings advance\nthe understanding of effective architectures for news video segmentation and\nprovide practical insights for implementing automated content organisation\nsystems in media applications. These include media archiving, personalised\ncontent delivery, and intelligent video search.\n","date":"2025-03-27"}
{"id":"2503.21851","title":"On Large Multimodal Models as Open-World Image Classifiers","abstract":"  Traditional image classification requires a predefined list of semantic\ncategories. In contrast, Large Multimodal Models (LMMs) can sidestep this\nrequirement by classifying images directly using natural language (e.g.,\nanswering the prompt \"What is the main object in the image?\"). Despite this\nremarkable capability, most existing studies on LMM classification performance\nare surprisingly limited in scope, often assuming a closed-world setting with a\npredefined set of categories. In this work, we address this gap by thoroughly\nevaluating LMM classification performance in a truly open-world setting. We\nfirst formalize the task and introduce an evaluation protocol, defining various\nmetrics to assess the alignment between predicted and ground truth classes. We\nthen evaluate 13 models across 10 benchmarks, encompassing prototypical,\nnon-prototypical, fine-grained, and very fine-grained classes, demonstrating\nthe challenges LMMs face in this task. Further analyses based on the proposed\nmetrics reveal the types of errors LMMs make, highlighting challenges related\nto granularity and fine-grained capabilities, showing how tailored prompting\nand reasoning can alleviate them.\n","date":"2025-03-27"}
{"id":"2503.21854","title":"Foveated Instance Segmentation","abstract":"  Instance segmentation is essential for augmented reality and virtual reality\n(AR\/VR) as it enables precise object recognition and interaction, enhancing the\nintegration of virtual and real-world elements for an immersive experience.\nHowever, the high computational overhead of segmentation limits its application\non resource-constrained AR\/VR devices, causing large processing latency and\ndegrading user experience. In contrast to conventional scenarios, AR\/VR users\ntypically focus on only a few regions within their field of view before\nshifting perspective, allowing segmentation to be concentrated on gaze-specific\nareas. This insight drives the need for efficient segmentation methods that\nprioritize processing instance of interest, reducing computational load and\nenhancing real-time performance. In this paper, we present a foveated instance\nsegmentation (FovealSeg) framework that leverages real-time user gaze data to\nperform instance segmentation exclusively on instance of interest, resulting in\nsubstantial computational savings. Evaluation results show that FSNet achieves\nan IoU of 0.56 on ADE20K and 0.54 on LVIS, notably outperforming the baseline.\nThe code is available at https:\/\/github.com\/SAI-\n","date":"2025-03-27"}
{"id":"2503.21860","title":"ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via\n  Residual Learning","abstract":"  Human hands play a central role in interacting, motivating increasing\nresearch in dexterous robotic manipulation. Data-driven embodied AI algorithms\ndemand precise, large-scale, human-like manipulation sequences, which are\nchallenging to obtain with conventional reinforcement learning or real-world\nteleoperation. To address this, we introduce ManipTrans, a novel two-stage\nmethod for efficiently transferring human bimanual skills to dexterous robotic\nhands in simulation. ManipTrans first pre-trains a generalist trajectory\nimitator to mimic hand motion, then fine-tunes a specific residual module under\ninteraction constraints, enabling efficient learning and accurate execution of\ncomplex bimanual tasks. Experiments show that ManipTrans surpasses\nstate-of-the-art methods in success rate, fidelity, and efficiency. Leveraging\nManipTrans, we transfer multiple hand-object datasets to robotic hands,\ncreating DexManipNet, a large-scale dataset featuring previously unexplored\ntasks like pen capping and bottle unscrewing. DexManipNet comprises 3.3K\nepisodes of robotic manipulation and is easily extensible, facilitating further\npolicy training for dexterous hands and enabling real-world deployments.\n","date":"2025-03-27"}
{"id":"2503.21878","title":"Is Best-of-N the Best of Them? Coverage, Scaling, and Optimality in\n  Inference-Time Alignment","abstract":"  Inference-time computation provides an important axis for scaling language\nmodel performance, but naively scaling compute through techniques like\nBest-of-$N$ sampling can cause performance to degrade due to reward hacking.\nToward a theoretical understanding of how to best leverage additional\ncomputation, we focus on inference-time alignment which we formalize as the\nproblem of improving a pre-trained policy's responses for a prompt of interest,\ngiven access to an imperfect reward model. We analyze the performance of\ninference-time alignment algorithms in terms of (i) response quality, and (ii)\ncompute, and provide new results that highlight the importance of the\npre-trained policy's coverage over high-quality responses for performance and\ncompute scaling:\n  1. We show that Best-of-$N$ alignment with an ideal choice for $N$ can\nachieve optimal performance under stringent notions of coverage, but provably\nsuffers from reward hacking when $N$ is large, and fails to achieve tight\nguarantees under more realistic coverage conditions.\n  2. We introduce $\\texttt{InferenceTimePessimism}$, a new algorithm which\nmitigates reward hacking through deliberate use of inference-time compute,\nimplementing the principle of pessimism in the face of uncertainty via\nrejection sampling; we prove that its performance is optimal and does not\ndegrade with $N$, meaning it is scaling-monotonic.\n  We complement our theoretical results with an experimental evaluation that\ndemonstrate the benefits of $\\texttt{InferenceTimePessimism}$ across a variety\nof tasks and models.\n","date":"2025-03-27"}
{"id":"2503.21886","title":"Refined Geometry-guided Head Avatar Reconstruction from Monocular RGB\n  Video","abstract":"  High-fidelity reconstruction of head avatars from monocular videos is highly\ndesirable for virtual human applications, but it remains a challenge in the\nfields of computer graphics and computer vision. In this paper, we propose a\ntwo-phase head avatar reconstruction network that incorporates a refined 3D\nmesh representation. Our approach, in contrast to existing methods that rely on\ncoarse template-based 3D representations derived from 3DMM, aims to learn a\nrefined mesh representation suitable for a NeRF that captures complex facial\nnuances. In the first phase, we train 3DMM-stored NeRF with an initial mesh to\nutilize geometric priors and integrate observations across frames using a\nconsistent set of latent codes. In the second phase, we leverage a novel mesh\nrefinement procedure based on an SDF constructed from the density field of the\ninitial NeRF. To mitigate the typical noise in the NeRF density field without\ncompromising the features of the 3DMM, we employ Laplace smoothing on the\ndisplacement field. Subsequently, we apply a second-phase training with these\nrefined meshes, directing the learning process of the network towards capturing\nintricate facial details. Our experiments demonstrate that our method further\nenhances the NeRF rendering based on the initial mesh and achieves performance\nsuperior to state-of-the-art methods in reconstructing high-fidelity head\navatars with such input.\n","date":"2025-03-27"}
{"id":"2503.21888","title":"RedditESS: A Mental Health Social Support Interaction Dataset --\n  Understanding Effective Social Support to Refine AI-Driven Support Tools","abstract":"  Effective mental health support is crucial for alleviating psychological\ndistress. While large language model (LLM)-based assistants have shown promise\nin mental health interventions, existing research often defines \"effective\"\nsupport primarily in terms of empathetic acknowledgments, overlooking other\nessential dimensions such as informational guidance, community validation, and\ntangible coping strategies. To address this limitation and better understand\nwhat constitutes effective support, we introduce RedditESS, a novel real-world\ndataset derived from Reddit posts, including supportive comments and original\nposters' follow-up responses. Grounded in established social science theories,\nwe develop an ensemble labeling mechanism to annotate supportive comments as\neffective or not and perform qualitative assessments to ensure the reliability\nof the annotations. Additionally, we demonstrate the practical utility of\nRedditESS by using it to guide LLM alignment toward generating more\ncontext-sensitive and genuinely helpful supportive responses. By broadening the\nunderstanding of effective support, our study paves the way for advanced\nAI-driven mental health interventions.\n","date":"2025-03-27"}
{"id":"2503.21889","title":"StarFlow: Generating Structured Workflow Outputs From Sketch Images","abstract":"  Workflows are a fundamental component of automation in enterprise platforms,\nenabling the orchestration of tasks, data processing, and system integrations.\nDespite being widely used, building workflows can be complex, often requiring\nmanual configuration through low-code platforms or visual programming tools. To\nsimplify this process, we explore the use of generative foundation models,\nparticularly vision-language models (VLMs), to automatically generate\nstructured workflows from visual inputs. Translating hand-drawn sketches or\ncomputer-generated diagrams into executable workflows is challenging due to the\nambiguity of free-form drawings, variations in diagram styles, and the\ndifficulty of inferring execution logic from visual elements. To address this,\nwe introduce StarFlow, a framework for generating structured workflow outputs\nfrom sketches using vision-language models. We curate a diverse dataset of\nworkflow diagrams -- including synthetic, manually annotated, and real-world\nsamples -- to enable robust training and evaluation. We finetune and benchmark\nmultiple vision-language models, conducting a series of ablation studies to\nanalyze the strengths and limitations of our approach. Our results show that\nfinetuning significantly enhances structured workflow generation, outperforming\nlarge vision-language models on this task.\n","date":"2025-03-27"}
{"id":"2503.21893","title":"Exponentially Weighted Instance-Aware Repeat Factor Sampling for\n  Long-Tailed Object Detection Model Training in Unmanned Aerial Vehicles\n  Surveillance Scenarios","abstract":"  Object detection models often struggle with class imbalance, where rare\ncategories appear significantly less frequently than common ones. Existing\nsampling-based rebalancing strategies, such as Repeat Factor Sampling (RFS) and\nInstance-Aware Repeat Factor Sampling (IRFS), mitigate this issue by adjusting\nsample frequencies based on image and instance counts. However, these methods\nare based on linear adjustments, which limit their effectiveness in long-tailed\ndistributions. This work introduces Exponentially Weighted Instance-Aware\nRepeat Factor Sampling (E-IRFS), an extension of IRFS that applies exponential\nscaling to better differentiate between rare and frequent classes. E-IRFS\nadjusts sampling probabilities using an exponential function applied to the\ngeometric mean of image and instance frequencies, ensuring a more adaptive\nrebalancing strategy. We evaluate E-IRFS on a dataset derived from the\nFireman-UAV-RGBT Dataset and four additional public datasets, using YOLOv11\nobject detection models to identify fire, smoke, people and lakes in emergency\nscenarios. The results show that E-IRFS improves detection performance by 22\\%\nover the baseline and outperforms RFS and IRFS, particularly for rare\ncategories. The analysis also highlights that E-IRFS has a stronger effect on\nlightweight models with limited capacity, as these models rely more on data\nsampling strategies to address class imbalance. The findings demonstrate that\nE-IRFS improves rare object detection in resource-constrained environments,\nmaking it a suitable solution for real-time applications such as UAV-based\nemergency monitoring.\n","date":"2025-03-27"}
{"id":"2503.21902","title":"OntoAligner: A Comprehensive Modular and Robust Python Toolkit for\n  Ontology Alignment","abstract":"  Ontology Alignment (OA) is fundamental for achieving semantic\ninteroperability across diverse knowledge systems. We present OntoAligner, a\ncomprehensive, modular, and robust Python toolkit for ontology alignment,\ndesigned to address current limitations with existing tools faced by\npractitioners. Existing tools are limited in scalability, modularity, and ease\nof integration with recent AI advances. OntoAligner provides a flexible\narchitecture integrating existing lightweight OA techniques such as fuzzy\nmatching but goes beyond by supporting contemporary methods with\nretrieval-augmented generation and large language models for OA. The framework\nprioritizes extensibility, enabling researchers to integrate custom alignment\nalgorithms and datasets. This paper details the design principles,\narchitecture, and implementation of the OntoAligner, demonstrating its utility\nthrough benchmarks on standard OA tasks. Our evaluation highlights\nOntoAligner's ability to handle large-scale ontologies efficiently with few\nlines of code while delivering high alignment quality. By making OntoAligner\nopen-source, we aim to provide a resource that fosters innovation and\ncollaboration within the OA community, empowering researchers and practitioners\nwith a toolkit for reproducible OA research and real-world applications.\n","date":"2025-03-27"}
{"id":"2503.21904","title":"AssistPDA: An Online Video Surveillance Assistant for Video Anomaly\n  Prediction, Detection, and Analysis","abstract":"  The rapid advancements in large language models (LLMs) have spurred growing\ninterest in LLM-based video anomaly detection (VAD). However, existing\napproaches predominantly focus on video-level anomaly question answering or\noffline detection, ignoring the real-time nature essential for practical VAD\napplications. To bridge this gap and facilitate the practical deployment of\nLLM-based VAD, we introduce AssistPDA, the first online video anomaly\nsurveillance assistant that unifies video anomaly prediction, detection, and\nanalysis (VAPDA) within a single framework. AssistPDA enables real-time\ninference on streaming videos while supporting interactive user engagement.\nNotably, we introduce a novel event-level anomaly prediction task, enabling\nproactive anomaly forecasting before anomalies fully unfold. To enhance the\nability to model intricate spatiotemporal relationships in anomaly events, we\npropose a Spatio-Temporal Relation Distillation (STRD) module. STRD transfers\nthe long-term spatiotemporal modeling capabilities of vision-language models\n(VLMs) from offline settings to real-time scenarios. Thus it equips AssistPDA\nwith a robust understanding of complex temporal dependencies and long-sequence\nmemory. Additionally, we construct VAPDA-127K, the first large-scale benchmark\ndesigned for VLM-based online VAPDA. Extensive experiments demonstrate that\nAssistPDA outperforms existing offline VLM-based approaches, setting a new\nstate-of-the-art for real-time VAPDA. Our dataset and code will be open-sourced\nto facilitate further research in the community.\n","date":"2025-03-27"}
{"id":"2503.21907","title":"KernelFusion: Assumption-Free Blind Super-Resolution via Patch Diffusion","abstract":"  Traditional super-resolution (SR) methods assume an ``ideal'' downscaling\nSR-kernel (e.g., bicubic downscaling) between the high-resolution (HR) image\nand the low-resolution (LR) image. Such methods fail once the LR images are\ngenerated differently. Current blind-SR methods aim to remove this assumption,\nbut are still fundamentally restricted to rather simplistic downscaling\nSR-kernels (e.g., anisotropic Gaussian kernels), and fail on more complex (out\nof distribution) downscaling degradations. However, using the correct SR-kernel\nis often more important than using a sophisticated SR algorithm. In\n``KernelFusion'' we introduce a zero-shot diffusion-based method that makes no\nassumptions about the kernel. Our method recovers the unique image-specific\nSR-kernel directly from the LR input image, while simultaneously recovering its\ncorresponding HR image. KernelFusion exploits the principle that the correct\nSR-kernel is the one that maximizes patch similarity across different scales of\nthe LR image. We first train an image-specific patch-based diffusion model on\nthe single LR input image, capturing its unique internal patch statistics. We\nthen reconstruct a larger HR image with the same learned patch distribution,\nwhile simultaneously recovering the correct downscaling SR-kernel that\nmaintains this cross-scale relation between the HR and LR images. Empirical\nresults show that KernelFusion vastly outperforms all SR baselines on complex\ndownscaling degradations, where existing SotA Blind-SR methods fail miserably.\nBy breaking free from predefined kernel assumptions, KernelFusion pushes\nBlind-SR into a new assumption-free paradigm, handling downscaling kernels\npreviously thought impossible.\n","date":"2025-03-27"}
{"id":"2503.21910","title":"JEEM: Vision-Language Understanding in Four Arabic Dialects","abstract":"  We introduce JEEM, a benchmark designed to evaluate Vision-Language Models\n(VLMs) on visual understanding across four Arabic-speaking countries: Jordan,\nThe Emirates, Egypt, and Morocco. JEEM includes the tasks of image captioning\nand visual question answering, and features culturally rich and regionally\ndiverse content. This dataset aims to assess the ability of VLMs to generalize\nacross dialects and accurately interpret cultural elements in visual contexts.\nIn an evaluation of five prominent open-source Arabic VLMs and GPT-4V, we find\nthat the Arabic VLMs consistently underperform, struggling with both visual\nunderstanding and dialect-specific generation. While GPT-4V ranks best in this\ncomparison, the model's linguistic competence varies across dialects, and its\nvisual understanding capabilities lag behind. This underscores the need for\nmore inclusive models and the value of culturally-diverse evaluation paradigms.\n","date":"2025-03-27"}
{"id":"2503.21911","title":"AutoPsyC: Automatic Recognition of Psychodynamic Conflicts from\n  Semi-structured Interviews with Large Language Models","abstract":"  Psychodynamic conflicts are persistent, often unconscious themes that shape a\nperson's behaviour and experiences. Accurate diagnosis of psychodynamic\nconflicts is crucial for effective patient treatment and is commonly done via\nlong, manually scored semi-structured interviews. Existing automated solutions\nfor psychiatric diagnosis tend to focus on the recognition of broad disorder\ncategories such as depression, and it is unclear to what extent psychodynamic\nconflicts which even the patient themselves may not have conscious access to\ncould be automatically recognised from conversation. In this paper, we propose\nAutoPsyC, the first method for recognising the presence and significance of\npsychodynamic conflicts from full-length Operationalized Psychodynamic\nDiagnostics (OPD) interviews using Large Language Models (LLMs). Our approach\ncombines recent advances in parameter-efficient fine-tuning and\nRetrieval-Augmented Generation (RAG) with a summarisation strategy to\neffectively process entire 90 minute long conversations. In evaluations on a\ndataset of 141 diagnostic interviews we show that AutoPsyC consistently\noutperforms all baselines and ablation conditions on the recognition of four\nhighly relevant psychodynamic conflicts.\n","date":"2025-03-27"}
{"id":"2503.21914","title":"PyUAT: Open-source Python framework for efficient and scalable cell\n  tracking","abstract":"  Tracking individual cells in live-cell imaging provides fundamental insights,\ninevitable for studying causes and consequences of phenotypic heterogeneity,\nresponses to changing environmental conditions or stressors. Microbial cell\ntracking, characterized by stochastic cell movements and frequent cell\ndivisions, remains a challenging task when imaging frame rates must be limited\nto avoid counterfactual results. A promising way to overcome this limitation is\nuncertainty-aware tracking (UAT), which uses statistical models, calibrated to\nempirically observed cell behavior, to predict likely cell associations. We\npresent PyUAT, an efficient and modular Python implementation of UAT for\ntracking microbial cells in time-lapse imaging. We demonstrate its performance\non a large 2D+t data set and investigate the influence of modular biological\nmodels and imaging intervals on the tracking performance. The open-source PyUAT\nsoftware is available at https:\/\/github.com\/JuBiotech\/PyUAT, including example\nnotebooks for immediate use in Google Colab.\n","date":"2025-03-27"}
{"id":"2503.21927","title":"Hybrid Emotion Recognition: Enhancing Customer Interactions Through\n  Acoustic and Textual Analysis","abstract":"  This research presents a hybrid emotion recognition system integrating\nadvanced Deep Learning, Natural Language Processing (NLP), and Large Language\nModels (LLMs) to analyze audio and textual data for enhancing customer\ninteractions in contact centers. By combining acoustic features with textual\nsentiment analysis, the system achieves nuanced emotion detection, addressing\nthe limitations of traditional approaches in understanding complex emotional\nstates. Leveraging LSTM and CNN models for audio analysis and DistilBERT for\ntextual evaluation, the methodology accommodates linguistic and cultural\nvariations while ensuring real-time processing. Rigorous testing on diverse\ndatasets demonstrates the system's robustness and accuracy, highlighting its\npotential to transform customer service by enabling personalized, empathetic\ninteractions and improving operational efficiency. This research establishes a\nfoundation for more intelligent and human-centric digital communication,\nredefining customer service standards.\n","date":"2025-03-27"}
{"id":"2503.21928","title":"An Efficient Training Algorithm for Models with Block-wise Sparsity","abstract":"  Large-scale machine learning (ML) models are increasingly being used in\ncritical domains like education, lending, recruitment, healthcare, criminal\njustice, etc. However, the training, deployment, and utilization of these\nmodels demand substantial computational resources. To decrease computation and\nmemory costs, machine learning models with sparse weight matrices are widely\nused in the literature. Among sparse models, those with special sparse\nstructures (e.g., models with block-wise sparse weight matrices) fit better\nwith the hardware accelerators and can decrease the memory and computation\ncosts during the inference. Unfortunately, while there are several efficient\ntraining methods, none of them are designed to train a block-wise sparse model\nefficiently. As a result, the current methods for training block-wise sparse\nmodels start with full and dense models leading to inefficient training. In\nthis work, we focus on training models with \\textit{block-wise sparse matrices}\nand propose an efficient training algorithm to decrease both computation and\nmemory costs during training and inference. In addition, we will show that our\nproposed method enables us to efficiently find the right block size for the\nsparsity pattern during the training process. Our extensive empirical and\ntheoretical analyses show that our algorithms can decrease the computation and\nmemory costs significantly without a performance drop compared to baselines.\n","date":"2025-03-27"}
{"id":"2503.21929","title":"Local Normalization Distortion and the Thermodynamic Formalism of\n  Decoding Strategies for Large Language Models","abstract":"  Advances in hardware and language model architecture have spurred a\nrevolution in natural language generation. However, autoregressive models\ncompute probability distributions over next-token choices, and sampling from\nthese distributions, known as decoding, has received significantly less\nattention than other design choices. Existing decoding strategies are largely\nbased on heuristics, resulting in methods that are hard to apply or improve in\na principled manner. We develop the theory of decoding strategies for language\nmodels by expressing popular decoding algorithms as equilibrium states in the\nlanguage of ergodic theory and stating the functions they optimize. Using this,\nwe analyze the effect of the local normalization step of top-k, nucleus, and\ntemperature sampling, used to make probabilities sum to one. We argue that\nlocal normalization distortion is a fundamental defect of decoding strategies\nand quantify the size of this distortion and its effect on mathematical proxies\nfor the quality and diversity of generated text. Contrary to the prevailing\nexplanation, we argue that the major cause of the under-performance of top-k\nsampling relative to nucleus sampling is local normalization distortion. This\nyields conclusions for the future design of decoding algorithms and the\ndetection of machine-generated text.\n","date":"2025-03-27"}
{"id":"2503.21931","title":"Locally Orderless Images for Optimization in Differentiable Rendering","abstract":"  Problems in differentiable rendering often involve optimizing scene\nparameters that cause motion in image space. The gradients for such parameters\ntend to be sparse, leading to poor convergence. While existing methods address\nthis sparsity through proxy gradients such as topological derivatives or\nlagrangian derivatives, they make simplifying assumptions about rendering.\nMulti-resolution image pyramids offer an alternative approach but prove\nunreliable in practice. We introduce a method that uses locally orderless\nimages, where each pixel maps to a histogram of intensities that preserves\nlocal variations in appearance. Using an inverse rendering objective that\nminimizes histogram distance, our method extends support for sparsely defined\nimage gradients and recovers optimal parameters. We validate our method on\nvarious inverse problems using both synthetic and real data.\n","date":"2025-03-27"}
{"id":"2503.21932","title":"Multimodal Data Integration for Sustainable Indoor Gardening: Tracking\n  Anyplant with Time Series Foundation Model","abstract":"  Indoor gardening within sustainable buildings offers a transformative\nsolution to urban food security and environmental sustainability. By 2030,\nurban farming, including Controlled Environment Agriculture (CEA) and vertical\nfarming, is expected to grow at a compound annual growth rate (CAGR) of 13.2%\nfrom 2024 to 2030, according to market reports. This growth is fueled by\nadvancements in Internet of Things (IoT) technologies, sustainable innovations\nsuch as smart growing systems, and the rising interest in green interior\ndesign. This paper presents a novel framework that integrates computer vision,\nmachine learning (ML), and environmental sensing for the automated monitoring\nof plant health and growth. Unlike previous approaches, this framework combines\nRGB imagery, plant phenotyping data, and environmental factors such as\ntemperature and humidity, to predict plant water stress in a controlled growth\nenvironment. The system utilizes high-resolution cameras to extract phenotypic\nfeatures, such as RGB, plant area, height, and width while employing the\nLag-Llama time series model to analyze and predict water stress. Experimental\nresults demonstrate that integrating RGB, size ratios, and environmental data\nsignificantly enhances predictive accuracy, with the Fine-tuned model achieving\nthe lowest errors (MSE = 0.420777, MAE = 0.595428) and reduced uncertainty.\nThese findings highlight the potential of multimodal data and intelligent\nsystems to automate plant care, optimize resource consumption, and align indoor\ngardening with sustainable building management practices, paving the way for\nresilient, green urban spaces.\n","date":"2025-03-27"}
{"id":"2503.21934","title":"Proof or Bluff? Evaluating LLMs on 2025 USA Math Olympiad","abstract":"  Recent math benchmarks for large language models (LLMs) such as MathArena\nindicate that state-of-the-art reasoning models achieve impressive performance\non mathematical competitions like AIME, with the leading model, o3-mini,\nachieving scores comparable to top human competitors. However, these benchmarks\nevaluate models solely based on final numerical answers, neglecting rigorous\nreasoning and proof generation which are essential for real-world mathematical\ntasks. To address this, we introduce the first comprehensive evaluation of\nfull-solution reasoning for challenging mathematical problems. Using expert\nhuman annotators, we evaluated several state-of-the-art reasoning models on the\nsix problems from the 2025 USAMO within hours of their release. Our results\nreveal that all tested models struggled significantly, achieving less than 5%\non average. Through detailed analysis of reasoning traces, we identify the most\ncommon failure modes and find several unwanted artifacts arising from the\noptimization strategies employed during model training. Overall, our results\nsuggest that current LLMs are inadequate for rigorous mathematical reasoning\ntasks, highlighting the need for substantial improvements in reasoning and\nproof generation capabilities.\n","date":"2025-03-27"}
{"id":"2503.21937","title":"Lobster: A GPU-Accelerated Framework for Neurosymbolic Programming","abstract":"  Neurosymbolic programs combine deep learning with symbolic reasoning to\nachieve better data efficiency, interpretability, and generalizability compared\nto standalone deep learning approaches. However, existing neurosymbolic\nlearning frameworks implement an uneasy marriage between a highly scalable,\nGPU-accelerated neural component with a slower symbolic component that runs on\nCPUs. We propose Lobster, a unified framework for harnessing GPUs in an\nend-to-end manner for neurosymbolic learning. Lobster maps a general\nneurosymbolic language based on Datalog to the GPU programming paradigm. This\nmapping is implemented via compilation to a new intermediate language called\nAPM. The extra abstraction provided by APM allows Lobster to be both flexible,\nsupporting discrete, probabilistic, and differentiable modes of reasoning on\nGPU hardware with a library of provenance semirings, and performant,\nimplementing new optimization passes. We demonstrate that Lobster programs can\nsolve interesting problems spanning the domains of natural language processing,\nimage processing, program reasoning, bioinformatics, and planning. On a suite\nof 8 applications, Lobster achieves an average speedup of 5.3x over Scallop, a\nstate-of-the-art neurosymbolic framework, and enables scaling of neurosymbolic\nsolutions to previously infeasible tasks.\n","date":"2025-03-27"}
{"id":"2503.21939","title":"Flexible Moment-Invariant Bases from Irreducible Tensors","abstract":"  Moment invariants are a powerful tool for the generation of\nrotation-invariant descriptors needed for many applications in pattern\ndetection, classification, and machine learning. A set of invariants is optimal\nif it is complete, independent, and robust against degeneracy in the input. In\nthis paper, we show that the current state of the art for the generation of\nthese bases of moment invariants, despite being robust against moment tensors\nbeing identically zero, is vulnerable to a degeneracy that is common in\nreal-world applications, namely spherical functions. We show how to overcome\nthis vulnerability by combining two popular moment invariant approaches: one\nbased on spherical harmonics and one based on Cartesian tensor algebra.\n","date":"2025-03-27"}
{"id":"2503.21943","title":"Parametric Shadow Control for Portrait Generationin Text-to-Image\n  Diffusion Models","abstract":"  Text-to-image diffusion models excel at generating diverse portraits, but\nlack intuitive shadow control. Existing editing approaches, as post-processing,\nstruggle to offer effective manipulation across diverse styles. Additionally,\nthese methods either rely on expensive real-world light-stage data collection\nor require extensive computational resources for training. To address these\nlimitations, we introduce Shadow Director, a method that extracts and\nmanipulates hidden shadow attributes within well-trained diffusion models. Our\napproach uses a small estimation network that requires only a few thousand\nsynthetic images and hours of training-no costly real-world light-stage data\nneeded. Shadow Director enables parametric and intuitive control over shadow\nshape, placement, and intensity during portrait generation while preserving\nartistic integrity and identity across diverse styles. Despite training only on\nsynthetic data built on real-world identities, it generalizes effectively to\ngenerated portraits with diverse styles, making it a more accessible and\nresource-friendly solution.\n","date":"2025-03-27"}
{"id":"2503.21949","title":"Reward Design for Reinforcement Learning Agents","abstract":"  Reward functions are central in reinforcement learning (RL), guiding agents\ntowards optimal decision-making. The complexity of RL tasks requires\nmeticulously designed reward functions that effectively drive learning while\navoiding unintended consequences. Effective reward design aims to provide\nsignals that accelerate the agent's convergence to optimal behavior. Crafting\nrewards that align with task objectives, foster desired behaviors, and prevent\nundesirable actions is inherently challenging. This thesis delves into the\ncritical role of reward signals in RL, highlighting their impact on the agent's\nbehavior and learning dynamics and addressing challenges such as delayed,\nambiguous, or intricate rewards. In this thesis work, we tackle different\naspects of reward shaping. First, we address the problem of designing\ninformative and interpretable reward signals from a teacher's\/expert's\nperspective (teacher-driven). Here, the expert, equipped with the optimal\npolicy and the corresponding value function, designs reward signals that\nexpedite the agent's convergence to optimal behavior. Second, we build on this\nteacher-driven approach by introducing a novel method for adaptive\ninterpretable reward design. In this scenario, the expert tailors the rewards\nbased on the learner's current policy, ensuring alignment and optimal\nprogression. Third, we propose a meta-learning approach, enabling the agent to\nself-design its reward signals online without expert input (agent-driven). This\nself-driven method considers the agent's learning and exploration to establish\na self-improving feedback loop.\n","date":"2025-03-27"}
{"id":"2503.21955","title":"Comprehensive segmentation of deep grey nuclei from structural MRI data","abstract":"  Motivation: Lack of tools for comprehensive and complete segmentation of deep\ngrey nuclei using a single software for reproducibility and repeatability\nGoal(s): A fast accurate and robust method for segmentation of deep grey nuclei\n(thalamic nuclei, basal ganglia, claustrum, red nucleus) from structural T1 MRI\ndata at conventional field strengths Approach: We leverage the improved\ncontrast of white-matter-nulled imaging by using the recently proposed\nHistogram-based Polynomial Synthesis (HIPS) to synthesize WMn-like images from\nstandard T1 and then use a multi-atlas segmentation with joint label fusion to\nsegment deep grey nuclei. Results: The method worked robustly on all field\nstrengths (1.5\/3\/7) and Dice coefficients of 0.7 or more were achieved for all\nstructures compared against manual segmentation ground truth. Impact: This\nmethod facilitates careful investigation of the role of deep grey nuclei by\nenabling the use of conventional T1 data from large public databases, which has\nnot been possible, hitherto, due to lack of robust reproducible segmentation\ntools.\n","date":"2025-03-27"}
{"id":"2503.21956","title":"Enhancing Pavement Crack Classification with Bidirectional Cascaded\n  Neural Networks","abstract":"  Pavement distress, such as cracks and potholes, is a significant issue\naffecting road safety and maintenance. In this study, we present the\nimplementation and evaluation of Bidirectional Cascaded Neural Networks (BCNNs)\nfor the classification of pavement crack images following image augmentation.\nWe classified pavement cracks into three main categories: linear cracks,\npotholes, and fatigue cracks on an enhanced dataset utilizing U-Net 50 for\nimage augmentation. The augmented dataset comprised 599 images. Our proposed\nBCNN model was designed to leverage both forward and backward information\nflows, with detection accuracy enhanced by its cascaded structure wherein each\nlayer progressively refines the output of the preceding one. Our model achieved\nan overall accuracy of 87%, with precision, recall, and F1-score measures\nindicating high effectiveness across the categories. For fatigue cracks, the\nmodel recorded a precision of 0.87, recall of 0.83, and F1-score of 0.85 on 205\nimages. Linear cracks were detected with a precision of 0.81, recall of 0.89,\nand F1-score of 0.85 on 205 images, and potholes with a precision of 0.96,\nrecall of 0.90, and F1-score of 0.93 on 189 images. The macro and weighted\naverage of precision, recall, and F1-score were identical at 0.88, confirming\nthe BCNN's excellent performance in classifying complex pavement crack\npatterns. This research demonstrates the potential of BCNNs to significantly\nenhance the accuracy and reliability of pavement distress classification,\nresulting in more effective and efficient pavement maintenance and management\nsystems.\n","date":"2025-03-27"}
{"id":"2503.21958","title":"NeRF-based Point Cloud Reconstruction using a Stationary Camera for\n  Agricultural Applications","abstract":"  This paper presents a NeRF-based framework for point cloud (PCD)\nreconstruction, specifically designed for indoor high-throughput plant\nphenotyping facilities. Traditional NeRF-based reconstruction methods require\ncameras to move around stationary objects, but this approach is impractical for\nhigh-throughput environments where objects are rapidly imaged while moving on\nconveyors or rotating pedestals. To address this limitation, we develop a\nvariant of NeRF-based PCD reconstruction that uses a single stationary camera\nto capture images as the object rotates on a pedestal. Our workflow comprises\nCOLMAP-based pose estimation, a straightforward pose transformation to simulate\ncamera movement, and subsequent standard NeRF training. A defined Region of\nInterest (ROI) excludes irrelevant scene data, enabling the generation of\nhigh-resolution point clouds (10M points). Experimental results demonstrate\nexcellent reconstruction fidelity, with precision-recall analyses yielding an\nF-score close to 100.00 across all evaluated plant objects. Although pose\nestimation remains computationally intensive with a stationary camera setup,\noverall training and reconstruction times are competitive, validating the\nmethod's feasibility for practical high-throughput indoor phenotyping\napplications. Our findings indicate that high-quality NeRF-based 3D\nreconstructions are achievable using a stationary camera, eliminating the need\nfor complex camera motion or costly imaging equipment. This approach is\nespecially beneficial when employing expensive and delicate instruments, such\nas hyperspectral cameras, for 3D plant phenotyping. Future work will focus on\noptimizing pose estimation techniques and further streamlining the methodology\nto facilitate seamless integration into automated, high-throughput 3D\nphenotyping pipelines.\n","date":"2025-03-27"}
{"id":"2503.21961","title":"Entropy-Aware Branching for Improved Mathematical Reasoning","abstract":"  While Large Language Models (LLMs) are effectively aligned through extensive\npre-training and fine-tuning, they still struggle with varying levels of\nuncertainty during token generation. In our investigation of mathematical\nreasoning, we observe that errors are more likely to arise at tokens exhibiting\nhigh entropy and variance of entropy in the model's output distribution. Based\non the observation, we propose a novel approach that dynamically branches the\ngeneration process on demand instead of defaulting to the single most probable\ntoken. By exploring in parallel multiple branches stemming from high\nprobability tokens of critical decision points, the model can discover diverse\nreasoning paths that might otherwise be missed. We further harness external\nfeedback from larger models to rank and select the most coherent and accurate\nreasoning branch. Our experimental results on mathematical word problems and\ncalculation questions show that this branching strategy boosts the reasoning\ncapabilities of small LLMs up to 4.6% compared to conventional argmax decoding.\n","date":"2025-03-27"}
{"id":"2503.21964","title":"NeuroLIP: Interpretable and Fair Cross-Modal Alignment of fMRI and\n  Phenotypic Text","abstract":"  Integrating functional magnetic resonance imaging (fMRI) connectivity data\nwith phenotypic textual descriptors (e.g., disease label, demographic data)\nholds significant potential to advance our understanding of neurological\nconditions. However, existing cross-modal alignment methods often lack\ninterpretability and risk introducing biases by encoding sensitive attributes\ntogether with diagnostic-related features. In this work, we propose NeuroLIP, a\nnovel cross-modal contrastive learning framework. We introduce text\ntoken-conditioned attention (TTCA) and cross-modal alignment via localized\ntokens (CALT) to the brain region-level embeddings with each disease-related\nphenotypic token. It improves interpretability via token-level attention maps,\nrevealing brain region-disease associations. To mitigate bias, we propose a\nloss for sensitive attribute disentanglement that maximizes the attention\ndistance between disease tokens and sensitive attribute tokens, reducing\nunintended correlations in downstream predictions. Additionally, we incorporate\na negative gradient technique that reverses the sign of CALT loss on sensitive\nattributes, further discouraging the alignment of these features. Experiments\non neuroimaging datasets (ABIDE and ADHD-200) demonstrate NeuroLIP's\nsuperiority in terms of fairness metrics while maintaining the overall best\nstandard metric performance. Qualitative visualization of attention maps\nhighlights neuroanatomical patterns aligned with diagnostic characteristics,\nvalidated by the neuroscientific literature. Our work advances the development\nof transparent and equitable neuroimaging AI.\n","date":"2025-03-27"}
{"id":"2503.21969","title":"Data-Agnostic Robotic Long-Horizon Manipulation with\n  Vision-Language-Guided Closed-Loop Feedback","abstract":"  Recent advances in language-conditioned robotic manipulation have leveraged\nimitation and reinforcement learning to enable robots to execute tasks from\nhuman commands. However, these methods often suffer from limited\ngeneralization, adaptability, and the lack of large-scale specialized datasets,\nunlike data-rich domains such as computer vision, making long-horizon task\nexecution challenging. To address these gaps, we introduce DAHLIA, a\ndata-agnostic framework for language-conditioned long-horizon robotic\nmanipulation, leveraging large language models (LLMs) for real-time task\nplanning and execution. DAHLIA employs a dual-tunnel architecture, where an\nLLM-powered planner collaborates with co-planners to decompose tasks and\ngenerate executable plans, while a reporter LLM provides closed-loop feedback,\nenabling adaptive re-planning and ensuring task recovery from potential\nfailures. Moreover, DAHLIA integrates chain-of-thought (CoT) in task reasoning\nand temporal abstraction for efficient action execution, enhancing traceability\nand robustness. Our framework demonstrates state-of-the-art performance across\ndiverse long-horizon tasks, achieving strong generalization in both simulated\nand real-world scenarios. Videos and code are available at\nhttps:\/\/ghiara.github.io\/DAHLIA\/.\n","date":"2025-03-27"}
{"id":"2503.21970","title":"Q-MambaIR: Accurate Quantized Mamba for Efficient Image Restoration","abstract":"  State-Space Models (SSMs) have attracted considerable attention in Image\nRestoration (IR) due to their ability to scale linearly sequence length while\neffectively capturing long-distance dependencies. However, deploying SSMs to\nedge devices is challenging due to the constraints in memory, computing\ncapacity, and power consumption, underscoring the need for efficient\ncompression strategies. While low-bit quantization is an efficient model\ncompression strategy for reducing size and accelerating IR tasks, SSM suffers\nsubstantial performance drops at ultra-low bit-widths (2-4 bits), primarily due\nto outliers that exacerbate quantization error. To address this challenge, we\npropose Q-MambaIR, an accurate, efficient, and flexible Quantized Mamba for IR\ntasks. Specifically, we introduce a Statistical Dynamic-balancing Learnable\nScalar (DLS) to dynamically adjust the quantization mapping range, thereby\nmitigating the peak truncation loss caused by extreme values. Furthermore, we\ndesign a Range-floating Flexible Allocator (RFA) with an adaptive threshold to\nflexibly round values. This approach preserves high-frequency details and\nmaintains the SSM's feature extraction capability. Notably, RFA also enables\npre-deployment weight quantization, striking a balance between computational\nefficiency and model accuracy. Extensive experiments on IR tasks demonstrate\nthat Q-MambaIR consistently outperforms existing quantized SSMs, achieving much\nhigher state-of-the-art (SOTA) accuracy results with only a negligible increase\nin training computation and storage saving.\n","date":"2025-03-27"}
{"id":"2503.21971","title":"RocketPPA: Ultra-Fast LLM-Based PPA Estimator at Code-Level Abstraction","abstract":"  Large language models have recently transformed hardware design, yet bridging\nthe gap between code synthesis and PPA (power, performance, and area)\nestimation remains a challenge. In this work, we introduce a novel framework\nthat leverages a 21k dataset of thoroughly cleaned and synthesizable Verilog\nmodules, each annotated with detailed power, delay, and area metrics. By\nemploying chain-of-thought techniques, we automatically debug and curate this\ndataset to ensure high fidelity in downstream applications. We then fine-tune\nCodeLlama using LoRA-based parameter-efficient methods, framing the task as a\nregression problem to accurately predict PPA metrics from Verilog code.\nFurthermore, we augment our approach with a mixture-of-experts\narchitecture-integrating both LoRA and an additional MLP expert layer-to\nfurther refine predictions. Experimental results demonstrate significant\nimprovements: power estimation accuracy is enhanced by 5.9% at a 20% error\nthreshold and by 7.2% at a 10% threshold, delay estimation improves by 5.1% and\n3.9%, and area estimation sees gains of 4% and 7.9% for the 20% and 10%\nthresholds, respectively. Notably, the incorporation of the mixture-of-experts\nmodule contributes an additional 3--4% improvement across these tasks. Our\nresults establish a new benchmark for PPA-aware Verilog generation,\nhighlighting the effectiveness of our integrated dataset and modeling\nstrategies for next-generation EDA workflows.\n","date":"2025-03-27"}
{"id":"2503.21975","title":"Pretrained Bayesian Non-parametric Knowledge Prior in Robotic\n  Long-Horizon Reinforcement Learning","abstract":"  Reinforcement learning (RL) methods typically learn new tasks from scratch,\noften disregarding prior knowledge that could accelerate the learning process.\nWhile some methods incorporate previously learned skills, they usually rely on\na fixed structure, such as a single Gaussian distribution, to define skill\npriors. This rigid assumption can restrict the diversity and flexibility of\nskills, particularly in complex, long-horizon tasks. In this work, we introduce\na method that models potential primitive skill motions as having non-parametric\nproperties with an unknown number of underlying features. We utilize a Bayesian\nnon-parametric model, specifically Dirichlet Process Mixtures, enhanced with\nbirth and merge heuristics, to pre-train a skill prior that effectively\ncaptures the diverse nature of skills. Additionally, the learned skills are\nexplicitly trackable within the prior space, enhancing interpretability and\ncontrol. By integrating this flexible skill prior into an RL framework, our\napproach surpasses existing methods in long-horizon manipulation tasks,\nenabling more efficient skill transfer and task success in complex\nenvironments. Our findings show that a richer, non-parametric representation of\nskill priors significantly improves both the learning and execution of\nchallenging robotic tasks. All data, code, and videos are available at\nhttps:\/\/ghiara.github.io\/HELIOS\/.\n","date":"2025-03-27"}
{"id":"2503.21979","title":"Harmonizing Visual Representations for Unified Multimodal Understanding\n  and Generation","abstract":"  Unifying visual understanding and generation within a single multimodal\nframework remains a significant challenge, as the two inherently heterogeneous\ntasks require representations at different levels of granularity. Current\napproaches that utilize vector quantization (VQ) or variational autoencoders\n(VAE) for unified visual representation prioritize intrinsic imagery features\nover semantics, compromising understanding performance. In this work, we take\ninspiration from masked image modelling (MIM) that learns rich semantics via a\nmask-and-reconstruct pre-training and its successful extension to masked\nautoregressive (MAR) image generation. A preliminary study on the MAR encoder's\nrepresentation reveals exceptional linear probing accuracy and precise feature\nresponse to visual concepts, which indicates MAR's potential for visual\nunderstanding tasks beyond its original generation role. Based on these\ninsights, we present \\emph{Harmon}, a unified autoregressive framework that\nharmonizes understanding and generation tasks with a shared MAR encoder.\nThrough a three-stage training procedure that progressively optimizes\nunderstanding and generation capabilities, Harmon achieves state-of-the-art\nimage generation results on the GenEval, MJHQ30K and WISE benchmarks while\nmatching the performance of methods with dedicated semantic encoders (e.g.,\nJanus) on image understanding benchmarks. Our code and models will be available\nat https:\/\/github.com\/wusize\/Harmon.\n","date":"2025-03-27"}
{"id":"2503.21980","title":"Rolled Gaussian process models for curves on manifolds","abstract":"  Given a planar curve, imagine rolling a sphere along that curve without\nslipping or twisting, and by this means tracing out a curve on the sphere. It\nis well known that such a rolling operation induces a local isometry between\nthe sphere and the plane so that the two curves uniquely determine each other,\nand moreover, the operation extends to a general class of manifolds in any\ndimension. We use rolling to construct an analogue of a Gaussian process on a\nmanifold starting from a Euclidean Gaussian process. The resulting model is\ngenerative, and is amenable to statistical inference given data as curves on a\nmanifold. We illustrate with examples on the unit sphere, symmetric\npositive-definite matrices, and with a robotics application involving 3D\norientations.\n","date":"2025-03-27"}
{"id":"2503.21981","title":"An Artificial Trend Index for Private Consumption Using Google Trends","abstract":"  In recent years, the use of databases that analyze trends, sentiments or news\nto make economic projections or create indicators has gained significant\npopularity, particularly with the Google Trends platform. This article explores\nthe potential of Google search data to develop a new index that improves\neconomic forecasts, with a particular focus on one of the key components of\neconomic activity: private consumption (64\\% of GDP in Peru). By selecting and\nestimating categorized variables, machine learning techniques are applied,\ndemonstrating that Google data can identify patterns to generate a leading\nindicator in real time and improve the accuracy of forecasts. Finally, the\nresults show that Google's \"Food\" and \"Tourism\" categories significantly reduce\nprojection errors, highlighting the importance of using this information in a\nsegmented manner to improve macroeconomic forecasts.\n","date":"2025-03-27"}
{"id":"2503.21984","title":"Differential Evolution for Grassmann Manifold Optimization: A Projection\n  Approach","abstract":"  We propose a novel evolutionary algorithm for optimizing real-valued\nobjective functions defined on the Grassmann manifold Gr}(k,n), the space of\nall k-dimensional linear subspaces of R^n. While existing optimization\ntechniques on Gr}(k,n) predominantly rely on first- or second-order Riemannian\nmethods, these inherently local methods often struggle with nonconvex or\nmultimodal landscapes. To address this limitation, we adapt the Differential\nEvolution algorithm - a global, population based optimization method - to\noperate effectively on the Grassmannian. Our approach incorporates adaptive\ncontrol parameter schemes, and introduces a projection mechanism that maps\ntrial vectors onto the manifold via QR decomposition. The resulting algorithm\nmaintains feasibility with respect to the manifold structure while enabling\nexploration beyond local neighborhoods. This framework provides a flexible and\ngeometry-aware alternative to classical Riemannian optimization methods and is\nwell-suited to applications in machine learning, signal processing, and\nlow-rank matrix recovery where subspace representations play a central role. We\ntest the methodology on a number of examples of optimization problems on\nGrassmann manifolds.\n","date":"2025-03-27"}
{"id":"2503.21985","title":"Improving Equivariant Networks with Probabilistic Symmetry Breaking","abstract":"  Equivariance encodes known symmetries into neural networks, often enhancing\ngeneralization. However, equivariant networks cannot break symmetries: the\noutput of an equivariant network must, by definition, have at least the same\nself-symmetries as the input. This poses an important problem, both (1) for\nprediction tasks on domains where self-symmetries are common, and (2) for\ngenerative models, which must break symmetries in order to reconstruct from\nhighly symmetric latent spaces. This fundamental limitation can be addressed by\nconsidering equivariant conditional distributions, instead of equivariant\nfunctions. We present novel theoretical results that establish necessary and\nsufficient conditions for representing such distributions. Concretely, this\nrepresentation provides a practical framework for breaking symmetries in any\nequivariant network via randomized canonicalization. Our method, SymPE\n(Symmetry-breaking Positional Encodings), admits a simple interpretation in\nterms of positional encodings. This approach expands the representational power\nof equivariant networks while retaining the inductive bias of symmetry, which\nwe justify through generalization bounds. Experimental results demonstrate that\nSymPE significantly improves performance of group-equivariant and graph neural\nnetworks across diffusion models for graphs, graph autoencoders, and lattice\nspin system modeling.\n","date":"2025-03-27"}
{"id":"2503.21986","title":"Socially Constructed Treatment Plans: Analyzing Online Peer Interactions\n  to Understand How Patients Navigate Complex Medical Conditions","abstract":"  When faced with complex and uncertain medical conditions (e.g., cancer,\nmental health conditions, recovery from substance dependency), millions of\npatients seek online peer support. In this study, we leverage content analysis\nof online discourse and ethnographic studies with clinicians and patient\nrepresentatives to characterize how treatment plans for complex conditions are\n\"socially constructed.\" Specifically, we ground online conversation on\nmedication-assisted recovery treatment to medication guidelines and\nsubsequently surface when and why people deviate from the clinical guidelines.\nWe characterize the implications and effectiveness of socially constructed\ntreatment plans through in-depth interviews with clinical experts. Finally,\ngiven the enthusiasm around AI-powered solutions for patient communication, we\ninvestigate whether and how socially constructed treatment-related knowledge is\nreflected in a state-of-the-art large language model (LLM). Leveraging a novel\nmixed-method approach, this study highlights critical research directions for\npatient-centered communication in online health communities.\n","date":"2025-03-27"}
{"id":"2503.21989","title":"Bresa: Bio-inspired Reflexive Safe Reinforcement Learning for\n  Contact-Rich Robotic Tasks","abstract":"  Ensuring safety in reinforcement learning (RL)-based robotic systems is a\ncritical challenge, especially in contact-rich tasks within unstructured\nenvironments. While the state-of-the-art safe RL approaches mitigate risks\nthrough safe exploration or high-level recovery mechanisms, they often overlook\nlow-level execution safety, where reflexive responses to potential hazards are\ncrucial. Similarly, variable impedance control (VIC) enhances safety by\nadjusting the robot's mechanical response, yet lacks a systematic way to adapt\nparameters, such as stiffness and damping throughout the task. In this paper,\nwe propose Bresa, a Bio-inspired Reflexive Hierarchical Safe RL method inspired\nby biological reflexes. Our method decouples task learning from safety\nlearning, incorporating a safety critic network that evaluates action risks and\noperates at a higher frequency than the task solver. Unlike existing\nrecovery-based methods, our safety critic functions at a low-level control\nlayer, allowing real-time intervention when unsafe conditions arise. The\ntask-solving RL policy, running at a lower frequency, focuses on high-level\nplanning (decision-making), while the safety critic ensures instantaneous\nsafety corrections. We validate Bresa on multiple tasks including a\ncontact-rich robotic task, demonstrating its reflexive ability to enhance\nsafety, and adaptability in unforeseen dynamic environments. Our results show\nthat Bresa outperforms the baseline, providing a robust and reflexive safety\nmechanism that bridges the gap between high-level planning and low-level\nexecution. Real-world experiments and supplementary material are available at\nproject website https:\/\/jack-sherman01.github.io\/Bresa.\n","date":"2025-03-27"}
{"id":"2503.21990","title":"AgRowStitch: A High-fidelity Image Stitching Pipeline for Ground-based\n  Agricultural Images","abstract":"  Agricultural imaging often requires individual images to be stitched together\ninto a final mosaic for analysis. However, agricultural images can be\nparticularly challenging to stitch because feature matching across images is\ndifficult due to repeated textures, plants are non-planar, and mosaics built\nfrom many images can accumulate errors that cause drift. Although these issues\ncan be mitigated by using georeferenced images or taking images at high\naltitude, there is no general solution for images taken close to the crop. To\naddress this, we created a user-friendly and open source pipeline for stitching\nground-based images of a linear row of crops that does not rely on additional\ndata. First, we use SuperPoint and LightGlue to extract and match features\nwithin small batches of images. Then we stitch the images in each batch in\nseries while imposing constraints on the camera movement. After straightening\nand rescaling each batch mosaic, all batch mosaics are stitched together in\nseries and then straightened into a final mosaic. We tested the pipeline on\nimages collected along 72 m long rows of crops using two different agricultural\nrobots and a camera manually carried over the row. In all three cases, the\npipeline produced high-quality mosaics that could be used to georeference real\nworld positions with a mean absolute error of 20 cm. This approach provides\naccessible leaf-scale stitching to users who need to coarsely georeference\npositions within a row, but do not have access to accurate positional data or\nsophisticated imaging systems.\n","date":"2025-03-27"}
{"id":"2503.21991","title":"BOOTPLACE: Bootstrapped Object Placement with Detection Transformers","abstract":"  In this paper, we tackle the copy-paste image-to-image composition problem\nwith a focus on object placement learning. Prior methods have leveraged\ngenerative models to reduce the reliance for dense supervision. However, this\noften limits their capacity to model complex data distributions. Alternatively,\ntransformer networks with a sparse contrastive loss have been explored, but\ntheir over-relaxed regularization often leads to imprecise object placement. We\nintroduce BOOTPLACE, a novel paradigm that formulates object placement as a\nplacement-by-detection problem. Our approach begins by identifying suitable\nregions of interest for object placement. This is achieved by training a\nspecialized detection transformer on object-subtracted backgrounds, enhanced\nwith multi-object supervisions. It then semantically associates each target\ncompositing object with detected regions based on their complementary\ncharacteristics. Through a boostrapped training approach applied to randomly\nobject-subtracted images, our model enforces meaningful placements through\nextensive paired data augmentation. Experimental results on established\nbenchmarks demonstrate BOOTPLACE's superior performance in object\nrepositioning, markedly surpassing state-of-the-art baselines on Cityscapes and\nOPA datasets with notable improvements in IOU scores. Additional ablation\nstudies further showcase the compositionality and generalizability of our\napproach, supported by user study evaluations.\n","date":"2025-03-27"}
{"id":"2503.21999","title":"FACETS: Efficient Once-for-all Object Detection via Constrained\n  Iterative Search","abstract":"  Neural Architecture Search (NAS) for deep learning object detection\nframeworks typically involves multiple modules, each performing distinct tasks.\nThese modules contribute to a vast search space, resulting in searches that can\ntake several GPU hours or even days, depending on the complexity of the search\nspace. This makes joint optimization both challenging and computationally\nexpensive. Furthermore, satisfying target device constraints across modules\nadds additional complexity to the optimization process. To address these\nchallenges, we propose \\textbf{FACETS}, e\\textbf{\\underline{F}}ficient\nOnce-for-\\textbf{\\underline{A}}ll Object Detection via\n\\textbf{\\underline{C}}onstrained\nit\\textbf{\\underline{E}}ra\\textbf{\\underline{T}}ive\\textbf{\\underline{S}}earch,\na novel unified iterative NAS method that refines the architecture of all\nmodules in a cyclical manner. FACETS leverages feedback from previous\niterations, alternating between fixing one module's architecture and optimizing\nthe others. This approach reduces the overall search space while preserving\ninterdependencies among modules and incorporates constraints based on the\ntarget device's computational budget. In a controlled comparison against\nprogressive and single-module search strategies, FACETS achieves architectures\nwith up to $4.75\\%$ higher accuracy twice as fast as progressive search\nstrategies in earlier stages, while still being able to achieve a global\noptimum. Moreover, FACETS demonstrates the ability to iteratively refine the\nsearch space, producing better performing architectures over time. The refined\nsearch space yields candidates with a mean accuracy up to $27\\%$ higher than\nglobal search and $5\\%$ higher than progressive search methods via random\nsampling.\n","date":"2025-03-27"}
{"id":"2503.22000","title":"Cluster automata","abstract":"  We introduce a new class of clustered Moore automata (CMA), investigate their\ntemporal behavior, and describe some applications.\n","date":"2025-03-27"}
{"id":"2503.22002","title":"Monte Carlo Sampling for Analyzing In-Context Examples","abstract":"  Prior works have shown that in-context learning is brittle to presentation\nfactors such as the order, number, and choice of selected examples. However,\nablation-based guidance on selecting the number of examples may ignore the\ninterplay between different presentation factors. In this work we develop a\nMonte Carlo sampling-based method to study the impact of number of examples\nwhile explicitly accounting for effects from order and selected examples. We\nfind that previous guidance on how many in-context examples to select does not\nalways generalize across different sets of selected examples and orderings, and\nwhether one-shot settings outperform zero-shot settings is highly dependent on\nthe selected example. Additionally, inspired by data valuation, we apply our\nsampling method to in-context example selection to select examples that perform\nwell across different orderings. We find a negative result, that while\nperformance is robust to ordering and number of examples, there is an\nunexpected performance degradation compared to random sampling.\n","date":"2025-03-27"}
{"id":"2503.22006","title":"Enhancing Domain-Specific Encoder Models with LLM-Generated Data: How to\n  Leverage Ontologies, and How to Do Without Them","abstract":"  We investigate the use of LLM-generated data for continual pretraining of\nencoder models in specialized domains with limited training data, using the\nscientific domain of invasion biology as a case study. To this end, we leverage\ndomain-specific ontologies by enriching them with LLM-generated data and\npretraining the encoder model as an ontology-informed embedding model for\nconcept definitions. To evaluate the effectiveness of this method, we compile a\nbenchmark specifically designed for assessing model performance in invasion\nbiology. After demonstrating substantial improvements over standard LLM\npretraining, we investigate the feasibility of applying the proposed approach\nto domains without comprehensive ontologies by substituting ontological\nconcepts with concepts automatically extracted from a small corpus of\nscientific abstracts and establishing relationships between concepts through\ndistributional statistics. Our results demonstrate that this automated approach\nachieves comparable performance using only a small set of scientific abstracts,\nresulting in a fully automated pipeline for enhancing domain-specific\nunderstanding of small encoder models that is especially suited for application\nin low-resource settings and achieves performance comparable to masked language\nmodeling pretraining on much larger datasets.\n","date":"2025-03-27"}
{"id":"2503.22008","title":"Tune It Up: Music Genre Transfer and Prediction","abstract":"  Deep generative models have been used in style transfer tasks for images. In\nthis study, we adapt and improve CycleGAN model to perform music style transfer\non Jazz and Classic genres. By doing so, we aim to easily generate new songs,\ncover music to different music genres and reduce the arrangements needed in\nthose processes. We train and use music genre classifier to assess the\nperformance of the transfer models. To that end, we obtain 87.7% accuracy with\nMulti-layer Perceptron algorithm. To improve our style transfer baseline, we\nadd auxiliary discriminators and triplet loss to our model. According to our\nexperiments, we obtain the best accuracies as 69.4% in Jazz to Classic task and\n39.3% in Classic to Jazz task with our developed genre classifier. We also run\na subjective experiment and results of it show that the overall performance of\nour transfer model is good and it manages to conserve melody of inputs on the\ntransferred outputs. Our code is available at https:\/\/github.com\/\nfidansamet\/tune-it-up\n","date":"2025-03-27"}
{"id":"2503.22015","title":"DeCompress: Denoising via Neural Compression","abstract":"  Learning-based denoising algorithms achieve state-of-the-art performance\nacross various denoising tasks. However, training such models relies on access\nto large training datasets consisting of clean and noisy image pairs. On the\nother hand, in many imaging applications, such as microscopy, collecting ground\ntruth images is often infeasible. To address this challenge, researchers have\nrecently developed algorithms that can be trained without requiring access to\nground truth data. However, training such models remains computationally\nchallenging and still requires access to large noisy training samples. In this\nwork, inspired by compression-based denoising and recent advances in neural\ncompression, we propose a new compression-based denoising algorithm, which we\nname DeCompress, that i) does not require access to ground truth images, ii)\ndoes not require access to large training dataset - only a single noisy image\nis sufficient, iii) is robust to overfitting, and iv) achieves superior\nperformance compared with zero-shot or unsupervised learning-based denoisers.\n","date":"2025-03-27"}
{"id":"2503.22019","title":"AGILE: A Diffusion-Based Attention-Guided Image and Label Translation\n  for Efficient Cross-Domain Plant Trait Identification","abstract":"  Semantically consistent cross-domain image translation facilitates the\ngeneration of training data by transferring labels across different domains,\nmaking it particularly useful for plant trait identification in agriculture.\nHowever, existing generative models struggle to maintain object-level accuracy\nwhen translating images between domains, especially when domain gaps are\nsignificant. In this work, we introduce AGILE (Attention-Guided Image and Label\nTranslation for Efficient Cross-Domain Plant Trait Identification), a\ndiffusion-based framework that leverages optimized text embeddings and\nattention guidance to semantically constrain image translation. AGILE utilizes\npretrained diffusion models and publicly available agricultural datasets to\nimprove the fidelity of translated images while preserving critical object\nsemantics. Our approach optimizes text embeddings to strengthen the\ncorrespondence between source and target images and guides attention maps\nduring the denoising process to control object placement. We evaluate AGILE on\ncross-domain plant datasets and demonstrate its effectiveness in generating\nsemantically accurate translated images. Quantitative experiments show that\nAGILE enhances object detection performance in the target domain while\nmaintaining realism and consistency. Compared to prior image translation\nmethods, AGILE achieves superior semantic alignment, particularly in\nchallenging cases where objects vary significantly or domain gaps are\nsubstantial.\n","date":"2025-03-27"}
{"id":"2503.22020","title":"CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action\n  Models","abstract":"  Vision-language-action models (VLAs) have shown potential in leveraging\npretrained vision-language models and diverse robot demonstrations for learning\ngeneralizable sensorimotor control. While this paradigm effectively utilizes\nlarge-scale data from both robotic and non-robotic sources, current VLAs\nprimarily focus on direct input--output mappings, lacking the intermediate\nreasoning steps crucial for complex manipulation tasks. As a result, existing\nVLAs lack temporal planning or reasoning capabilities. In this paper, we\nintroduce a method that incorporates explicit visual chain-of-thought (CoT)\nreasoning into vision-language-action models (VLAs) by predicting future image\nframes autoregressively as visual goals before generating a short action\nsequence to achieve these goals. We introduce CoT-VLA, a state-of-the-art 7B\nVLA that can understand and generate visual and action tokens. Our experimental\nresults demonstrate that CoT-VLA achieves strong performance, outperforming the\nstate-of-the-art VLA model by 17% in real-world manipulation tasks and 6% in\nsimulation benchmarks. Project website: https:\/\/cot-vla.github.io\/\n","date":"2025-03-27"}
{"id":"2503.22023","title":"Safeguarding Autonomy: a Focus on Machine Learning Decision Systems","abstract":"  As global discourse on AI regulation gains momentum, this paper focuses on\ndelineating the impact of ML on autonomy and fostering awareness. Respect for\nautonomy is a basic principle in bioethics that establishes persons as\ndecision-makers. While the concept of autonomy in the context of ML appears in\nseveral European normative publications, it remains a theoretical concept that\nhas yet to be widely accepted in ML practice. Our contribution is to bridge the\ntheoretical and practical gap by encouraging the practical application of\nautonomy in decision-making within ML practice by identifying the conditioning\nfactors that currently prevent it. Consequently, we focus on the different\nstages of the ML pipeline to identify the potential effects on ML end-users'\nautonomy. To improve its practical utility, we propose a related question for\neach detected impact, offering guidance for identifying possible focus points\nto respect ML end-users autonomy in decision-making.\n","date":"2025-03-27"}
{"id":"2503.22026","title":"Multispectral Demosaicing via Dual Cameras","abstract":"  Multispectral (MS) images capture detailed scene information across a wide\nrange of spectral bands, making them invaluable for applications requiring rich\nspectral data. Integrating MS imaging into multi camera devices, such as\nsmartphones, has the potential to enhance both spectral applications and RGB\nimage quality. A critical step in processing MS data is demosaicing, which\nreconstructs color information from the mosaic MS images captured by the\ncamera. This paper proposes a method for MS image demosaicing specifically\ndesigned for dual-camera setups where both RGB and MS cameras capture the same\nscene. Our approach leverages co-captured RGB images, which typically have\nhigher spatial fidelity, to guide the demosaicing of lower-fidelity MS images.\nWe introduce the Dual-camera RGB-MS Dataset - a large collection of paired RGB\nand MS mosaiced images with ground-truth demosaiced outputs - that enables\ntraining and evaluation of our method. Experimental results demonstrate that\nour method achieves state-of-the-art accuracy compared to existing techniques.\n","date":"2025-03-27"}
{"id":"2503.22036","title":"Cognitive Prompts Using Guilford's Structure of Intellect Model","abstract":"  Large language models (LLMs) demonstrate strong language generation\ncapabilities but often struggle with structured reasoning, leading to\ninconsistent or suboptimal problem-solving. To mitigate this limitation,\nGuilford's Structure of Intellect (SOI) model - a foundational framework from\nintelligence theory - is leveraged as the basis for cognitive prompt\nengineering. The SOI model categorizes cognitive operations such as pattern\nrecognition, memory retrieval, and evaluation, offering a systematic approach\nto enhancing LLM reasoning and decision-making. This position paper presents a\nnovel cognitive prompting approach for enforcing SOI-inspired reasoning for\nimproving clarity, coherence, and adaptability in model responses.\n","date":"2025-03-27"}
{"id":"2503.22038","title":"Debate-Driven Multi-Agent LLMs for Phishing Email Detection","abstract":"  Phishing attacks remain a critical cybersecurity threat. Attackers constantly\nrefine their methods, making phishing emails harder to detect. Traditional\ndetection methods, including rule-based systems and supervised machine learning\nmodels, either rely on predefined patterns like blacklists, which can be\nbypassed with slight modifications, or require large datasets for training and\nstill can generate false positives and false negatives. In this work, we\npropose a multi-agent large language model (LLM) prompting technique that\nsimulates debates among agents to detect whether the content presented on an\nemail is phishing. Our approach uses two LLM agents to present arguments for or\nagainst the classification task, with a judge agent adjudicating the final\nverdict based on the quality of reasoning provided. This debate mechanism\nenables the models to critically analyze contextual cue and deceptive patterns\nin text, which leads to improved classification accuracy. The proposed\nframework is evaluated on multiple phishing email datasets and demonstrate that\nmixed-agent configurations consistently outperform homogeneous configurations.\nResults also show that the debate structure itself is sufficient to yield\naccurate decisions without extra prompting strategies.\n","date":"2025-03-27"}
{"id":"2503.22040","title":"The Risks of Using Large Language Models for Text Annotation in Social\n  Science Research","abstract":"  Generative artificial intelligence (GenAI) or large language models (LLMs)\nhave the potential to revolutionize computational social science, particularly\nin automated textual analysis. In this paper, we conduct a systematic\nevaluation of the promises and risks of using LLMs for diverse coding tasks,\nwith social movement studies serving as a case example. We propose a framework\nfor social scientists to incorporate LLMs into text annotation, either as the\nprimary coding decision-maker or as a coding assistant. This framework provides\ntools for researchers to develop the optimal prompt, and to examine and report\nthe validity and reliability of LLMs as a methodological tool. Additionally, we\ndiscuss the associated epistemic risks related to validity, reliability,\nreplicability, and transparency. We conclude with several practical guidelines\nfor using LLMs in text annotation tasks, and how we can better communicate the\nepistemic risks in research.\n","date":"2025-03-27"}
{"id":"2503.22048","title":"ThinkEdit: Interpretable Weight Editing to Mitigate Overly Short\n  Thinking in Reasoning Models","abstract":"  Recent studies have shown that Large Language Models (LLMs) augmented with\nchain-of-thought (CoT) reasoning demonstrate impressive problem-solving\nabilities. However, in this work, we identify a recurring issue where these\nmodels occasionally generate overly short reasoning, leading to degraded\nperformance on even simple mathematical problems. Specifically, we investigate\nhow reasoning length is embedded in the hidden representations of reasoning\nmodels and its impact on accuracy. Our analysis reveals that reasoning length\nis governed by a linear direction in the representation space, allowing us to\ninduce overly short reasoning by steering the model along this direction.\nBuilding on this insight, we introduce ThinkEdit, a simple yet effective\nweight-editing approach to mitigate the issue of overly short reasoning. We\nfirst identify a small subset of attention heads (approximately 2%) that\npredominantly drive short reasoning behavior. We then edit the output\nprojection weights of these heads to suppress the short reasoning direction.\nWith changes to only 0.1% of the model's parameters, ThinkEdit effectively\nreduces overly short reasoning and yields notable accuracy gains for short\nreasoning outputs (+5.44%), along with an overall improvement across multiple\nmath benchmarks (+2.43%). Our findings provide new mechanistic insights into\nhow reasoning length is controlled within LLMs and highlight the potential of\nfine-grained model interventions to improve reasoning quality. Our code is\navailable at https:\/\/github.com\/Trustworthy-ML-Lab\/ThinkEdit\n","date":"2025-03-27"}
{"id":"2503.22050","title":"A Deep Learning Framework for Boundary-Aware Semantic Segmentation","abstract":"  As a fundamental task in computer vision, semantic segmentation is widely\napplied in fields such as autonomous driving, remote sensing image analysis,\nand medical image processing. In recent years, Transformer-based segmentation\nmethods have demonstrated strong performance in global feature modeling.\nHowever, they still struggle with blurred target boundaries and insufficient\nrecognition of small targets. To address these issues, this study proposes a\nMask2Former-based semantic segmentation algorithm incorporating a boundary\nenhancement feature bridging module (BEFBM). The goal is to improve target\nboundary accuracy and segmentation consistency. Built upon the Mask2Former\nframework, this method constructs a boundary-aware feature map and introduces a\nfeature bridging mechanism. This enables effective cross-scale feature fusion,\nenhancing the model's ability to focus on target boundaries. Experiments on the\nCityscapes dataset demonstrate that, compared to mainstream segmentation\nmethods, the proposed approach achieves significant improvements in metrics\nsuch as mIOU, mDICE, and mRecall. It also exhibits superior boundary retention\nin complex scenes. Visual analysis further confirms the model's advantages in\nfine-grained regions. Future research will focus on optimizing computational\nefficiency and exploring its potential in other high-precision segmentation\ntasks.\n","date":"2025-03-28"}
{"id":"2503.22051","title":"Non-Monotonic Attention-based Read\/Write Policy Learning for\n  Simultaneous Translation","abstract":"  Simultaneous or streaming machine translation generates translation while\nreading the input stream. These systems face a quality\/latency trade-off,\naiming to achieve high translation quality similar to non-streaming models with\nminimal latency. We propose an approach that efficiently manages this\ntrade-off. By enhancing a pretrained non-streaming model, which was trained\nwith a seq2seq mechanism and represents the upper bound in quality, we convert\nit into a streaming model by utilizing the alignment between source and target\ntokens. This alignment is used to learn a read\/write decision boundary for\nreliable translation generation with minimal input. During training, the model\nlearns the decision boundary through a read\/write policy module, employing\nsupervised learning on the alignment points (pseudo labels). The read\/write\npolicy module, a small binary classification unit, can control the\nquality\/latency trade-off during inference. Experimental results show that our\nmodel outperforms several strong baselines and narrows the gap with the\nnon-streaming baseline model.\n","date":"2025-03-28"}
{"id":"2503.22052","title":"Improving the generalization of deep learning models in the segmentation\n  of mammography images","abstract":"  Mammography stands as the main screening method for detecting breast cancer\nearly, enhancing treatment success rates. The segmentation of landmark\nstructures in mammography images can aid the medical assessment in the\nevaluation of cancer risk and the image acquisition adequacy. We introduce a\nseries of data-centric strategies aimed at enriching the training data for deep\nlearning-based segmentation of landmark structures. Our approach involves\naugmenting the training samples through annotation-guided image intensity\nmanipulation and style transfer to achieve better generalization than standard\ntraining procedures. These augmentations are applied in a balanced manner to\nensure the model learns to process a diverse range of images generated by\ndifferent vendor equipments while retaining its efficacy on the original data.\nWe present extensive numerical and visual results that demonstrate the superior\ngeneralization capabilities of our methods when compared to the standard\ntraining. For this evaluation, we consider a large dataset that includes\nmammography images generated by different vendor equipments. Further, we\npresent complementary results that show both the strengths and limitations of\nour methods across various scenarios. The accuracy and robustness demonstrated\nin the experiments suggest that our method is well-suited for integration into\nclinical practice.\n","date":"2025-03-28"}
{"id":"2503.22054","title":"tempdisagg: A Python Framework for Temporal Disaggregation of Time\n  Series Data","abstract":"  tempdisagg is a modern, extensible, and production-ready Python framework for\ntemporal disaggregation of time series data. It transforms low-frequency\naggregates into consistent, high-frequency estimates using a wide array of\neconometric techniques-including Chow-Lin, Denton, Litterman, Fernandez, and\nuniform interpolation-as well as enhanced variants with automated estimation of\nkey parameters such as the autocorrelation coefficient rho. The package\nintroduces features beyond classical methods, including robust ensemble\nmodeling via non-negative least squares optimization, post-estimation\ncorrection of negative values under multiple aggregation rules, and optional\nregression-based imputation of missing values through a dedicated\nRetropolarizer module. Architecturally, it follows a modular design inspired by\nscikit-learn, offering a clean API for validation, modeling, visualization, and\nresult interpretation.\n","date":"2025-03-28"}
{"id":"2503.22059","title":"Low Rank and Sparse Fourier Structure in Recurrent Networks Trained on\n  Modular Addition","abstract":"  Modular addition tasks serve as a useful test bed for observing empirical\nphenomena in deep learning, including the phenomenon of \\emph{grokking}. Prior\nwork has shown that one-layer transformer architectures learn Fourier\nMultiplication circuits to solve modular addition tasks. In this paper, we show\nthat Recurrent Neural Networks (RNNs) trained on modular addition tasks also\nuse a Fourier Multiplication strategy. We identify low rank structures in the\nmodel weights, and attribute model components to specific Fourier frequencies,\nresulting in a sparse representation in the Fourier space. We also show\nempirically that the RNN is robust to removing individual frequencies, while\nthe performance degrades drastically as more frequencies are ablated from the\nmodel.\n","date":"2025-03-28"}
{"id":"2503.22060","title":"Deep Depth Estimation from Thermal Image: Dataset, Benchmark, and\n  Challenges","abstract":"  Achieving robust and accurate spatial perception under adverse weather and\nlighting conditions is crucial for the high-level autonomy of self-driving\nvehicles and robots. However, existing perception algorithms relying on the\nvisible spectrum are highly affected by weather and lighting conditions. A\nlong-wave infrared camera (i.e., thermal imaging camera) can be a potential\nsolution to achieve high-level robustness. However, the absence of large-scale\ndatasets and standardized benchmarks remains a significant bottleneck to\nprogress in active research for robust visual perception from thermal images.\nTo this end, this manuscript provides a large-scale Multi-Spectral Stereo\n(MS$^2$) dataset that consists of stereo RGB, stereo NIR, stereo thermal,\nstereo LiDAR data, and GNSS\/IMU information along with semi-dense depth ground\ntruth. MS$^2$ dataset includes 162K synchronized multi-modal data pairs\ncaptured across diverse locations (e.g., urban city, residential area, campus,\nand high-way road) at different times (e.g., morning, daytime, and nighttime)\nand under various weather conditions (e.g., clear-sky, cloudy, and rainy).\nSecondly, we conduct a thorough evaluation of monocular and stereo depth\nestimation networks across RGB, NIR, and thermal modalities to establish\nstandardized benchmark results on MS$^2$ depth test sets (e.g., day, night, and\nrainy). Lastly, we provide in-depth analyses and discuss the challenges\nrevealed by the benchmark results, such as the performance variability for each\nmodality under adverse conditions, domain shift between different sensor\nmodalities, and potential research direction for thermal perception. Our\ndataset and source code are publicly available at\nhttps:\/\/sites.google.com\/view\/multi-spectral-stereo-dataset and\nhttps:\/\/github.com\/UkcheolShin\/SupDepth4Thermal.\n","date":"2025-03-28"}
{"id":"2503.22063","title":"Arch-LLM: Taming LLMs for Neural Architecture Generation via\n  Unsupervised Discrete Representation Learning","abstract":"  Unsupervised representation learning has been widely explored across various\nmodalities, including neural architectures, where it plays a key role in\ndownstream applications like Neural Architecture Search (NAS). These methods\ntypically learn an unsupervised representation space before generating\/\nsampling architectures for the downstream search. A common approach involves\nthe use of Variational Autoencoders (VAEs) to map discrete architectures onto a\ncontinuous representation space, however, sampling from these spaces often\nleads to a high percentage of invalid or duplicate neural architectures. This\ncould be due to the unnatural mapping of inherently discrete architectural\nspace onto a continuous space, which emphasizes the need for a robust discrete\nrepresentation of these architectures. To address this, we introduce a Vector\nQuantized Variational Autoencoder (VQ-VAE) to learn a discrete latent space\nmore naturally aligned with the discrete neural architectures. In contrast to\nVAEs, VQ-VAEs (i) map each architecture into a discrete code sequence and (ii)\nallow the prior to be learned by any generative model rather than assuming a\nnormal distribution. We then represent these architecture latent codes as\nnumerical sequences and train a text-to-text model leveraging a Large Language\nModel to learn and generate sequences representing architectures. We experiment\nour method with Inception\/ ResNet-like cell-based search spaces, namely\nNAS-Bench-101 and NAS-Bench-201. Compared to VAE-based methods, our approach\nimproves the generation of valid and unique architectures by over 80% on\nNASBench-101 and over 8% on NASBench-201. Finally, we demonstrate the\napplicability of our method in NAS employing a sequence-modeling-based NAS\nalgorithm.\n","date":"2025-03-28"}
{"id":"2503.22064","title":"Multi-Task Semantic Communications via Large Models","abstract":"  Artificial intelligence (AI) promises to revolutionize the design,\noptimization and management of next-generation communication systems. In this\narticle, we explore the integration of large AI models (LAMs) into semantic\ncommunications (SemCom) by leveraging their multi-modal data processing and\ngeneration capabilities. Although LAMs bring unprecedented abilities to extract\nsemantics from raw data, this integration entails multifaceted challenges\nincluding high resource demands, model complexity, and the need for\nadaptability across diverse modalities and tasks. To overcome these challenges,\nwe propose a LAM-based multi-task SemCom (MTSC) architecture, which includes an\nadaptive model compression strategy and a federated split fine-tuning approach\nto facilitate the efficient deployment of LAM-based semantic models in\nresource-limited networks. Furthermore, a retrieval-augmented generation scheme\nis implemented to synthesize the most recent local and global knowledge bases\nto enhance the accuracy of semantic extraction and content generation, thereby\nimproving the inference performance. Finally, simulation results demonstrate\nthe efficacy of the proposed LAM-based MTSC architecture, highlighting the\nperformance enhancements across various downstream tasks under varying channel\nconditions.\n","date":"2025-03-28"}
{"id":"2503.22068","title":"A Proposal for Networks Capable of Continual Learning","abstract":"  We analyze the ability of computational units to retain past responses after\nparameter updates, a key property for system-wide continual learning. Neural\nnetworks trained with gradient descent lack this capability, prompting us to\npropose Modelleyen, an alternative approach with inherent response\npreservation. We demonstrate through experiments on modeling the dynamics of a\nsimple environment and on MNIST that, despite increased computational\ncomplexity and some representational limitations at its current stage,\nModelleyen achieves continual learning without relying on sample replay or\npredefined task boundaries.\n","date":"2025-03-28"}
{"id":"2503.22069","title":"Contrasting Low and High-Resolution Features for HER2 Scoring using Deep\n  Learning","abstract":"  Breast cancer, the most common malignancy among women, requires precise\ndetection and classification for effective treatment. Immunohistochemistry\n(IHC) biomarkers like HER2, ER, and PR are critical for identifying breast\ncancer subtypes. However, traditional IHC classification relies on\npathologists' expertise, making it labor-intensive and subject to significant\ninter-observer variability. To address these challenges, this study introduces\nthe India Pathology Breast Cancer Dataset (IPD-Breast), comprising of 1,272 IHC\nslides (HER2, ER, and PR) aimed at automating receptor status classification.\nThe primary focus is on developing predictive models for HER2 3-way\nclassification (0, Low, High) to enhance prognosis. Evaluation of multiple deep\nlearning models revealed that an end-to-end ConvNeXt network utilizing\nlow-resolution IHC images achieved an AUC, F1, and accuracy of 91.79%, 83.52%,\nand 83.56%, respectively, for 3-way classification, outperforming patch-based\nmethods by over 5.35% in F1 score. This study highlights the potential of\nsimple yet effective deep learning techniques to significantly improve accuracy\nand reproducibility in breast cancer classification, supporting their\nintegration into clinical workflows for better patient outcomes.\n","date":"2025-03-28"}
{"id":"2503.22074","title":"Penrose Tiled Low-Rank Compression and Section-Wise Q&A Fine-Tuning: A\n  General Framework for Domain-Specific Large Language Model Adaptation","abstract":"  Large language models (LLMs) hold great promise for specialized scientific\ndomains such as materials science, yet adapting them efficiently and accurately\nto domain-specific knowledge remains challenging due to limited data and high\nknowledge density. We propose a two-stage framework that combines structured\nmodel compression with a scientific fine-tuning regimen to address this\nchallenge. In the compression stage, we decompose the LLM's weight matrices\ninto local low-rank \"rank blocks\" and arrange these blocks in a Penrose-like\nnon-periodic tiling pattern. Each block is then compacted via spectral\ntransformations (e.g., discrete cosine or Fourier transforms), and a\nKullback-Leibler (KL) divergence-based alignment loss preserves the\ndistributional similarity between the compressed model's representations and\nthose of the original full model. In the adaptation stage, the compressed model\nis further tuned using a human-like scientific reading protocol: it processes\ntechnical materials science documents section by section, engaging in a\nstructured question-and-answer routine for each section. This section-wise Q&A\nfine-tuning strategy extracts explicit reasoning traces and gradually injects\ndomain knowledge, while minimizing catastrophic forgetting of the model's\ngeneral language capabilities. By balancing efficient compression with targeted\nadaptation, our two-stage approach enables precise specialization of LLMs to\nhigh-value domains under data-scarce conditions. We present this principled yet\nexploratory pipeline and outline its potential for advancing materials science\nknowledge integration, laying the groundwork for comprehensive empirical\nevaluation in future work.\n","date":"2025-03-28"}
{"id":"2503.22076","title":"Concise One-Layer Transformers Can Do Function Evaluation (Sometimes)","abstract":"  While transformers have proven enormously successful in a range of tasks,\ntheir fundamental properties as models of computation are not well understood.\nThis paper contributes to the study of the expressive capacity of transformers,\nfocusing on their ability to perform the fundamental computational task of\nevaluating an arbitrary function from $[n]$ to $[n]$ at a given argument. We\nprove that concise 1-layer transformers (i.e., with a polylog bound on the\nproduct of the number of heads, the embedding dimension, and precision) are\ncapable of doing this task under some representations of the input, but not\nwhen the function's inputs and values are only encoded in different input\npositions. Concise 2-layer transformers can perform the task even with the more\ndifficult input representation. Experimentally, we find a rough alignment\nbetween what we have proven can be computed by concise transformers and what\ncan be practically learned.\n","date":"2025-03-28"}
{"id":"2503.22079","title":"A Semantic-Enhanced Heterogeneous Graph Learning Method for Flexible\n  Objects Recognition","abstract":"  Flexible objects recognition remains a significant challenge due to its\ninherently diverse shapes and sizes, translucent attributes, and subtle\ninter-class differences. Graph-based models, such as graph convolution networks\nand graph vision models, are promising in flexible objects recognition due to\ntheir ability of capturing variable relations within the flexible objects.\nThese methods, however, often focus on global visual relationships or fail to\nalign semantic and visual information. To alleviate these limitations, we\npropose a semantic-enhanced heterogeneous graph learning method. First, an\nadaptive scanning module is employed to extract discriminative semantic\ncontext, facilitating the matching of flexible objects with varying shapes and\nsizes while aligning semantic and visual nodes to enhance cross-modal feature\ncorrelation. Second, a heterogeneous graph generation module aggregates global\nvisual and local semantic node features, improving the recognition of flexible\nobjects. Additionally, We introduce the FSCW, a large-scale flexible dataset\ncurated from existing sources. We validate our method through extensive\nexperiments on flexible datasets (FDA and FSCW), and challenge benchmarks\n(CIFAR-100 and ImageNet-Hard), demonstrating competitive performance.\n","date":"2025-03-28"}
{"id":"2503.22081","title":"A Survey on Remote Sensing Foundation Models: From Vision to\n  Multimodality","abstract":"  The rapid advancement of remote sensing foundation models, particularly\nvision and multimodal models, has significantly enhanced the capabilities of\nintelligent geospatial data interpretation. These models combine various data\nmodalities, such as optical, radar, and LiDAR imagery, with textual and\ngeographic information, enabling more comprehensive analysis and understanding\nof remote sensing data. The integration of multiple modalities allows for\nimproved performance in tasks like object detection, land cover classification,\nand change detection, which are often challenged by the complex and\nheterogeneous nature of remote sensing data. However, despite these\nadvancements, several challenges remain. The diversity in data types, the need\nfor large-scale annotated datasets, and the complexity of multimodal fusion\ntechniques pose significant obstacles to the effective deployment of these\nmodels. Moreover, the computational demands of training and fine-tuning\nmultimodal models require significant resources, further complicating their\npractical application in remote sensing image interpretation tasks. This paper\nprovides a comprehensive review of the state-of-the-art in vision and\nmultimodal foundation models for remote sensing, focusing on their\narchitecture, training methods, datasets and application scenarios. We discuss\nthe key challenges these models face, such as data alignment, cross-modal\ntransfer learning, and scalability, while also identifying emerging research\ndirections aimed at overcoming these limitations. Our goal is to provide a\nclear understanding of the current landscape of remote sensing foundation\nmodels and inspire future research that can push the boundaries of what these\nmodels can achieve in real-world applications. The list of resources collected\nby the paper can be found in the\nhttps:\/\/github.com\/IRIP-BUAA\/A-Review-for-remote-sensing-vision-language-models.\n","date":"2025-03-28"}
{"id":"2503.22082","title":"ReLU Networks as Random Functions: Their Distribution in Probability\n  Space","abstract":"  This paper presents a novel framework for understanding trained ReLU networks\nas random, affine functions, where the randomness is induced by the\ndistribution over the inputs. By characterizing the probability distribution of\nthe network's activation patterns, we derive the discrete probability\ndistribution over the affine functions realizable by the network. We extend\nthis analysis to describe the probability distribution of the network's\noutputs. Our approach provides explicit, numerically tractable expressions for\nthese distributions in terms of Gaussian orthant probabilities. Additionally,\nwe develop approximation techniques to identify the support of affine functions\na trained ReLU network can realize for a given distribution of inputs. Our work\nprovides a framework for understanding the behavior and performance of ReLU\nnetworks corresponding to stochastic inputs, paving the way for more\ninterpretable and reliable models.\n","date":"2025-03-28"}
{"id":"2503.22087","title":"Mitigating Trade-off: Stream and Query-guided Aggregation for Efficient\n  and Effective 3D Occupancy Prediction","abstract":"  3D occupancy prediction has emerged as a key perception task for autonomous\ndriving, as it reconstructs 3D environments to provide a comprehensive scene\nunderstanding. Recent studies focus on integrating spatiotemporal information\nobtained from past observations to improve prediction accuracy, using a\nmulti-frame fusion approach that processes multiple past frames together.\nHowever, these methods struggle with a trade-off between efficiency and\naccuracy, which significantly limits their practicality. To mitigate this\ntrade-off, we propose StreamOcc, a novel framework that aggregates\nspatio-temporal information in a stream-based manner. StreamOcc consists of two\nkey components: (i) Stream-based Voxel Aggregation, which effectively\naccumulates past observations while minimizing computational costs, and (ii)\nQuery-guided Aggregation, which recurrently aggregates instance-level features\nof dynamic objects into corresponding voxel features, refining fine-grained\ndetails of dynamic objects. Experiments on the Occ3D-nuScenes dataset show that\nStreamOcc achieves state-of-the-art performance in real-time settings, while\nreducing memory usage by more than 50% compared to previous methods.\n","date":"2025-03-28"}
{"id":"2503.22092","title":"Leveraging LLMs for Predicting Unknown Diagnoses from Clinical Notes","abstract":"  Electronic Health Records (EHRs) often lack explicit links between\nmedications and diagnoses, making clinical decision-making and research more\ndifficult. Even when links exist, diagnosis lists may be incomplete, especially\nduring early patient visits. Discharge summaries tend to provide more complete\ninformation, which can help infer accurate diagnoses, especially with the help\nof large language models (LLMs). This study investigates whether LLMs can\npredict implicitly mentioned diagnoses from clinical notes and link them to\ncorresponding medications. We address two research questions: (1) Does majority\nvoting across diverse LLM configurations outperform the best single\nconfiguration in diagnosis prediction? (2) How sensitive is majority voting\naccuracy to LLM hyperparameters such as temperature, top-p, and summary length?\nTo evaluate, we created a new dataset of 240 expert-annotated\nmedication-diagnosis pairs from 20 MIMIC-IV notes. Using GPT-3.5 Turbo, we ran\n18 prompting configurations across short and long summary lengths, generating\n8568 test cases. Results show that majority voting achieved 75 percent\naccuracy, outperforming the best single configuration at 66 percent. No single\nhyperparameter setting dominated, but combining deterministic, balanced, and\nexploratory strategies improved performance. Shorter summaries generally led to\nhigher accuracy.In conclusion, ensemble-style majority voting with diverse LLM\nconfigurations improves diagnosis prediction in EHRs and offers a promising\nmethod to link medications and diagnoses in clinical texts.\n","date":"2025-03-28"}
{"id":"2503.22093","title":"How Well Can Vison-Language Models Understand Humans' Intention? An\n  Open-ended Theory of Mind Question Evaluation Benchmark","abstract":"  Vision Language Models (VLMs) have demonstrated strong reasoning capabilities\nin Visual Question Answering (VQA) tasks; However, their ability to perform\nTheory of Mind (ToM) tasks such as accurately inferring human intentions,\nbeliefs, and other mental states remains underexplored. In this work, we\npropose an open-ended question framework to comprehensively evaluate VLMs'\nperformance across diverse categories of ToM tasks. We curated and annotated a\nbenchmark dataset composed of 30 images. We then assessed the performance of\nfour VLMs of varying sizes on this dataset. Our experimental results show that\nthe GPT-4 model outperformed all others, with only one smaller model,\nGPT-4o-mini, achieving comparable performance. Additionally, we observed that\nVLMs often struggle to accurately infer intentions in complex scenarios such as\nbullying or cheating. Moreover, our findings also reveal that smaller models\ncan sometimes infer correct intentions despite relying on incorrect visual\ncues.\n","date":"2025-03-28"}
{"id":"2503.22097","title":"Few-Shot Graph Out-of-Distribution Detection with LLMs","abstract":"  Existing methods for graph out-of-distribution (OOD) detection typically\ndepend on training graph neural network (GNN) classifiers using a substantial\namount of labeled in-distribution (ID) data. However, acquiring high-quality\nlabeled nodes in text-attributed graphs (TAGs) is challenging and costly due to\ntheir complex textual and structural characteristics. Large language models\n(LLMs), known for their powerful zero-shot capabilities in textual tasks, show\npromise but struggle to naturally capture the critical structural information\ninherent to TAGs, limiting their direct effectiveness.\n  To address these challenges, we propose LLM-GOOD, a general framework that\neffectively combines the strengths of LLMs and GNNs to enhance data efficiency\nin graph OOD detection. Specifically, we first leverage LLMs' strong zero-shot\ncapabilities to filter out likely OOD nodes, significantly reducing the human\nannotation burden. To minimize the usage and cost of the LLM, we employ it only\nto annotate a small subset of unlabeled nodes. We then train a lightweight GNN\nfilter using these noisy labels, enabling efficient predictions of ID status\nfor all other unlabeled nodes by leveraging both textual and structural\ninformation. After obtaining node embeddings from the GNN filter, we can apply\ninformativeness-based methods to select the most valuable nodes for precise\nhuman annotation. Finally, we train the target ID classifier using these\naccurately annotated ID nodes. Extensive experiments on four real-world TAG\ndatasets demonstrate that LLM-GOOD significantly reduces human annotation costs\nand outperforms state-of-the-art baselines in terms of both ID classification\naccuracy and OOD detection performance.\n","date":"2025-03-28"}
{"id":"2503.22115","title":"Beyond Single-Sentence Prompts: Upgrading Value Alignment Benchmarks\n  with Dialogues and Stories","abstract":"  Evaluating the value alignment of large language models (LLMs) has\ntraditionally relied on single-sentence adversarial prompts, which directly\nprobe models with ethically sensitive or controversial questions. However, with\nthe rapid advancements in AI safety techniques, models have become increasingly\nadept at circumventing these straightforward tests, limiting their\neffectiveness in revealing underlying biases and ethical stances. To address\nthis limitation, we propose an upgraded value alignment benchmark that moves\nbeyond single-sentence prompts by incorporating multi-turn dialogues and\nnarrative-based scenarios. This approach enhances the stealth and adversarial\nnature of the evaluation, making it more robust against superficial safeguards\nimplemented in modern LLMs. We design and implement a dataset that includes\nconversational traps and ethically ambiguous storytelling, systematically\nassessing LLMs' responses in more nuanced and context-rich settings.\nExperimental results demonstrate that this enhanced methodology can effectively\nexpose latent biases that remain undetected in traditional single-shot\nevaluations. Our findings highlight the necessity of contextual and dynamic\ntesting for value alignment in LLMs, paving the way for more sophisticated and\nrealistic assessments of AI ethics and safety.\n","date":"2025-03-28"}
{"id":"2503.22118","title":"Estimating City-wide operating mode Distribution of Light-Duty Vehicles:\n  A Neural Network-based Approach","abstract":"  Driving cycles are a set of driving conditions and are crucial for the\nexisting emission estimation model to evaluate vehicle performance, fuel\nefficiency, and emissions, by matching them with average speed to calculate the\noperating modes, such as braking, idling, and cruising. While existing emission\nestimation models, such as the Motor Vehicle Emission Simulator (MOVES), are\npowerful tools, their reliance on predefined driving cycles can be limiting, as\nthese cycles often do not accurately represent regional driving conditions,\nmaking the models less effective for city-wide analyses. To solve this problem,\nthis paper proposes a modular neural network (NN)-based framework to estimate\noperating mode distributions bypassing the driving cycle development phase,\nutilizing macroscopic variables such as speed, flow, and link infrastructure\nattributes. The proposed method is validated using a well-calibrated\nmicrosimulation model of Brookline MA, the United States. The results indicate\nthat the proposed framework outperforms the operating mode distribution\ncalculated by MOVES based on default driving cycles, providing a closer match\nto the actual operating mode distribution derived from trajectory data.\nSpecifically, the proposed model achieves an average RMSE of 0.04 in predicting\noperating mode distribution, compared to 0.08 for MOVES. The average error in\nemission estimation across pollutants is 8.57% for the proposed method, lower\nthan the 32.86% error for MOVES. In particular, for the estimation of CO2, the\nproposed method has an error of just 4%, compared to 35% for MOVES. The\nproposed model can be utilized for real-time emissions monitoring by providing\nrapid and accurate emissions estimates with easily accessible inputs.\n","date":"2025-03-28"}
{"id":"2503.22119","title":"Multimodal Machine Learning for Real Estate Appraisal: A Comprehensive\n  Survey","abstract":"  Real estate appraisal has undergone a significant transition from manual to\nautomated valuation and is entering a new phase of evolution. Leveraging\ncomprehensive attention to various data sources, a novel approach to automated\nvaluation, multimodal machine learning, has taken shape. This approach\nintegrates multimodal data to deeply explore the diverse factors influencing\nhousing prices. Furthermore, multimodal machine learning significantly\noutperforms single-modality or fewer-modality approaches in terms of prediction\naccuracy, with enhanced interpretability. However, systematic and comprehensive\nsurvey work on the application in the real estate domain is still lacking. In\nthis survey, we aim to bridge this gap by reviewing the research efforts. We\nbegin by reviewing the background of real estate appraisal and propose two\nresearch questions from the perspecve of performance and fusion aimed at\nimproving the accuracy of appraisal results. Subsequently, we explain the\nconcept of multimodal machine learning and provide a comprehensive\nclassification and definition of modalities used in real estate appraisal for\nthe first time. To ensure clarity, we explore works related to data and\ntechniques, along with their evaluation methods, under the framework of these\ntwo research questions. Furthermore, specific application domains are\nsummarized. Finally, we present insights into future research directions\nincluding multimodal complementarity, technology and modality contribution.\n","date":"2025-03-28"}
{"id":"2503.22120","title":"Camera Model Identification with SPAIR-Swin and Entropy based\n  Non-Homogeneous Patches","abstract":"  Source camera model identification (SCMI) plays a pivotal role in image\nforensics with applications including authenticity verification and copyright\nprotection. For identifying the camera model used to capture a given image, we\npropose SPAIR-Swin, a novel model combining a modified spatial attention\nmechanism and inverted residual block (SPAIR) with a Swin Transformer.\nSPAIR-Swin effectively captures both global and local features, enabling robust\nidentification of artifacts such as noise patterns that are particularly\neffective for SCMI. Additionally, unlike conventional methods focusing on\nhomogeneous patches, we propose a patch selection strategy for SCMI that\nemphasizes high-entropy regions rich in patterns and textures. Extensive\nevaluations on four benchmark SCMI datasets demonstrate that SPAIR-Swin\noutperforms existing methods, achieving patch-level accuracies of 99.45%,\n98.39%, 99.45%, and 97.46% and image-level accuracies of 99.87%, 99.32%, 100%,\nand 98.61% on the Dresden, Vision, Forchheim, and Socrates datasets,\nrespectively. Our findings highlight that high-entropy patches, which contain\nhigh-frequency information such as edge sharpness, noise, and compression\nartifacts, are more favorable in improving SCMI accuracy. Code will be made\navailable upon request.\n","date":"2025-03-28"}
{"id":"2503.22121","title":"Detecting Localized Deepfake Manipulations Using Action Unit-Guided\n  Video Representations","abstract":"  With rapid advancements in generative modeling, deepfake techniques are\nincreasingly narrowing the gap between real and synthetic videos, raising\nserious privacy and security concerns. Beyond traditional face swapping and\nreenactment, an emerging trend in recent state-of-the-art deepfake generation\nmethods involves localized edits such as subtle manipulations of specific\nfacial features like raising eyebrows, altering eye shapes, or modifying mouth\nexpressions. These fine-grained manipulations pose a significant challenge for\nexisting detection models, which struggle to capture such localized variations.\nTo the best of our knowledge, this work presents the first detection approach\nexplicitly designed to generalize to localized edits in deepfake videos by\nleveraging spatiotemporal representations guided by facial action units. Our\nmethod leverages a cross-attention-based fusion of representations learned from\npretext tasks like random masking and action unit detection, to create an\nembedding that effectively encodes subtle, localized changes. Comprehensive\nevaluations across multiple deepfake generation methods demonstrate that our\napproach, despite being trained solely on the traditional FF+ dataset, sets a\nnew benchmark in detecting recent deepfake-generated videos with fine-grained\nlocal edits, achieving a $20\\%$ improvement in accuracy over current\nstate-of-the-art detection methods. Additionally, our method delivers\ncompetitive performance on standard datasets, highlighting its robustness and\ngeneralization across diverse types of local and global forgeries.\n","date":"2025-03-28"}
{"id":"2503.22122","title":"REMAC: Self-Reflective and Self-Evolving Multi-Agent Collaboration for\n  Long-Horizon Robot Manipulation","abstract":"  Vision-language models (VLMs) have demonstrated remarkable capabilities in\nrobotic planning, particularly for long-horizon tasks that require a holistic\nunderstanding of the environment for task decomposition. Existing methods\ntypically rely on prior environmental knowledge or carefully designed\ntask-specific prompts, making them struggle with dynamic scene changes or\nunexpected task conditions, e.g., a robot attempting to put a carrot in the\nmicrowave but finds the door was closed. Such challenges underscore two\ncritical issues: adaptability and efficiency. To address them, in this work, we\npropose an adaptive multi-agent planning framework, termed REMAC, that enables\nefficient, scene-agnostic multi-robot long-horizon task planning and execution\nthrough continuous reflection and self-evolution. REMAC incorporates two key\nmodules: a self-reflection module performing pre-condition and post-condition\nchecks in the loop to evaluate progress and refine plans, and a self-evolvement\nmodule dynamically adapting plans based on scene-specific reasoning. It offers\nseveral appealing benefits: 1) Robots can initially explore and reason about\nthe environment without complex prompt design. 2) Robots can keep reflecting on\npotential planning errors and adapting the plan based on task-specific\ninsights. 3) After iterations, a robot can call another one to coordinate tasks\nin parallel, maximizing the task execution efficiency. To validate REMAC's\neffectiveness, we build a multi-agent environment for long-horizon robot\nmanipulation and navigation based on RoboCasa, featuring 4 task categories with\n27 task styles and 50+ different objects. Based on it, we further benchmark\nstate-of-the-art reasoning models, including DeepSeek-R1, o3-mini, QwQ, and\nGrok3, demonstrating REMAC's superiority by boosting average success rates by\n40% and execution efficiency by 52.7% over the single robot baseline.\n","date":"2025-03-28"}
{"id":"2503.22125","title":"Semantic segmentation for building houses from wooden cubes","abstract":"  Automated construction is one of the most promising areas that can improve\nefficiency, reduce costs and minimize errors in the process of building\nconstruction. In this paper, a comparative analysis of three neural network\nmodels for semantic segmentation, U-Net(light), LinkNet and PSPNet, is\nperformed. Two specialized datasets with images of houses built from wooden\ncubes were created for the experiments. The first dataset contains 4 classes\n(background, foundation, walls, roof ) and is designed for basic model\nevaluation, while the second dataset includes 44 classes where each cube is\nlabeled as a separate object. The models were trained with the same\nhyperparameters and their accuracy was evaluated using MeanIoU and F1 Score\nmetrics. According to the results obtained, U-Net(light) showed the best\nperformance with 78% MeanIoU and 87% F1 Score on the first dataset and 17% and\n25% respectively on the second dataset. The poor results on the second dataset\nare due to the limited amount of data, the complexity of the partitioning and\nthe imbalance of classes, making it difficult to accurately select individual\ncubes. In addition, overtraining was observed in all experiments, manifested by\nhigh accuracy on the training dataset and its significant decrease on the\nvalidation dataset. The present work is the basis for the development of\nalgorithms for automatic generation of staged building plans, which can be\nfurther scaled to design complete buildings. Future research is planned to\nextend the datasets and apply methods to combat overfitting (L1\/L2\nregularization, Early Stopping). The next stage of work will be the development\nof algorithms for automatic generation of a step-by-step plan for building\nhouses from cubes using manipulators. Index Terms-Deep Learning, Computer\nvision, CNN, Semantic segmentation, Construction materials.\n","date":"2025-03-28"}
{"id":"2503.22132","title":"Long-Term Electricity Demand Prediction Using Non-negative Tensor\n  Factorization and Genetic Algorithm-Driven Temporal Modeling","abstract":"  This study proposes a novel framework for long-term electricity demand\nprediction based solely on historical consumption data, without relying on\nexternal variables such as temperature or economic indicators. The method\ncombines Non-negative Tensor Factorization (NTF) to extract low-dimensional\ntemporal features from multi-way electricity usage data, with a Genetic\nAlgorithm that optimizes the hyperparameters of time series models applied to\nthe latent annual factors. We model the dataset as a third-order tensor\nspanning electric utilities, industrial sectors, and years, and apply canonical\npolyadic decomposition under non-negativity constraints. The annual component\nis forecasted using autoregressive models, with hyperparameter tuning guided by\nthe prediction error or reconstruction accuracy on a validation set.\nComparative experiments using real-world electricity data from Japan\ndemonstrate that the proposed method achieves lower mean squared error than\nbaseline approaches without tensor decomposition or evolutionary optimization.\nMoreover, we find that reducing the model's degrees of freedom via tensor\ndecomposition improves generalization performance, and that initialization\nsensitivity in NTF can be mitigated through multiple runs or ensemble\nstrategies. These findings suggest that the proposed framework offers an\ninterpretable, flexible, and scalable approach to long-term electricity demand\nprediction and can be extended to other structured time series forecasting\ntasks.\n","date":"2025-03-28"}
{"id":"2503.22135","title":"Convolutional optimization with convex kernel and power lift","abstract":"  We focus on establishing the foundational paradigm of a novel optimization\ntheory based on convolution with convex kernels. Our goal is to devise a\nmorally deterministic model of locating the global optima of an arbitrary\nfunction, which is distinguished from most commonly used statistical models.\nLimited preliminary numerical results are provided to test the efficiency of\nsome specific algorithms derived from our paradigm, which we hope to stimulate\nfurther practical interest.\n","date":"2025-03-28"}
{"id":"2503.22136","title":"Beyond Background Shift: Rethinking Instance Replay in Continual\n  Semantic Segmentation","abstract":"  In this work, we focus on continual semantic segmentation (CSS), where\nsegmentation networks are required to continuously learn new classes without\nerasing knowledge of previously learned ones. Although storing images of old\nclasses and directly incorporating them into the training of new models has\nproven effective in mitigating catastrophic forgetting in classification tasks,\nthis strategy presents notable limitations in CSS. Specifically, the stored and\nnew images with partial category annotations leads to confusion between\nunannotated categories and the background, complicating model fitting. To\ntackle this issue, this paper proposes a novel Enhanced Instance Replay (EIR)\nmethod, which not only preserves knowledge of old classes while simultaneously\neliminating background confusion by instance storage of old classes, but also\nmitigates background shifts in the new images by integrating stored instances\nwith new images. By effectively resolving background shifts in both stored and\nnew images, EIR alleviates catastrophic forgetting in the CSS task, thereby\nenhancing the model's capacity for CSS. Experimental results validate the\nefficacy of our approach, which significantly outperforms state-of-the-art CSS\nmethods.\n","date":"2025-03-28"}
{"id":"2503.22137","title":"Sharpe Ratio-Guided Active Learning for Preference Optimization in RLHF","abstract":"  Reinforcement learning from human feedback (RLHF) has become a cornerstone of\nthe training and alignment pipeline for large language models (LLMs). Recent\nadvances, such as direct preference optimization (DPO), have simplified the\npreference learning step. However, collecting preference data remains a\nchallenging and costly process, often requiring expert annotation. This cost\ncan be mitigated by carefully selecting the data points presented for\nannotation. In this work, we propose an active learning approach to efficiently\nselect prompt and preference pairs using a risk assessment strategy based on\nthe Sharpe Ratio. To address the challenge of unknown preferences prior to\nannotation, our method evaluates the gradients of all potential preference\nannotations to assess their impact on model updates. These gradient-based\nevaluations enable risk assessment of data points regardless of the annotation\noutcome. By leveraging the DPO loss derivations, we derive a closed-form\nexpression for computing these Sharpe ratios on a per-tuple basis, ensuring our\napproach remains both tractable and computationally efficient. We also\nintroduce two variants of our method, each making different assumptions about\nprior information. Experimental results demonstrate that our method outperforms\nthe baseline by up to 5% in win rates against the chosen completion with\nlimited human preference data across several language models and real-world\ndatasets.\n","date":"2025-03-28"}
{"id":"2503.22138","title":"Enhancing Dance-to-Music Generation via Negative Conditioning Latent\n  Diffusion Model","abstract":"  Conditional diffusion models have gained increasing attention since their\nimpressive results for cross-modal synthesis, where the strong alignment\nbetween conditioning input and generated output can be achieved by training a\ntime-conditioned U-Net augmented with cross-attention mechanism. In this paper,\nwe focus on the problem of generating music synchronized with rhythmic visual\ncues of the given dance video. Considering that bi-directional guidance is more\nbeneficial for training a diffusion model, we propose to enhance the quality of\ngenerated music and its synchronization with dance videos by adopting both\npositive rhythmic information and negative ones (PN-Diffusion) as conditions,\nwhere a dual diffusion and reverse processes is devised. Specifically, to train\na sequential multi-modal U-Net structure, PN-Diffusion consists of a noise\nprediction objective for positive conditioning and an additional noise\nprediction objective for negative conditioning. To accurately define and select\nboth positive and negative conditioning, we ingeniously utilize temporal\ncorrelations in dance videos, capturing positive and negative rhythmic cues by\nplaying them forward and backward, respectively. Through subjective and\nobjective evaluations of input-output correspondence in terms of dance-music\nbeat alignment and the quality of generated music, experimental results on the\nAIST++ and TikTok dance video datasets demonstrate that our model outperforms\nSOTA dance-to-music generation models.\n","date":"2025-03-28"}
{"id":"2503.22139","title":"Time-resolved dynamic CBCT reconstruction using prior-model-free\n  spatiotemporal Gaussian representation (PMF-STGR)","abstract":"  Time-resolved CBCT imaging, which reconstructs a dynamic sequence of CBCTs\nreflecting intra-scan motion (one CBCT per x-ray projection without phase\nsorting or binning), is highly desired for regular and irregular motion\ncharacterization, patient setup, and motion-adapted radiotherapy. Representing\npatient anatomy and associated motion fields as 3D Gaussians, we developed a\nGaussian representation-based framework (PMF-STGR) for fast and accurate\ndynamic CBCT reconstruction. PMF-STGR comprises three major components: a dense\nset of 3D Gaussians to reconstruct a reference-frame CBCT for the dynamic\nsequence; another 3D Gaussian set to capture three-level, coarse-to-fine\nmotion-basis-components (MBCs) to model the intra-scan motion; and a CNN-based\nmotion encoder to solve projection-specific temporal coefficients for the MBCs.\nScaled by the temporal coefficients, the learned MBCs will combine into\ndeformation vector fields to deform the reference CBCT into\nprojection-specific, time-resolved CBCTs to capture the dynamic motion. Due to\nthe strong representation power of 3D Gaussians, PMF-STGR can reconstruct\ndynamic CBCTs in a 'one-shot' training fashion from a standard 3D CBCT scan,\nwithout using any prior anatomical or motion model. We evaluated PMF-STGR using\nXCAT phantom simulations and real patient scans. Metrics including the image\nrelative error, structural-similarity-index-measure, tumor\ncenter-of-mass-error, and landmark localization error were used to evaluate the\naccuracy of solved dynamic CBCTs and motion. PMF-STGR shows clear advantages\nover a state-of-the-art, INR-based approach, PMF-STINR. Compared with\nPMF-STINR, PMF-STGR reduces reconstruction time by 50% while reconstructing\nless blurred images with better motion accuracy. With improved efficiency and\naccuracy, PMF-STGR enhances the applicability of dynamic CBCT imaging for\npotential clinical translation.\n","date":"2025-03-28"}
{"id":"2503.22140","title":"Score-Based Turbo Message Passing for Plug-and-Play Compressive Image\n  Recovery","abstract":"  Message passing algorithms have been tailored for compressive imaging\napplications by plugging in different types of off-the-shelf image denoisers.\nThese off-the-shelf denoisers mostly rely on some generic or hand-crafted\npriors for denoising. Due to their insufficient accuracy in capturing the true\nimage prior, these methods often fail to produce satisfactory results,\nespecially in largely underdetermined scenarios. On the other hand, score-based\ngenerative modeling offers a promising way to accurately characterize the\nsophisticated image distribution. In this paper, by exploiting the close\nrelation between score-based modeling and empirical Bayes-optimal denoising, we\ndevise a message passing framework that integrates a score-based minimum mean\nsquared error (MMSE) denoiser for compressive image recovery. This framework is\nfirmly rooted in Bayesian formalism, in which state evolution (SE) equations\naccurately predict its asymptotic performance. Experiments on the FFHQ dataset\ndemonstrate that our method strikes a significantly better\nperformance-complexity tradeoff than conventional message passing, regularized\nlinear regression, and score-based posterior sampling baselines. Remarkably,\nour method typically requires less than 20 neural function evaluations (NFEs)\nto converge.\n","date":"2025-03-28"}
{"id":"2503.22141","title":"Integrating Artificial Intelligence with Human Expertise: An In-depth\n  Analysis of ChatGPT's Capabilities in Generating Metamorphic Relations","abstract":"  Context: This paper provides an in-depth examination of the generation and\nevaluation of Metamorphic Relations (MRs) using GPT models developed by OpenAI,\nwith a particular focus on the capabilities of GPT-4 in software testing\nenvironments.\n  Objective: The aim is to examine the quality of MRs produced by GPT-3.5 and\nGPT-4 for a specific System Under Test (SUT) adopted from an earlier study, and\nto introduce and apply an improved set of evaluation criteria for a diverse\nrange of SUTs.\n  Method: The initial phase evaluates MRs generated by GPT-3.5 and GPT-4 using\ncriteria from a prior study, followed by an application of an enhanced\nevaluation framework on MRs created by GPT-4 for a diverse range of nine SUTs,\nvarying from simple programs to complex systems incorporating AI\/ML components.\nA custom-built GPT evaluator, alongside human evaluators, assessed the MRs,\nenabling a direct comparison between automated and human evaluation methods.\n  Results: The study finds that GPT-4 outperforms GPT-3.5 in generating\naccurate and useful MRs. With the advanced evaluation criteria, GPT-4\ndemonstrates a significant ability to produce high-quality MRs across a wide\nrange of SUTs, including complex systems incorporating AI\/ML components.\n  Conclusions: GPT-4 exhibits advanced capabilities in generating MRs suitable\nfor various applications. The research underscores the growing potential of AI\nin software testing, particularly in the generation and evaluation of MRs, and\npoints towards the complementarity of human and AI skills in this domain.\n","date":"2025-03-28"}
{"id":"2503.22143","title":"A Self-Supervised Learning of a Foundation Model for Analog Layout\n  Design Automation","abstract":"  We propose a UNet-based foundation model and its self-supervised learning\nmethod to address two key challenges: 1) lack of qualified annotated analog\nlayout data, and 2) excessive variety in analog layout design tasks. For\nself-supervised learning, we propose random patch sampling and random masking\ntechniques automatically to obtain enough training data from a small\nunannotated layout dataset. The obtained data are greatly augmented, less\nbiased, equally sized, and contain enough information for excessive varieties\nof qualified layout patterns. By pre-training with the obtained data, the\nproposed foundation model can learn implicit general knowledge on layout\npatterns so that it can be fine-tuned for various downstream layout tasks with\nsmall task-specific datasets. Fine-tuning provides an efficient and\nconsolidated methodology for diverse downstream tasks, reducing the enormous\nhuman effort to develop a model per task separately. In experiments, the\nfoundation model was pre-trained using 324,000 samples obtained from 6\nsilicon-proved manually designed analog circuits, then it was fine-tuned for\nthe five example downstream tasks: generating contacts, vias, dummy fingers,\nN-wells, and metal routings. The fine-tuned models successfully performed these\ntasks for more than one thousand unseen layout inputs, generating DRC\/LVS-clean\nlayouts for 96.6% of samples. Compared with training the model from scratch for\nthe metal routing task, fine-tuning required only 1\/8 of the data to achieve\nthe same dice score of 0.95. With the same data, fine-tuning achieved a 90%\nlower validation loss and a 40% higher benchmark score than training from\nscratch.\n","date":"2025-03-28"}
{"id":"2503.22144","title":"FRASE: Structured Representations for Generalizable SPARQL Query\n  Generation","abstract":"  Translating natural language questions into SPARQL queries enables Knowledge\nBase querying for factual and up-to-date responses. However, existing datasets\nfor this task are predominantly template-based, leading models to learn\nsuperficial mappings between question and query templates rather than\ndeveloping true generalization capabilities. As a result, models struggle when\nencountering naturally phrased, template-free questions. This paper introduces\nFRASE (FRAme-based Semantic Enhancement), a novel approach that leverages Frame\nSemantic Role Labeling (FSRL) to address this limitation. We also present\nLC-QuAD 3.0, a new dataset derived from LC-QuAD 2.0, in which each question is\nenriched using FRASE through frame detection and the mapping of frame-elements\nto their argument. We evaluate the impact of this approach through extensive\nexperiments on recent large language models (LLMs) under different fine-tuning\nconfigurations. Our results demonstrate that integrating frame-based structured\nrepresentations consistently improves SPARQL generation performance,\nparticularly in challenging generalization scenarios when test questions\nfeature unseen templates (unknown template splits) and when they are all\nnaturally phrased (reformulated questions).\n","date":"2025-03-28"}
{"id":"2503.22145","title":"Tokenization of Gaze Data","abstract":"  A considerable part of the performance of today's large language models\n(LLM's) and multimodal large language models (MLLM's) depends on their\ntokenization strategies. While tokenizers are extensively researched for\ntextual and visual input, there is no research on tokenization strategies for\ngaze data due to its nature. However, a corresponding tokenization strategy\nwould allow using the vision capabilities of pre-trained MLLM's for gaze data,\nfor example, through fine-tuning.\n  In this paper, we aim to close this research gap by analyzing five different\ntokenizers for gaze data on three different datasets for the forecasting and\ngeneration of gaze data through LLMs (cf.~\\cref{fig:teaser}). We evaluate the\ntokenizers regarding their reconstruction and compression abilities. Further,\nwe train an LLM for each tokenization strategy, measuring its generative and\npredictive performance. Overall, we found that a quantile tokenizer outperforms\nall others in predicting the gaze positions and k-means is best when predicting\ngaze velocities.\n","date":"2025-03-28"}
{"id":"2503.22147","title":"Characterizing Non-Markovian Dynamics of Open Quantum Systems","abstract":"  Characterizing non-Markovian quantum dynamics is essential for accurately\nmodeling open quantum systems, particularly in near-term quantum technologies.\nIn this work, we develop a structure-preserving approach to characterizing\nnon-Markovian evolution using the time-convolutionless (TCL) master equation,\nconsidering both linear and nonlinear formulations. To parameterize the master\nequation, we explore two distinct techniques: the Karhunen-Loeve (KL)\nexpansion, which provides an optimal basis representation of the dynamics, and\nneural networks, which offer a data-driven approach to learning\nsystem-environment interactions. We demonstrate our methodology using\nexperimental data from a superconducting qubit at the Quantum Device\nIntegration Testbed (QuDIT) at Lawrence Livermore National Laboratory (LLNL).\nOur results show that while neural networks can capture complex dependencies,\nthe KL expansion yields the most accurate predictions of the qubit's\nnon-Markovian dynamics, highlighting its effectiveness in structure-preserving\nquantum system characterization. These findings provide valuable insights into\nefficient modeling strategies for open quantum systems, with implications for\nquantum control and error mitigation in near-term quantum processors.\n","date":"2025-03-28"}
{"id":"2503.22151","title":"When Autonomy Breaks: The Hidden Existential Risk of AI","abstract":"  AI risks are typically framed around physical threats to humanity, a loss of\ncontrol or an accidental error causing humanity's extinction. However, I argue\nin line with the gradual disempowerment thesis, that there is an\nunderappreciated risk in the slow and irrevocable decline of human autonomy. As\nAI starts to outcompete humans in various areas of life, a tipping point will\nbe reached where it no longer makes sense to rely on human decision-making,\ncreativity, social care or even leadership.\n  What may follow is a process of gradual de-skilling, where we lose skills\nthat we currently take for granted. Traditionally, it is argued that AI will\ngain human skills over time, and that these skills are innate and immutable in\nhumans. By contrast, I argue that humans may lose such skills as critical\nthinking, decision-making and even social care in an AGI world. The biggest\nthreat to humanity is therefore not that machines will become more like humans,\nbut that humans will become more like machines.\n","date":"2025-03-28"}
{"id":"2503.22152","title":"EgoToM: Benchmarking Theory of Mind Reasoning from Egocentric Videos","abstract":"  We introduce EgoToM, a new video question-answering benchmark that extends\nTheory-of-Mind (ToM) evaluation to egocentric domains. Using a causal ToM\nmodel, we generate multi-choice video QA instances for the Ego4D dataset to\nbenchmark the ability to predict a camera wearer's goals, beliefs, and next\nactions. We study the performance of both humans and state of the art\nmultimodal large language models (MLLMs) on these three interconnected\ninference problems. Our evaluation shows that MLLMs achieve close to\nhuman-level accuracy on inferring goals from egocentric videos. However, MLLMs\n(including the largest ones we tested with over 100B parameters) fall short of\nhuman performance when inferring the camera wearers' in-the-moment belief\nstates and future actions that are most consistent with the unseen video\nfuture. We believe that our results will shape the future design of an\nimportant class of egocentric digital assistants which are equipped with a\nreasonable model of the user's internal mental states.\n","date":"2025-03-28"}
{"id":"2503.22154","title":"Permutation-Invariant and Orientation-Aware Dataset Distillation for 3D\n  Point Clouds","abstract":"  We should collect large amount of data to train deep neural networks for\nvarious applications. Recently, the dataset distillation for images and texts\nhas been attracting a lot of attention, that reduces the original dataset to a\nsynthetic dataset while preserving essential task-relevant information.\nHowever, 3D point clouds distillation is almost unexplored due to the\nchallenges of unordered structures of points. In this paper, we propose a novel\ndistribution matching-based dataset distillation method for 3D point clouds\nthat jointly optimizes the geometric structures of synthetic dataset as well as\nthe orientations of synthetic models. To ensure the consistent feature\nalignment between different 3D point cloud models, we devise a permutation\ninvariant distribution matching loss with the sorted feature vectors. We also\nemploy learnable rotation angles to transform each syntheic model according to\nthe optimal orientation best representing the original feature distribution.\nExtensive experimental results on widely used four benchmark datasets,\nincluding ModelNet10, ModelNet40, ShapeNet, and ScanObjectNN, demonstrate that\nthe proposed method consistently outperforms the existing methods.\n","date":"2025-03-28"}
{"id":"2503.22159","title":"Disentangled 4D Gaussian Splatting: Towards Faster and More Efficient\n  Dynamic Scene Rendering","abstract":"  Novel-view synthesis (NVS) for dynamic scenes from 2D images presents\nsignificant challenges due to the spatial complexity and temporal variability\nof such scenes. Recently, inspired by the remarkable success of NVS using 3D\nGaussian Splatting (3DGS), researchers have sought to extend 3D Gaussian models\nto four dimensions (4D) for dynamic novel-view synthesis. However, methods\nbased on 4D rotation and scaling introduce spatiotemporal deformation into the\n4D covariance matrix, necessitating the slicing of 4D Gaussians into 3D\nGaussians. This process increases redundant computations as timestamps\nchange-an inherent characteristic of dynamic scene rendering. Additionally,\nperforming calculations on a four-dimensional matrix is computationally\nintensive. In this paper, we introduce Disentangled 4D Gaussian Splatting\n(Disentangled4DGS), a novel representation and rendering approach that\ndisentangles temporal and spatial deformations, thereby eliminating the\nreliance on 4D matrix computations. We extend the 3DGS rendering process to 4D,\nenabling the projection of temporal and spatial deformations into dynamic 2D\nGaussians in ray space. Consequently, our method facilitates faster dynamic\nscene synthesis. Moreover, it reduces storage requirements by at least 4.5\\%\ndue to our efficient presentation method. Our approach achieves an\nunprecedented average rendering speed of 343 FPS at a resolution of\n$1352\\times1014$ on an RTX 3090 GPU, with experiments across multiple\nbenchmarks demonstrating its competitive performance in both monocular and\nmulti-view scenarios.\n","date":"2025-03-28"}
{"id":"2503.22163","title":"T-CIL: Temperature Scaling using Adversarial Perturbation for\n  Calibration in Class-Incremental Learning","abstract":"  We study model confidence calibration in class-incremental learning, where\nmodels learn from sequential tasks with different class sets. While existing\nworks primarily focus on accuracy, maintaining calibrated confidence has been\nlargely overlooked. Unfortunately, most post-hoc calibration techniques are not\ndesigned to work with the limited memories of old-task data typical in\nclass-incremental learning, as retaining a sufficient validation set would be\nimpractical. Thus, we propose T-CIL, a novel temperature scaling approach for\nclass-incremental learning without a validation set for old tasks, that\nleverages adversarially perturbed exemplars from memory. Directly using\nexemplars is inadequate for temperature optimization, since they are already\nused for training. The key idea of T-CIL is to perturb exemplars more strongly\nfor old tasks than for the new task by adjusting the perturbation direction\nbased on feature distance, with the single magnitude determined using the\nnew-task validation set. This strategy makes the perturbation magnitude\ncomputed from the new task also applicable to old tasks, leveraging the\ntendency that the accuracy of old tasks is lower than that of the new task. We\nempirically show that T-CIL significantly outperforms various baselines in\nterms of calibration on real datasets and can be integrated with existing\nclass-incremental learning techniques with minimal impact on accuracy.\n","date":"2025-03-28"}
{"id":"2503.22164","title":"PharmAgents: Building a Virtual Pharma with Large Language Model Agents","abstract":"  The discovery of novel small molecule drugs remains a critical scientific\nchallenge with far-reaching implications for treating diseases and advancing\nhuman health. Traditional drug development--especially for small molecule\ntherapeutics--is a highly complex, resource-intensive, and time-consuming\nprocess that requires multidisciplinary collaboration. Recent breakthroughs in\nartificial intelligence (AI), particularly the rise of large language models\n(LLMs), present a transformative opportunity to streamline and accelerate this\nprocess. In this paper, we introduce PharmAgents, a virtual pharmaceutical\necosystem driven by LLM-based multi-agent collaboration. PharmAgents simulates\nthe full drug discovery workflow--from target discovery to preclinical\nevaluation--by integrating explainable, LLM-driven agents equipped with\nspecialized machine learning models and computational tools. Through structured\nknowledge exchange and automated optimization, PharmAgents identifies potential\ntherapeutic targets, discovers promising lead compounds, enhances binding\naffinity and key molecular properties, and performs in silico analyses of\ntoxicity and synthetic feasibility. Additionally, the system supports\ninterpretability, agent interaction, and self-evolvement, enabling it to refine\nfuture drug designs based on prior experience. By showcasing the potential of\nLLM-powered multi-agent systems in drug discovery, this work establishes a new\nparadigm for autonomous, explainable, and scalable pharmaceutical research,\nwith future extensions toward comprehensive drug lifecycle management.\n","date":"2025-03-28"}
{"id":"2503.22165","title":"Landscape of Thoughts: Visualizing the Reasoning Process of Large\n  Language Models","abstract":"  Numerous applications of large language models (LLMs) rely on their ability\nto perform step-by-step reasoning. However, the reasoning behavior of LLMs\nremains poorly understood, posing challenges to research, development, and\nsafety. To address this gap, we introduce landscape of thoughts-the first\nvisualization tool for users to inspect the reasoning paths of chain-of-thought\nand its derivatives on any multi-choice dataset. Specifically, we represent the\nstates in a reasoning path as feature vectors that quantify their distances to\nall answer choices. These features are then visualized in two-dimensional plots\nusing t-SNE. Qualitative and quantitative analysis with the landscape of\nthoughts effectively distinguishes between strong and weak models, correct and\nincorrect answers, as well as different reasoning tasks. It also uncovers\nundesirable reasoning patterns, such as low consistency and high uncertainty.\nAdditionally, users can adapt our tool to a model that predicts the property\nthey observe. We showcase this advantage by adapting our tool to a lightweight\nverifier that evaluates the correctness of reasoning paths. The code is\npublicly available at: https:\/\/github.com\/tmlr-group\/landscape-of-thoughts.\n","date":"2025-03-28"}
{"id":"2503.22166","title":"Reasoning of Large Language Models over Knowledge Graphs with\n  Super-Relations","abstract":"  While large language models (LLMs) have made significant progress in\nprocessing and reasoning over knowledge graphs, current methods suffer from a\nhigh non-retrieval rate. This limitation reduces the accuracy of answering\nquestions based on these graphs. Our analysis reveals that the combination of\ngreedy search and forward reasoning is a major contributor to this issue. To\novercome these challenges, we introduce the concept of super-relations, which\nenables both forward and backward reasoning by summarizing and connecting\nvarious relational paths within the graph. This holistic approach not only\nexpands the search space, but also significantly improves retrieval efficiency.\nIn this paper, we propose the ReKnoS framework, which aims to Reason over\nKnowledge Graphs with Super-Relations. Our framework's key advantages include\nthe inclusion of multiple relation paths through super-relations, enhanced\nforward and backward reasoning capabilities, and increased efficiency in\nquerying LLMs. These enhancements collectively lead to a substantial\nimprovement in the successful retrieval rate and overall reasoning performance.\nWe conduct extensive experiments on nine real-world datasets to evaluate\nReKnoS, and the results demonstrate the superior performance of ReKnoS over\nexisting state-of-the-art baselines, with an average accuracy gain of 2.92%.\n","date":"2025-03-28"}
{"id":"2503.22168","title":"Spatial Transport Optimization by Repositioning Attention Map for\n  Training-Free Text-to-Image Synthesis","abstract":"  Diffusion-based text-to-image (T2I) models have recently excelled in\nhigh-quality image generation, particularly in a training-free manner, enabling\ncost-effective adaptability and generalization across diverse tasks. However,\nwhile the existing methods have been continuously focusing on several\nchallenges, such as \"missing objects\" and \"mismatched attributes,\" another\ncritical issue of \"mislocated objects\" remains where generated spatial\npositions fail to align with text prompts. Surprisingly, ensuring such\nseemingly basic functionality remains challenging in popular T2I models due to\nthe inherent difficulty of imposing explicit spatial guidance via text forms.\nTo address this, we propose STORM (Spatial Transport Optimization by\nRepositioning Attention Map), a novel training-free approach for spatially\ncoherent T2I synthesis. STORM employs Spatial Transport Optimization (STO),\nrooted in optimal transport theory, to dynamically adjust object attention maps\nfor precise spatial adherence, supported by a Spatial Transport (ST) Cost\nfunction that enhances spatial understanding. Our analysis shows that\nintegrating spatial awareness is most effective in the early denoising stages,\nwhile later phases refine details. Extensive experiments demonstrate that STORM\nsurpasses existing methods, effectively mitigating mislocated objects while\nimproving missing and mismatched attributes, setting a new benchmark for\nspatial alignment in T2I synthesis.\n","date":"2025-03-28"}
{"id":"2503.22171","title":"An Empirical Study of Validating Synthetic Data for Text-Based Person\n  Retrieval","abstract":"  Data plays a pivotal role in Text-Based Person Retrieval (TBPR) research.\nMainstream research paradigm necessitates real-world person images with manual\ntextual annotations for training models, posing privacy-sensitive and\nlabor-intensive issues. Several pioneering efforts explore synthetic data for\nTBPR but still rely on real data, keeping the aforementioned issues and also\nresulting in diversity-deficient issue in synthetic datasets, thus impacting\nTBPR performance. Moreover, these works tend to explore synthetic data for TBPR\nthrough limited perspectives, leading to exploration-restricted issue. In this\npaper, we conduct an empirical study to explore the potential of synthetic data\nfor TBPR, highlighting three key aspects. (1) We propose an inter-class image\ngeneration pipeline, in which an automatic prompt construction strategy is\nintroduced to guide generative Artificial Intelligence (AI) models in\ngenerating various inter-class images without reliance on original data. (2) We\ndevelop an intra-class image augmentation pipeline, in which the generative AI\nmodels are applied to further edit the images for obtaining various intra-class\nimages. (3) Building upon the proposed pipelines and an automatic text\ngeneration pipeline, we explore the effectiveness of synthetic data in diverse\nscenarios through extensive experiments. Additionally, we experimentally\ninvestigate various noise-robust learning strategies to mitigate the inherent\nnoise in synthetic data. We will release the code, along with the synthetic\nlarge-scale dataset generated by our pipelines, which are expected to advance\npractical TBPR research.\n","date":"2025-03-28"}
{"id":"2503.22172","title":"Concept-Aware LoRA for Domain-Aligned Segmentation Dataset Generation","abstract":"  This paper addresses the challenge of data scarcity in semantic segmentation\nby generating datasets through text-to-image (T2I) generation models, reducing\nimage acquisition and labeling costs. Segmentation dataset generation faces two\nkey challenges: 1) aligning generated samples with the target domain and 2)\nproducing informative samples beyond the training data. Fine-tuning T2I models\ncan help generate samples aligned with the target domain. However, it often\noverfits and memorizes training data, limiting their ability to generate\ndiverse and well-aligned samples. To overcome these issues, we propose\nConcept-Aware LoRA (CA-LoRA), a novel fine-tuning approach that selectively\nidentifies and updates only the weights associated with necessary concepts\n(e.g., style or viewpoint) for domain alignment while preserving the pretrained\nknowledge of the T2I model to produce informative samples. We demonstrate its\neffectiveness in generating datasets for urban-scene segmentation,\noutperforming baseline and state-of-the-art methods in in-domain (few-shot and\nfully-supervised) settings, as well as in domain generalization tasks,\nespecially under challenging conditions such as adverse weather and varying\nillumination, further highlighting its superiority.\n","date":"2025-03-28"}
{"id":"2503.22174","title":"Synergistic Bleeding Region and Point Detection in Surgical Videos","abstract":"  Intraoperative bleeding in laparoscopic surgery causes rapid obscuration of\nthe operative field to hinder the surgical process. Intelligent detection of\nbleeding regions can quantify the blood loss to assist decision-making, while\nlocating the bleeding point helps surgeons quickly identify the source of\nbleeding and achieve hemostasis in time. In this study, we first construct a\nreal-world surgical bleeding detection dataset, named SurgBlood, comprising\n5,330 frames from 95 surgical video clips with bleeding region and point\nannotations. Accordingly, we develop a dual-task synergistic online detector\ncalled BlooDet, designed to perform simultaneous detection of bleeding regions\nand points in surgical videos. Our framework embraces a dual-branch\nbidirectional guidance design based on Segment Anything Model 2 (SAM 2). The\nmask branch detects bleeding regions through adaptive edge and point prompt\nembeddings, while the point branch leverages mask memory to induce bleeding\npoint memory modeling and captures the direction of bleed point movement\nthrough inter-frame optical flow. By interactive guidance and prompts, the two\nbranches explore potential spatial-temporal relationships while leveraging\nmemory modeling from previous frames to infer the current bleeding condition.\nExtensive experiments demonstrate that our approach outperforms other\ncounterparts on SurgBlood in both bleeding region and point detection tasks,\ne.g., achieving 64.88% IoU for bleeding region detection and 83.69% PCK-10% for\nbleeding point detection.\n","date":"2025-03-28"}
{"id":"2503.22175","title":"Efficient Continual Learning through Frequency Decomposition and\n  Integration","abstract":"  Continual learning (CL) aims to learn new tasks while retaining past\nknowledge, addressing the challenge of forgetting during task adaptation.\nRehearsal-based methods, which replay previous samples, effectively mitigate\nforgetting. However, research on enhancing the efficiency of these methods,\nespecially in resource-constrained environments, remains limited, hindering\ntheir application in real-world systems with dynamic data streams. The human\nperceptual system processes visual scenes through complementary frequency\nchannels: low-frequency signals capture holistic cues, while high-frequency\ncomponents convey structural details vital for fine-grained discrimination.\nInspired by this, we propose the Frequency Decomposition and Integration\nNetwork (FDINet), a novel framework that decomposes and integrates information\nacross frequencies. FDINet designs two lightweight networks to independently\nprocess low- and high-frequency components of images. When integrated with\nrehearsal-based methods, this frequency-aware design effectively enhances\ncross-task generalization through low-frequency information, preserves\nclass-specific details using high-frequency information, and facilitates\nefficient training due to its lightweight architecture. Experiments demonstrate\nthat FDINet reduces backbone parameters by 78%, improves accuracy by up to\n7.49% over state-of-the-art (SOTA) methods, and decreases peak memory usage by\nup to 80%. Additionally, on edge devices, FDINet accelerates training by up to\n5$\\times$.\n","date":"2025-03-28"}
{"id":"2503.22176","title":"A Multi-Site Study on AI-Driven Pathology Detection and Osteoarthritis\n  Grading from Knee X-Ray","abstract":"  Introduction: Bone health disorders like osteoarthritis and osteoporosis pose\nmajor global health challenges, often leading to delayed diagnoses due to\nlimited diagnostic tools. This study presents an AI-powered system that\nanalyzes knee X-rays to detect key pathologies, including joint space\nnarrowing, sclerosis, osteophytes, tibial spikes, alignment issues, and soft\ntissue anomalies. It also grades osteoarthritis severity, enabling timely,\npersonalized treatment.\n  Study Design: The research used 1.3 million knee X-rays from a multi-site\nIndian clinical trial across government, private, and SME hospitals. The\ndataset ensured diversity in demographics, imaging equipment, and clinical\nsettings. Rigorous annotation and preprocessing yielded high-quality training\ndatasets for pathology-specific models like ResNet15 for joint space narrowing\nand DenseNet for osteoarthritis grading.\n  Performance: The AI system achieved strong diagnostic accuracy across diverse\nimaging environments. Pathology-specific models excelled in precision, recall,\nand NPV, validated using Mean Squared Error (MSE), Intersection over Union\n(IoU), and Dice coefficient. Subgroup analyses across age, gender, and\nmanufacturer variations confirmed generalizability for real-world applications.\n  Conclusion: This scalable, cost-effective solution for bone health\ndiagnostics demonstrated robust performance in a multi-site trial. It holds\npromise for widespread adoption, especially in resource-limited healthcare\nsettings, transforming bone health management and enabling proactive patient\ncare.\n","date":"2025-03-28"}
{"id":"2503.22177","title":"3D Acetabular Surface Reconstruction from 2D Pre-operative X-ray Images\n  using SRVF Elastic Registration and Deformation Graph","abstract":"  Accurate and reliable selection of the appropriate acetabular cup size is\ncrucial for restoring joint biomechanics in total hip arthroplasty (THA). This\npaper proposes a novel framework that integrates square-root velocity function\n(SRVF)-based elastic shape registration technique with an embedded deformation\n(ED) graph approach to reconstruct the 3D articular surface of the acetabulum\nby fusing multiple views of 2D pre-operative pelvic X-ray images and a\nhemispherical surface model. The SRVF-based elastic registration establishes\n2D-3D correspondences between the parametric hemispherical model and X-ray\nimages, and the ED framework incorporates the SRVF-derived correspondences as\nconstraints to optimize the 3D acetabular surface reconstruction using\nnonlinear least-squares optimization. Validations using both simulation and\nreal patient datasets are performed to demonstrate the robustness and the\npotential clinical value of the proposed algorithm. The reconstruction result\ncan assist surgeons in selecting the correct acetabular cup on the first\nattempt in primary THA, minimising the need for revision surgery.\n","date":"2025-03-28"}
{"id":"2503.22178","title":"AdaRank: Adaptive Rank Pruning for Enhanced Model Merging","abstract":"  Model merging has emerged as a promising approach for unifying independently\nfine-tuned models into an integrated framework, significantly enhancing\ncomputational efficiency in multi-task learning. Recently, several SVD-based\ntechniques have been introduced to exploit low-rank structures for enhanced\nmerging, but their reliance on such manually designed rank selection often\nleads to cross-task interference and suboptimal performance. In this paper, we\npropose AdaRank, a novel model merging framework that adaptively selects the\nmost beneficial singular directions of task vectors to merge multiple models.\nWe empirically show that the dominant singular components of task vectors can\ncause critical interference with other tasks, and that naive truncation across\ntasks and layers degrades performance. In contrast, AdaRank dynamically prunes\nthe singular components that cause interference and offers an optimal amount of\ninformation to each task vector by learning to prune ranks during test-time via\nentropy minimization. Our analysis demonstrates that such method mitigates\ndetrimental overlaps among tasks, while empirical results show that AdaRank\nconsistently achieves state-of-the-art performance with various backbones and\nnumber of tasks, reducing the performance gap between fine-tuned models to\nnearly 1%.\n","date":"2025-03-28"}
{"id":"2503.22179","title":"High-Fidelity Diffusion Face Swapping with ID-Constrained Facial\n  Conditioning","abstract":"  Face swapping aims to seamlessly transfer a source facial identity onto a\ntarget while preserving target attributes such as pose and expression.\nDiffusion models, known for their superior generative capabilities, have\nrecently shown promise in advancing face-swapping quality. This paper addresses\ntwo key challenges in diffusion-based face swapping: the prioritized\npreservation of identity over target attributes and the inherent conflict\nbetween identity and attribute conditioning. To tackle these issues, we\nintroduce an identity-constrained attribute-tuning framework for face swapping\nthat first ensures identity preservation and then fine-tunes for attribute\nalignment, achieved through a decoupled condition injection. We further enhance\nfidelity by incorporating identity and adversarial losses in a post-training\nrefinement stage. Our proposed identity-constrained diffusion-based\nface-swapping model outperforms existing methods in both qualitative and\nquantitative evaluations, demonstrating superior identity similarity and\nattribute consistency, achieving a new state-of-the-art performance in\nhigh-fidelity face swapping.\n","date":"2025-03-28"}
{"id":"2503.22180","title":"Knowledge Rectification for Camouflaged Object Detection: Unlocking\n  Insights from Low-Quality Data","abstract":"  Low-quality data often suffer from insufficient image details, introducing an\nextra implicit aspect of camouflage that complicates camouflaged object\ndetection (COD). Existing COD methods focus primarily on high-quality data,\noverlooking the challenges posed by low-quality data, which leads to\nsignificant performance degradation. Therefore, we propose KRNet, the first\nframework explicitly designed for COD on low-quality data. KRNet presents a\nLeader-Follower framework where the Leader extracts dual gold-standard\ndistributions: conditional and hybrid, from high-quality data to drive the\nFollower in rectifying knowledge learned from low-quality data. The framework\nfurther benefits from a cross-consistency strategy that improves the\nrectification of these distributions and a time-dependent conditional encoder\nthat enriches the distribution diversity. Extensive experiments on benchmark\ndatasets demonstrate that KRNet outperforms state-of-the-art COD methods and\nsuper-resolution-assisted COD approaches, proving its effectiveness in tackling\nthe challenges of low-quality data in COD.\n","date":"2025-03-28"}
{"id":"2503.22181","title":"e-person Architecture and Framework for Human-AI Co-adventure\n  Relationship","abstract":"  This paper proposes the e-person architecture for constructing a unified and\nincremental development of AI ethics. The e-person architecture takes the\nreduction of uncertainty through collaborative cognition and action with others\nas a unified basis for ethics. By classifying and defining uncertainty along\ntwo axes - (1) first, second, and third person perspectives, and (2) the\ndifficulty of inference based on the depth of information - we support the\ndevelopment of unified and incremental development of AI ethics. In addition,\nwe propose the e-person framework based on the free energy principle, which\nconsiders the reduction of uncertainty as a unifying principle of brain\nfunction, with the aim of implementing the e-person architecture, and we show\nour previous works and future challenges based on the proposed framework.\n","date":"2025-03-28"}
{"id":"2503.22182","title":"Sell It Before You Make It: Revolutionizing E-Commerce with Personalized\n  AI-Generated Items","abstract":"  E-commerce has revolutionized retail, yet its traditional workflows remain\ninefficient, with significant time and resource costs tied to product design\nand manufacturing inventory. This paper introduces a novel system deployed at\nAlibaba that leverages AI-generated items (AIGI) to address these challenges\nwith personalized text-to-image generation for e-commercial product design.\nAIGI enables an innovative business mode called \"sell it before you make it\",\nwhere merchants can design fashion items and generate photorealistic images\nwith digital models based on textual descriptions. Only when the items have\nreceived a certain number of orders, do the merchants start to produce them,\nwhich largely reduces reliance on physical prototypes and thus accelerates time\nto market. For such a promising application, we identify the underlying key\nscientific challenge, i.e., capturing the users' group-level personalized\npreferences towards multiple generated candidate images. To this end, we\npropose a Personalized Group-Level Preference Alignment Framework for Diffusion\nModels (i.e., PerFusion). We first design PerFusion Reward Model for user\npreference estimation with a feature-crossing-based personalized plug-in. Then\nwe develop PerFusion with a personalized adaptive network to model diverse\npreferences across users, and meanwhile derive the group-level preference\noptimization objective to capture the comparative behaviors among multiple\ncandidates. Both offline and online experiments demonstrate the effectiveness\nof our proposed algorithm. The AI-generated items have achieved over 13%\nrelative improvements for both click-through rate and conversion rate compared\nto their human-designed counterparts, validating the revolutionary potential of\nAI-generated items for e-commercial platforms.\n","date":"2025-03-28"}
{"id":"2503.22192","title":"An Advanced Ensemble Deep Learning Framework for Stock Price Prediction\n  Using VAE, Transformer, and LSTM Model","abstract":"  This research proposes a cutting-edge ensemble deep learning framework for\nstock price prediction by combining three advanced neural network\narchitectures: The particular areas of interest for the research include but\nare not limited to: Variational Autoencoder (VAE), Transformer, and Long\nShort-Term Memory (LSTM) networks. The presented framework is aimed to\nsubstantially utilize the advantages of each model which would allow for\nachieving the identification of both linear and non-linear relations in stock\nprice movements. To improve the accuracy of its predictions it uses rich set of\ntechnical indicators and it scales its predictors based on the current market\nsituation. By trying out the framework on several stock data sets, and\nbenchmarking the results against single models and conventional forecasting,\nthe ensemble method exhibits consistently high accuracy and reliability. The\nVAE is able to learn linear representation on high-dimensional data while the\nTransformer outstandingly perform in recognizing long-term patterns on the\nstock price data. LSTM, based on its characteristics of being a model that can\ndeal with sequences, brings additional improvements to the given framework,\nespecially regarding temporal dynamics and fluctuations. Combined, these\ncomponents provide exceptional directional performance and a very small\ndisparity in the predicted results. The present solution has given a probable\nconcept that can handle the inherent problem of stock price prediction with\nhigh reliability and scalability. Compared to the performance of individual\nproposals based on the neural network, as well as classical methods, the\nproposed ensemble framework demonstrates the advantages of combining different\narchitectures. It has a very important application in algorithmic trading, risk\nanalysis, and control and decision-making for finance professions and scholars.\n","date":"2025-03-28"}
{"id":"2503.22193","title":"Unbiased Max-Min Embedding Classification for Transductive Few-Shot\n  Learning: Clustering and Classification Are All You Need","abstract":"  Convolutional neural networks and supervised learning have achieved\nremarkable success in various fields but are limited by the need for large\nannotated datasets. Few-shot learning (FSL) addresses this limitation by\nenabling models to generalize from only a few labeled examples. Transductive\nfew-shot learning (TFSL) enhances FSL by leveraging both labeled and unlabeled\ndata, though it faces challenges like the hubness problem. To overcome these\nlimitations, we propose the Unbiased Max-Min Embedding Classification (UMMEC)\nMethod, which addresses the key challenges in few-shot learning through three\ninnovative contributions. First, we introduce a decentralized covariance matrix\nto mitigate the hubness problem, ensuring a more uniform distribution of\nembeddings. Second, our method combines local alignment and global uniformity\nthrough adaptive weighting and nonlinear transformation, balancing intra-class\nclustering with inter-class separation. Third, we employ a Variational Sinkhorn\nFew-Shot Classifier to optimize the distances between samples and class\nprototypes, enhancing classification accuracy and robustness. These combined\ninnovations allow the UMMEC method to achieve superior performance with minimal\nlabeled data. Our UMMEC method significantly improves classification\nperformance with minimal labeled data, advancing the state-of-the-art in TFSL.\n","date":"2025-03-28"}
{"id":"2503.22194","title":"ORIGEN: Zero-Shot 3D Orientation Grounding in Text-to-Image Generation","abstract":"  We introduce ORIGEN, the first zero-shot method for 3D orientation grounding\nin text-to-image generation across multiple objects and diverse categories.\nWhile previous work on spatial grounding in image generation has mainly focused\non 2D positioning, it lacks control over 3D orientation. To address this, we\npropose a reward-guided sampling approach using a pretrained discriminative\nmodel for 3D orientation estimation and a one-step text-to-image generative\nflow model. While gradient-ascent-based optimization is a natural choice for\nreward-based guidance, it struggles to maintain image realism. Instead, we\nadopt a sampling-based approach using Langevin dynamics, which extends gradient\nascent by simply injecting random noise--requiring just a single additional\nline of code. Additionally, we introduce adaptive time rescaling based on the\nreward function to accelerate convergence. Our experiments show that ORIGEN\noutperforms both training-based and test-time guidance methods across\nquantitative metrics and user studies.\n","date":"2025-03-28"}
{"id":"2503.22196","title":"EdgeInfinite: A Memory-Efficient Infinite-Context Transformer for Edge\n  Devices","abstract":"  Transformer-based large language models (LLMs) encounter challenges in\nprocessing long sequences on edge devices due to the quadratic complexity of\nattention mechanisms and growing memory demands from Key-Value (KV) cache.\nExisting KV cache optimizations struggle with irreversible token eviction in\nlong-output tasks, while alternative sequence modeling architectures prove\ncostly to adopt within established Transformer infrastructure. We present\nEdgeInfinite, a memory-efficient solution for infinite contexts that integrates\ncompressed memory into Transformer-based LLMs through a trainable memory-gating\nmodule. This approach maintains full compatibility with standard Transformer\narchitectures, requiring fine-tuning only a small part of parameters, and\nenables selective activation of the memory-gating module for long and short\ncontext task routing. The experimental result shows that EdgeInfinite achieves\ncomparable performance to baseline Transformer-based LLM on long context\nbenchmarks while optimizing memory consumption and time to first token.\n","date":"2025-03-28"}
{"id":"2503.22197","title":"Extremely Simple Out-of-distribution Detection for Audio-visual\n  Generalized Zero-shot Learning","abstract":"  Zero-shot Learning(ZSL) attains knowledge transfer from seen classes to\nunseen classes by exploring auxiliary category information, which is a\npromising yet difficult research topic. In this field, Audio-Visual Generalized\nZero-Shot Learning~(AV-GZSL) has aroused researchers' great interest in which\nintricate relations within triple modalities~(audio, video, and natural\nlanguage) render this task quite challenging but highly research-worthy.\nHowever, both existing embedding-based and generative-based AV-GZSL methods\ntend to suffer from domain shift problem a lot and we propose an extremely\nsimple Out-of-distribution~(OOD) detection based AV-GZSL method~(EZ-AVOOD) to\nfurther mitigate bias problem by differentiating seen and unseen samples at the\ninitial beginning. EZ-AVOOD accomplishes effective seen-unseen separation by\nexploiting the intrinsic discriminative information held in class-specific\nlogits and class-agnostic feature subspace without training an extra OOD\ndetector network. Followed by seen-unseen binary classification, we employ two\nexpert models to classify seen samples and unseen samples separately. Compared\nto existing state-of-the-art methods, our model achieves superior ZSL and GZSL\nperformances on three audio-visual datasets and becomes the new SOTA, which\ncomprehensively demonstrates the effectiveness of the proposed EZ-AVOOD.\n","date":"2025-03-28"}
{"id":"2503.22199","title":"Hyperspectral Adapter for Object Tracking based on Hyperspectral Video","abstract":"  Object tracking based on hyperspectral video attracts increasing attention to\nthe rich material and motion information in the hyperspectral videos. The\nprevailing hyperspectral methods adapt pretrained RGB-based object tracking\nnetworks for hyperspectral tasks by fine-tuning the entire network on\nhyperspectral datasets, which achieves impressive results in challenging\nscenarios. However, the performance of hyperspectral trackers is limited by the\nloss of spectral information during the transformation, and fine-tuning the\nentire pretrained network is inefficient for practical applications. To address\nthe issues, a new hyperspectral object tracking method, hyperspectral adapter\nfor tracking (HyA-T), is proposed in this work. The hyperspectral adapter for\nthe self-attention (HAS) and the hyperspectral adapter for the multilayer\nperceptron (HAM) are proposed to generate the adaption information and to\ntransfer the multi-head self-attention (MSA) module and the multilayer\nperceptron (MLP) in pretrained network for the hyperspectral object tracking\ntask by augmenting the adaption information into the calculation of the MSA and\nMLP. Additionally, the hyperspectral enhancement of input (HEI) is proposed to\naugment the original spectral information into the input of the tracking\nnetwork. The proposed methods extract spectral information directly from the\nhyperspectral images, which prevent the loss of the spectral information.\nMoreover, only the parameters in the proposed methods are fine-tuned, which is\nmore efficient than the existing methods. Extensive experiments were conducted\non four datasets with various spectral bands, verifing the effectiveness of the\nproposed methods. The HyA-T achieves state-of-the-art performance on all the\ndatasets.\n","date":"2025-03-28"}
{"id":"2503.22200","title":"Enhance Generation Quality of Flow Matching V2A Model via Multi-Step\n  CoT-Like Guidance and Combined Preference Optimization","abstract":"  Creating high-quality sound effects from videos and text prompts requires\nprecise alignment between visual and audio domains, both semantically and\ntemporally, along with step-by-step guidance for professional audio generation.\nHowever, current state-of-the-art video-guided audio generation models often\nfall short of producing high-quality audio for both general and specialized use\ncases. To address this challenge, we introduce a multi-stage, multi-modal,\nend-to-end generative framework with Chain-of-Thought-like (CoT-like) guidance\nlearning, termed Chain-of-Perform (CoP). First, we employ a transformer-based\nnetwork architecture designed to achieve CoP guidance, enabling the generation\nof both general and professional audio. Second, we implement a multi-stage\ntraining framework that follows step-by-step guidance to ensure the generation\nof high-quality sound effects. Third, we develop a CoP multi-modal dataset,\nguided by video, to support step-by-step sound effects generation. Evaluation\nresults highlight the advantages of the proposed multi-stage CoP generative\nframework compared to the state-of-the-art models on a variety of datasets,\nwith FAD 0.79 to 0.74 (+6.33%), CLIP 16.12 to 17.70 (+9.80%) on VGGSound,\nSI-SDR 1.98dB to 3.35dB (+69.19%), MOS 2.94 to 3.49(+18.71%) on PianoYT-2h, and\nSI-SDR 2.22dB to 3.21dB (+44.59%), MOS 3.07 to 3.42 (+11.40%) on Piano-10h.\n","date":"2025-03-28"}
{"id":"2503.22201","title":"Multi-modal Knowledge Distillation-based Human Trajectory Forecasting","abstract":"  Pedestrian trajectory forecasting is crucial in various applications such as\nautonomous driving and mobile robot navigation. In such applications,\ncamera-based perception enables the extraction of additional modalities (human\npose, text) to enhance prediction accuracy. Indeed, we find that textual\ndescriptions play a crucial role in integrating additional modalities into a\nunified understanding. However, online extraction of text requires the use of\nVLM, which may not be feasible for resource-constrained systems. To address\nthis challenge, we propose a multi-modal knowledge distillation framework: a\nstudent model with limited modality is distilled from a teacher model trained\nwith full range of modalities. The comprehensive knowledge of a teacher model\ntrained with trajectory, human pose, and text is distilled into a student model\nusing only trajectory or human pose as a sole supplement. In doing so, we\nseparately distill the core locomotion insights from intra-agent multi-modality\nand inter-agent interaction. Our generalizable framework is validated with two\nstate-of-the-art models across three datasets on both ego-view (JRDB, SIT) and\nBEV-view (ETH\/UCY) setups, utilizing both annotated and VLM-generated text\ncaptions. Distilled student models show consistent improvement in all\nprediction metrics for both full and instantaneous observations, improving up\nto ~13%. The code is available at https:\/\/github.com\/Jaewoo97\/KDTF.\n","date":"2025-03-28"}
{"id":"2503.22204","title":"Segment then Splat: A Unified Approach for 3D Open-Vocabulary\n  Segmentation based on Gaussian Splatting","abstract":"  Open-vocabulary querying in 3D space is crucial for enabling more intelligent\nperception in applications such as robotics, autonomous systems, and augmented\nreality. However, most existing methods rely on 2D pixel-level parsing, leading\nto multi-view inconsistencies and poor 3D object retrieval. Moreover, they are\nlimited to static scenes and struggle with dynamic scenes due to the\ncomplexities of motion modeling. In this paper, we propose Segment then Splat,\na 3D-aware open vocabulary segmentation approach for both static and dynamic\nscenes based on Gaussian Splatting. Segment then Splat reverses the long\nestablished approach of \"segmentation after reconstruction\" by dividing\nGaussians into distinct object sets before reconstruction. Once the\nreconstruction is complete, the scene is naturally segmented into individual\nobjects, achieving true 3D segmentation. This approach not only eliminates\nGaussian-object misalignment issues in dynamic scenes but also accelerates the\noptimization process, as it eliminates the need for learning a separate\nlanguage field. After optimization, a CLIP embedding is assigned to each object\nto enable open-vocabulary querying. Extensive experiments on various datasets\ndemonstrate the effectiveness of our proposed method in both static and dynamic\nscenarios.\n","date":"2025-03-28"}
{"id":"2503.22205","title":"Data-Free Universal Attack by Exploiting the Intrinsic Vulnerability of\n  Deep Models","abstract":"  Deep neural networks (DNNs) are susceptible to Universal Adversarial\nPerturbations (UAPs), which are instance agnostic perturbations that can\ndeceive a target model across a wide range of samples. Unlike instance-specific\nadversarial examples, UAPs present a greater challenge as they must generalize\nacross different samples and models. Generating UAPs typically requires access\nto numerous examples, which is a strong assumption in real-world tasks. In this\npaper, we propose a novel data-free method called Intrinsic UAP (IntriUAP), by\nexploiting the intrinsic vulnerabilities of deep models. We analyze a series of\npopular deep models composed of linear and nonlinear layers with a Lipschitz\nconstant of 1, revealing that the vulnerability of these models is\npredominantly influenced by their linear components. Based on this observation,\nwe leverage the ill-conditioned nature of the linear components by aligning the\nUAP with the right singular vectors corresponding to the maximum singular value\nof each linear layer. Remarkably, our method achieves highly competitive\nperformance in attacking popular image classification deep models without using\nany image samples. We also evaluate the black-box attack performance of our\nmethod, showing that it matches the state-of-the-art baseline for data-free\nmethods on models that conform to our theoretical framework. Beyond the\ndata-free assumption, IntriUAP also operates under a weaker assumption, where\nthe adversary only can access a few of the victim model's layers. Experiments\ndemonstrate that the attack success rate decreases by only 4% when the\nadversary has access to just 50% of the linear layers in the victim model.\n","date":"2025-03-28"}
{"id":"2503.22208","title":"DeepSound-V1: Start to Think Step-by-Step in the Audio Generation from\n  Videos","abstract":"  Currently, high-quality, synchronized audio is synthesized from video and\noptional text inputs using various multi-modal joint learning frameworks.\nHowever, the precise alignment between the visual and generated audio domains\nremains far from satisfactory. One key factor is the lack of sufficient\ntemporal and semantic alignment annotations in open-source video-audio and\ntext-audio benchmarks. Therefore, we propose a framework for audio generation\nfrom videos, leveraging the internal chain-of-thought (CoT) of a multi-modal\nlarge language model (MLLM) to enable step-by-step reasoning without requiring\nadditional annotations. Additionally, a corresponding multi-modal reasoning\ndataset is constructed to facilitate the learning of initial reasoning in audio\ngeneration. In the experiments, we demonstrate the effectiveness of the\nproposed framework in reducing misalignment (voice-over) in generated audio and\nachieving competitive performance compared to various state-of-the-art models.\nThe evaluation results show that the proposed method outperforms\nstate-of-the-art approaches across multiple metrics. Specifically, the F DP\naSST indicator is reduced by up to 10.07%, the F DP AN N s indicator by up to\n11.62%, and the F DV GG indicator by up to 38.61%. Furthermore, the IS\nindicator improves by up to 4.95%, the IB-score indicator increases by up to\n6.39%, and the DeSync indicator is reduced by up to 0.89%.\n","date":"2025-03-28"}
{"id":"2503.22209","title":"Intrinsic Image Decomposition for Robust Self-supervised Monocular Depth\n  Estimation on Reflective Surfaces","abstract":"  Self-supervised monocular depth estimation (SSMDE) has gained attention in\nthe field of deep learning as it estimates depth without requiring ground truth\ndepth maps. This approach typically uses a photometric consistency loss between\na synthesized image, generated from the estimated depth, and the original\nimage, thereby reducing the need for extensive dataset acquisition. However,\nthe conventional photometric consistency loss relies on the Lambertian\nassumption, which often leads to significant errors when dealing with\nreflective surfaces that deviate from this model. To address this limitation,\nwe propose a novel framework that incorporates intrinsic image decomposition\ninto SSMDE. Our method synergistically trains for both monocular depth\nestimation and intrinsic image decomposition. The accurate depth estimation\nfacilitates multi-image consistency for intrinsic image decomposition by\naligning different view coordinate systems, while the decomposition process\nidentifies reflective areas and excludes corrupted gradients from the depth\ntraining process. Furthermore, our framework introduces a pseudo-depth\ngeneration and knowledge distillation technique to further enhance the\nperformance of the student model across both reflective and non-reflective\nsurfaces. Comprehensive evaluations on multiple datasets show that our approach\nsignificantly outperforms existing SSMDE baselines in depth prediction,\nespecially on reflective surfaces.\n","date":"2025-03-28"}
{"id":"2503.22211","title":"Fuzzy Cluster-Aware Contrastive Clustering for Time Series","abstract":"  The rapid growth of unlabeled time series data, driven by the Internet of\nThings (IoT), poses significant challenges in uncovering underlying patterns.\nTraditional unsupervised clustering methods often fail to capture the complex\nnature of time series data. Recent deep learning-based clustering approaches,\nwhile effective, struggle with insufficient representation learning and the\nintegration of clustering objectives. To address these issues, we propose a\nfuzzy cluster-aware contrastive clustering framework (FCACC) that jointly\noptimizes representation learning and clustering.\n  Our approach introduces a novel three-view data augmentation strategy to\nenhance feature extraction by leveraging various characteristics of time series\ndata. Additionally, we propose a cluster-aware hard negative sample generation\nmechanism that dynamically constructs high-quality negative samples using\nclustering structure information, thereby improving the model's discriminative\nability.\n  By leveraging fuzzy clustering, FCACC dynamically generates cluster\nstructures to guide the contrastive learning process, resulting in more\naccurate clustering. Extensive experiments on 40 benchmark datasets show that\nFCACC outperforms the selected baseline methods (eight in total), providing an\neffective solution for unsupervised time series learning.\n","date":"2025-03-28"}
{"id":"2503.22214","title":"Interpretable Deep Learning Paradigm for Airborne Transient\n  Electromagnetic Inversion","abstract":"  The extraction of geoelectric structural information from airborne transient\nelectromagnetic(ATEM)data primarily involves data processing and inversion.\nConventional methods rely on empirical parameter selection, making it difficult\nto process complex field data with high noise levels. Additionally, inversion\ncomputations are time consuming and often suffer from multiple local minima.\nExisting deep learning-based approaches separate the data processing steps,\nwhere independently trained denoising networks struggle to ensure the\nreliability of subsequent inversions. Moreover, end to end networks lack\ninterpretability. To address these issues, we propose a unified and\ninterpretable deep learning inversion paradigm based on disentangled\nrepresentation learning. The network explicitly decomposes noisy data into\nnoise and signal factors, completing the entire data processing workflow based\non the signal factors while incorporating physical information for guidance.\nThis approach enhances the network's reliability and interpretability. The\ninversion results on field data demonstrate that our method can directly use\nnoisy data to accurately reconstruct the subsurface electrical structure.\nFurthermore, it effectively processes data severely affected by environmental\nnoise, which traditional methods struggle with, yielding improved lateral\nstructural resolution.\n","date":"2025-03-28"}
{"id":"2503.22215","title":"Learning to Instruct for Visual Instruction Tuning","abstract":"  We propose LIT, an advancement of visual instruction tuning (VIT). While VIT\nequips Multimodal LLMs (MLLMs) with promising multimodal capabilities, the\ncurrent design choices for VIT often result in overfitting and shortcut\nlearning, potentially degrading performance. This gap arises from an\noveremphasis on instruction-following abilities, while neglecting the proactive\nunderstanding of visual information. Inspired by this, LIT adopts a simple yet\neffective approach by incorporating the loss function into both the instruction\nand response sequences. It seamlessly expands the training data, and\nregularizes the MLLMs from overly relying on language priors. Based on this\nmerit, LIT achieves a significant relative improvement of up to 9% on\ncomprehensive multimodal benchmarks, requiring no additional training data and\nincurring negligible computational overhead. Surprisingly, LIT attains\nexceptional fundamental visual capabilities, yielding up to an 18% improvement\nin captioning performance, while simultaneously alleviating hallucination in\nMLLMs.\n","date":"2025-03-28"}
{"id":"2503.22218","title":"ABC-GS: Alignment-Based Controllable Style Transfer for 3D Gaussian\n  Splatting","abstract":"  3D scene stylization approaches based on Neural Radiance Fields (NeRF)\nachieve promising results by optimizing with Nearest Neighbor Feature Matching\n(NNFM) loss. However, NNFM loss does not consider global style information. In\naddition, the implicit representation of NeRF limits their fine-grained control\nover the resulting scenes. In this paper, we introduce ABC-GS, a novel\nframework based on 3D Gaussian Splatting to achieve high-quality 3D style\ntransfer. To this end, a controllable matching stage is designed to achieve\nprecise alignment between scene content and style features through segmentation\nmasks. Moreover, a style transfer loss function based on feature alignment is\nproposed to ensure that the outcomes of style transfer accurately reflect the\nglobal style of the reference image. Furthermore, the original geometric\ninformation of the scene is preserved with the depth loss and Gaussian\nregularization terms. Extensive experiments show that our ABC-GS provides\ncontrollability of style transfer and achieves stylization results that are\nmore faithfully aligned with the global style of the chosen artistic reference.\nOur homepage is available at https:\/\/vpx-ecnu.github.io\/ABC-GS-website.\n","date":"2025-03-28"}
{"id":"2503.22223","title":"DREMnet: An Interpretable Denoising Framework for Semi-Airborne\n  Transient Electromagnetic Signal","abstract":"  The semi-airborne transient electromagnetic method (SATEM) is capable of\nconducting rapid surveys over large-scale and hard-to-reach areas. However, the\nacquired signals are often contaminated by complex noise, which can compromise\nthe accuracy of subsequent inversion interpretations. Traditional denoising\ntechniques primarily rely on parameter selection strategies, which are\ninsufficient for processing field data in noisy environments. With the advent\nof deep learning, various neural networks have been employed for SATEM signal\ndenoising. However, existing deep learning methods typically use single-mapping\nlearning approaches that struggle to effectively separate signal from noise.\nThese methods capture only partial information and lack interpretability. To\novercome these limitations, we propose an interpretable decoupled\nrepresentation learning framework, termed DREMnet, that disentangles data into\ncontent and context factors, enabling robust and interpretable denoising in\ncomplex conditions. To address the limitations of CNN and Transformer\narchitectures, we utilize the RWKV architecture for data processing and\nintroduce the Contextual-WKV mechanism, which allows unidirectional WKV to\nperform bidirectional signal modeling. Our proposed Covering Embedding\ntechnique retains the strong local perception of convolutional networks through\nstacked embedding. Experimental results on test datasets demonstrate that the\nDREMnet method outperforms existing techniques, with processed field data that\nmore accurately reflects the theoretical signal, offering improved\nidentification of subsurface electrical structures.\n","date":"2025-03-28"}
{"id":"2503.22224","title":"Composite Indicator-Guided Infilling Sampling for Expensive\n  Multi-Objective Optimization","abstract":"  In expensive multi-objective optimization, where the evaluation budget is\nstrictly limited, selecting promising candidate solutions for expensive fitness\nevaluations is critical for accelerating convergence and improving algorithmic\nperformance. However, designing an optimization strategy that effectively\nbalances convergence, diversity, and distribution remains a challenge. To\ntackle this issue, we propose a composite indicator-based evolutionary\nalgorithm (CI-EMO) for expensive multi-objective optimization. In each\ngeneration of the optimization process, CI-EMO first employs NSGA-III to\nexplore the solution space based on fitness values predicted by surrogate\nmodels, generating a candidate population. Subsequently, we design a novel\ncomposite performance indicator to guide the selection of candidates for real\nfitness evaluation. This indicator simultaneously considers convergence,\ndiversity, and distribution to improve the efficiency of identifying promising\ncandidate solutions, which significantly improves algorithm performance. The\ncomposite indicator-based candidate selection strategy is easy to achieve and\ncomputes efficiency. Component analysis experiments confirm the effectiveness\nof each element in the composite performance indicator. Comparative experiments\non benchmark problems demonstrate that the proposed algorithm outperforms five\nstate-of-the-art expensive multi-objective optimization algorithms.\n","date":"2025-03-28"}
{"id":"2503.22225","title":"Follow Your Motion: A Generic Temporal Consistency Portrait Editing\n  Framework with Trajectory Guidance","abstract":"  Pre-trained conditional diffusion models have demonstrated remarkable\npotential in image editing. However, they often face challenges with temporal\nconsistency, particularly in the talking head domain, where continuous changes\nin facial expressions intensify the level of difficulty. These issues stem from\nthe independent editing of individual images and the inherent loss of temporal\ncontinuity during the editing process. In this paper, we introduce Follow Your\nMotion (FYM), a generic framework for maintaining temporal consistency in\nportrait editing. Specifically, given portrait images rendered by a pre-trained\n3D Gaussian Splatting model, we first develop a diffusion model that\nintuitively and inherently learns motion trajectory changes at different scales\nand pixel coordinates, from the first frame to each subsequent frame. This\napproach ensures that temporally inconsistent edited avatars inherit the motion\ninformation from the rendered avatars. Secondly, to maintain fine-grained\nexpression temporal consistency in talking head editing, we propose a dynamic\nre-weighted attention mechanism. This mechanism assigns higher weight\ncoefficients to landmark points in space and dynamically updates these weights\nbased on landmark loss, achieving more consistent and refined facial\nexpressions. Extensive experiments demonstrate that our method outperforms\nexisting approaches in terms of temporal consistency and can be used to\noptimize and compensate for temporally inconsistent outputs in a range of\napplications, such as text-driven editing, relighting, and various other\napplications.\n","date":"2025-03-28"}
{"id":"2503.22228","title":"MFH: A Multi-faceted Heuristic Algorithm Selection Approach for Software\n  Verification","abstract":"  Currently, many verification algorithms are available to improve the\nreliability of software systems. Selecting the appropriate verification\nalgorithm typically demands domain expertise and non-trivial manpower. An\nautomated algorithm selector is thus desired. However, existing selectors,\neither depend on machine-learned strategies or manually designed heuristics,\nencounter issues such as reliance on high-quality samples with algorithm labels\nand limited scalability. In this paper, an automated algorithm selection\napproach, namely MFH, is proposed for software verification. Our approach\nleverages the heuristics that verifiers producing correct results typically\nimplement certain appropriate algorithms, and the supported algorithms by these\nverifiers indirectly reflect which ones are potentially applicable.\nSpecifically, MFH embeds the code property graph (CPG) of a semantic-preserving\ntransformed program to enhance the robustness of the prediction model.\nFurthermore, our approach decomposes the selection task into the sub-tasks of\npredicting potentially applicable algorithms and matching the most appropriate\nverifiers. Additionally, MFH also introduces a feedback loop on incorrect\npredictions to improve model prediction accuracy. We evaluate MFH on 20\nverifiers and over 15,000 verification tasks. Experimental results demonstrate\nthe effectiveness of MFH, achieving a prediction accuracy of 91.47% even\nwithout ground truth algorithm labels provided during the training phase.\nMoreover, the prediction accuracy decreases only by 0.84% when introducing 10\nnew verifiers, indicating the strong scalability of the proposed approach.\n","date":"2025-03-28"}
{"id":"2503.22230","title":"Exploring Data Scaling Trends and Effects in Reinforcement Learning from\n  Human Feedback","abstract":"  Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning\nlarge language models with human preferences. While recent research has focused\non algorithmic improvements, the importance of prompt-data construction has\nbeen overlooked. This paper addresses this gap by exploring data-driven\nbottlenecks in RLHF performance scaling, particularly reward hacking and\ndecreasing response diversity. We introduce a hybrid reward system combining\nreasoning task verifiers (RTV) and a generative reward model (GenRM) to\nmitigate reward hacking. We also propose a novel prompt-selection method,\nPre-PPO, to maintain response diversity and enhance learning effectiveness.\nAdditionally, we find that prioritizing mathematical and coding tasks early in\nRLHF training significantly improves performance. Experiments across two model\nsizes validate our methods' effectiveness and scalability. Results show that\nRTV is most resistant to reward hacking, followed by GenRM with ground truth,\nand then GenRM with SFT Best-of-N responses. Our strategies enable rapid\ncapture of subtle task-specific distinctions, leading to substantial\nimprovements in overall RLHF performance. This work highlights the importance\nof careful data construction and provides practical methods to overcome\nperformance barriers in RLHF.\n","date":"2025-03-28"}
{"id":"2503.22231","title":"CoGen: 3D Consistent Video Generation via Adaptive Conditioning for\n  Autonomous Driving","abstract":"  Recent progress in driving video generation has shown significant potential\nfor enhancing self-driving systems by providing scalable and controllable\ntraining data. Although pretrained state-of-the-art generation models, guided\nby 2D layout conditions (e.g., HD maps and bounding boxes), can produce\nphotorealistic driving videos, achieving controllable multi-view videos with\nhigh 3D consistency remains a major challenge. To tackle this, we introduce a\nnovel spatial adaptive generation framework, CoGen, which leverages advances in\n3D generation to improve performance in two key aspects: (i) To ensure 3D\nconsistency, we first generate high-quality, controllable 3D conditions that\ncapture the geometry of driving scenes. By replacing coarse 2D conditions with\nthese fine-grained 3D representations, our approach significantly enhances the\nspatial consistency of the generated videos. (ii) Additionally, we introduce a\nconsistency adapter module to strengthen the robustness of the model to\nmulti-condition control. The results demonstrate that this method excels in\npreserving geometric fidelity and visual realism, offering a reliable video\ngeneration solution for autonomous driving.\n","date":"2025-03-28"}
{"id":"2503.22233","title":"Process Reward Modeling with Entropy-Driven Uncertainty","abstract":"  This paper presents the Entropy-Driven Unified Process Reward Model\n(EDU-PRM), a novel framework that approximates state-of-the-art performance in\nprocess supervision while drastically reducing training costs. EDU-PRM\nintroduces an entropy-guided dynamic step partitioning mechanism, using logit\ndistribution entropy to pinpoint high-uncertainty regions during token\ngeneration dynamically. This self-assessment capability enables precise\nstep-level feedback without manual fine-grained annotation, addressing a\ncritical challenge in process supervision. Experiments on the Qwen2.5-72B model\nwith only 7,500 EDU-PRM-generated training queries demonstrate accuracy closely\napproximating the full Qwen2.5-72B-PRM (71.1% vs. 71.6%), achieving a 98%\nreduction in query cost compared to prior methods. This work establishes\nEDU-PRM as an efficient approach for scalable process reward model training.\n","date":"2025-03-28"}
{"id":"2503.22235","title":"WeatherMesh-3: Fast and accurate operational global weather forecasting","abstract":"  We present WeatherMesh-3 (WM-3), an operational transformer-based global\nweather forecasting system that improves the state of the art in both accuracy\nand computational efficiency. We introduce the following advances: 1) a latent\nrollout that enables arbitrary-length predictions in latent space without\nintermediate encoding or decoding; and 2) a modular architecture that flexibly\nutilizes mixed-horizon processors and encodes multiple real-time analyses to\ncreate blended initial conditions. WM-3 generates 14-day global forecasts at\n0.25-degree resolution in 12 seconds on a single RTX 4090. This represents a\n>100,000-fold speedup over traditional NWP approaches while achieving superior\naccuracy with up to 37.7% improvement in RMSE over operational models,\nrequiring only a single consumer-grade GPU for deployment. We aim for WM-3 to\ndemocratize weather forecasting by providing an accessible, lightweight model\nfor operational use while pushing the performance boundaries of machine\nlearning-based weather prediction.\n","date":"2025-03-28"}
{"id":"2503.22236","title":"Hi3DGen: High-fidelity 3D Geometry Generation from Images via Normal\n  Bridging","abstract":"  With the growing demand for high-fidelity 3D models from 2D images, existing\nmethods still face significant challenges in accurately reproducing\nfine-grained geometric details due to limitations in domain gaps and inherent\nambiguities in RGB images. To address these issues, we propose Hi3DGen, a novel\nframework for generating high-fidelity 3D geometry from images via normal\nbridging. Hi3DGen consists of three key components: (1) an image-to-normal\nestimator that decouples the low-high frequency image pattern with noise\ninjection and dual-stream training to achieve generalizable, stable, and sharp\nestimation; (2) a normal-to-geometry learning approach that uses\nnormal-regularized latent diffusion learning to enhance 3D geometry generation\nfidelity; and (3) a 3D data synthesis pipeline that constructs a high-quality\ndataset to support training. Extensive experiments demonstrate the\neffectiveness and superiority of our framework in generating rich geometric\ndetails, outperforming state-of-the-art methods in terms of fidelity. Our work\nprovides a new direction for high-fidelity 3D geometry generation from images\nby leveraging normal maps as an intermediate representation.\n","date":"2025-03-28"}
{"id":"2503.22237","title":"SCHNet: SAM Marries CLIP for Human Parsing","abstract":"  Vision Foundation Model (VFM) such as the Segment Anything Model (SAM) and\nContrastive Language-Image Pre-training Model (CLIP) has shown promising\nperformance for segmentation and detection tasks. However, although SAM excels\nin fine-grained segmentation, it faces major challenges when applying it to\nsemantic-aware segmentation. While CLIP exhibits a strong semantic\nunderstanding capability via aligning the global features of language and\nvision, it has deficiencies in fine-grained segmentation tasks. Human parsing\nrequires to segment human bodies into constituent parts and involves both\naccurate fine-grained segmentation and high semantic understanding of each\npart. Based on traits of SAM and CLIP, we formulate high efficient modules to\neffectively integrate features of them to benefit human parsing. We propose a\nSemantic-Refinement Module to integrate semantic features of CLIP with SAM\nfeatures to benefit parsing. Moreover, we formulate a high efficient\nFine-tuning Module to adjust the pretrained SAM for human parsing that needs\nhigh semantic information and simultaneously demands spatial details, which\nsignificantly reduces the training time compared with full-time training and\nachieves notable performance. Extensive experiments demonstrate the\neffectiveness of our method on LIP, PPP, and CIHP databases.\n","date":"2025-03-28"}
{"id":"2503.22241","title":"Agent-Centric Personalized Multiple Clustering with Multi-Modal LLMs","abstract":"  Personalized multiple clustering aims to generate diverse partitions of a\ndataset based on different user-specific aspects, rather than a single\nclustering. It has recently drawn research interest for accommodating varying\nuser preferences. Recent approaches primarily use CLIP embeddings with proxy\nlearning to extract representations biased toward user clustering preferences.\nHowever, CLIP primarily focuses on coarse image-text alignment, lacking a deep\ncontextual understanding of user interests. To overcome these limitations, we\npropose an agent-centric personalized clustering framework that leverages\nmulti-modal large language models (MLLMs) as agents to comprehensively traverse\na relational graph to search for clusters based on user interests. Due to the\nadvanced reasoning mechanism of MLLMs, the obtained clusters align more closely\nwith user-defined criteria than those obtained from CLIP-based representations.\nTo reduce computational overhead, we shorten the agents' traversal path by\nconstructing a relational graph using user-interest-biased embeddings extracted\nby MLLMs. A large number of weakly connected edges can be filtered out based on\nembedding similarity, facilitating an efficient traversal search for agents.\nExperimental results show that the proposed method achieves NMI scores of\n0.9667 and 0.9481 on the Card Order and Card Suits benchmarks, respectively,\nlargely improving the SOTA model by over 140%.\n","date":"2025-03-28"}
{"id":"2503.22244","title":"Analysis of On-policy Policy Gradient Methods under the Distribution\n  Mismatch","abstract":"  Policy gradient methods are one of the most successful methods for solving\nchallenging reinforcement learning problems. However, despite their empirical\nsuccesses, many SOTA policy gradient algorithms for discounted problems deviate\nfrom the theoretical policy gradient theorem due to the existence of a\ndistribution mismatch. In this work, we analyze the impact of this mismatch on\nthe policy gradient methods. Specifically, we first show that in the case of\ntabular parameterizations, the methods under the mismatch remain globally\noptimal. Then, we extend this analysis to more general parameterizations by\nleveraging the theory of biased stochastic gradient descent. Our findings offer\nnew insights into the robustness of policy gradient methods as well as the gap\nbetween theoretical foundations and practical implementations.\n","date":"2025-03-28"}
{"id":"2503.22248","title":"CRLLK: Constrained Reinforcement Learning for Lane Keeping in Autonomous\n  Driving","abstract":"  Lane keeping in autonomous driving systems requires scenario-specific weight\ntuning for different objectives. We formulate lane-keeping as a constrained\nreinforcement learning problem, where weight coefficients are automatically\nlearned along with the policy, eliminating the need for scenario-specific\ntuning. Empirically, our approach outperforms traditional RL in efficiency and\nreliability. Additionally, real-world demonstrations validate its practical\nvalue for real-world autonomous driving.\n","date":"2025-03-28"}
{"id":"2503.22249","title":"FLAM: Foundation Model-Based Body Stabilization for Humanoid Locomotion\n  and Manipulation","abstract":"  Humanoid robots have attracted significant attention in recent years.\nReinforcement Learning (RL) is one of the main ways to control the whole body\nof humanoid robots. RL enables agents to complete tasks by learning from\nenvironment interactions, guided by task rewards. However, existing RL methods\nrarely explicitly consider the impact of body stability on humanoid locomotion\nand manipulation. Achieving high performance in whole-body control remains a\nchallenge for RL methods that rely solely on task rewards. In this paper, we\npropose a Foundation model-based method for humanoid Locomotion And\nManipulation (FLAM for short). FLAM integrates a stabilizing reward function\nwith a basic policy. The stabilizing reward function is designed to encourage\nthe robot to learn stable postures, thereby accelerating the learning process\nand facilitating task completion. Specifically, the robot pose is first mapped\nto the 3D virtual human model. Then, the human pose is stabilized and\nreconstructed through a human motion reconstruction model. Finally, the pose\nbefore and after reconstruction is used to compute the stabilizing reward. By\ncombining this stabilizing reward with the task reward, FLAM effectively guides\npolicy learning. Experimental results on a humanoid robot benchmark demonstrate\nthat FLAM outperforms state-of-the-art RL methods, highlighting its\neffectiveness in improving stability and overall performance.\n","date":"2025-03-28"}
{"id":"2503.22250","title":"Beyond the Script: Testing LLMs for Authentic Patient Communication\n  Styles in Healthcare","abstract":"  Effective patient communication is pivotal in healthcare, yet traditional\nmedical training often lacks exposure to diverse, challenging interpersonal\ndynamics. To bridge this gap, this study proposes the use of Large Language\nModels (LLMs) to simulate authentic patient communication styles, specifically\nthe \"accuser\" and \"rationalizer\" personas derived from the Satir model, while\nalso ensuring multilingual applicability to accommodate diverse cultural\ncontexts and enhance accessibility for medical professionals. Leveraging\nadvanced prompt engineering, including behavioral prompts, author's notes, and\nstubbornness mechanisms, we developed virtual patients (VPs) that embody\nnuanced emotional and conversational traits. Medical professionals evaluated\nthese VPs, rating their authenticity (accuser: $3.8 \\pm 1.0$; rationalizer:\n$3.7 \\pm 0.8$ on a 5-point Likert scale (from one to five)) and correctly\nidentifying their styles. Emotion analysis revealed distinct profiles: the\naccuser exhibited pain, anger, and distress, while the rationalizer displayed\ncontemplation and calmness, aligning with predefined, detailed patient\ndescription including medical history. Sentiment scores (on a scale from zero\nto nine) further validated these differences in the communication styles, with\nthe accuser adopting negative ($3.1 \\pm 0.6$) and the rationalizer more neutral\n($4.0 \\pm 0.4$) tone. These results underscore LLMs' capability to replicate\ncomplex communication styles, offering transformative potential for medical\neducation. This approach equips trainees to navigate challenging clinical\nscenarios by providing realistic, adaptable patient interactions, enhancing\nempathy and diagnostic acumen. Our findings advocate for AI-driven tools as\nscalable, cost-effective solutions to cultivate nuanced communication skills,\nsetting a foundation for future innovations in healthcare training.\n","date":"2025-03-28"}
{"id":"2503.22251","title":"Efficient Building Roof Type Classification: A Domain-Specific\n  Self-Supervised Approach","abstract":"  Accurate classification of building roof types from aerial imagery is crucial\nfor various remote sensing applications, including urban planning, disaster\nmanagement, and infrastructure monitoring. However, this task is often hindered\nby the limited availability of labeled data for supervised learning approaches.\nTo address this challenge, this paper investigates the effectiveness of self\nsupervised learning with EfficientNet architectures, known for their\ncomputational efficiency, for building roof type classification. We propose a\nnovel framework that incorporates a Convolutional Block Attention Module (CBAM)\nto enhance the feature extraction capabilities of EfficientNet. Furthermore, we\nexplore the benefits of pretraining on a domain-specific dataset, the Aerial\nImage Dataset (AID), compared to ImageNet pretraining. Our experimental results\ndemonstrate the superiority of our approach. Employing Simple Framework for\nContrastive Learning of Visual Representations (SimCLR) with EfficientNet-B3\nand CBAM achieves a 95.5% accuracy on our validation set, matching the\nperformance of state-of-the-art transformer-based models while utilizing\nsignificantly fewer parameters. We also provide a comprehensive evaluation on\ntwo challenging test sets, demonstrating the generalization capability of our\nmethod. Notably, our findings highlight the effectiveness of domain-specific\npretraining, consistently leading to higher accuracy compared to models\npretrained on the generic ImageNet dataset. Our work establishes EfficientNet\nbased self-supervised learning as a computationally efficient and highly\neffective approach for building roof type classification, particularly\nbeneficial in scenarios with limited labeled data.\n","date":"2025-03-28"}
{"id":"2503.22252","title":"Data-driven modeling of fluid flow around rotating structures with graph\n  neural networks","abstract":"  Graph neural networks, recently introduced into the field of fluid flow\nsurrogate modeling, have been successfully applied to model the temporal\nevolution of various fluid flow systems. Existing applications, however, are\nmostly restricted to cases where the domain is time-invariant. The present work\nextends the application of graph neural network-based modeling to fluid flow\naround structures rotating with respect to a certain axis. Specifically, we\npropose to apply a graph neural network-based surrogate modeling for fluid flow\nwith the mesh corotating with the structure. Unlike conventional data-driven\napproaches that rely on structured Cartesian meshes, our framework operates on\nunstructured co-rotating meshes, enforcing rotation equivariance of the learned\nmodel by leveraging co-rotating polar (2D) and cylindrical (3D) coordinate\nsystems. To model the pressure for systems without Dirichlet pressure\nboundaries, we propose a novel local directed pressure difference formulation\nthat is invariant to the reference pressure point and value. For flow systems\nwith large mesh sizes, we introduce a scheme to train the network in single or\ndistributed graphics processing units by accumulating the backpropagated\ngradients from partitions of the mesh. The effectiveness of our proposed\nframework is examined on two test cases: (i) fluid flow in a 2D rotating mixer,\nand (ii) the flow past a 3D rotating cube. Our results show that the model\nachieves stable and accurate rollouts for over 2000 time steps in periodic\nregimes while capturing accurate short-term dynamics in chaotic flow regimes.\nIn addition, the drag and lift force predictions closely match the CFD\ncalculations, highlighting the potential of the framework for modeling both\nperiodic and chaotic fluid flow around rotating structures.\n","date":"2025-03-28"}
{"id":"2503.22257","title":"DynaGraph: Interpretable Multi-Label Prediction from EHRs via Dynamic\n  Graph Learning and Contrastive Augmentation","abstract":"  Learning from longitudinal electronic health records is limited if it does\nnot capture the temporal trajectories of the patient's state in a clinical\nsetting. Graph models allow us to capture the hidden dependencies of the\nmultivariate time-series when the graphs are constructed in a similar dynamic\nmanner. Previous dynamic graph models require a pre-defined and\/or static graph\nstructure, which is unknown in most cases, or they only capture the spatial\nrelations between the features. Furthermore in healthcare, the interpretability\nof the model is an essential requirement to build trust with clinicians. In\naddition to previously proposed attention mechanisms, there has not been an\ninterpretable dynamic graph framework for data from multivariate electronic\nhealth records (EHRs). Here, we propose DynaGraph, an end-to-end interpretable\ncontrastive graph model that learns the dynamics of multivariate time-series\nEHRs as part of optimisation. We validate our model in four real-world clinical\ndatasets, ranging from primary care to secondary care settings with broad\ndemographics, in challenging settings where tasks are imbalanced and\nmulti-labelled. Compared to state-of-the-art models, DynaGraph achieves\nsignificant improvements in balanced accuracy and sensitivity over the nearest\ncomplex competitors in time-series or dynamic graph modelling across three ICU\nand one primary care datasets. Through a pseudo-attention approach to graph\nconstruction, our model also indicates the importance of clinical covariates\nover time, providing means for clinical validation.\n","date":"2025-03-28"}
{"id":"2503.22262","title":"Mono2Stereo: A Benchmark and Empirical Study for Stereo Conversion","abstract":"  With the rapid proliferation of 3D devices and the shortage of 3D content,\nstereo conversion is attracting increasing attention. Recent works introduce\npretrained Diffusion Models (DMs) into this task. However, due to the scarcity\nof large-scale training data and comprehensive benchmarks, the optimal\nmethodologies for employing DMs in stereo conversion and the accurate\nevaluation of stereo effects remain largely unexplored. In this work, we\nintroduce the Mono2Stereo dataset, providing high-quality training data and\nbenchmark to support in-depth exploration of stereo conversion. With this\ndataset, we conduct an empirical study that yields two primary findings. 1) The\ndifferences between the left and right views are subtle, yet existing metrics\nconsider overall pixels, failing to concentrate on regions critical to stereo\neffects. 2) Mainstream methods adopt either one-stage left-to-right generation\nor warp-and-inpaint pipeline, facing challenges of degraded stereo effect and\nimage distortion respectively. Based on these findings, we introduce a new\nevaluation metric, Stereo Intersection-over-Union, which prioritizes disparity\nand achieves a high correlation with human judgments on stereo effect.\nMoreover, we propose a strong baseline model, harmonizing the stereo effect and\nimage quality simultaneously, and notably surpassing current mainstream\nmethods. Our code and data will be open-sourced to promote further research in\nstereo conversion. Our models are available at mono2stereo-bench.github.io.\n","date":"2025-03-28"}
{"id":"2503.22263","title":"FLIP: Towards Comprehensive and Reliable Evaluation of Federated Prompt\n  Learning","abstract":"  The increasing emphasis on privacy and data security has driven the adoption\nof federated learning, a decentralized approach to train machine learning\nmodels without sharing raw data. Prompt learning, which fine-tunes prompt\nembeddings of pretrained models, offers significant advantages in federated\nsettings by reducing computational costs and communication overheads while\nleveraging the strong performance and generalization capabilities of\nvision-language models such as CLIP. This paper addresses the intersection of\nfederated learning and prompt learning, particularly for vision-language\nmodels. In this work, we introduce a comprehensive framework, named FLIP, to\nevaluate federated prompt learning algorithms. FLIP assesses the performance of\n8 state-of-the-art federated prompt learning methods across 4 federated\nlearning protocols and 12 open datasets, considering 6 distinct evaluation\nscenarios. Our findings demonstrate that prompt learning maintains strong\ngeneralization performance in both in-distribution and out-of-distribution\nsettings with minimal resource consumption. This work highlights the\neffectiveness of federated prompt learning in environments characterized by\ndata scarcity, unseen classes, and cross-domain distributional shifts. We\nopen-source the code for all implemented algorithms in FLIP to facilitate\nfurther research in this domain.\n","date":"2025-03-28"}
{"id":"2503.22265","title":"DeepAudio-V1:Towards Multi-Modal Multi-Stage End-to-End Video to Speech\n  and Audio Generation","abstract":"  Currently, high-quality, synchronized audio is synthesized using various\nmulti-modal joint learning frameworks, leveraging video and optional text\ninputs. In the video-to-audio benchmarks, video-to-audio quality, semantic\nalignment, and audio-visual synchronization are effectively achieved. However,\nin real-world scenarios, speech and audio often coexist in videos\nsimultaneously, and the end-to-end generation of synchronous speech and audio\ngiven video and text conditions are not well studied. Therefore, we propose an\nend-to-end multi-modal generation framework that simultaneously produces speech\nand audio based on video and text conditions. Furthermore, the advantages of\nvideo-to-audio (V2A) models for generating speech from videos remain unclear.\nThe proposed framework, DeepAudio, consists of a video-to-audio (V2A) module, a\ntext-to-speech (TTS) module, and a dynamic mixture of modality fusion (MoF)\nmodule. In the evaluation, the proposed end-to-end framework achieves\nstate-of-the-art performance on the video-audio benchmark, video-speech\nbenchmark, and text-speech benchmark. In detail, our framework achieves\ncomparable results in the comparison with state-of-the-art models for the\nvideo-audio and text-speech benchmarks, and surpassing state-of-the-art models\nin the video-speech benchmark, with WER 16.57% to 3.15% (+80.99%), SPK-SIM\n78.30% to 89.38% (+14.15%), EMO-SIM 66.24% to 75.56% (+14.07%), MCD 8.59 to\n7.98 (+7.10%), MCD SL 11.05 to 9.40 (+14.93%) across a variety of dubbing\nsettings.\n","date":"2025-03-28"}
{"id":"2503.22268","title":"Segment Any Motion in Videos","abstract":"  Moving object segmentation is a crucial task for achieving a high-level\nunderstanding of visual scenes and has numerous downstream applications. Humans\ncan effortlessly segment moving objects in videos. Previous work has largely\nrelied on optical flow to provide motion cues; however, this approach often\nresults in imperfect predictions due to challenges such as partial motion,\ncomplex deformations, motion blur and background distractions. We propose a\nnovel approach for moving object segmentation that combines long-range\ntrajectory motion cues with DINO-based semantic features and leverages SAM2 for\npixel-level mask densification through an iterative prompting strategy. Our\nmodel employs Spatio-Temporal Trajectory Attention and Motion-Semantic\nDecoupled Embedding to prioritize motion while integrating semantic support.\nExtensive testing on diverse datasets demonstrates state-of-the-art\nperformance, excelling in challenging scenarios and fine-grained segmentation\nof multiple objects. Our code is available at https:\/\/motion-seg.github.io\/.\n","date":"2025-03-28"}
{"id":"2503.22271","title":"Efficient Epistemic Uncertainty Estimation in Cerebrovascular\n  Segmentation","abstract":"  Brain vessel segmentation of MR scans is a critical step in the diagnosis of\ncerebrovascular diseases. Due to the fine vessel structure, manual vessel\nsegmentation is time consuming. Therefore, automatic deep learning (DL) based\nsegmentation techniques are intensively investigated. As conventional DL models\nyield a high complexity and lack an indication of decision reliability, they\nare often considered as not trustworthy. This work aims to increase trust in DL\nbased models by incorporating epistemic uncertainty quantification into\ncerebrovascular segmentation models for the first time. By implementing an\nefficient ensemble model combining the advantages of Bayesian Approximation and\nDeep Ensembles, we aim to overcome the high computational costs of conventional\nprobabilistic networks. Areas of high model uncertainty and erroneous\npredictions are aligned which demonstrates the effectiveness and reliability of\nthe approach. We perform extensive experiments applying the ensemble model on\nout-of-distribution (OOD) data. We demonstrate that for OOD-images, the\nestimated uncertainty increases. Additionally, omitting highly uncertain areas\nimproves the segmentation quality, both for in- and out-of-distribution data.\nThe ensemble model explains its limitations in a reliable manner and can\nmaintain trustworthiness also for OOD data and could be considered in clinical\napplications\n","date":"2025-03-28"}
{"id":"2503.22275","title":"Make Some Noise: Towards LLM audio reasoning and generation using sound\n  tokens","abstract":"  Integrating audio comprehension and generation into large language models\n(LLMs) remains challenging due to the continuous nature of audio and the\nresulting high sampling rates. Here, we introduce a novel approach that\ncombines Variational Quantization with Conditional Flow Matching to convert\naudio into ultra-low bitrate discrete tokens of 0.23kpbs, allowing for seamless\nintegration with text tokens in LLMs. We fine-tuned a pretrained text-based LLM\nusing Low-Rank Adaptation (LoRA) to assess its effectiveness in achieving true\nmultimodal capabilities, i.e., audio comprehension and generation. Our\ntokenizer outperforms a traditional VQ-VAE across various datasets with diverse\nacoustic events. Despite the substantial loss of fine-grained details through\naudio tokenization, our multimodal LLM trained with discrete tokens achieves\ncompetitive results in audio comprehension with state-of-the-art methods,\nthough audio generation is poor. Our results highlight the need for larger,\nmore diverse datasets and improved evaluation metrics to advance multimodal LLM\nperformance.\n","date":"2025-03-28"}
{"id":"2503.22276","title":"Machine Learning Models for Soil Parameter Prediction Based on\n  Satellite, Weather, Clay and Yield Data","abstract":"  Efficient nutrient management and precise fertilization are essential for\nadvancing modern agriculture, particularly in regions striving to optimize crop\nyields sustainably. The AgroLens project endeavors to address this challenge by\ndevelop ing Machine Learning (ML)-based methodologies to predict soil nutrient\nlevels without reliance on laboratory tests. By leveraging state of the art\ntechniques, the project lays a foundation for acionable insights to improve\nagricultural productivity in resource-constrained areas, such as Africa. The\napproach begins with the development of a robust European model using the LUCAS\nSoil dataset and Sentinel-2 satellite imagery to estimate key soil properties,\nincluding phosphorus, potassium, nitrogen, and pH levels. This model is then\nenhanced by integrating supplementary features, such as weather data, harvest\nrates, and Clay AI-generated embeddings. This report details the methodological\nframework, data preprocessing strategies, and ML pipelines employed in this\nproject. Advanced algorithms, including Random Forests, Extreme Gradient\nBoosting (XGBoost), and Fully Connected Neural Networks (FCNN), were\nimplemented and finetuned for precise nutrient prediction. Results showcase\nrobust model performance, with root mean square error values meeting stringent\naccuracy thresholds. By establishing a reproducible and scalable pipeline for\nsoil nutrient prediction, this research paves the way for transformative\nagricultural applications, including precision fertilization and improved\nresource allocation in underresourced regions like Africa.\n","date":"2025-03-28"}
{"id":"2503.22277","title":"CFiCS: Graph-Based Classification of Common Factors and Microcounseling\n  Skills","abstract":"  Common factors and microcounseling skills are critical to the effectiveness\nof psychotherapy. Understanding and measuring these elements provides valuable\ninsights into therapeutic processes and outcomes. However, automatic\nidentification of these change principles from textual data remains challenging\ndue to the nuanced and context-dependent nature of therapeutic dialogue. This\npaper introduces CFiCS, a hierarchical classification framework integrating\ngraph machine learning with pretrained contextual embeddings. We represent\ncommon factors, intervention concepts, and microcounseling skills as a\nheterogeneous graph, where textual information from ClinicalBERT enriches each\nnode. This structure captures both the hierarchical relationships (e.g.,\nskill-level nodes linking to broad factors) and the semantic properties of\ntherapeutic concepts. By leveraging graph neural networks, CFiCS learns\ninductive node embeddings that generalize to unseen text samples lacking\nexplicit connections. Our results demonstrate that integrating ClinicalBERT\nnode features and graph structure significantly improves classification\nperformance, especially in fine-grained skill prediction. CFiCS achieves\nsubstantial gains in both micro and macro F1 scores across all tasks compared\nto baselines, including random forests, BERT-based multi-task models, and\ngraph-based methods.\n","date":"2025-03-28"}
{"id":"2503.22280","title":"MultiClaimNet: A Massively Multilingual Dataset of Fact-Checked Claim\n  Clusters","abstract":"  In the context of fact-checking, claims are often repeated across various\nplatforms and in different languages, which can benefit from a process that\nreduces this redundancy. While retrieving previously fact-checked claims has\nbeen investigated as a solution, the growing number of unverified claims and\nexpanding size of fact-checked databases calls for alternative, more efficient\nsolutions. A promising solution is to group claims that discuss the same\nunderlying facts into clusters to improve claim retrieval and validation.\nHowever, research on claim clustering is hindered by the lack of suitable\ndatasets. To bridge this gap, we introduce \\textit{MultiClaimNet}, a collection\nof three multilingual claim cluster datasets containing claims in 86 languages\nacross diverse topics. Claim clusters are formed automatically from\nclaim-matching pairs with limited manual intervention. We leverage two existing\nclaim-matching datasets to form the smaller datasets within\n\\textit{MultiClaimNet}. To build the larger dataset, we propose and validate an\napproach involving retrieval of approximate nearest neighbors to form candidate\nclaim pairs and an automated annotation of claim similarity using large\nlanguage models. This larger dataset contains 85.3K fact-checked claims written\nin 78 languages. We further conduct extensive experiments using various\nclustering techniques and sentence embedding models to establish baseline\nperformance. Our datasets and findings provide a strong foundation for scalable\nclaim clustering, contributing to efficient fact-checking pipelines.\n","date":"2025-03-28"}
{"id":"2503.22281","title":"Divide to Conquer: A Field Decomposition Approach for Multi-Organ\n  Whole-Body CT Image Registration","abstract":"  Image registration is an essential technique for the analysis of Computed\nTomography (CT) images in clinical practice. However, existing methodologies\nare predominantly tailored to a specific organ of interest and often exhibit\nlower performance on other organs, thus limiting their generalizability and\napplicability. Multi-organ registration addresses these limitations, but the\nsimultaneous alignment of multiple organs with diverse shapes, sizes and\nlocations requires a highly complex deformation field with a multi-layer\ncomposition of individual deformations. This study introduces a novel field\ndecomposition approach to address the high complexity of deformations in\nmulti-organ whole-body CT image registration. The proposed method is trained\nand evaluated on a longitudinal dataset of 691 patients, each with two CT\nimages obtained at distinct time points. These scans fully encompass the\nthoracic, abdominal, and pelvic regions. Two baseline registration methods are\nselected for this study: one based on optimization techniques and another based\non deep learning. Experimental results demonstrate that the proposed approach\noutperforms baseline methods in handling complex deformations in multi-organ\nwhole-body CT image registration.\n","date":"2025-03-28"}
{"id":"2503.22285","title":"RUNA: Object-level Out-of-Distribution Detection via Regional\n  Uncertainty Alignment of Multimodal Representations","abstract":"  Enabling object detectors to recognize out-of-distribution (OOD) objects is\nvital for building reliable systems. A primary obstacle stems from the fact\nthat models frequently do not receive supervisory signals from unfamiliar data,\nleading to overly confident predictions regarding OOD objects. Despite previous\nprogress that estimates OOD uncertainty based on the detection model and\nin-distribution (ID) samples, we explore using pre-trained vision-language\nrepresentations for object-level OOD detection. We first discuss the\nlimitations of applying image-level CLIP-based OOD detection methods to\nobject-level scenarios. Building upon these insights, we propose RUNA, a novel\nframework that leverages a dual encoder architecture to capture rich contextual\ninformation and employs a regional uncertainty alignment mechanism to\ndistinguish ID from OOD objects effectively. We introduce a few-shot\nfine-tuning approach that aligns region-level semantic representations to\nfurther improve the model's capability to discriminate between similar objects.\nOur experiments show that RUNA substantially surpasses state-of-the-art methods\nin object-level OOD detection, particularly in challenging scenarios with\ndiverse and complex object instances.\n","date":"2025-03-28"}
{"id":"2503.22291","title":"VisTa: Visual-contextual and Text-augmented Zero-shot Object-level OOD\n  Detection","abstract":"  As object detectors are increasingly deployed as black-box cloud services or\npre-trained models with restricted access to the original training data, the\nchallenge of zero-shot object-level out-of-distribution (OOD) detection arises.\nThis task becomes crucial in ensuring the reliability of detectors in\nopen-world settings. While existing methods have demonstrated success in\nimage-level OOD detection using pre-trained vision-language models like CLIP,\ndirectly applying such models to object-level OOD detection presents challenges\ndue to the loss of contextual information and reliance on image-level\nalignment. To tackle these challenges, we introduce a new method that leverages\nvisual prompts and text-augmented in-distribution (ID) space construction to\nadapt CLIP for zero-shot object-level OOD detection. Our method preserves\ncritical contextual information and improves the ability to differentiate\nbetween ID and OOD objects, achieving competitive performance across different\nbenchmarks.\n","date":"2025-03-28"}
{"id":"2503.22303","title":"Preference-based Learning with Retrieval Augmented Generation for\n  Conversational Question Answering","abstract":"  Conversational Question Answering (ConvQA) involves multiple subtasks, i) to\nunderstand incomplete questions in their context, ii) to retrieve relevant\ninformation, and iii) to generate answers. This work presents PRAISE, a\npipeline-based approach for ConvQA that trains LLM adapters for each of the\nthree subtasks. As labeled training data for individual subtasks is unavailable\nin practice, PRAISE learns from its own generations using the final answering\nperformance as feedback signal without human intervention and treats\nintermediate information, like relevant evidence, as weakly labeled data. We\napply Direct Preference Optimization by contrasting successful and unsuccessful\nsamples for each subtask. In our experiments, we show the effectiveness of this\ntraining paradigm: PRAISE shows improvements per subtask and achieves new\nstate-of-the-art performance on a popular ConvQA benchmark, by gaining 15.5\npercentage points increase in precision over baselines.\n","date":"2025-03-28"}
{"id":"2503.22309","title":"A Dataset for Semantic Segmentation in the Presence of Unknowns","abstract":"  Before deployment in the real-world deep neural networks require thorough\nevaluation of how they handle both knowns, inputs represented in the training\ndata, and unknowns (anomalies). This is especially important for scene\nunderstanding tasks with safety critical applications, such as in autonomous\ndriving. Existing datasets allow evaluation of only knowns or unknowns - but\nnot both, which is required to establish \"in the wild\" suitability of deep\nneural network models. To bridge this gap, we propose a novel anomaly\nsegmentation dataset, ISSU, that features a diverse set of anomaly inputs from\ncluttered real-world environments. The dataset is twice larger than existing\nanomaly segmentation datasets, and provides a training, validation and test set\nfor controlled in-domain evaluation. The test set consists of a static and\ntemporal part, with the latter comprised of videos. The dataset provides\nannotations for both closed-set (knowns) and anomalies, enabling closed-set and\nopen-set evaluation. The dataset covers diverse conditions, such as domain and\ncross-sensor shift, illumination variation and allows ablation of anomaly\ndetection methods with respect to these variations. Evaluation results of\ncurrent state-of-the-art methods confirm the need for improvements especially\nin domain-generalization, small and large object segmentation.\n","date":"2025-03-28"}
{"id":"2503.22313","title":"Hybrid Time-Domain Behavior Model Based on Neural Differential Equations\n  and RNNs","abstract":"  Nonlinear dynamics system identification is crucial for circuit emulation.\nTraditional continuous-time domain modeling approaches have limitations in\nfitting capability and computational efficiency when used for modeling circuit\nIPs and device behaviors.This paper presents a novel continuous-time domain\nhybrid modeling paradigm. It integrates neural network differential models with\nrecurrent neural networks (RNNs), creating NODE-RNN and NCDE-RNN models based\non neural ordinary differential equations (NODE) and neural controlled\ndifferential equations (NCDE), respectively.Theoretical analysis shows that\nthis hybrid model has mathematical advantages in event-driven dynamic mutation\nresponse and gradient propagation stability. Validation using real data from\nPIN diodes in high-power microwave environments shows NCDE-RNN improves fitting\naccuracy by 33\\% over traditional NCDE, and NODE-RNN by 24\\% over CTRNN,\nespecially in capturing nonlinear memory effects.The model has been\nsuccessfully deployed in Verilog-A and validated through circuit emulation,\nconfirming its compatibility with existing platforms and practical value.This\nhybrid dynamics paradigm, by restructuring the neural differential equation\nsolution path, offers new ideas for high-precision circuit time-domain modeling\nand is significant for complex nonlinear circuit system modeling.\n","date":"2025-03-28"}
{"id":"2503.22324","title":"AH-GS: Augmented 3D Gaussian Splatting for High-Frequency Detail\n  Representation","abstract":"  The 3D Gaussian Splatting (3D-GS) is a novel method for scene representation\nand view synthesis. Although Scaffold-GS achieves higher quality real-time\nrendering compared to the original 3D-GS, its fine-grained rendering of the\nscene is extremely dependent on adequate viewing angles. The spectral bias of\nneural network learning results in Scaffold-GS's poor ability to perceive and\nlearn high-frequency information in the scene. In this work, we propose\nenhancing the manifold complexity of input features and using network-based\nfeature map loss to improve the image reconstruction quality of 3D-GS models.\nWe introduce AH-GS, which enables 3D Gaussians in structurally complex regions\nto obtain higher-frequency encodings, allowing the model to more effectively\nlearn the high-frequency information of the scene. Additionally, we incorporate\nhigh-frequency reinforce loss to further enhance the model's ability to capture\ndetailed frequency information. Our result demonstrates that our model\nsignificantly improves rendering fidelity, and in specific scenarios (e.g.,\nMipNeRf360-garden), our method exceeds the rendering quality of Scaffold-GS in\njust 15K iterations.\n","date":"2025-03-28"}
{"id":"2503.22328","title":"VoteFlow: Enforcing Local Rigidity in Self-Supervised Scene Flow","abstract":"  Scene flow estimation aims to recover per-point motion from two adjacent\nLiDAR scans. However, in real-world applications such as autonomous driving,\npoints rarely move independently of others, especially for nearby points\nbelonging to the same object, which often share the same motion. Incorporating\nthis locally rigid motion constraint has been a key challenge in\nself-supervised scene flow estimation, which is often addressed by\npost-processing or appending extra regularization. While these approaches are\nable to improve the rigidity of predicted flows, they lack an architectural\ninductive bias for local rigidity within the model structure, leading to\nsuboptimal learning efficiency and inferior performance. In contrast, we\nenforce local rigidity with a lightweight add-on module in neural network\ndesign, enabling end-to-end learning. We design a discretized voting space that\naccommodates all possible translations and then identify the one shared by\nnearby points by differentiable voting. Additionally, to ensure computational\nefficiency, we operate on pillars rather than points and learn representative\nfeatures for voting per pillar. We plug the Voting Module into popular model\ndesigns and evaluate its benefit on Argoverse 2 and Waymo datasets. We\noutperform baseline works with only marginal compute overhead. Code is\navailable at https:\/\/github.com\/tudelft-iv\/VoteFlow.\n","date":"2025-03-28"}
{"id":"2503.22329","title":"A Refined Analysis of Massive Activations in LLMs","abstract":"  Motivated in part by their relevance for low-precision training and\nquantization, massive activations in large language models (LLMs) have recently\nemerged as a topic of interest. However, existing analyses are limited in\nscope, and generalizability across architectures is unclear. This paper helps\naddress some of these gaps by conducting an analysis of massive activations\nacross a broad range of LLMs, including both GLU-based and non-GLU-based\narchitectures. Our findings challenge several prior assumptions, most\nimportantly: (1) not all massive activations are detrimental, i.e. suppressing\nthem does not lead to an explosion of perplexity or a collapse in downstream\ntask performance; (2) proposed mitigation strategies such as Attention KV bias\nare model-specific and ineffective in certain cases. We consequently\ninvestigate novel hybrid mitigation strategies; in particular pairing Target\nVariance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT)\nsuccessfully balances the mitigation of massive activations with preserved\ndownstream model performance in the scenarios we investigated. Our code is\navailable at: https:\/\/github.com\/bluorion-com\/refine_massive_activations.\n","date":"2025-03-28"}
{"id":"2503.22330","title":"Imperceptible but Forgeable: Practical Invisible Watermark Forgery via\n  Diffusion Models","abstract":"  Invisible watermarking is critical for content provenance and accountability\nin Generative AI. Although commercial companies have increasingly committed to\nusing watermarks, the robustness of existing watermarking schemes against\nforgery attacks is understudied. This paper proposes DiffForge, the first\nwatermark forgery framework capable of forging imperceptible watermarks under a\nno-box setting. We estimate the watermark distribution using an unconditional\ndiffusion model and introduce shallow inversion to inject the watermark into a\nnon-watermarked image seamlessly. This approach facilitates watermark injection\nwhile preserving image quality by adaptively selecting the depth of inversion\nsteps, leveraging our key insight that watermarks degrade with added noise\nduring the early diffusion phases. Comprehensive evaluations show that\nDiffForge deceives open-source watermark detectors with a 96.38% success rate\nand misleads a commercial watermark system with over 97% success rate,\nachieving high confidence.1 This work reveals fundamental security limitations\nin current watermarking paradigms.\n","date":"2025-03-28"}
{"id":"2503.22338","title":"SKDU at De-Factify 4.0: Natural Language Features for AI-Generated\n  Text-Detection","abstract":"  The rapid advancement of large language models (LLMs) has introduced new\nchallenges in distinguishing human-written text from AI-generated content. In\nthis work, we explored a pipelined approach for AI-generated text detection\nthat includes a feature extraction step (i.e. prompt-based rewriting features\ninspired by RAIDAR and content-based features derived from the NELA toolkit)\nfollowed by a classification module. Comprehensive experiments were conducted\non the Defactify4.0 dataset, evaluating two tasks: binary classification to\ndifferentiate human-written and AI-generated text, and multi-class\nclassification to identify the specific generative model used to generate the\ninput text. Our findings reveal that NELA features significantly outperform\nRAIDAR features in both tasks, demonstrating their ability to capture nuanced\nlinguistic, stylistic, and content-based differences. Combining RAIDAR and NELA\nfeatures provided minimal improvement, highlighting the redundancy introduced\nby less discriminative features. Among the classifiers tested, XGBoost emerged\nas the most effective, leveraging the rich feature sets to achieve high\naccuracy and generalisation.\n","date":"2025-03-28"}
{"id":"2503.22342","title":"CPPO: Accelerating the Training of Group Relative Policy\n  Optimization-Based Reasoning Models","abstract":"  This paper introduces Completion Pruning Policy Optimization (CPPO) to\naccelerate the training of reasoning models based on Group Relative Policy\nOptimization (GRPO). GRPO, while effective, incurs high training costs due to\nthe need for sampling multiple completions for each question. Our experiment\nand theoretical analysis reveals that the number of completions impacts model\naccuracy yet increases training time multiplicatively, and not all completions\ncontribute equally to policy training -- their contribution depends on their\nrelative advantage. To address these issues, we propose CPPO, which prunes\ncompletions with low absolute advantages, significantly reducing the number\nneeded for gradient calculation and updates. Additionally, we introduce a\ndynamic completion allocation strategy to maximize GPU utilization by\nincorporating additional questions, further enhancing training efficiency.\nExperimental results demonstrate that CPPO achieves up to $8.32\\times$ speedup\non GSM8K and $3.51\\times$ on Math while preserving or even enhancing the\naccuracy compared to the original GRPO. We release our code at\nhttps:\/\/github.com\/lzhxmu\/CPPO.\n","date":"2025-03-28"}
{"id":"2503.22344","title":"Semantix: An Energy Guided Sampler for Semantic Style Transfer","abstract":"  Recent advances in style and appearance transfer are impressive, but most\nmethods isolate global style and local appearance transfer, neglecting semantic\ncorrespondence. Additionally, image and video tasks are typically handled in\nisolation, with little focus on integrating them for video transfer. To address\nthese limitations, we introduce a novel task, Semantic Style Transfer, which\ninvolves transferring style and appearance features from a reference image to a\ntarget visual content based on semantic correspondence. We subsequently propose\na training-free method, Semantix an energy-guided sampler designed for Semantic\nStyle Transfer that simultaneously guides both style and appearance transfer\nbased on semantic understanding capacity of pre-trained diffusion models.\nAdditionally, as a sampler, Semantix be seamlessly applied to both image and\nvideo models, enabling semantic style transfer to be generic across various\nvisual media. Specifically, once inverting both reference and context images or\nvideos to noise space by SDEs, Semantix utilizes a meticulously crafted energy\nfunction to guide the sampling process, including three key components: Style\nFeature Guidance, Spatial Feature Guidance and Semantic Distance as a\nregularisation term. Experimental results demonstrate that Semantix not only\neffectively accomplishes the task of semantic style transfer across images and\nvideos, but also surpasses existing state-of-the-art solutions in both fields.\nThe project website is available at https:\/\/huiang-he.github.io\/semantix\/\n","date":"2025-03-28"}
{"id":"2503.22346","title":"ArchCAD-400K: An Open Large-Scale Architectural CAD Dataset and New\n  Baseline for Panoptic Symbol Spotting","abstract":"  Recognizing symbols in architectural CAD drawings is critical for various\nadvanced engineering applications. In this paper, we propose a novel CAD data\nannotation engine that leverages intrinsic attributes from systematically\narchived CAD drawings to automatically generate high-quality annotations, thus\nsignificantly reducing manual labeling efforts. Utilizing this engine, we\nconstruct ArchCAD-400K, a large-scale CAD dataset consisting of 413,062 chunks\nfrom 5538 highly standardized drawings, making it over 26 times larger than the\nlargest existing CAD dataset. ArchCAD-400K boasts an extended drawing diversity\nand broader categories, offering line-grained annotations. Furthermore, we\npresent a new baseline model for panoptic symbol spotting, termed Dual-Pathway\nSymbol Spotter (DPSS). It incorporates an adaptive fusion module to enhance\nprimitive features with complementary image features, achieving\nstate-of-the-art performance and enhanced robustness. Extensive experiments\nvalidate the effectiveness of DPSS, demonstrating the value of ArchCAD-400K and\nits potential to drive innovation in architectural design and construction.\n","date":"2025-03-28"}
{"id":"2503.22349","title":"GCRayDiffusion: Pose-Free Surface Reconstruction via Geometric\n  Consistent Ray Diffusion","abstract":"  Accurate surface reconstruction from unposed images is crucial for efficient\n3D object or scene creation. However, it remains challenging, particularly for\nthe joint camera pose estimation. Previous approaches have achieved impressive\npose-free surface reconstruction results in dense-view settings, but could\neasily fail for sparse-view scenarios without sufficient visual overlap. In\nthis paper, we propose a new technique for pose-free surface reconstruction,\nwhich follows triplane-based signed distance field (SDF) learning but\nregularizes the learning by explicit points sampled from ray-based diffusion of\ncamera pose estimation. Our key contribution is a novel Geometric Consistent\nRay Diffusion model (GCRayDiffusion), where we represent camera poses as neural\nbundle rays and regress the distribution of noisy rays via a diffusion model.\nMore importantly, we further condition the denoising process of RGRayDiffusion\nusing the triplane-based SDF of the entire scene, which provides effective 3D\nconsistent regularization to achieve multi-view consistent camera pose\nestimation. Finally, we incorporate RGRayDiffusion into the triplane-based SDF\nlearning by introducing on-surface geometric regularization from the sampling\npoints of the neural bundle rays, which leads to highly accurate pose-free\nsurface reconstruction results even for sparse-view inputs. Extensive\nevaluations on public datasets show that our GCRayDiffusion achieves more\naccurate camera pose estimation than previous approaches, with geometrically\nmore consistent surface reconstruction results, especially given sparse-view\ninputs.\n","date":"2025-03-28"}
{"id":"2503.22351","title":"One Look is Enough: A Novel Seamless Patchwise Refinement for Zero-Shot\n  Monocular Depth Estimation Models on High-Resolution Images","abstract":"  Zero-shot depth estimation (DE) models exhibit strong generalization\nperformance as they are trained on large-scale datasets. However, existing\nmodels struggle with high-resolution images due to the discrepancy in image\nresolutions of training (with smaller resolutions) and inference (for high\nresolutions). Processing them at full resolution leads to decreased estimation\naccuracy on depth with tremendous memory consumption, while downsampling to the\ntraining resolution results in blurred edges in the estimated depth images.\nPrevailing high-resolution depth estimation methods adopt a patch-based\napproach, which introduces depth discontinuity issues when reassembling the\nestimated depth patches and results in test-time inefficiency. Additionally, to\nobtain fine-grained depth details, these methods rely on synthetic datasets due\nto the real-world sparse ground truth depth, leading to poor generalizability.\nTo tackle these limitations, we propose Patch Refine Once (PRO), an efficient\nand generalizable tile-based framework. Our PRO consists of two key components:\n(i) Grouped Patch Consistency Training that enhances test-time efficiency while\nmitigating the depth discontinuity problem by jointly processing four\noverlapping patches and enforcing a consistency loss on their overlapping\nregions within a single backpropagation step, and (ii) Bias Free Masking that\nprevents the DE models from overfitting to dataset-specific biases, enabling\nbetter generalization to real-world datasets even after training on synthetic\ndata. Zero-shot evaluation on Booster, ETH3D, Middlebury 2014, and NuScenes\ndemonstrates into which our PRO can be well harmonized, making their DE\ncapabilities still effective for the grid input of high-resolution images with\nlittle depth discontinuities at the grid boundaries. Our PRO runs fast at\ninference time.\n","date":"2025-03-28"}
{"id":"2503.22352","title":"Meta-LoRA: Meta-Learning LoRA Components for Domain-Aware ID\n  Personalization","abstract":"  Recent advancements in text-to-image generative models, particularly latent\ndiffusion models (LDMs), have demonstrated remarkable capabilities in\nsynthesizing high-quality images from textual prompts. However, achieving\nidentity personalization-ensuring that a model consistently generates\nsubject-specific outputs from limited reference images-remains a fundamental\nchallenge. To address this, we introduce Meta-Low-Rank Adaptation (Meta-LoRA),\na novel framework that leverages meta-learning to encode domain-specific priors\ninto LoRA-based identity personalization. Our method introduces a structured\nthree-layer LoRA architecture that separates identity-agnostic knowledge from\nidentity-specific adaptation. In the first stage, the LoRA Meta-Down layers are\nmeta-trained across multiple subjects, learning a shared manifold that captures\ngeneral identity-related features. In the second stage, only the LoRA-Mid and\nLoRA-Up layers are optimized to specialize on a given subject, significantly\nreducing adaptation time while improving identity fidelity. To evaluate our\napproach, we introduce Meta-PHD, a new benchmark dataset for identity\npersonalization, and compare Meta-LoRA against state-of-the-art methods. Our\nresults demonstrate that Meta-LoRA achieves superior identity retention,\ncomputational efficiency, and adaptability across diverse identity conditions.\nThe code, model weights, and dataset will be released publicly upon acceptance.\n","date":"2025-03-28"}
{"id":"2503.22353","title":"Firm or Fickle? Evaluating Large Language Models Consistency in\n  Sequential Interactions","abstract":"  Large Language Models (LLMs) have shown remarkable capabilities across\nvarious tasks, but their deployment in high-stake domains requires consistent\nperformance across multiple interaction rounds. This paper introduces a\ncomprehensive framework for evaluating and improving LLM response consistency,\nmaking three key contributions. First, we propose a novel Position-Weighted\nConsistency (PWC) score that captures both the importance of early-stage\nstability and recovery patterns in multi-turn interactions. Second, we present\na carefully curated benchmark dataset spanning diverse domains and difficulty\nlevels, specifically designed to evaluate LLM consistency under various\nchallenging follow-up scenarios. Third, we introduce Confidence-Aware Response\nGeneration (CARG), a framework that significantly improves response stability\nby incorporating model confidence signals into the generation process.\nEmpirical results demonstrate that CARG significantly improves response\nstability without sacrificing accuracy, underscoring its potential for reliable\nLLM deployment in critical applications.\n","date":"2025-03-28"}
{"id":"2503.22357","title":"EchoFlow: A Foundation Model for Cardiac Ultrasound Image and Video\n  Generation","abstract":"  Advances in deep learning have significantly enhanced medical image analysis,\nyet the availability of large-scale medical datasets remains constrained by\npatient privacy concerns. We present EchoFlow, a novel framework designed to\ngenerate high-quality, privacy-preserving synthetic echocardiogram images and\nvideos. EchoFlow comprises four key components: an adversarial variational\nautoencoder for defining an efficient latent representation of cardiac\nultrasound images, a latent image flow matching model for generating accurate\nlatent echocardiogram images, a latent re-identification model to ensure\nprivacy by filtering images anatomically, and a latent video flow matching\nmodel for animating latent images into realistic echocardiogram videos\nconditioned on ejection fraction. We rigorously evaluate our synthetic datasets\non the clinically relevant task of ejection fraction regression and\ndemonstrate, for the first time, that downstream models trained exclusively on\nEchoFlow-generated synthetic datasets achieve performance parity with models\ntrained on real datasets. We release our models and synthetic datasets,\nenabling broader, privacy-compliant research in medical ultrasound imaging at\nhttps:\/\/huggingface.co\/spaces\/HReynaud\/EchoFlow.\n","date":"2025-03-28"}
{"id":"2503.22358","title":"Shapley Revisited: Tractable Responsibility Measures for Query Answers","abstract":"  The Shapley value, originating from cooperative game theory, has been\nemployed to define responsibility measures that quantify the contributions of\ndatabase facts to obtaining a given query answer. For non-numeric queries, this\nis done by considering a cooperative game whose players are the facts and whose\nwealth function assigns 1 or 0 to each subset of the database, depending on\nwhether the query answer holds in the given subset. While conceptually simple,\nthis approach suffers from a notable drawback: the problem of computing such\nShapley values is #P-hard in data complexity, even for simple conjunctive\nqueries. This motivates us to revisit the question of what constitutes a\nreasonable responsibility measure and to introduce a new family of\nresponsibility measures -- weighted sums of minimal supports (WSMS) -- which\nsatisfy intuitive properties. Interestingly, while the definition of WSMSs is\nsimple and bears no obvious resemblance to the Shapley value formula, we prove\nthat every WSMS measure can be equivalently seen as the Shapley value of a\nsuitably defined cooperative game. Moreover, WSMS measures enjoy tractable data\ncomplexity for a large class of queries, including all unions of conjunctive\nqueries. We further explore the combined complexity of WSMS computation and\nestablish (in)tractability results for various subclasses of conjunctive\nqueries.\n","date":"2025-03-28"}
{"id":"2503.22359","title":"Mitigating Knowledge Discrepancies among Multiple Datasets for\n  Task-agnostic Unified Face Alignment","abstract":"  Despite the similar structures of human faces, existing face alignment\nmethods cannot learn unified knowledge from multiple datasets with different\nlandmark annotations. The limited training samples in a single dataset commonly\nresult in fragile robustness in this field. To mitigate knowledge discrepancies\namong different datasets and train a task-agnostic unified face alignment\n(TUFA) framework, this paper presents a strategy to unify knowledge from\nmultiple datasets. Specifically, we calculate a mean face shape for each\ndataset. To explicitly align these mean shapes on an interpretable plane based\non their semantics, each shape is then incorporated with a group of semantic\nalignment embeddings. The 2D coordinates of these aligned shapes can be viewed\nas the anchors of the plane. By encoding them into structure prompts and\nfurther regressing the corresponding facial landmarks using image features, a\nmapping from the plane to the target faces is finally established, which\nunifies the learning target of different datasets. Consequently, multiple\ndatasets can be utilized to boost the generalization ability of the model. The\nsuccessful mitigation of discrepancies also enhances the efficiency of\nknowledge transferring to a novel dataset, significantly boosts the performance\nof few-shot face alignment. Additionally, the interpretable plane endows TUFA\nwith a task-agnostic characteristic, enabling it to locate landmarks unseen\nduring training in a zero-shot manner. Extensive experiments are carried on\nseven benchmarks and the results demonstrate an impressive improvement in face\nalignment brought by knowledge discrepancies mitigation.\n","date":"2025-03-28"}
{"id":"2503.22362","title":"Supposedly Equivalent Facts That Aren't? Entity Frequency in\n  Pre-training Induces Asymmetry in LLMs","abstract":"  Understanding and mitigating hallucinations in Large Language Models (LLMs)\nis crucial for ensuring reliable content generation. While previous research\nhas primarily focused on \"when\" LLMs hallucinate, our work explains \"why\" and\ndirectly links model behaviour to the pre-training data that forms their prior\nknowledge. Specifically, we demonstrate that an asymmetry exists in the\nrecognition of logically equivalent facts, which can be attributed to frequency\ndiscrepancies of entities appearing as subjects versus objects. Given that most\npre-training datasets are inaccessible, we leverage the fully open-source OLMo\nseries by indexing its Dolma dataset to estimate entity frequencies. Using\nrelational facts (represented as triples) from Wikidata5M, we construct probing\ndatasets to isolate this effect. Our experiments reveal that facts with a\nhigh-frequency subject and a low-frequency object are better recognised than\ntheir inverse, despite their logical equivalence. The pattern reverses in\nlow-to-high frequency settings, and no statistically significant asymmetry\nemerges when both entities are high-frequency. These findings highlight the\ninfluential role of pre-training data in shaping model predictions and provide\ninsights for inferring the characteristics of pre-training data in closed or\npartially closed LLMs.\n","date":"2025-03-28"}
{"id":"2503.22363","title":"ForcePose: A Deep Learning Approach for Force Calculation Based on\n  Action Recognition Using MediaPipe Pose Estimation Combined with Object\n  Detection","abstract":"  Force estimation in human-object interactions is crucial for various fields\nlike ergonomics, physical therapy, and sports science. Traditional methods\ndepend on specialized equipment such as force plates and sensors, which makes\naccurate assessments both expensive and restricted to laboratory settings. In\nthis paper, we introduce ForcePose, a novel deep learning framework that\nestimates applied forces by combining human pose estimation with object\ndetection. Our approach leverages MediaPipe for skeletal tracking and SSD\nMobileNet for object recognition to create a unified representation of\nhuman-object interaction. We've developed a specialized neural network that\nprocesses both spatial and temporal features to predict force magnitude and\ndirection without needing any physical sensors. After training on our dataset\nof 850 annotated videos with corresponding force measurements, our model\nachieves a mean absolute error of 5.83 N in force magnitude and 7.4 degrees in\nforce direction. When compared to existing computer vision approaches, our\nmethod performs 27.5% better while still offering real-time performance on\nstandard computing hardware. ForcePose opens up new possibilities for force\nanalysis in diverse real-world scenarios where traditional measurement tools\nare impractical or intrusive. This paper discusses our methodology, the dataset\ncreation process, evaluation metrics, and potential applications across\nrehabilitation, ergonomics assessment, and athletic performance analysis.\n","date":"2025-03-28"}
{"id":"2503.22370","title":"Grasping a Handful: Sequential Multi-Object Dexterous Grasp Generation","abstract":"  We introduce the sequential multi-object robotic grasp sampling algorithm\nSeqGrasp that can robustly synthesize stable grasps on diverse objects using\nthe robotic hand's partial Degrees of Freedom (DoF). We use SeqGrasp to\nconstruct the large-scale Allegro Hand sequential grasping dataset SeqDataset\nand use it for training the diffusion-based sequential grasp generator\nSeqDiffuser. We experimentally evaluate SeqGrasp and SeqDiffuser against the\nstate-of-the-art non-sequential multi-object grasp generation method MultiGrasp\nin simulation and on a real robot. The experimental results demonstrate that\nSeqGrasp and SeqDiffuser reach an 8.71%-43.33% higher grasp success rate than\nMultiGrasp. Furthermore, SeqDiffuser is approximately 1000 times faster at\ngenerating grasps than SeqGrasp and MultiGrasp.\n","date":"2025-03-28"}
{"id":"2503.22374","title":"ViSketch-GPT: Collaborative Multi-Scale Feature Extraction for Sketch\n  Recognition and Generation","abstract":"  Understanding the nature of human sketches is challenging because of the wide\nvariation in how they are created. Recognizing complex structural patterns\nimproves both the accuracy in recognizing sketches and the fidelity of the\ngenerated sketches. In this work, we introduce ViSketch-GPT, a novel algorithm\ndesigned to address these challenges through a multi-scale context extraction\napproach. The model captures intricate details at multiple scales and combines\nthem using an ensemble-like mechanism, where the extracted features work\ncollaboratively to enhance the recognition and generation of key details\ncrucial for classification and generation tasks.\n  The effectiveness of ViSketch-GPT is validated through extensive experiments\non the QuickDraw dataset. Our model establishes a new benchmark, significantly\noutperforming existing methods in both classification and generation tasks,\nwith substantial improvements in accuracy and the fidelity of generated\nsketches.\n  The proposed algorithm offers a robust framework for understanding complex\nstructures by extracting features that collaborate to recognize intricate\ndetails, enhancing the understanding of structures like sketches and making it\na versatile tool for various applications in computer vision and machine\nlearning.\n","date":"2025-03-28"}
{"id":"2503.22375","title":"Data Quality Matters: Quantifying Image Quality Impact on Machine\n  Learning Performance","abstract":"  Precise perception of the environment is essential in highly automated\ndriving systems, which rely on machine learning tasks such as object detection\nand segmentation. Compression of sensor data is commonly used for data\nhandling, while virtualization is used for hardware-in-the-loop validation.\nBoth methods can alter sensor data and degrade model performance. This\nnecessitates a systematic approach to quantifying image validity. This paper\npresents a four-step framework to evaluate the impact of image modifications on\nmachine learning tasks. First, a dataset with modified images is prepared to\nensure one-to-one matching image pairs, enabling measurement of deviations\nresulting from compression and virtualization. Second, image deviations are\nquantified by comparing the effects of compression and virtualization against\noriginal camera-based sensor data. Third, the performance of state-of-the-art\nobject detection models is analyzed to determine how altered input data affects\nperception tasks, including bounding box accuracy and reliability. Finally, a\ncorrelation analysis is performed to identify relationships between image\nquality and model performance. As a result, the LPIPS metric achieves the\nhighest correlation between image deviation and machine learning performance\nacross all evaluated machine learning tasks.\n","date":"2025-03-28"}
{"id":"2503.22379","title":"Spend Your Budget Wisely: Towards an Intelligent Distribution of the\n  Privacy Budget in Differentially Private Text Rewriting","abstract":"  The task of $\\textit{Differentially Private Text Rewriting}$ is a class of\ntext privatization techniques in which (sensitive) input textual documents are\n$\\textit{rewritten}$ under Differential Privacy (DP) guarantees. The motivation\nbehind such methods is to hide both explicit and implicit identifiers that\ncould be contained in text, while still retaining the semantic meaning of the\noriginal text, thus preserving utility. Recent years have seen an uptick in\nresearch output in this field, offering a diverse array of word-, sentence-,\nand document-level DP rewriting methods. Common to these methods is the\nselection of a privacy budget (i.e., the $\\varepsilon$ parameter), which\ngoverns the degree to which a text is privatized. One major limitation of\nprevious works, stemming directly from the unique structure of language itself,\nis the lack of consideration of $\\textit{where}$ the privacy budget should be\nallocated, as not all aspects of language, and therefore text, are equally\nsensitive or personal. In this work, we are the first to address this\nshortcoming, asking the question of how a given privacy budget can be\nintelligently and sensibly distributed amongst a target document. We construct\nand evaluate a toolkit of linguistics- and NLP-based methods used to allocate a\nprivacy budget to constituent tokens in a text document. In a series of privacy\nand utility experiments, we empirically demonstrate that given the same privacy\nbudget, intelligent distribution leads to higher privacy levels and more\npositive trade-offs than a naive distribution of $\\varepsilon$. Our work\nhighlights the intricacies of text privatization with DP, and furthermore, it\ncalls for further work on finding more efficient ways to maximize the\nprivatization benefits offered by DP in text rewriting.\n","date":"2025-03-28"}
{"id":"2503.22388","title":"Why Stop at One Error? Benchmarking LLMs as Data Science Code Debuggers\n  for Multi-Hop and Multi-Bug Errors","abstract":"  LLMs are transforming software development, yet current code generation and\ncode repair benchmarks mainly assess syntactic and functional correctness in\nsimple, single-error cases. LLMs' capabilities to autonomously find and fix\nruntime logical errors in complex data science code remain largely unexplored.\nTo address this gap, we introduce DSDBench: the Data Science Debugging\nBenchmark, the first benchmark for systematic evaluation of LLMs on multi-hop\nerror tracing and multi-bug detection in data science code debugging. DSDBench\nadapts datasets from existing data science task benchmarks, such as DABench and\nMatPlotBench, featuring realistic data science debugging tasks with\nautomatically synthesized multi-hop, multi-bug code snippets. DSDBench includes\n1,117 annotated samples with 741 cause-effect error pairs and runtime error\nmessages. Evaluations of state-of-the-art LLMs on DSDBench show significant\nperformance gaps, highlighting challenges in debugging logical runtime errors\nin data science code. DSDBench offers a crucial resource to evaluate and\nimprove LLMs' debugging and reasoning capabilities, enabling more reliable\nAI-assisted data science in the future.DSDBench is publicly available at\nhttps:\/\/github.com\/KevinCL16\/DSDBench.\n","date":"2025-03-28"}
{"id":"2503.22389","title":"MASCOTS: Model-Agnostic Symbolic COunterfactual explanations for Time\n  Series","abstract":"  Counterfactual explanations provide an intuitive way to understand model\ndecisions by identifying minimal changes required to alter an outcome. However,\napplying counterfactual methods to time series models remains challenging due\nto temporal dependencies, high dimensionality, and the lack of an intuitive\nhuman-interpretable representation. We introduce MASCOTS, a method that\nleverages the Bag-of-Receptive-Fields representation alongside symbolic\ntransformations inspired by Symbolic Aggregate Approximation. By operating in a\nsymbolic feature space, it enhances interpretability while preserving fidelity\nto the original data and model. Unlike existing approaches that either depend\non model structure or autoencoder-based sampling, MASCOTS directly generates\nmeaningful and diverse counterfactual observations in a model-agnostic manner,\noperating on both univariate and multivariate data. We evaluate MASCOTS on\nunivariate and multivariate benchmark datasets, demonstrating comparable\nvalidity, proximity, and plausibility to state-of-the-art methods, while\nsignificantly improving interpretability and sparsity. Its symbolic nature\nallows for explanations that can be expressed visually, in natural language, or\nthrough semantic representations, making counterfactual reasoning more\naccessible and actionable.\n","date":"2025-03-28"}
{"id":"2503.22394","title":"Endo-TTAP: Robust Endoscopic Tissue Tracking via Multi-Facet Guided\n  Attention and Hybrid Flow-point Supervision","abstract":"  Accurate tissue point tracking in endoscopic videos is critical for\nrobotic-assisted surgical navigation and scene understanding, but remains\nchallenging due to complex deformations, instrument occlusion, and the scarcity\nof dense trajectory annotations. Existing methods struggle with long-term\ntracking under these conditions due to limited feature utilization and\nannotation dependence. We present Endo-TTAP, a novel framework addressing these\nchallenges through: (1) A Multi-Facet Guided Attention (MFGA) module that\nsynergizes multi-scale flow dynamics, DINOv2 semantic embeddings, and explicit\nmotion patterns to jointly predict point positions with uncertainty and\nocclusion awareness; (2) A two-stage curriculum learning strategy employing an\nAuxiliary Curriculum Adapter (ACA) for progressive initialization and hybrid\nsupervision. Stage I utilizes synthetic data with optical flow ground truth for\nuncertainty-occlusion regularization, while Stage II combines unsupervised flow\nconsistency and semi-supervised learning with refined pseudo-labels from\noff-the-shelf trackers. Extensive validation on two MICCAI Challenge datasets\nand our collected dataset demonstrates that Endo-TTAP achieves state-of-the-art\nperformance in tissue point tracking, particularly in scenarios characterized\nby complex endoscopic conditions. The source code and dataset will be available\nat https:\/\/anonymous.4open.science\/r\/Endo-TTAP-36E5.\n","date":"2025-03-28"}
{"id":"2503.22395","title":"Negation: A Pink Elephant in the Large Language Models' Room?","abstract":"  Negations are key to determining sentence meaning, making them essential for\nlogical reasoning. Despite their importance, negations pose a substantial\nchallenge for large language models (LLMs) and remain underexplored.\n  We construct two multilingual natural language inference (NLI) datasets with\n\\textit{paired} examples differing in negation. We investigate how model size\nand language impact its ability to handle negation correctly by evaluating\npopular LLMs.\n  Contrary to previous work, we show that increasing the model size\nconsistently improves the models' ability to handle negations. Furthermore, we\nfind that both the models' reasoning accuracy and robustness to negation are\nlanguage-dependent and that the length and explicitness of the premise have a\ngreater impact on robustness than language.\n  Our datasets can facilitate further research and improvements of language\nmodel reasoning in multilingual settings.\n","date":"2025-03-28"}
{"id":"2503.22396","title":"On-site estimation of battery electrochemical parameters via transfer\n  learning based physics-informed neural network approach","abstract":"  This paper presents a novel physical parameter estimation framework for\non-site model characterization, using a two-phase modelling strategy with\nPhysics-Informed Neural Networks (PINNs) and transfer learning (TL). In the\nfirst phase, a PINN is trained using only the physical principles of the single\nparticle model (SPM) equations. In the second phase, the majority of the PINN\nparameters are frozen, while critical electrochemical parameters are set as\ntrainable and adjusted using real-world voltage profile data. The proposed\napproach significantly reduces computational costs, making it suitable for\nreal-time implementation on Battery Management Systems (BMS). Additionally, as\nthe initial phase does not require field data, the model is easy to deploy with\nminimal setup requirements. With the proposed methodology, we have been able to\neffectively estimate relevant electrochemical parameters with operating data.\nThis has been proved estimating diffusivities and active material volume\nfractions with charge data in different degradation conditions. The methodology\nis experimentally validated in a Raspberry Pi device using data from a standard\ncharge profile with a 3.89\\% relative accuracy estimating the active material\nvolume fractions of a NMC cell with 82.09\\% of its nominal capacity.\n","date":"2025-03-28"}
{"id":"2503.22397","title":"GAITGen: Disentangled Motion-Pathology Impaired Gait Generative Model --\n  Bringing Motion Generation to the Clinical Domain","abstract":"  Gait analysis is crucial for the diagnosis and monitoring of movement\ndisorders like Parkinson's Disease. While computer vision models have shown\npotential for objectively evaluating parkinsonian gait, their effectiveness is\nlimited by scarce clinical datasets and the challenge of collecting large and\nwell-labelled data, impacting model accuracy and risk of bias. To address these\ngaps, we propose GAITGen, a novel framework that generates realistic gait\nsequences conditioned on specified pathology severity levels. GAITGen employs a\nConditional Residual Vector Quantized Variational Autoencoder to learn\ndisentangled representations of motion dynamics and pathology-specific factors,\ncoupled with Mask and Residual Transformers for conditioned sequence\ngeneration. GAITGen generates realistic, diverse gait sequences across severity\nlevels, enriching datasets and enabling large-scale model training in\nparkinsonian gait analysis. Experiments on our new PD-GaM (real) dataset\ndemonstrate that GAITGen outperforms adapted state-of-the-art models in both\nreconstruction fidelity and generation quality, accurately capturing critical\npathology-specific gait features. A clinical user study confirms the realism\nand clinical relevance of our generated sequences. Moreover, incorporating\nGAITGen-generated data into downstream tasks improves parkinsonian gait\nseverity estimation, highlighting its potential for advancing clinical gait\nanalysis.\n","date":"2025-03-28"}
{"id":"2503.22398","title":"DF-Net: The Digital Forensics Network for Image Forgery Detection","abstract":"  The orchestrated manipulation of public opinion, particularly through\nmanipulated images, often spread via online social networks (OSN), has become a\nserious threat to society. In this paper we introduce the Digital Forensics Net\n(DF-Net), a deep neural network for pixel-wise image forgery detection. The\nreleased model outperforms several state-of-the-art methods on four established\nbenchmark datasets. Most notably, DF-Net's detection is robust against lossy\nimage operations (e.g resizing, compression) as they are automatically\nperformed by social networks.\n","date":"2025-03-28"}
{"id":"2503.22399","title":"VITAL: More Understandable Feature Visualization through Distribution\n  Alignment and Relevant Information Flow","abstract":"  Neural networks are widely adopted to solve complex and challenging tasks.\nEspecially in high-stakes decision-making, understanding their reasoning\nprocess is crucial, yet proves challenging for modern deep networks. Feature\nvisualization (FV) is a powerful tool to decode what information neurons are\nresponding to and hence to better understand the reasoning behind such\nnetworks. In particular, in FV we generate human-understandable images that\nreflect the information detected by neurons of interest. However, current\nmethods often yield unrecognizable visualizations, exhibiting repetitive\npatterns and visual artifacts that are hard to understand for a human. To\naddress these problems, we propose to guide FV through statistics of real image\nfeatures combined with measures of relevant network flow to generate\nprototypical images. Our approach yields human-understandable visualizations\nthat both qualitatively and quantitatively improve over state-of-the-art FVs\nacross various architectures. As such, it can be used to decode which\ninformation the network uses, complementing mechanistic circuits that identify\nwhere it is encoded. Code is available at: https:\/\/github.com\/adagorgun\/VITAL\n","date":"2025-03-28"}
{"id":"2503.22401","title":"Generative Reliability-Based Design Optimization Using In-Context\n  Learning Capabilities of Large Language Models","abstract":"  Large Language Models (LLMs) have demonstrated remarkable in-context learning\ncapabilities, enabling flexible utilization of limited historical information\nto play pivotal roles in reasoning, problem-solving, and complex pattern\nrecognition tasks. Inspired by the successful applications of LLMs in multiple\ndomains, this paper proposes a generative design method by leveraging the\nin-context learning capabilities of LLMs with the iterative search mechanisms\nof metaheuristic algorithms for solving reliability-based design optimization\nproblems. In detail, reliability analysis is performed by engaging the LLMs and\nKriging surrogate modeling to overcome the computational burden. By dynamically\nproviding critical information of design points to the LLMs with prompt\nengineering, the method enables rapid generation of high-quality design\nalternatives that satisfy reliability constraints while achieving performance\noptimization. With the Deepseek-V3 model, three case studies are used to\ndemonstrated the performance of the proposed approach. Experimental results\nindicate that the proposed LLM-RBDO method successfully identifies feasible\nsolutions that meet reliability constraints while achieving a comparable\nconvergence rate compared to traditional genetic algorithms.\n","date":"2025-03-28"}
{"id":"2503.22402","title":"EllieSQL: Cost-Efficient Text-to-SQL with Complexity-Aware Routing","abstract":"  Text-to-SQL automatically translates natural language queries to SQL,\nallowing non-technical users to retrieve data from databases without\nspecialized SQL knowledge. Despite the success of advanced LLM-based\nText-to-SQL approaches on leaderboards, their unsustainable computational\ncosts--often overlooked--stand as the \"elephant in the room\" in current\nleaderboard-driven research, limiting their economic practicability for\nreal-world deployment and widespread adoption. To tackle this, we exploratively\npropose EllieSQL, a complexity-aware routing framework that assigns queries to\nsuitable SQL generation pipelines based on estimated complexity. We investigate\nmultiple routers to direct simple queries to efficient approaches while\nreserving computationally intensive methods for complex cases. Drawing from\neconomics, we introduce the Token Elasticity of Performance (TEP) metric,\ncapturing cost-efficiency by quantifying the responsiveness of performance\ngains relative to token investment in SQL generation. Experiments show that\ncompared to always using the most advanced methods in our study, EllieSQL with\nthe Qwen2.5-0.5B-DPO router reduces token use by over 40% without compromising\nperformance on Bird development set, achieving more than a 2x boost in TEP over\nnon-routing approaches. This not only advances the pursuit of cost-efficient\nText-to-SQL but also invites the community to weigh resource efficiency\nalongside performance, contributing to progress in sustainable Text-to-SQL.\n","date":"2025-03-28"}
{"id":"2503.22405","title":"Modeling Multiple Normal Action Representations for Error Detection in\n  Procedural Tasks","abstract":"  Error detection in procedural activities is essential for consistent and\ncorrect outcomes in AR-assisted and robotic systems. Existing methods often\nfocus on temporal ordering errors or rely on static prototypes to represent\nnormal actions. However, these approaches typically overlook the common\nscenario where multiple, distinct actions are valid following a given sequence\nof executed actions. This leads to two issues: (1) the model cannot effectively\ndetect errors using static prototypes when the inference environment or action\nexecution distribution differs from training; and (2) the model may also use\nthe wrong prototypes to detect errors if the ongoing action label is not the\nsame as the predicted one. To address this problem, we propose an Adaptive\nMultiple Normal Action Representation (AMNAR) framework. AMNAR predicts all\nvalid next actions and reconstructs their corresponding normal action\nrepresentations, which are compared against the ongoing action to detect\nerrors. Extensive experiments demonstrate that AMNAR achieves state-of-the-art\nperformance, highlighting the effectiveness of AMNAR and the importance of\nmodeling multiple valid next actions in error detection. The code is available\nat https:\/\/github.com\/iSEE-Laboratory\/AMNAR.\n","date":"2025-03-28"}
{"id":"2503.22406","title":"Training Large Language Models for Advanced Typosquatting Detection","abstract":"  Typosquatting is a long-standing cyber threat that exploits human error in\ntyping URLs to deceive users, distribute malware, and conduct phishing attacks.\nWith the proliferation of domain names and new Top-Level Domains (TLDs),\ntyposquatting techniques have grown more sophisticated, posing significant\nrisks to individuals, businesses, and national cybersecurity infrastructure.\nTraditional detection methods primarily focus on well-known impersonation\npatterns, leaving gaps in identifying more complex attacks. This study\nintroduces a novel approach leveraging large language models (LLMs) to enhance\ntyposquatting detection. By training an LLM on character-level transformations\nand pattern-based heuristics rather than domain-specific data, a more adaptable\nand resilient detection mechanism develops. Experimental results indicate that\nthe Phi-4 14B model outperformed other tested models when properly fine tuned\nachieving a 98% accuracy rate with only a few thousand training samples. This\nresearch highlights the potential of LLMs in cybersecurity applications,\nspecifically in mitigating domain-based deception tactics, and provides\ninsights into optimizing machine learning strategies for threat detection.\n","date":"2025-03-28"}
{"id":"2503.22411","title":"Elite Political Discourse has Become More Toxic in Western Countries","abstract":"  Toxic and uncivil politics is widely seen as a growing threat to democratic\nvalues and governance, yet our understanding of the drivers and evolution of\npolitical incivility remains limited. Leveraging a novel dataset of nearly 18\nmillion Twitter messages from parliamentarians in 17 countries over five years,\nthis paper systematically investigates whether politics internationally is\nbecoming more uncivil, and what are the determinants of political incivility.\nOur analysis reveals a marked increase in toxic discourse among political\nelites, and that it is associated to radical-right parties and parties in\nopposition. Toxicity diminished markedly during the early phase of the COVID-19\npandemic and, surprisingly, during election campaigns. Furthermore, our results\nindicate that posts relating to ``culture war'' topics, such as migration and\nLGBTQ+ rights, are substantially more toxic than debates focused on welfare or\neconomic issues. These findings underscore a troubling shift in international\ndemocracies toward an erosion of constructive democratic dialogue.\n","date":"2025-03-28"}
{"id":"2503.22413","title":"Instance-Level Data-Use Auditing of Visual ML Models","abstract":"  The growing trend of legal disputes over the unauthorized use of data in\nmachine learning (ML) systems highlights the urgent need for reliable data-use\nauditing mechanisms to ensure accountability and transparency in ML. In this\npaper, we present the first proactive instance-level data-use auditing method\ndesigned to enable data owners to audit the use of their individual data\ninstances in ML models, providing more fine-grained auditing results. Our\napproach integrates any black-box membership inference technique with a\nsequential hypothesis test, providing a quantifiable and tunable\nfalse-detection rate. We evaluate our method on three types of visual ML\nmodels: image classifiers, visual encoders, and Contrastive Image-Language\nPretraining (CLIP) models. In additional, we apply our method to evaluate the\nperformance of two state-of-the-art approximate unlearning methods. Our\nfindings reveal that neither method successfully removes the influence of the\nunlearned data instances from image classifiers and CLIP models even if\nsacrificing model utility by $10.33\\%$.\n","date":"2025-03-28"}
{"id":"2503.22417","title":"DF2023: The Digital Forensics 2023 Dataset for Image Forgery Detection","abstract":"  The deliberate manipulation of public opinion, especially through altered\nimages, which are frequently disseminated through online social networks, poses\na significant danger to society. To fight this issue on a technical level we\nsupport the research community by releasing the Digital Forensics 2023 (DF2023)\ntraining and validation dataset, comprising one million images from four major\nforgery categories: splicing, copy-move, enhancement and removal. This dataset\nenables an objective comparison of network architectures and can significantly\nreduce the time and effort of researchers preparing datasets.\n","date":"2025-03-28"}
{"id":"2503.22418","title":"Robustness quantification and how it allows for reliable classification,\n  even in the presence of distribution shift and for small training sets","abstract":"  Based on existing ideas in the field of imprecise probabilities, we present a\nnew approach for assessing the reliability of the individual predictions of a\ngenerative probabilistic classifier. We call this approach robustness\nquantification, compare it to uncertainty quantification, and demonstrate that\nit continues to work well even for classifiers that are learned from small\ntraining sets that are sampled from a shifted distribution.\n","date":"2025-03-28"}
{"id":"2503.22420","title":"Unveiling the Mist over 3D Vision-Language Understanding: Object-centric\n  Evaluation with Chain-of-Analysis","abstract":"  Existing 3D vision-language (3D-VL) benchmarks fall short in evaluating 3D-VL\nmodels, creating a \"mist\" that obscures rigorous insights into model\ncapabilities and 3D-VL tasks. This mist persists due to three key limitations.\nFirst, flawed test data, like ambiguous referential text in the grounding task,\ncan yield incorrect and unreliable test results. Second, oversimplified metrics\nsuch as simply averaging accuracy per question answering (QA) pair, cannot\nreveal true model capability due to their vulnerability to language variations.\nThird, existing benchmarks isolate the grounding and QA tasks, disregarding the\nunderlying coherence that QA should be based on solid grounding capabilities.\nTo unveil the \"mist\", we propose Beacon3D, a benchmark for 3D-VL grounding and\nQA tasks, delivering a perspective shift in the evaluation of 3D-VL\nunderstanding. Beacon3D features (i) high-quality test data with precise and\nnatural language, (ii) object-centric evaluation with multiple tests per object\nto ensure robustness, and (iii) a novel chain-of-analysis paradigm to address\nlanguage robustness and model performance coherence across grounding and QA.\nOur evaluation of state-of-the-art 3D-VL models on Beacon3D reveals that (i)\nobject-centric evaluation elicits true model performance and particularly weak\ngeneralization in QA; (ii) grounding-QA coherence remains fragile in current\n3D-VL models, and (iii) incorporating large language models (LLMs) to 3D-VL\nmodels, though as a prevalent practice, hinders grounding capabilities and has\nyet to elevate QA capabilities. We hope Beacon3D and our comprehensive analysis\ncould benefit the 3D-VL community towards faithful developments.\n","date":"2025-03-28"}
{"id":"2503.22424","title":"CoSIL: Software Issue Localization via LLM-Driven Code Repository Graph\n  Searching","abstract":"  Large language models (LLMs) have significantly advanced autonomous software\nengineering, leading to a growing number of software engineering agents that\nassist developers in automatic program repair. Issue localization forms the\nbasis for accurate patch generation. However, because of limitations caused by\nthe context window length of LLMs, existing issue localization methods face\nchallenges in balancing concise yet effective contexts and adequately\ncomprehensive search spaces. In this paper, we introduce CoSIL, an LLM driven,\nsimple yet powerful function level issue localization method without training\nor indexing. CoSIL reduces the search space through module call graphs,\niteratively searches the function call graph to obtain relevant contexts, and\nuses context pruning to control the search direction and manage contexts\neffectively. Importantly, the call graph is dynamically constructed by the LLM\nduring search, eliminating the need for pre-parsing. Experiment results\ndemonstrate that CoSIL achieves a Top-1 localization success rate of 43 percent\nand 44.6 percent on SWE bench Lite and SWE bench Verified, respectively, using\nQwen2.5 Coder 32B, outperforming existing methods by 8.6 to 98.2 percent. When\nCoSIL is applied to guide the patch generation stage, the resolved rate further\nimproves by 9.3 to 31.5 percent.\n","date":"2025-03-28"}
{"id":"2503.22426","title":"Long-Tail Crisis in Nearest Neighbor Language Models","abstract":"  The $k$-nearest-neighbor language model ($k$NN-LM), one of the\nretrieval-augmented language models, improves the perplexity for given text by\ndirectly accessing a large datastore built from any text data during inference.\nA widely held hypothesis for the success of $k$NN-LM is that its explicit\nmemory, i.e., the datastore, enhances predictions for long-tail phenomena.\nHowever, prior works have primarily shown its ability to retrieve long-tail\ncontexts, leaving the model's performance remain underexplored in estimating\nthe probabilities of long-tail target tokens during inference. In this paper,\nwe investigate the behavior of $k$NN-LM on low-frequency tokens, examining\nprediction probability, retrieval accuracy, token distribution in the\ndatastore, and approximation error of the product quantization. Our\nexperimental results reveal that $k$NN-LM does not improve prediction\nperformance for low-frequency tokens but mainly benefits high-frequency tokens\nregardless of long-tail contexts in the datastore.\n","date":"2025-03-28"}
{"id":"2503.22430","title":"MVSAnywhere: Zero-Shot Multi-View Stereo","abstract":"  Computing accurate depth from multiple views is a fundamental and\nlongstanding challenge in computer vision. However, most existing approaches do\nnot generalize well across different domains and scene types (e.g. indoor vs.\noutdoor). Training a general-purpose multi-view stereo model is challenging and\nraises several questions, e.g. how to best make use of transformer-based\narchitectures, how to incorporate additional metadata when there is a variable\nnumber of input views, and how to estimate the range of valid depths which can\nvary considerably across different scenes and is typically not known a priori?\nTo address these issues, we introduce MVSA, a novel and versatile Multi-View\nStereo architecture that aims to work Anywhere by generalizing across diverse\ndomains and depth ranges. MVSA combines monocular and multi-view cues with an\nadaptive cost volume to deal with scale-related issues. We demonstrate\nstate-of-the-art zero-shot depth estimation on the Robust Multi-View Depth\nBenchmark, surpassing existing multi-view stereo and monocular baselines.\n","date":"2025-03-28"}
{"id":"2503.22436","title":"NuGrounding: A Multi-View 3D Visual Grounding Framework in Autonomous\n  Driving","abstract":"  Multi-view 3D visual grounding is critical for autonomous driving vehicles to\ninterpret natural languages and localize target objects in complex\nenvironments. However, existing datasets and methods suffer from coarse-grained\nlanguage instructions, and inadequate integration of 3D geometric reasoning\nwith linguistic comprehension. To this end, we introduce NuGrounding, the first\nlarge-scale benchmark for multi-view 3D visual grounding in autonomous driving.\nWe present a Hierarchy of Grounding (HoG) method to construct NuGrounding to\ngenerate hierarchical multi-level instructions, ensuring comprehensive coverage\nof human instruction patterns. To tackle this challenging dataset, we propose a\nnovel paradigm that seamlessly combines instruction comprehension abilities of\nmulti-modal LLMs (MLLMs) with precise localization abilities of specialist\ndetection models. Our approach introduces two decoupled task tokens and a\ncontext query to aggregate 3D geometric information and semantic instructions,\nfollowed by a fusion decoder to refine spatial-semantic feature fusion for\nprecise localization. Extensive experiments demonstrate that our method\nsignificantly outperforms the baselines adapted from representative 3D scene\nunderstanding methods by a significant margin and achieves 0.59 in precision\nand 0.64 in recall, with improvements of 50.8% and 54.7%.\n","date":"2025-03-28"}
{"id":"2503.22437","title":"EndoLRMGS: Complete Endoscopic Scene Reconstruction combining Large\n  Reconstruction Modelling and Gaussian Splatting","abstract":"  Complete reconstruction of surgical scenes is crucial for robot-assisted\nsurgery (RAS). Deep depth estimation is promising but existing works struggle\nwith depth discontinuities, resulting in noisy predictions at object boundaries\nand do not achieve complete reconstruction omitting occluded surfaces. To\naddress these issues we propose EndoLRMGS, that combines Large Reconstruction\nModelling (LRM) and Gaussian Splatting (GS), for complete surgical scene\nreconstruction. GS reconstructs deformable tissues and LRM generates 3D models\nfor surgical tools while position and scale are subsequently optimized by\nintroducing orthogonal perspective joint projection optimization (OPjPO) to\nenhance accuracy. In experiments on four surgical videos from three public\ndatasets, our method improves the Intersection-over-union (IoU) of tool 3D\nmodels in 2D projections by>40%. Additionally, EndoLRMGS improves the PSNR of\nthe tools projection from 3.82% to 11.07%. Tissue rendering quality also\nimproves, with PSNR increasing from 0.46% to 49.87%, and SSIM from 1.53% to\n29.21% across all test videos.\n","date":"2025-03-28"}
{"id":"2503.22444","title":"Scaling Laws in Scientific Discovery with AI and Robot Scientists","abstract":"  Scientific discovery is poised for rapid advancement through advanced\nrobotics and artificial intelligence. Current scientific practices face\nsubstantial limitations as manual experimentation remains time-consuming and\nresource-intensive, while multidisciplinary research demands knowledge\nintegration beyond individual researchers' expertise boundaries. Here, we\nenvision an autonomous generalist scientist (AGS) concept combines agentic AI\nand embodied robotics to automate the entire research lifecycle. This system\ncould dynamically interact with both physical and virtual environments while\nfacilitating the integration of knowledge across diverse scientific\ndisciplines. By deploying these technologies throughout every research stage --\nspanning literature review, hypothesis generation, experimentation, and\nmanuscript writing -- and incorporating internal reflection alongside external\nfeedback, this system aims to significantly reduce the time and resources\nneeded for scientific discovery. Building on the evolution from virtual AI\nscientists to versatile generalist AI-based robot scientists, AGS promises\ngroundbreaking potential. As these autonomous systems become increasingly\nintegrated into the research process, we hypothesize that scientific discovery\nmight adhere to new scaling laws, potentially shaped by the number and\ncapabilities of these autonomous systems, offering novel perspectives on how\nknowledge is generated and evolves. The adaptability of embodied robots to\nextreme environments, paired with the flywheel effect of accumulating\nscientific knowledge, holds the promise of continually pushing beyond both\nphysical and intellectual frontiers.\n","date":"2025-03-28"}
{"id":"2503.22448","title":"Comparison between neural network clustering, hierarchical clustering\n  and k-means clustering: Applications using fluidic lenses","abstract":"  A comparison between neural network clustering (NNC), hierarchical clustering\n(HC) and K-means clustering (KMC) is performed to evaluate the computational\nsuperiority of these three machine learning (ML) techniques for organizing\nlarge datasets into clusters. For NNC, a self-organizing map (SOM) training was\napplied to a collection of wavefront sensor reconstructions, decomposed in\nterms of 15 Zernike coefficients, characterizing the optical aberrations of the\nphase front transmitted by fluidic lenses. In order to understand the\ndistribution and structure of the 15 Zernike variables within an input space,\nSOM-neighboring weight distances, SOM-sample hits, SOM-weight positions and\nSOM-weight planes were analyzed to form a visual interpretation of the system's\nstructural properties. In the case of HC, the data was partitioned using a\ncombined dissimilarity-linkage matrix computation. The effectiveness of this\nmethod was confirmed by a high cophenetic correlation coefficient value\n(c=0.9651). Additionally, a maximum number of clusters was established by\nsetting an inconsistency cutoff of 0.8, yielding a total of 7 clusters for\nsystem segmentation. In addition, a KMC approach was employed to establish a\nquantitative measure of clustering segmentation efficiency, obtaining a\nsillhoute average value of 0.905 for data segmentation into K=5 non-overlapping\nclusters. On the other hand, the NNC analysis revealed that the 15 variables\ncould be characterized through the collective influence of 8 clusters. It was\nestablished that the formation of clusters through the combined linkage and\ndissimilarity algorithms of HC alongside KMC is a more dependable clustering\nsolution than separate assessment via NNC or HC, where altering the SOM size or\ninconsistency cutoff can lead to completely new clustering configurations.\n","date":"2025-03-28"}
{"id":"2503.22451","title":"STADE: Standard Deviation as a Pruning Metric","abstract":"  Recently, Large Language Models (LLMs) have become very widespread and are\nused to solve a wide variety of tasks. To successfully handle these tasks, LLMs\nrequire longer training times and larger model sizes. This makes LLMs ideal\ncandidates for pruning methods that reduce computational demands while\nmaintaining performance. Previous methods require a retraining phase after\npruning to maintain the original model's performance. However, state-of-the-art\npruning methods, such as Wanda, prune the model without retraining, making the\npruning process faster and more efficient. Building upon Wanda's work, this\nstudy provides a theoretical explanation of why the method is effective and\nleverages these insights to enhance the pruning process. Specifically, a\ntheoretical analysis of the pruning problem reveals a common scenario in\nMachine Learning where Wanda is the optimal pruning method. Furthermore, this\nanalysis is extended to cases where Wanda is no longer optimal, leading to the\ndevelopment of a new method, STADE, based on the standard deviation of the\ninput. From a theoretical standpoint, STADE demonstrates better generality\nacross different scenarios. Finally, extensive experiments on Llama and Open\nPre-trained Transformers (OPT) models validate these theoretical findings,\nshowing that depending on the training conditions, Wanda's optimal performance\nvaries as predicted by the theoretical framework. These insights contribute to\na more robust understanding of pruning strategies and their practical\nimplications. Code is available at: https:\/\/github.com\/Coello-dev\/STADE\/\n","date":"2025-03-28"}
{"id":"2503.22454","title":"A Causal Framework to Measure and Mitigate Non-binary Treatment\n  Discrimination","abstract":"  Fairness studies of algorithmic decision-making systems often simplify\ncomplex decision processes, such as bail or loan approvals, into binary\nclassification tasks. However, these approaches overlook that such decisions\nare not inherently binary (e.g., approve or not approve bail or loan); they\nalso involve non-binary treatment decisions (e.g., bail conditions or loan\nterms) that can influence the downstream outcomes (e.g., loan repayment or\nreoffending). In this paper, we argue that non-binary treatment decisions are\nintegral to the decision process and controlled by decision-makers and,\ntherefore, should be central to fairness analyses in algorithmic\ndecision-making. We propose a causal framework that extends fairness analyses\nand explicitly distinguishes between decision-subjects' covariates and the\ntreatment decisions. This specification allows decision-makers to use our\nframework to (i) measure treatment disparity and its downstream effects in\nhistorical data and, using counterfactual reasoning, (ii) mitigate the impact\nof past unfair treatment decisions when automating decision-making. We use our\nframework to empirically analyze four widely used loan approval datasets to\nreveal potential disparity in non-binary treatment decisions and their\ndiscriminatory impact on outcomes, highlighting the need to incorporate\ntreatment decisions in fairness assessments. Moreover, by intervening in\ntreatment decisions, we show that our framework effectively mitigates treatment\ndiscrimination from historical data to ensure fair risk score estimation and\n(non-binary) decision-making processes that benefit all stakeholders.\n","date":"2025-03-28"}
{"id":"2503.22456","title":"Entropy-guided sequence weighting for efficient exploration in RL-based\n  LLM fine-tuning","abstract":"  We introduce Entropy-Guided Sequence Weighting (EGSW), a novel approach that\nenhances the exploration-exploitation tradeoff by dynamically assigning weights\nto generated outputs based on their advantage and entropy for Reinforcement\nLearning-based Large Language Model fine-tuning. EGSW integrates entropy\nregularization with advantage-based weighting to balance policy updates,\nenabling efficient exploration in high-dimensional state spaces. By employing\ntemperature-scaled softmax weighting over sequences, EGSW prioritizing\nhigh-reward, high-uncertainty steps while maintaining training stability.\nAlthough originally developed to improve Group Relative Policy Optimization\n(GRPO) during large language model (LLM) fine-tuning, EGSW is generalizable to\nother reinforcement learning (RL) algorithms and can be implemented in both\nstep-wise and trajectory-wise settings. Empirical evaluations demonstrate that\nEGSW enhances GRPO reasoning ability, yielding improvements in sample\nefficiency. Future work will explore the application of EGSW to advanced RL\nmethodologies.\n","date":"2025-03-28"}
{"id":"2503.22458","title":"Evaluating LLM-based Agents for Multi-Turn Conversations: A Survey","abstract":"  This survey examines evaluation methods for large language model (LLM)-based\nagents in multi-turn conversational settings. Using a PRISMA-inspired\nframework, we systematically reviewed nearly 250 scholarly sources, capturing\nthe state of the art from various venues of publication, and establishing a\nsolid foundation for our analysis. Our study offers a structured approach by\ndeveloping two interrelated taxonomy systems: one that defines \\emph{what to\nevaluate} and another that explains \\emph{how to evaluate}. The first taxonomy\nidentifies key components of LLM-based agents for multi-turn conversations and\ntheir evaluation dimensions, including task completion, response quality, user\nexperience, memory and context retention, as well as planning and tool\nintegration. These components ensure that the performance of conversational\nagents is assessed in a holistic and meaningful manner. The second taxonomy\nsystem focuses on the evaluation methodologies. It categorizes approaches into\nannotation-based evaluations, automated metrics, hybrid strategies that combine\nhuman assessments with quantitative measures, and self-judging methods\nutilizing LLMs. This framework not only captures traditional metrics derived\nfrom language understanding, such as BLEU and ROUGE scores, but also\nincorporates advanced techniques that reflect the dynamic, interactive nature\nof multi-turn dialogues.\n","date":"2025-03-28"}
{"id":"2503.22462","title":"SemAlign3D: Semantic Correspondence between RGB-Images through Aligning\n  3D Object-Class Representations","abstract":"  Semantic correspondence made tremendous progress through the recent\nadvancements of large vision models (LVM). While these LVMs have been shown to\nreliably capture local semantics, the same can currently not be said for\ncapturing global geometric relationships between semantic object regions. This\nproblem leads to unreliable performance for semantic correspondence between\nimages with extreme view variation. In this work, we aim to leverage monocular\ndepth estimates to capture these geometric relationships for more robust and\ndata-efficient semantic correspondence. First, we introduce a simple but\neffective method to build 3D object-class representations from monocular depth\nestimates and LVM features using a sparsely annotated image correspondence\ndataset. Second, we formulate an alignment energy that can be minimized using\ngradient descent to obtain an alignment between the 3D object-class\nrepresentation and the object-class instance in the input RGB-image. Our method\nachieves state-of-the-art matching accuracy in multiple categories on the\nchallenging SPair-71k dataset, increasing the PCK@0.1 score by more than 10\npoints on three categories and overall by 3.3 points from 85.6% to 88.9%.\nAdditional resources and code are available at https:\/\/dub.sh\/semalign3d.\n","date":"2025-03-28"}
{"id":"2503.22473","title":"WorkTeam: Constructing Workflows from Natural Language with Multi-Agents","abstract":"  Workflows play a crucial role in enhancing enterprise efficiency by\norchestrating complex processes with multiple tools or components. However,\nhand-crafted workflow construction requires expert knowledge, presenting\nsignificant technical barriers. Recent advancements in Large Language Models\n(LLMs) have improved the generation of workflows from natural language\ninstructions (aka NL2Workflow), yet existing single LLM agent-based methods\nface performance degradation on complex tasks due to the need for specialized\nknowledge and the strain of task-switching. To tackle these challenges, we\npropose WorkTeam, a multi-agent NL2Workflow framework comprising a supervisor,\norchestrator, and filler agent, each with distinct roles that collaboratively\nenhance the conversion process. As there are currently no publicly available\nNL2Workflow benchmarks, we also introduce the HW-NL2Workflow dataset, which\nincludes 3,695 real-world business samples for training and evaluation.\nExperimental results show that our approach significantly increases the success\nrate of workflow construction, providing a novel and effective solution for\nenterprise NL2Workflow services.\n","date":"2025-03-28"}
{"id":"2503.22475","title":"DeepOFormer: Deep Operator Learning with Domain-informed Features for\n  Fatigue Life Prediction","abstract":"  Fatigue life characterizes the duration a material can function before\nfailure under specific environmental conditions, and is traditionally assessed\nusing stress-life (S-N) curves. While machine learning and deep learning offer\npromising results for fatigue life prediction, they face the overfitting\nchallenge because of the small size of fatigue experimental data in specific\nmaterials. To address this challenge, we propose, DeepOFormer, by formulating\nS-N curve prediction as an operator learning problem. DeepOFormer improves the\ndeep operator learning framework with a transformer-based encoder and a mean L2\nrelative error loss function. We also consider Stussi, Weibull, and Pascual and\nMeeker (PM) features as domain-informed features. These features are motivated\nby empirical fatigue models. To evaluate the performance of our DeepOFormer, we\ncompare it with different deep learning models and XGBoost on a dataset with 54\nS-N curves of aluminum alloys. With seven different aluminum alloys selected\nfor testing, our DeepOFormer achieves an R2 of 0.9515, a mean absolute error of\n0.2080, and a mean relative error of 0.5077, significantly outperforming\nstate-of-the-art deep\/machine learning methods including DeepONet,\nTabTransformer, and XGBoost, etc. The results highlight that our Deep0Former\nintegrating with domain-informed features substantially improves prediction\naccuracy and generalization capabilities for fatigue life prediction in\naluminum alloys.\n","date":"2025-03-28"}
{"id":"2503.22478","title":"Almost Bayesian: The Fractal Dynamics of Stochastic Gradient Descent","abstract":"  We show that the behavior of stochastic gradient descent is related to\nBayesian statistics by showing that SGD is effectively diffusion on a fractal\nlandscape, where the fractal dimension can be accounted for in a purely\nBayesian way. By doing this we show that SGD can be regarded as a modified\nBayesian sampler which accounts for accessibility constraints induced by the\nfractal structure of the loss landscape. We verify our results experimentally\nby examining the diffusion of weights during training. These results offer\ninsight into the factors which determine the learning process, and seemingly\nanswer the question of how SGD and purely Bayesian sampling are related.\n","date":"2025-03-28"}
{"id":"2503.22480","title":"Probabilistic Uncertain Reward Model: A Natural Generalization of\n  Bradley-Terry Reward Model","abstract":"  Reinforcement Learning from Human Feedback (RLHF) has emerged as a critical\ntechnique for training large language models. However, reward hacking-a\nphenomenon where models exploit flaws in the reward model-remains a significant\nbarrier to achieving robust and scalable intelligence through long-term\ntraining. Existing studies have proposed uncertain reward model to address\nreward hacking, however, they often lack systematic or theoretical foundations,\nfailing to model the uncertainty intrinsically emerging from preference data.\nIn this paper, we propose the Probabilistic Uncertain Reward Model (PURM), a\nnatural generalization of the classical Bradley-Terry reward model. PURM learns\nreward distributions directly from preference data and quantifies per-sample\nuncertainty via the average overlap area between reward distributions. To\nmitigate reward hacking, we further introduce an uncertainty-aware penalty into\nProximal Policy Optimization (PPO), which leverages the learned uncertainty to\ndynamically balance reward optimization and exploration. We propose a\nlightweight and easy-to-use implementation of PURM. Experiments demonstrate\nthat PURM significantly delays the onset of reward hacking while improving\nfinal reward performance, outperforming baseline methods in both stability and\neffectiveness.\n","date":"2025-03-28"}
{"id":"2503.22485","title":"SPDNet: Seasonal-Periodic Decomposition Network for Advanced Residential\n  Demand Forecasting","abstract":"  Residential electricity demand forecasting is critical for efficient energy\nmanagement and grid stability. Accurate predictions enable utility companies to\noptimize planning and operations. However, real-world residential electricity\ndemand data often exhibit intricate temporal variability, including multiple\nseasonalities, periodicities, and abrupt fluctuations, which pose significant\nchallenges for forecasting models. Previous models that rely on statistical\nmethods, recurrent, convolutional neural networks, and transformers often\nstruggle to capture these intricate temporal dynamics. To address these\nchallenges, we propose the Seasonal-Periodic Decomposition Network (SPDNet), a\nnovel deep learning framework consisting of two main modules. The first is the\nSeasonal-Trend Decomposition Module (STDM), which decomposes the input data\ninto trend, seasonal, and residual components. The second is the Periodical\nDecomposition Module (PDM), which employs the Fast Fourier Transform to\nidentify the dominant periods. For each dominant period, 1D input data is\nreshaped into a 2D tensor, where rows represent periods and columns correspond\nto frequencies. The 2D representations are then processed through three\nsubmodules: a 1D convolution to capture sharp fluctuations, a transformer-based\nencoder to model global patterns, and a 2D convolution to capture interactions\nbetween periods. Extensive experiments conducted on real-world residential\nelectricity load data demonstrate that SPDNet outperforms traditional and\nadvanced models in both forecasting accuracy and computational efficiency. The\ncode is available in this repository: https:\/\/github.com\/Tims2D\/SPDNet.\n","date":"2025-03-28"}
{"id":"2503.22496","title":"Scenario Dreamer: Vectorized Latent Diffusion for Generating Driving\n  Simulation Environments","abstract":"  We introduce Scenario Dreamer, a fully data-driven generative simulator for\nautonomous vehicle planning that generates both the initial traffic scene -\ncomprising a lane graph and agent bounding boxes - and closed-loop agent\nbehaviours. Existing methods for generating driving simulation environments\nencode the initial traffic scene as a rasterized image and, as such, require\nparameter-heavy networks that perform unnecessary computation due to many empty\npixels in the rasterized scene. Moreover, we find that existing methods that\nemploy rule-based agent behaviours lack diversity and realism. Scenario Dreamer\ninstead employs a novel vectorized latent diffusion model for initial scene\ngeneration that directly operates on the vectorized scene elements and an\nautoregressive Transformer for data-driven agent behaviour simulation. Scenario\nDreamer additionally supports scene extrapolation via diffusion inpainting,\nenabling the generation of unbounded simulation environments. Extensive\nexperiments show that Scenario Dreamer outperforms existing generative\nsimulators in realism and efficiency: the vectorized scene-generation base\nmodel achieves superior generation quality with around 2x fewer parameters, 6x\nlower generation latency, and 10x fewer GPU training hours compared to the\nstrongest baseline. We confirm its practical utility by showing that\nreinforcement learning planning agents are more challenged in Scenario Dreamer\nenvironments than traditional non-generative simulation environments,\nespecially on long and adversarial driving environments.\n","date":"2025-03-28"}
{"id":"2503.22498","title":"Learnable cut flow","abstract":"  Neural networks have emerged as a powerful paradigm for tasks in high energy\nphysics, yet their opaque training process renders them as a black box. In\ncontrast, the traditional cut flow method offers simplicity and\ninterpretability but demands human effort to identify optimal boundaries. To\nmerge the strengths of both approaches, we propose the Learnable Cut Flow\n(LCF), a neural network that transforms the traditional cut selection into a\nfully differentiable, data-driven process. LCF implements two cut\nstrategies-parallel, where observable distributions are treated independently,\nand sequential, where prior cuts shape subsequent ones-to flexibly determine\noptimal boundaries. Building on this, we introduce the Learnable Importance, a\nmetric that quantifies feature importance and adjusts their contributions to\nthe loss accordingly, offering model-driven insights unlike ad-hoc metrics. To\nensure differentiability, a modified loss function replaces hard cuts with mask\noperations, preserving data shape throughout the training process. LCF is\ntested on six varied mock datasets and a realistic diboson vs. QCD dataset.\nResults demonstrate that LCF (1) accurately learns cut boundaries across\ntypical feature distributions in both parallel and sequential strategies, (2)\nassigns higher importance to discriminative features with minimal overlap, (3)\nhandles redundant or correlated features robustly, and (4) performs effectively\nin real-world scenarios. In diboson dataset, LCF initially underperforms\nboosted decision trees and multiplayer perceptrons when using all observables.\nHowever, pruning less critical features-guided by learned importance-boosts its\nperformance to match or exceed these baselines. LCF bridges the gap between\ntraditional cut flow method and modern black-box neural networks, delivering\nactionable insights into the training process and feature importance.\n","date":"2025-03-28"}
{"id":"2503.22513","title":"Masked Self-Supervised Pre-Training for Text Recognition Transformers on\n  Large-Scale Datasets","abstract":"  Self-supervised learning has emerged as a powerful approach for leveraging\nlarge-scale unlabeled data to improve model performance in various domains. In\nthis paper, we explore masked self-supervised pre-training for text recognition\ntransformers. Specifically, we propose two modifications to the pre-training\nphase: progressively increasing the masking probability, and modifying the loss\nfunction to incorporate both masked and non-masked patches. We conduct\nextensive experiments using a dataset of 50M unlabeled text lines for\npre-training and four differently sized annotated datasets for fine-tuning.\nFurthermore, we compare our pre-trained models against those trained with\ntransfer learning, demonstrating the effectiveness of the self-supervised\npre-training. In particular, pre-training consistently improves the character\nerror rate of models, in some cases up to 30 % relatively. It is also on par\nwith transfer learning but without relying on extra annotated text lines.\n","date":"2025-03-28"}
{"id":"2503.22516","title":"Assessing Foundation Models for Sea Ice Type Segmentation in Sentinel-1\n  SAR Imagery","abstract":"  Accurate segmentation of sea ice types is essential for mapping and\noperational forecasting of sea ice conditions for safe navigation and resource\nextraction in ice-covered waters, as well as for understanding polar climate\nprocesses. While deep learning methods have shown promise in automating sea ice\nsegmentation, they often rely on extensive labeled datasets which require\nexpert knowledge and are time-consuming to create. Recently, foundation models\n(FMs) have shown excellent results for segmenting remote sensing images by\nutilizing pre-training on large datasets using self-supervised techniques.\nHowever, their effectiveness for sea ice segmentation remains unexplored,\nespecially given sea ice's complex structures, seasonal changes, and unique\nspectral signatures, as well as peculiar Synthetic Aperture Radar (SAR) imagery\ncharacteristics including banding and scalloping noise, and varying ice\nbackscatter characteristics, which are often missing in standard remote sensing\npre-training datasets. In particular, SAR images over polar regions are\nacquired using different modes than used to capture the images at lower\nlatitudes by the same sensors that form training datasets for FMs. This study\nevaluates ten remote sensing FMs for sea ice type segmentation using Sentinel-1\nSAR imagery, focusing on their seasonal and spatial generalization. Among the\nselected models, Prithvi-600M outperforms the baseline models, while CROMA\nachieves a very similar performance in F1-score. Our contributions include\noffering a systematic methodology for selecting FMs for sea ice data analysis,\na comprehensive benchmarking study on performances of FMs for sea ice\nsegmentation with tailored performance metrics, and insights into existing gaps\nand future directions for improving domain-specific models in polar\napplications using SAR data.\n","date":"2025-03-28"}
{"id":"2503.22517","title":"Exploiting Mixture-of-Experts Redundancy Unlocks Multimodal Generative\n  Abilities","abstract":"  In this work, we undertake the challenge of augmenting the existing\ngenerative capabilities of pre-trained text-only large language models (LLMs)\nwith multi-modal generation capability while satisfying two core constraints:\nC1 preserving the preservation of original language generative capabilities\nwith negligible performance degradation, and C2 adhering to a small parameter\nbudget to learn the new modality, ensuring scalability and efficiency. In\ncontrast to current approaches that add dedicated modules, thereby\nsignificantly increasing the parameter count, we propose a method that\nleverages the underutilized capacity inherent in deep models. Specifically, we\nexploit the parameter redundancy within Mixture-of-Experts (MoEs) as a source\nof additional capacity for learning a new modality, enabling better parameter\nefficiency (C1). Moreover, we preserve the original language generation\ncapabilities by applying low-rank adaptation exclusively to the tokens of the\nnew modality (C2). Furthermore, we introduce a novel parameter initialization\nscheme based on the Gromov-Wasserstein distance to improve convergence and\ntraining stability. Through an extensive analysis of the routing mechanism, we\nuncover the emergence of modality-specific pathways and decreased redundancy\nwithin the experts that can efficiently unlock multi-modal generative\ncapabilities. Overall, our method can be seamlessly applied to a wide range of\ncontemporary LLMs, providing a new pathway for transitioning from uni-modal to\nmulti-modal architectures.\n","date":"2025-03-28"}
{"id":"2503.22524","title":"Robust Offline Imitation Learning Through State-level Trajectory\n  Stitching","abstract":"  Imitation learning (IL) has proven effective for enabling robots to acquire\nvisuomotor skills through expert demonstrations. However, traditional IL\nmethods are limited by their reliance on high-quality, often scarce, expert\ndata, and suffer from covariate shift. To address these challenges, recent\nadvances in offline IL have incorporated suboptimal, unlabeled datasets into\nthe training. In this paper, we propose a novel approach to enhance policy\nlearning from mixed-quality offline datasets by leveraging task-relevant\ntrajectory fragments and rich environmental dynamics. Specifically, we\nintroduce a state-based search framework that stitches state-action pairs from\nimperfect demonstrations, generating more diverse and informative training\ntrajectories. Experimental results on standard IL benchmarks and real-world\nrobotic tasks showcase that our proposed method significantly improves both\ngeneralization and performance.\n","date":"2025-03-28"}
{"id":"2503.22526","title":"AnnoPage Dataset: Dataset of Non-Textual Elements in Documents with\n  Fine-Grained Categorization","abstract":"  We introduce the AnnoPage Dataset, a novel collection of 7550 pages from\nhistorical documents, primarily in Czech and German, spanning from 1485 to the\npresent, focusing on the late 19th and early 20th centuries. The dataset is\ndesigned to support research in document layout analysis and object detection.\nEach page is annotated with axis-aligned bounding boxes (AABB) representing\nelements of 25 categories of non-textual elements, such as images, maps,\ndecorative elements, or charts, following the Czech Methodology of image\ndocument processing. The annotations were created by expert librarians to\nensure accuracy and consistency. The dataset also incorporates pages from\nmultiple, mainly historical, document datasets to enhance variability and\nmaintain continuity. The dataset is divided into development and test subsets,\nwith the test set carefully selected to maintain the category distribution. We\nprovide baseline results using YOLO and DETR object detectors, offering a\nreference point for future research. The AnnoPage Dataset is publicly available\non Zenodo (https:\/\/doi.org\/10.5281\/zenodo.12788419), along with ground-truth\nannotations in YOLO format.\n","date":"2025-03-28"}
{"id":"2503.22528","title":"MixFunn: A Neural Network for Differential Equations with Improved\n  Generalization and Interpretability","abstract":"  We introduce MixFunn, a novel neural network architecture designed to solve\ndifferential equations with enhanced precision, interpretability, and\ngeneralization capability. The architecture comprises two key components: the\nmixed-function neuron, which integrates multiple parameterized nonlinear\nfunctions to improve representational flexibility, and the second-order neuron,\nwhich combines a linear transformation of its inputs with a quadratic term to\ncapture cross-combinations of input variables. These features significantly\nenhance the expressive power of the network, enabling it to achieve comparable\nor superior results with drastically fewer parameters and a reduction of up to\nfour orders of magnitude compared to conventional approaches. We applied\nMixFunn in a physics-informed setting to solve differential equations in\nclassical mechanics, quantum mechanics, and fluid dynamics, demonstrating its\neffectiveness in achieving higher accuracy and improved generalization to\nregions outside the training domain relative to standard machine learning\nmodels. Furthermore, the architecture facilitates the extraction of\ninterpretable analytical expressions, offering valuable insights into the\nunderlying solutions.\n","date":"2025-03-28"}
{"id":"2503.22531","title":"Deterministic Medical Image Translation via High-fidelity Brownian\n  Bridges","abstract":"  Recent studies have shown that diffusion models produce superior synthetic\nimages when compared to Generative Adversarial Networks (GANs). However, their\noutputs are often non-deterministic and lack high fidelity to the ground truth\ndue to the inherent randomness. In this paper, we propose a novel High-fidelity\nBrownian bridge model (HiFi-BBrg) for deterministic medical image translations.\nOur model comprises two distinct yet mutually beneficial mappings: a generation\nmapping and a reconstruction mapping. The Brownian bridge training process is\nguided by the fidelity loss and adversarial training in the reconstruction\nmapping. This ensures that translated images can be accurately reversed to\ntheir original forms, thereby achieving consistent translations with high\nfidelity to the ground truth. Our extensive experiments on multiple datasets\nshow HiFi-BBrg outperforms state-of-the-art methods in multi-modal image\ntranslation and multi-image super-resolution.\n","date":"2025-03-28"}
{"id":"2503.22537","title":"LIM: Large Interpolator Model for Dynamic Reconstruction","abstract":"  Reconstructing dynamic assets from video data is central to many in computer\nvision and graphics tasks. Existing 4D reconstruction approaches are limited by\ncategory-specific models or slow optimization-based methods. Inspired by the\nrecent Large Reconstruction Model (LRM), we present the Large Interpolation\nModel (LIM), a transformer-based feed-forward solution, guided by a novel\ncausal consistency loss, for interpolating implicit 3D representations across\ntime. Given implicit 3D representations at times $t_0$ and $t_1$, LIM produces\na deformed shape at any continuous time $t\\in[t_0,t_1]$, delivering\nhigh-quality interpolated frames in seconds. Furthermore, LIM allows explicit\nmesh tracking across time, producing a consistently uv-textured mesh sequence\nready for integration into existing production pipelines. We also use LIM, in\nconjunction with a diffusion-based multiview generator, to produce dynamic 4D\nreconstructions from monocular videos. We evaluate LIM on various dynamic\ndatasets, benchmarking against image-space interpolation methods (e.g., FiLM)\nand direct triplane linear interpolation, and demonstrate clear advantages. In\nsummary, LIM is the first feed-forward model capable of high-speed tracked 4D\nasset reconstruction across diverse categories.\n","date":"2025-03-28"}
{"id":"2503.22539","title":"Efficient Verified Machine Unlearning For Distillation","abstract":"  Growing data privacy demands, driven by regulations like GDPR and CCPA,\nrequire machine unlearning methods capable of swiftly removing the influence of\nspecific training points. Although verified approaches like SISA, using data\nslicing and checkpointing, achieve efficient unlearning for single models by\nreverting to intermediate states, these methods struggle in teacher-student\nknowledge distillation settings. Unlearning in the teacher typically forces\ncostly, complete student retraining due to pervasive information propagation\nduring distillation. Our primary contribution is PURGE (Partitioned Unlearning\nwith Retraining Guarantee for Ensembles), a novel framework integrating\nverified unlearning with distillation. We introduce constituent mapping and an\nincremental multi-teacher strategy that partitions the distillation process,\nconfines each teacher constituent's impact to distinct student data subsets,\nand crucially maintains data isolation. The PURGE framework substantially\nreduces retraining overhead, requiring only partial student updates when\nteacher-side unlearning occurs. We provide both theoretical analysis,\nquantifying significant speed-ups in the unlearning process, and empirical\nvalidation on multiple datasets, demonstrating that PURGE achieves these\nefficiency gains while maintaining student accuracy comparable to standard\nbaselines.\n","date":"2025-03-28"}
{"id":"2503.22541","title":"SafeCast: Risk-Responsive Motion Forecasting for Autonomous Vehicles","abstract":"  Accurate motion forecasting is essential for the safety and reliability of\nautonomous driving (AD) systems. While existing methods have made significant\nprogress, they often overlook explicit safety constraints and struggle to\ncapture the complex interactions among traffic agents, environmental factors,\nand motion dynamics. To address these challenges, we present SafeCast, a\nrisk-responsive motion forecasting model that integrates safety-aware\ndecision-making with uncertainty-aware adaptability. SafeCast is the first to\nincorporate the Responsibility-Sensitive Safety (RSS) framework into motion\nforecasting, encoding interpretable safety rules--such as safe distances and\ncollision avoidance--based on traffic norms and physical principles. To further\nenhance robustness, we introduce the Graph Uncertainty Feature (GUF), a\ngraph-based module that injects learnable noise into Graph Attention Networks,\ncapturing real-world uncertainties and enhancing generalization across diverse\nscenarios. We evaluate SafeCast on four real-world benchmark datasets--Next\nGeneration Simulation (NGSIM), Highway Drone (HighD), ApolloScape, and the\nMacao Connected Autonomous Driving (MoCAD)--covering highway, urban, and\nmixed-autonomy traffic environments. Our model achieves state-of-the-art (SOTA)\naccuracy while maintaining a lightweight architecture and low inference\nlatency, underscoring its potential for real-time deployment in safety-critical\nAD systems.\n","date":"2025-03-28"}
{"id":"2503.22547","title":"Bridging the Dimensional Chasm: Uncover Layer-wise Dimensional Reduction\n  in Transformers through Token Correlation","abstract":"  The geometric evolution of token representations in large language models\n(LLMs) presents a fundamental paradox: while human language inherently\norganizes semantic information in low-dimensional spaces ($\\sim 10^1$\ndimensions), modern LLMs employ high-dimensional embeddings ($\\sim 10^3$\ndimensions) processed through Transformer architectures. To resolve this\nparadox, this work bridges this conceptual gap by developing a geometric\nframework that tracks token dynamics across Transformers layers. Through\nlayer-wise analysis of intrinsic dimensions across multiple architectures, we\nreveal an expansion-contraction pattern where tokens diffuse to a \"working\nspace\" and then progressively project onto lower-dimensional submanifolds. Our\nfinding implies a negative correlation between the working space dimension and\nparameter-sensitive performance of the LLMs, and indicates that effective\nmodels tend to compress tokens into approximately 10-dimensional submanifolds,\nclosely resembling human semantic spaces. This work not only advances LLM\ninterpretability by reframing Transformers layers as projectors that mediate\nbetween high-dimensional computation and low-dimensional semantics, but also\nprovides practical tools for model diagnostics that do not rely on\ntask-specific evaluations.\n","date":"2025-03-28"}
{"id":"2503.22557","title":"MO-CTranS: A unified multi-organ segmentation model learning from\n  multiple heterogeneously labelled datasets","abstract":"  Multi-organ segmentation holds paramount significance in many clinical tasks.\nIn practice, compared to large fully annotated datasets, multiple small\ndatasets are often more accessible and organs are not labelled consistently.\nNormally, an individual model is trained for each of these datasets, which is\nnot an effective way of using data for model learning. It remains challenging\nto train a single model that can robustly learn from several partially labelled\ndatasets due to label conflict and data imbalance problems. We propose\nMO-CTranS: a single model that can overcome such problems. MO-CTranS contains a\nCNN-based encoder and a Transformer-based decoder, which are connected in a\nmulti-resolution manner. Task-specific tokens are introduced in the decoder to\nhelp differentiate label discrepancies. Our method was evaluated and compared\nto several baseline models and state-of-the-art (SOTA) solutions on abdominal\nMRI datasets that were acquired in different views (i.e. axial and coronal) and\nannotated for different organs (i.e. liver, kidney, spleen). Our method\nachieved better performance (most were statistically significant) than the\ncompared methods. Github link: https:\/\/github.com\/naisops\/MO-CTranS.\n","date":"2025-03-28"}
{"id":"2503.22560","title":"Image Decomposition with G-norm Weighted by Total Symmetric Variation","abstract":"  In this paper, we propose a novel variational model for decomposing images\ninto their respective cartoon and texture parts. Our model characterizes\ncertain non-local features of any Bounded Variation (BV) image by its Total\nSymmetric Variation (TSV). We demonstrate that TSV is effective in identifying\nregional boundaries. Based on this property, we introduce a weighted Meyer's\n$G$-norm to identify texture interiors without including contour edges. For BV\nimages with bounded TSV, we show that the proposed model admits a solution.\nAdditionally, we design a fast algorithm based on operator-splitting to tackle\nthe associated non-convex optimization problem. The performance of our method\nis validated by a series of numerical experiments.\n","date":"2025-03-28"}
{"id":"2503.22562","title":"Niyama : Breaking the Silos of LLM Inference Serving","abstract":"  The widespread adoption of Large Language Models (LLMs) has enabled diverse\napplications with very different latency requirements. Existing LLM serving\nframeworks rely on siloed infrastructure with coarse-grained workload\nsegregation -- interactive and batch -- leading to inefficient resource\nutilization and limited support for fine-grained Quality-of-Service (QoS)\ndifferentiation. This results in operational inefficiencies, over-provisioning\nand poor load management during traffic surges.\n  We present Niyama, a novel QoS-driven inference serving system that enables\nefficient co-scheduling of diverse workloads on shared infrastructure. Niyama\nintroduces fine-grained QoS classification allowing applications to specify\nprecise latency requirements, and dynamically adapts scheduling decisions based\non real-time system state. Leveraging the predictable execution characteristics\nof LLM inference, Niyama implements a dynamic chunking mechanism to improve\noverall throughput while maintaining strict QoS guarantees. Additionally,\nNiyama employs a hybrid prioritization policy that balances fairness and\nefficiency, and employs selective request relegation that enables graceful\nservice degradation during overload conditions. Our evaluation demonstrates\nthat Niyama increases serving capacity by 32% compared to current siloed\ndeployments, while maintaining QoS guarantees. Notably, under extreme load, our\nsystem reduces SLO violations by an order of magnitude compared to current\nstrategies.\n","date":"2025-03-28"}
{"id":"2503.22563","title":"RELD: Regularization by Latent Diffusion Models for Image Restoration","abstract":"  In recent years, Diffusion Models have become the new state-of-the-art in\ndeep generative modeling, ending the long-time dominance of Generative\nAdversarial Networks. Inspired by the Regularization by Denoising principle, we\nintroduce an approach that integrates a Latent Diffusion Model, trained for the\ndenoising task, into a variational framework using Half-Quadratic Splitting,\nexploiting its regularization properties. This approach, under appropriate\nconditions that can be easily met in various imaging applications, allows for\nreduced computational cost while achieving high-quality results. The proposed\nstrategy, called Regularization by Latent Denoising (RELD), is then tested on a\ndataset of natural images, for image denoising, deblurring, and\nsuper-resolution tasks. The numerical experiments show that RELD is competitive\nwith other state-of-the-art methods, particularly achieving remarkable results\nwhen evaluated using perceptual quality metrics.\n","date":"2025-03-28"}
{"id":"2503.22567","title":"Benchmarking Ultra-Low-Power $\\mu$NPUs","abstract":"  Efficient on-device neural network (NN) inference has various advantages over\ncloud-based processing, including predictable latency, enhanced privacy,\ngreater reliability, and reduced operating costs for vendors. This has sparked\nthe recent rapid development of microcontroller-scale NN accelerators, often\nreferred to as neural processing units ($\\mu$NPUs), designed specifically for\nultra-low-power applications.\n  In this paper we present the first comparative evaluation of a number of\ncommercially-available $\\mu$NPUs, as well as the first independent benchmarks\nfor several of these platforms. We develop and open-source a model compilation\nframework to enable consistent benchmarking of quantized models across diverse\n$\\mu$NPU hardware. Our benchmark targets end-to-end performance and includes\nmodel inference latency, power consumption, and memory overhead, alongside\nother factors. The resulting analysis uncovers both expected performance trends\nas well as surprising disparities between hardware specifications and actual\nperformance, including $\\mu$NPUs exhibiting unexpected scaling behaviors with\nincreasing model complexity. Our framework provides a foundation for further\nevaluation of $\\mu$NPU platforms alongside valuable insights for both hardware\ndesigners and software developers in this rapidly evolving space.\n","date":"2025-03-28"}
{"id":"2503.22569","title":"Comparing Methods for Bias Mitigation in Graph Neural Networks","abstract":"  This paper examines the critical role of Graph Neural Networks (GNNs) in data\npreparation for generative artificial intelligence (GenAI) systems, with a\nparticular focus on addressing and mitigating biases. We present a comparative\nanalysis of three distinct methods for bias mitigation: data sparsification,\nfeature modification, and synthetic data augmentation. Through experimental\nanalysis using the german credit dataset, we evaluate these approaches using\nmultiple fairness metrics, including statistical parity, equality of\nopportunity, and false positive rates. Our research demonstrates that while all\nmethods improve fairness metrics compared to the original dataset, stratified\nsampling and synthetic data augmentation using GraphSAGE prove particularly\neffective in balancing demographic representation while maintaining model\nperformance. The results provide practical insights for developing more\nequitable AI systems while maintaining model performance.\n","date":"2025-03-28"}
{"id":"2503.22573","title":"A Framework for Cryptographic Verifiability of End-to-End AI Pipelines","abstract":"  The increasing integration of Artificial Intelligence across multiple\nindustry sectors necessitates robust mechanisms for ensuring transparency,\ntrust, and auditability of its development and deployment. This topic is\nparticularly important in light of recent calls in various jurisdictions to\nintroduce regulation and legislation on AI safety. In this paper, we propose a\nframework for complete verifiable AI pipelines, identifying key components and\nanalyzing existing cryptographic approaches that contribute to verifiability\nacross different stages of the AI lifecycle, from data sourcing to training,\ninference, and unlearning. This framework could be used to combat\nmisinformation by providing cryptographic proofs alongside AI-generated assets\nto allow downstream verification of their provenance and correctness. Our\nfindings underscore the importance of ongoing research to develop cryptographic\ntools that are not only efficient for isolated AI processes, but that are\nefficiently `linkable' across different processes within the AI pipeline, to\nsupport the development of end-to-end verifiable AI technologies.\n","date":"2025-03-28"}
{"id":"2503.22575","title":"On the Mistaken Assumption of Interchangeable Deep Reinforcement\n  Learning Implementations","abstract":"  Deep Reinforcement Learning (DRL) is a paradigm of artificial intelligence\nwhere an agent uses a neural network to learn which actions to take in a given\nenvironment. DRL has recently gained traction from being able to solve complex\nenvironments like driving simulators, 3D robotic control, and\nmultiplayer-online-battle-arena video games. Numerous implementations of the\nstate-of-the-art algorithms responsible for training these agents, like the\nDeep Q-Network (DQN) and Proximal Policy Optimization (PPO) algorithms,\ncurrently exist. However, studies make the mistake of assuming implementations\nof the same algorithm to be consistent and thus, interchangeable. In this\npaper, through a differential testing lens, we present the results of studying\nthe extent of implementation inconsistencies, their effect on the\nimplementations' performance, as well as their impact on the conclusions of\nprior studies under the assumption of interchangeable implementations. The\noutcomes of our differential tests showed significant discrepancies between the\ntested algorithm implementations, indicating that they are not interchangeable.\nIn particular, out of the five PPO implementations tested on 56 games, three\nimplementations achieved superhuman performance for 50% of their total trials\nwhile the other two implementations only achieved superhuman performance for\nless than 15% of their total trials. As part of a meticulous manual analysis of\nthe implementations' source code, we analyzed implementation discrepancies and\ndetermined that code-level inconsistencies primarily caused these\ndiscrepancies. Lastly, we replicated a study and showed that this assumption of\nimplementation interchangeability was sufficient to flip experiment outcomes.\nTherefore, this calls for a shift in how implementations are being used.\n","date":"2025-03-28"}
{"id":"2503.22577","title":"Breaking Language Barriers in Visual Language Models via Multilingual\n  Textual Regularization","abstract":"  Rapid advancements in Visual Language Models (VLMs) have transformed\nmultimodal understanding but are often constrained by generating English\nresponses regardless of the input language. This phenomenon has been termed as\nImage-induced Fidelity Loss (IFL) and stems from limited multimodal\nmultilingual training data. To address this, we propose a continuous\nmultilingual integration strategy that injects text-only multilingual data\nduring visual instruction tuning, preserving the language model's original\nmultilingual capabilities. Extensive evaluations demonstrate that our approach\nsignificantly improves linguistic fidelity across languages without degradation\nin visual performance. We also explore model merging, which improves language\nfidelity but comes at the cost of visual performance. In contrast, our core\nmethod achieves robust multilingual alignment without trade-offs, offering a\nscalable and effective path to mitigating IFL for global VLM adoption.\n","date":"2025-03-28"}
{"id":"2503.22582","title":"Beyond Vanilla Fine-Tuning: Leveraging Multistage, Multilingual, and\n  Domain-Specific Methods for Low-Resource Machine Translation","abstract":"  Fine-tuning multilingual sequence-to-sequence large language models (msLLMs)\nhas shown promise in developing neural machine translation (NMT) systems for\nlow-resource languages (LRLs). However, conventional single-stage fine-tuning\nmethods struggle in extremely low-resource NMT settings, where training data is\nvery limited. This paper contributes to artificial intelligence by proposing\ntwo approaches for adapting msLLMs in these challenging scenarios: (1)\ncontinual pre-training (CPT), where the msLLM is further trained with\ndomain-specific monolingual data to compensate for the under-representation of\nLRLs, and (2) intermediate task transfer learning (ITTL), a method that\nfine-tunes the msLLM with both in-domain and out-of-domain parallel data to\nenhance its translation capabilities across various domains and tasks. As an\napplication in engineering, these methods are implemented in NMT systems for\nSinhala, Tamil, and English (six language pairs) in domain-specific, extremely\nlow-resource settings (datasets containing fewer than 100,000 samples). Our\nexperiments reveal that these approaches enhance translation performance by an\naverage of +1.47 bilingual evaluation understudy (BLEU) score compared to the\nstandard single-stage fine-tuning baseline across all translation directions.\nAdditionally, a multi-model ensemble further improves performance by an\nadditional BLEU score.\n","date":"2025-03-28"}
{"id":"2503.22585","title":"Historical Ink: Exploring Large Language Models for Irony Detection in\n  19th-Century Spanish","abstract":"  This study explores the use of large language models (LLMs) to enhance\ndatasets and improve irony detection in 19th-century Latin American newspapers.\nTwo strategies were employed to evaluate the efficacy of BERT and GPT-4o models\nin capturing the subtle nuances nature of irony, through both multi-class and\nbinary classification tasks. First, we implemented dataset enhancements focused\non enriching emotional and contextual cues; however, these showed limited\nimpact on historical language analysis. The second strategy, a semi-automated\nannotation process, effectively addressed class imbalance and augmented the\ndataset with high-quality annotations. Despite the challenges posed by the\ncomplexity of irony, this work contributes to the advancement of sentiment\nanalysis through two key contributions: introducing a new historical Spanish\ndataset tagged for sentiment analysis and irony detection, and proposing a\nsemi-automated annotation methodology where human expertise is crucial for\nrefining LLMs results, enriched by incorporating historical and cultural\ncontexts as core features.\n","date":"2025-03-28"}
{"id":"2503.22588","title":"Next-Best-Trajectory Planning of Robot Manipulators for Effective\n  Observation and Exploration","abstract":"  Visual observation of objects is essential for many robotic applications,\nsuch as object reconstruction and manipulation, navigation, and scene\nunderstanding. Machine learning algorithms constitute the state-of-the-art in\nmany fields but require vast data sets, which are costly and time-intensive to\ncollect. Automated strategies for observation and exploration are crucial to\nenhance the efficiency of data gathering. Therefore, a novel strategy utilizing\nthe Next-Best-Trajectory principle is developed for a robot manipulator\noperating in dynamic environments. Local trajectories are generated to maximize\nthe information gained from observations along the path while avoiding\ncollisions. We employ a voxel map for environment modeling and utilize\nraycasting from perspectives around a point of interest to estimate the\ninformation gain. A global ergodic trajectory planner provides an optional\nreference trajectory to the local planner, improving exploration and helping to\navoid local minima. To enhance computational efficiency, raycasting for\nestimating the information gain in the environment is executed in parallel on\nthe graphics processing unit. Benchmark results confirm the efficiency of the\nparallelization, while real-world experiments demonstrate the strategy's\neffectiveness.\n","date":"2025-03-28"}
{"id":"2503.22589","title":"Using AI to Summarize US Presidential Campaign TV Advertisement Videos,\n  1952-2012","abstract":"  This paper introduces the largest and most comprehensive dataset of US\npresidential campaign television advertisements, available in digital format.\nThe dataset also includes machine-searchable transcripts and high-quality\nsummaries designed to facilitate a variety of academic research. To date, there\nhas been great interest in collecting and analyzing US presidential campaign\nadvertisements, but the need for manual procurement and annotation led many to\nrely on smaller subsets. We design a large-scale parallelized, AI-based\nanalysis pipeline that automates the laborious process of preparing,\ntranscribing, and summarizing videos. We then apply this methodology to the\n9,707 presidential ads from the Julian P. Kanter Political Commercial Archive.\nWe conduct extensive human evaluations to show that these transcripts and\nsummaries match the quality of manually generated alternatives. We illustrate\nthe value of this data by including an application that tracks the genesis and\nevolution of current focal issue areas over seven decades of presidential\nelections. Our analysis pipeline and codebase also show how to use LLM-based\ntools to obtain high-quality summaries for other video datasets.\n","date":"2025-03-28"}
{"id":"2503.22592","title":"KEVS: Enhancing Segmentation of Visceral Adipose Tissue in\n  Pre-Cystectomy CT with Gaussian Kernel Density Estimation","abstract":"  Purpose: The distribution of visceral adipose tissue (VAT) in cystectomy\npatients is indicative of the incidence of post-operative complications.\nExisting VAT segmentation methods for computed tomography (CT) employing\nintensity thresholding have limitations relating to inter-observer variability.\nMoreover, the difficulty in creating ground-truth masks limits the development\nof deep learning (DL) models for this task. This paper introduces a novel\nmethod for VAT prediction in pre-cystectomy CT, which is fully automated and\ndoes not require ground-truth VAT masks for training, overcoming aforementioned\nlimitations. Methods: We introduce the Kernel density Enhanced VAT Segmentator\n( KEVS), combining a DL semantic segmentation model, for multi-body feature\nprediction, with Gaussian kernel density estimation analysis of predicted\nsubcutaneous adipose tissue to achieve accurate scan-specific predictions of\nVAT in the abdominal cavity. Uniquely for a DL pipeline, KEVS does not require\nground-truth VAT masks. Results: We verify the ability of KEVS to accurately\nsegment abdominal organs in unseen CT data and compare KEVS VAT segmentation\npredictions to existing state-of-the-art (SOTA) approaches in a dataset of 20\npre-cystectomy CT scans, collected from University College London Hospital\n(UCLH-Cyst), with expert ground-truth annotations. KEVS presents a 4.80% and\n6.02% improvement in Dice Coefficient over the second best DL and\nthresholding-based VAT segmentation techniques respectively when evaluated on\nUCLH-Cyst. Conclusion: This research introduces KEVS; an automated, SOTA method\nfor the prediction of VAT in pre-cystectomy CT which eliminates inter-observer\nvariability and is trained entirely on open-source CT datasets which do not\ncontain ground-truth VAT masks.\n","date":"2025-03-28"}
{"id":"2503.22595","title":"Reinforcement Learning for Machine Learning Model Deployment: Evaluating\n  Multi-Armed Bandits in ML Ops Environments","abstract":"  In modern ML Ops environments, model deployment is a critical process that\ntraditionally relies on static heuristics such as validation error comparisons\nand A\/B testing. However, these methods require human intervention to adapt to\nreal-world deployment challenges, such as model drift or unexpected performance\ndegradation. We investigate whether reinforcement learning, specifically\nmulti-armed bandit (MAB) algorithms, can dynamically manage model deployment\ndecisions more effectively. Our approach enables more adaptive production\nenvironments by continuously evaluating deployed models and rolling back\nunderperforming ones in real-time. We test six model selection strategies\nacross two real-world datasets and find that RL based approaches match or\nexceed traditional methods in performance. Our findings suggest that\nreinforcement learning (RL)-based model management can improve automation,\nreduce reliance on manual interventions, and mitigate risks associated with\npost-deployment model failures.\n","date":"2025-03-28"}
{"id":"2503.22600","title":"Generative Latent Neural PDE Solver using Flow Matching","abstract":"  Autoregressive next-step prediction models have become the de-facto standard\nfor building data-driven neural solvers to forecast time-dependent partial\ndifferential equations (PDEs). Denoise training that is closely related to\ndiffusion probabilistic model has been shown to enhance the temporal stability\nof neural solvers, while its stochastic inference mechanism enables ensemble\npredictions and uncertainty quantification. In principle, such training\ninvolves sampling a series of discretized diffusion timesteps during both\ntraining and inference, inevitably increasing computational overhead. In\naddition, most diffusion models apply isotropic Gaussian noise on structured,\nuniform grids, limiting their adaptability to irregular domains. We propose a\nlatent diffusion model for PDE simulation that embeds the PDE state in a\nlower-dimensional latent space, which significantly reduces computational\ncosts. Our framework uses an autoencoder to map different types of meshes onto\na unified structured latent grid, capturing complex geometries. By analyzing\ncommon diffusion paths, we propose to use a coarsely sampled noise schedule\nfrom flow matching for both training and testing. Numerical experiments show\nthat the proposed model outperforms several deterministic baselines in both\naccuracy and long-term stability, highlighting the potential of diffusion-based\napproaches for robust data-driven PDE learning.\n","date":"2025-03-28"}
{"id":"2503.22605","title":"Audio-Plane: Audio Factorization Plane Gaussian Splatting for Real-Time\n  Talking Head Synthesis","abstract":"  Talking head synthesis has become a key research area in computer graphics\nand multimedia, yet most existing methods often struggle to balance generation\nquality with computational efficiency. In this paper, we present a novel\napproach that leverages an Audio Factorization Plane (Audio-Plane) based\nGaussian Splatting for high-quality and real-time talking head generation. For\nmodeling a dynamic talking head, 4D volume representation is needed. However,\ndirectly storing a dense 4D grid is impractical due to the high cost and lack\nof scalability for longer durations. We overcome this challenge with the\nproposed Audio-Plane, where the 4D volume representation is decomposed into\naudio-independent space planes and audio-dependent planes. This provides a\ncompact and interpretable feature representation for talking head, facilitating\nmore precise audio-aware spatial encoding and enhanced audio-driven lip dynamic\nmodeling. To further improve speech dynamics, we develop a dynamic splatting\nmethod that helps the network more effectively focus on modeling the dynamics\nof the mouth region. Extensive experiments demonstrate that by integrating\nthese innovations with the powerful Gaussian Splatting, our method is capable\nof synthesizing highly realistic talking videos in real time while ensuring\nprecise audio-lip synchronization. Synthesized results are available in\nhttps:\/\/sstzal.github.io\/Audio-Plane\/.\n","date":"2025-03-28"}
{"id":"2503.22610","title":"Evaluating Multimodal Language Models as Visual Assistants for Visually\n  Impaired Users","abstract":"  This paper explores the effectiveness of Multimodal Large Language models\n(MLLMs) as assistive technologies for visually impaired individuals. We conduct\na user survey to identify adoption patterns and key challenges users face with\nsuch technologies. Despite a high adoption rate of these models, our findings\nhighlight concerns related to contextual understanding, cultural sensitivity,\nand complex scene understanding, particularly for individuals who may rely\nsolely on them for visual interpretation. Informed by these results, we collate\nfive user-centred tasks with image and video inputs, including a novel task on\nOptical Braille Recognition. Our systematic evaluation of twelve MLLMs reveals\nthat further advancements are necessary to overcome limitations related to\ncultural context, multilingual support, Braille reading comprehension,\nassistive object recognition, and hallucinations. This work provides critical\ninsights into the future direction of multimodal AI for accessibility,\nunderscoring the need for more inclusive, robust, and trustworthy visual\nassistance technologies.\n","date":"2025-03-28"}
{"id":"2503.22617","title":"Using Machine Learning for Lunar Mineralogy-I: Hyperspectral Imaging of\n  Volcanic Samples","abstract":"  This study examines the mineral composition of volcanic samples similar to\nlunar materials, focusing on olivine and pyroxene. Using hyperspectral imaging\nfrom 400 to 1000 nm, we created data cubes to analyze the reflectance\ncharacteristics of samples from samples from Vulcano, a volcanically active\nisland in the Aeolian Archipelago, north of Sicily, Italy, categorizing them\ninto nine regions of interest and analyzing spectral data for each. We applied\nvarious unsupervised clustering algorithms, including K-Means, Hierarchical\nClustering, GMM, and Spectral Clustering, to classify the spectral profiles.\nPrincipal Component Analysis revealed distinct spectral signatures associated\nwith specific minerals, facilitating precise identification. Clustering\nperformance varied by region, with K-Means achieving the highest\nsilhouette-score of 0.47, whereas GMM performed poorly with a score of only\n0.25. Non-negative Matrix Factorization aided in identifying similarities among\nclusters across different methods and reference spectra for olivine and\npyroxene. Hierarchical clustering emerged as the most reliable technique,\nachieving a 94\\% similarity with the olivine spectrum in one sample, whereas\nGMM exhibited notable variability. Overall, the analysis indicated that both\nHierarchical and K-Means methods yielded lower errors in total measurements,\nwith K-Means demonstrating superior performance in estimated dispersion and\nclustering. Additionally, GMM showed a higher root mean square error compared\nto the other models. The RMSE analysis confirmed K-Means as the most consistent\nalgorithm across all samples, suggesting a predominance of olivine in the\nVulcano region relative to pyroxene. This predominance is likely linked to\nhistorical formation conditions similar to volcanic processes on the Moon,\nwhere olivine-rich compositions are common in ancient lava flows and impact\nmelt rocks.\n","date":"2025-03-28"}
{"id":"2503.22622","title":"Zero4D: Training-Free 4D Video Generation From Single Video Using\n  Off-the-Shelf Video Diffusion Model","abstract":"  Recently, multi-view or 4D video generation has emerged as a significant\nresearch topic. Nonetheless, recent approaches to 4D generation still struggle\nwith fundamental limitations, as they primarily rely on harnessing multiple\nvideo diffusion models with additional training or compute-intensive training\nof a full 4D diffusion model with limited real-world 4D data and large\ncomputational costs. To address these challenges, here we propose the first\ntraining-free 4D video generation method that leverages the off-the-shelf video\ndiffusion models to generate multi-view videos from a single input video. Our\napproach consists of two key steps: (1) By designating the edge frames in the\nspatio-temporal sampling grid as key frames, we first synthesize them using a\nvideo diffusion model, leveraging a depth-based warping technique for guidance.\nThis approach ensures structural consistency across the generated frames,\npreserving spatial and temporal coherence. (2) We then interpolate the\nremaining frames using a video diffusion model, constructing a fully populated\nand temporally coherent sampling grid while preserving spatial and temporal\nconsistency. Through this approach, we extend a single video into a multi-view\nvideo along novel camera trajectories while maintaining spatio-temporal\nconsistency. Our method is training-free and fully utilizes an off-the-shelf\nvideo diffusion model, offering a practical and effective solution for\nmulti-view video generation.\n","date":"2025-03-28"}
{"id":"2503.22625","title":"Challenges and Paths Towards AI for Software Engineering","abstract":"  AI for software engineering has made remarkable progress recently, becoming a\nnotable success within generative AI. Despite this, there are still many\nchallenges that need to be addressed before automated software engineering\nreaches its full potential. It should be possible to reach high levels of\nautomation where humans can focus on the critical decisions of what to build\nand how to balance difficult tradeoffs while most routine development effort is\nautomated away. Reaching this level of automation will require substantial\nresearch and engineering efforts across academia and industry. In this paper,\nwe aim to discuss progress towards this in a threefold manner. First, we\nprovide a structured taxonomy of concrete tasks in AI for software engineering,\nemphasizing the many other tasks in software engineering beyond code generation\nand completion. Second, we outline several key bottlenecks that limit current\napproaches. Finally, we provide an opinionated list of promising research\ndirections toward making progress on these bottlenecks, hoping to inspire\nfuture research in this rapidly maturing field.\n","date":"2025-03-28"}
{"id":"2503.22629","title":"Sentiment Classification of Thai Central Bank Press Releases Using\n  Supervised Learning","abstract":"  Central bank communication plays a critical role in shaping economic\nexpectations and monetary policy effectiveness. This study applies supervised\nmachine learning techniques to classify the sentiment of press releases from\nthe Bank of Thailand, addressing gaps in research that primarily focus on\nlexicon-based approaches. My findings show that supervised learning can be an\neffective method, even with smaller datasets, and serves as a starting point\nfor further automation. However, achieving higher accuracy and better\ngeneralization requires a substantial amount of labeled data, which is\ntime-consuming and demands expertise. Using models such as Na\\\"ive Bayes,\nRandom Forest and SVM, this study demonstrates the applicability of machine\nlearning for central bank sentiment analysis, with English-language\ncommunications from the Thai Central Bank as a case study.\n","date":"2025-03-28"}
{"id":"2503.22634","title":"Empirical Analysis of Sim-and-Real Cotraining Of Diffusion Policies For\n  Planar Pushing from Pixels","abstract":"  In imitation learning for robotics, cotraining with demonstration data\ngenerated both in simulation and on real hardware has emerged as a powerful\nrecipe to overcome the sim2real gap. This work seeks to elucidate basic\nprinciples of this sim-and-real cotraining to help inform simulation design,\nsim-and-real dataset creation, and policy training. Focusing narrowly on the\ncanonical task of planar pushing from camera inputs enabled us to be thorough\nin our study. These experiments confirm that cotraining with simulated data\n\\emph{can} dramatically improve performance in real, especially when real data\nis limited. Performance gains scale with simulated data, but eventually\nplateau; real-world data increases this performance ceiling. The results also\nsuggest that reducing the domain gap in physics may be more important than\nvisual fidelity for non-prehensile manipulation tasks. Perhaps surprisingly,\nhaving some visual domain gap actually helps the cotrained policy -- binary\nprobes reveal that high-performing policies learn to distinguish simulated\ndomains from real. We conclude by investigating this nuance and mechanisms that\nfacilitate positive transfer between sim-and-real. In total, our experiments\nspan over 40 real-world policies (evaluated on 800+ trials) and 200 simulated\npolicies (evaluated on 40,000+ trials).\n","date":"2025-03-28"}
{"id":"2503.22653","title":"Tropical Bisectors and Carlini-Wagner Attacks","abstract":"  Pasque et al. showed that using a tropical symmetric metric as an activation\nfunction in the last layer can improve the robustness of convolutional neural\nnetworks (CNNs) against state-of-the-art attacks, including the Carlini-Wagner\nattack. This improvement occurs when the attacks are not specifically adapted\nto the non-differentiability of the tropical layer. Moreover, they showed that\nthe decision boundary of a tropical CNN is defined by tropical bisectors. In\nthis paper, we explore the combinatorics of tropical bisectors and analyze how\nthe tropical embedding layer enhances robustness against Carlini-Wagner\nattacks. We prove an upper bound on the number of linear segments the decision\nboundary of a tropical CNN can have. We then propose a refined version of the\nCarlini-Wagner attack, specifically tailored for the tropical architecture.\nComputational experiments with MNIST and LeNet5 showcase our attacks improved\nsuccess rate.\n","date":"2025-03-28"}
{"id":"2503.22655","title":"Unicorn: Text-Only Data Synthesis for Vision Language Model Training","abstract":"  Training vision-language models (VLMs) typically requires large-scale,\nhigh-quality image-text pairs, but collecting or synthesizing such data is\ncostly. In contrast, text data is abundant and inexpensive, prompting the\nquestion: can high-quality multimodal training data be synthesized purely from\ntext? To tackle this, we propose a cross-integrated three-stage multimodal data\nsynthesis framework, which generates two datasets: Unicorn-1.2M and\nUnicorn-471K-Instruction. In Stage 1: Diverse Caption Data Synthesis, we\nconstruct 1.2M semantically diverse high-quality captions by expanding sparse\ncaption seeds using large language models (LLMs). In Stage 2:\nInstruction-Tuning Data Generation, we further process 471K captions into\nmulti-turn instruction-tuning tasks to support complex reasoning. Finally, in\nStage 3: Modality Representation Transfer, these textual captions\nrepresentations are transformed into visual representations, resulting in\ndiverse synthetic image representations. This three-stage process enables us to\nconstruct Unicorn-1.2M for pretraining and Unicorn-471K-Instruction for\ninstruction-tuning, without relying on real images. By eliminating the\ndependency on real images while maintaining data quality and diversity, our\nframework offers a cost-effective and scalable solution for VLMs training. Code\nis available at https:\/\/github.com\/Yu-xm\/Unicorn.git.\n","date":"2025-03-28"}
{"id":"2503.22656","title":"Differential equation quantum solvers: engineering measurements to\n  reduce cost","abstract":"  Quantum computers have been proposed as a solution for efficiently solving\nnon-linear differential equations (DEs), a fundamental task across diverse\ntechnological and scientific domains. However, a crucial milestone in this\nregard is to design protocols that are hardware-aware, making efficient use of\nlimited available quantum resources. We focus here on promising variational\nmethods derived from scientific machine learning: differentiable quantum\ncircuits (DQC), addressing specifically their cost in number of circuit\nevaluations. Reducing the number of quantum circuit evaluations is particularly\nvaluable in hybrid quantum\/classical protocols, where the time required to\ninterface and run quantum hardware at each cycle can impact the total wall-time\nmuch more than relatively inexpensive classical post-processing overhead. Here,\nwe propose and test two sample-efficient protocols for solving non-linear DEs,\nachieving exponential savings in quantum circuit evaluations. These protocols\nare based on redesigning the extraction of information from DQC in a\n``measure-first\" approach, by introducing engineered cost operators similar to\nthe randomized-measurement toolbox (i.e. classical shadows). In benchmark\nsimulations on one and two-dimensional DEs, we report up to $\\sim$ 100 fold\nreductions in circuit evaluations. Our protocols thus hold the promise to\nunlock larger and more challenging non-linear differential equation\ndemonstrations with existing quantum hardware.\n","date":"2025-03-28"}
{"id":"2503.22658","title":"Evaluation of Machine-generated Biomedical Images via A Tally-based\n  Similarity Measure","abstract":"  Super-resolution, in-painting, whole-image generation, unpaired\nstyle-transfer, and network-constrained image reconstruction each include an\naspect of machine-learned image synthesis where the actual ground truth is not\nknown at time of use. It is generally difficult to quantitatively and\nauthoritatively evaluate the quality of synthetic images; however, in\nmission-critical biomedical scenarios robust evaluation is paramount. In this\nwork, all practical image-to-image comparisons really are relative\nqualifications, not absolute difference quantifications; and, therefore,\nmeaningful evaluation of generated image quality can be accomplished using the\nTversky Index, which is a well-established measure for assessing perceptual\nsimilarity. This evaluation procedure is developed and then demonstrated using\nmultiple image data sets, both real and simulated. The main result is that when\nthe subjectivity and intrinsic deficiencies of any feature-encoding choice are\nput upfront, Tversky's method leads to intuitive results, whereas traditional\nmethods based on summarizing distances in deep feature spaces do not.\n","date":"2025-03-28"}
{"id":"2503.22668","title":"Understanding Co-speech Gestures in-the-wild","abstract":"  Co-speech gestures play a vital role in non-verbal communication. In this\npaper, we introduce a new framework for co-speech gesture understanding in the\nwild. Specifically, we propose three new tasks and benchmarks to evaluate a\nmodel's capability to comprehend gesture-text-speech associations: (i)\ngesture-based retrieval, (ii) gestured word spotting, and (iii) active speaker\ndetection using gestures. We present a new approach that learns a tri-modal\nspeech-text-video-gesture representation to solve these tasks. By leveraging a\ncombination of global phrase contrastive loss and local gesture-word coupling\nloss, we demonstrate that a strong gesture representation can be learned in a\nweakly supervised manner from videos in the wild. Our learned representations\noutperform previous methods, including large vision-language models (VLMs),\nacross all three tasks. Further analysis reveals that speech and text\nmodalities capture distinct gesture-related signals, underscoring the\nadvantages of learning a shared tri-modal embedding space. The dataset, model,\nand code are available at: https:\/\/www.robots.ox.ac.uk\/~vgg\/research\/jegal\n","date":"2025-03-28"}
{"id":"2503.22672","title":"Exploring the Effectiveness of Multi-stage Fine-tuning for Cross-encoder\n  Re-rankers","abstract":"  State-of-the-art cross-encoders can be fine-tuned to be highly effective in\npassage re-ranking. The typical fine-tuning process of cross-encoders as\nre-rankers requires large amounts of manually labelled data, a contrastive\nlearning objective, and a set of heuristically sampled negatives. An\nalternative recent approach for fine-tuning instead involves teaching the model\nto mimic the rankings of a highly effective large language model using a\ndistillation objective. These fine-tuning strategies can be applied either\nindividually, or in sequence. In this work, we systematically investigate the\neffectiveness of point-wise cross-encoders when fine-tuned independently in a\nsingle stage, or sequentially in two stages. Our experiments show that the\neffectiveness of point-wise cross-encoders fine-tuned using contrastive\nlearning is indeed on par with that of models fine-tuned with multi-stage\napproaches. Code is available for reproduction at\nhttps:\/\/github.com\/fpezzuti\/multistage-finetuning.\n","date":"2025-03-28"}
{"id":"2503.22673","title":"ActionStudio: A Lightweight Framework for Data and Training of Large\n  Action Models","abstract":"  Action models are essential for enabling autonomous agents to perform complex\ntasks. However, training large action models remains challenging due to the\ndiversity of agent environments and the complexity of agentic data. Despite\ngrowing interest, existing infrastructure provides limited support for\nscalable, agent-specific fine-tuning. We present ActionStudio, a lightweight\nand extensible data and training framework designed for large action models.\nActionStudio unifies heterogeneous agent trajectories through a standardized\nformat, supports diverse training paradigms including LoRA, full fine-tuning,\nand distributed setups, and integrates robust preprocessing and verification\ntools. We validate its effectiveness across both public and realistic industry\nbenchmarks, demonstrating strong performance and practical scalability. We\nopen-sourced code and data at https:\/\/github.com\/SalesforceAIResearch\/xLAM to\nfacilitate research in the community.\n","date":"2025-03-28"}
{"id":"2503.22674","title":"QuestBench: Can LLMs ask the right question to acquire information in\n  reasoning tasks?","abstract":"  Recently, a large amount of work has focused on improving large language\nmodels' (LLMs') performance on reasoning benchmarks such as math and logic.\nHowever, past work has largely assumed that tasks are well-defined. In the real\nworld, queries to LLMs are often underspecified, only solvable through\nacquiring missing information. We formalize this as a constraint satisfaction\nproblem (CSP) with missing variable assignments. Using a special case of this\nformalism where only one necessary variable assignment is missing, we can\nrigorously evaluate an LLM's ability to identify the minimal necessary question\nto ask and quantify axes of difficulty levels for each problem. We present\nQuestBench, a set of underspecified reasoning tasks solvable by asking at most\none question, which includes: (1) Logic-Q: Logical reasoning tasks with one\nmissing proposition, (2) Planning-Q: PDDL planning problems with initial states\nthat are partially-observed, (3) GSM-Q: Human-annotated grade school math\nproblems with one missing variable assignment, and (4) GSME-Q: a version of\nGSM-Q where word problems are translated into equations by human annotators.\nThe LLM is tasked with selecting the correct clarification question(s) from a\nlist of options. While state-of-the-art models excel at GSM-Q and GSME-Q, their\naccuracy is only 40-50% on Logic-Q and Planning-Q. Analysis demonstrates that\nthe ability to solve well-specified reasoning problems may not be sufficient\nfor success on our benchmark: models have difficulty identifying the right\nquestion to ask, even when they can solve the fully specified version of the\nproblem. Furthermore, in the Planning-Q domain, LLMs tend not to hedge, even\nwhen explicitly presented with the option to predict ``not sure.'' This\nhighlights the need for deeper investigation into models' information\nacquisition capabilities.\n","date":"2025-03-28"}
{"id":"2503.22675","title":"Think Before Recommend: Unleashing the Latent Reasoning Power for\n  Sequential Recommendation","abstract":"  Sequential Recommendation (SeqRec) aims to predict the next item by capturing\nsequential patterns from users' historical interactions, playing a crucial role\nin many real-world recommender systems. However, existing approaches\npredominantly adopt a direct forward computation paradigm, where the final\nhidden state of the sequence encoder serves as the user representation. We\nargue that this inference paradigm, due to its limited computational depth,\nstruggles to model the complex evolving nature of user preferences and lacks a\nnuanced understanding of long-tail items, leading to suboptimal performance. To\naddress this issue, we propose \\textbf{ReaRec}, the first inference-time\ncomputing framework for recommender systems, which enhances user\nrepresentations through implicit multi-step reasoning. Specifically, ReaRec\nautoregressively feeds the sequence's last hidden state into the sequential\nrecommender while incorporating special reasoning position embeddings to\ndecouple the original item encoding space from the multi-step reasoning space.\nMoreover, we introduce two lightweight reasoning-based learning methods,\nEnsemble Reasoning Learning (ERL) and Progressive Reasoning Learning (PRL), to\nfurther effectively exploit ReaRec's reasoning potential. Extensive experiments\non five public real-world datasets and different SeqRec architectures\ndemonstrate the generality and effectiveness of our proposed ReaRec.\nRemarkably, post-hoc analyses reveal that ReaRec significantly elevates the\nperformance ceiling of multiple sequential recommendation backbones by\napproximately 30\\%-50\\%. Thus, we believe this work can open a new and\npromising avenue for future research in inference-time computing for sequential\nrecommendation.\n","date":"2025-03-28"}
{"id":"2503.22676","title":"TranSplat: Lighting-Consistent Cross-Scene Object Transfer with 3D\n  Gaussian Splatting","abstract":"  We present TranSplat, a 3D scene rendering algorithm that enables realistic\ncross-scene object transfer (from a source to a target scene) based on the\nGaussian Splatting framework. Our approach addresses two critical challenges:\n(1) precise 3D object extraction from the source scene, and (2) faithful\nrelighting of the transferred object in the target scene without explicit\nmaterial property estimation. TranSplat fits a splatting model to the source\nscene, using 2D object masks to drive fine-grained 3D segmentation. Following\nuser-guided insertion of the object into the target scene, along with automatic\nrefinement of position and orientation, TranSplat derives per-Gaussian radiance\ntransfer functions via spherical harmonic analysis to adapt the object's\nappearance to match the target scene's lighting environment. This relighting\nstrategy does not require explicitly estimating physical scene properties such\nas BRDFs. Evaluated on several synthetic and real-world scenes and objects,\nTranSplat yields excellent 3D object extractions and relighting performance\ncompared to recent baseline methods and visually convincing cross-scene object\ntransfers. We conclude by discussing the limitations of the approach.\n","date":"2025-03-28"}
{"id":"2503.22677","title":"DSO: Aligning 3D Generators with Simulation Feedback for Physical\n  Soundness","abstract":"  Most 3D object generators focus on aesthetic quality, often neglecting\nphysical constraints necessary in applications. One such constraint is that the\n3D object should be self-supporting, i.e., remains balanced under gravity.\nPrior approaches to generating stable 3D objects used differentiable physics\nsimulators to optimize geometry at test-time, which is slow, unstable, and\nprone to local optima. Inspired by the literature on aligning generative models\nto external feedback, we propose Direct Simulation Optimization (DSO), a\nframework to use the feedback from a (non-differentiable) simulator to increase\nthe likelihood that the 3D generator outputs stable 3D objects directly. We\nconstruct a dataset of 3D objects labeled with a stability score obtained from\nthe physics simulator. We can then fine-tune the 3D generator using the\nstability score as the alignment metric, via direct preference optimization\n(DPO) or direct reward optimization (DRO), a novel objective, which we\nintroduce, to align diffusion models without requiring pairwise preferences.\nOur experiments show that the fine-tuned feed-forward generator, using either\nDPO or DRO objective, is much faster and more likely to produce stable objects\nthan test-time optimization. Notably, the DSO framework works even without any\nground-truth 3D objects for training, allowing the 3D generator to self-improve\nby automatically collecting simulation feedback on its own outputs.\n","date":"2025-03-28"}
{"id":"2503.22678","title":"Self-Evolving Multi-Agent Simulations for Realistic Clinical\n  Interactions","abstract":"  In this work, we introduce MedAgentSim, an open-source simulated clinical\nenvironment with doctor, patient, and measurement agents designed to evaluate\nand enhance LLM performance in dynamic diagnostic settings. Unlike prior\napproaches, our framework requires doctor agents to actively engage with\npatients through multi-turn conversations, requesting relevant medical\nexaminations (e.g., temperature, blood pressure, ECG) and imaging results\n(e.g., MRI, X-ray) from a measurement agent to mimic the real-world diagnostic\nprocess. Additionally, we incorporate self improvement mechanisms that allow\nmodels to iteratively refine their diagnostic strategies. We enhance LLM\nperformance in our simulated setting by integrating multi-agent discussions,\nchain-of-thought reasoning, and experience-based knowledge retrieval,\nfacilitating progressive learning as doctor agents interact with more patients.\nWe also introduce an evaluation benchmark for assessing the LLM's ability to\nengage in dynamic, context-aware diagnostic interactions. While MedAgentSim is\nfully automated, it also supports a user-controlled mode, enabling human\ninteraction with either the doctor or patient agent. Comprehensive evaluations\nin various simulated diagnostic scenarios demonstrate the effectiveness of our\napproach. Our code, simulation tool, and benchmark are available at\n\\href{https:\/\/medagentsim.netlify.app\/}.\n","date":"2025-03-28"}
{"id":"2503.22679","title":"Q-Insight: Understanding Image Quality via Visual Reinforcement Learning","abstract":"  Image quality assessment (IQA) focuses on the perceptual visual quality of\nimages, playing a crucial role in downstream tasks such as image\nreconstruction, compression, and generation. The rapid advancement of\nmulti-modal large language models (MLLMs) has significantly broadened the scope\nof IQA, moving toward comprehensive image quality understanding that\nincorporates content analysis, degradation perception, and comparison reasoning\nbeyond mere numerical scoring. Previous MLLM-based methods typically either\ngenerate numerical scores lacking interpretability or heavily rely on\nsupervised fine-tuning (SFT) using large-scale annotated datasets to provide\ndescriptive assessments, limiting their flexibility and applicability. In this\npaper, we propose Q-Insight, a reinforcement learning-based model built upon\ngroup relative policy optimization (GRPO), which demonstrates strong visual\nreasoning capability for image quality understanding while requiring only a\nlimited amount of rating scores and degradation labels. By jointly optimizing\nscore regression and degradation perception tasks with carefully designed\nreward functions, our approach effectively exploits their mutual benefits for\nenhanced performance. Extensive experiments demonstrate that Q-Insight\nsubstantially outperforms existing state-of-the-art methods in both score\nregression and degradation perception tasks, while exhibiting impressive\nzero-shot generalization to comparison reasoning tasks. Code will be available\nat https:\/\/github.com\/lwq20020127\/Q-Insight.\n","date":"2025-03-28"}
{"id":"2503.22711","title":"Modeling speech emotion with label variance and analyzing performance\n  across speakers and unseen acoustic conditions","abstract":"  Spontaneous speech emotion data usually contain perceptual grades where\ngraders assign emotion score after listening to the speech files. Such\nperceptual grades introduce uncertainty in labels due to grader opinion\nvariation. Grader variation is addressed by using consensus grades as\ngroundtruth, where the emotion with the highest vote is selected. Consensus\ngrades fail to consider ambiguous instances where a speech sample may contain\nmultiple emotions, as captured through grader opinion uncertainty. We\ndemonstrate that using the probability density function of the emotion grades\nas targets instead of the commonly used consensus grades, provide better\nperformance on benchmark evaluation sets compared to results reported in the\nliterature. We show that a saliency driven foundation model (FM) representation\nselection helps to train a state-of-the-art speech emotion model for both\ndimensional and categorical emotion recognition. Comparing representations\nobtained from different FMs, we observed that focusing on overall test-set\nperformance can be deceiving, as it fails to reveal the models generalization\ncapacity across speakers and gender. We demonstrate that performance evaluation\nacross multiple test-sets and performance analysis across gender and speakers\nare useful in assessing usefulness of emotion models. Finally, we demonstrate\nthat label uncertainty and data-skew pose a challenge to model evaluation,\nwhere instead of using the best hypothesis, it is useful to consider the 2- or\n3-best hypotheses.\n","date":"2025-03-24"}
{"id":"2503.22712","title":"Risk-Calibrated Affective Speech Recognition via Conformal Coverage\n  Guarantees: A Stochastic Calibrative Framework for Emergent Uncertainty\n  Quantification","abstract":"  Traffic safety challenges arising from extreme driver emotions highlight the\nurgent need for reliable emotion recognition systems. Traditional deep learning\napproaches in speech emotion recognition suffer from overfitting and poorly\ncalibrated confidence estimates. We propose a framework integrating Conformal\nPrediction (CP) and Risk Control,using Mel-spectrogram features processed\nthrough a pre-trained convolutional neural network. Our key innovation is the\ndevelopment of a nonconformity score that heuristically measures how closely a\nclassifier's predictions align with given inputs. Through calibration samples,\nwe compute this score and derive a statistically rigorous threshold based on\nuser-specified risk level $\\alpha$, constructing prediction sets with provable\ncoverage guarantees ($\\geq 1-\\alpha$). The Risk Control framework enables\ntask-specific adaptation through customizable loss functions, dynamically\nadjusting prediction set sizes while maintaining coverage guarantees.\nCross-dataset experiments on IEMOCAP and TESS demonstrate: 1) Strict coverage\nguarantee, 2) Significant negative correlation between Average Prediction Set\nSize (APSS) and $\\alpha$, revealing reduced model uncertainty under high-risk\nconditions. We further propose APSS as a novel metric for evaluating\nclassification uncertainty. This approach enhances speech emotion recognition\nreliability, with direct applications in intelligent transportation systems and\nreal-time emotion monitoring.\n","date":"2025-03-24"}
{"id":"2503.22713","title":"Chirp Localization via Fine-Tuned Transformer Model: A Proof-of-Concept\n  Study","abstract":"  Spectrograms are pivotal in time-frequency signal analysis, widely used in\naudio processing and computational neuroscience. Chirp-like patterns in\nelectroencephalogram (EEG) spectrograms (marked by linear or exponential\nfrequency sweep) are key biomarkers for seizure dynamics, but automated tools\nfor their detection, localization, and feature extraction are lacking. This\nstudy bridges this gap by fine-tuning a Vision Transformer (ViT) model on\nsynthetic spectrograms, augmented with Low-Rank Adaptation (LoRA) to boost\nadaptability. We generated 100000 synthetic spectrograms with chirp parameters,\ncreating the first large-scale benchmark for chirp localization. These\nspectrograms mimic neural chirps using linear or exponential frequency sweep,\nGaussian noise, and smoothing. A ViT model, adapted for regression, predicted\nchirp parameters. LoRA fine-tuned the attention layers, enabling efficient\nupdates to the pre-trained backbone. Training used MSE loss and the AdamW\noptimizer, with a learning rate scheduler and early stopping to curb\noverfitting. Only three features were targeted: Chirp Start Time (Onset Time),\nChirp Start Frequency (Onset Frequency), and Chirp End Frequency (Offset\nFrequency). Performance was evaluated via Pearson correlation between predicted\nand actual labels. Results showed strong alignment: 0.9841 correlation for\nchirp start time, with stable inference times (137 to 140s) and minimal bias in\nerror distributions. This approach offers a tool for chirp analysis in EEG\ntime-frequency representation, filling a critical methodological void.\n","date":"2025-03-24"}
{"id":"2503.22714","title":"TRIDIS: A Comprehensive Medieval and Early Modern Corpus for HTR and NER","abstract":"  This paper introduces TRIDIS (Tria Digita Scribunt), an open-source corpus of\nmedieval and early modern manuscripts. TRIDIS aggregates multiple legacy\ncollections (all published under open licenses) and incorporates large metadata\ndescriptions. While prior publications referenced some portions of this corpus,\nhere we provide a unified overview with a stronger focus on its constitution.\nWe describe (i) the narrative, chronological, and editorial background of each\nmajor sub-corpus, (ii) its semi-diplomatic transcription rules (expansion,\nnormalization, punctuation), (iii) a strategy for challenging out-of-domain\ntest splits driven by outlier detection in a joint embedding space, and (iv)\npreliminary baseline experiments using TrOCR and MiniCPM2.5 comparing random\nand outlier-based test partitions. Overall, TRIDIS is designed to stimulate\njoint robust Handwritten Text Recognition (HTR) and Named Entity Recognition\n(NER) research across medieval and early modern textual heritage.\n","date":"2025-03-25"}
{"id":"2503.22715","title":"Hierarchical Adaptive Expert for Multimodal Sentiment Analysis","abstract":"  Multimodal sentiment analysis has emerged as a critical tool for\nunderstanding human emotions across diverse communication channels. While\nexisting methods have made significant strides, they often struggle to\neffectively differentiate and integrate modality-shared and modality-specific\ninformation, limiting the performance of multimodal learning. To address this\nchallenge, we propose the Hierarchical Adaptive Expert for Multimodal Sentiment\nAnalysis (HAEMSA), a novel framework that synergistically combines evolutionary\noptimization, cross-modal knowledge transfer, and multi-task learning. HAEMSA\nemploys a hierarchical structure of adaptive experts to capture both global and\nlocal modality representations, enabling more nuanced sentiment analysis. Our\napproach leverages evolutionary algorithms to dynamically optimize network\narchitectures and modality combinations, adapting to both partial and full\nmodality scenarios. Extensive experiments demonstrate HAEMSA's superior\nperformance across multiple benchmark datasets. On CMU-MOSEI, HAEMSA achieves a\n2.6% increase in 7-class accuracy and a 0.059 decrease in MAE compared to the\nprevious best method. For CMU-MOSI, we observe a 6.3% improvement in 7-class\naccuracy and a 0.058 reduction in MAE. On IEMOCAP, HAEMSA outperforms the\nstate-of-the-art by 2.84% in weighted-F1 score for emotion recognition. These\nresults underscore HAEMSA's effectiveness in capturing complex multimodal\ninteractions and generalizing across different emotional contexts.\n","date":"2025-03-25"}
{"id":"2503.22719","title":"LLM-based Agent Simulation for Maternal Health Interventions:\n  Uncertainty Estimation and Decision-focused Evaluation","abstract":"  Agent-based simulation is crucial for modeling complex human behavior, yet\ntraditional approaches require extensive domain knowledge and large datasets.\nIn data-scarce healthcare settings where historic and counterfactual data are\nlimited, large language models (LLMs) offer a promising alternative by\nleveraging broad world knowledge. This study examines an LLM-driven simulation\nof a maternal mobile health program, predicting beneficiaries' listening\nbehavior when they receive health information via automated messages (control)\nor live representatives (intervention). Since uncertainty quantification is\ncritical for decision-making in health interventions, we propose an LLM\nepistemic uncertainty estimation method based on binary entropy across multiple\nsamples. We enhance model robustness through ensemble approaches, improving F1\nscore and model calibration compared to individual models. Beyond direct\nevaluation, we take a decision-focused approach, demonstrating how LLM\npredictions inform intervention feasibility and trial implementation in\ndata-limited settings. The proposed method extends to public health, disaster\nresponse, and other domains requiring rapid intervention assessment under\nsevere data constraints. All code and prompts used for this work can be found\nat https:\/\/github.com\/sarahmart\/LLM-ABS-ARMMAN-prediction.\n","date":"2025-03-25"}
{"id":"2503.22720","title":"Why Representation Engineering Works: A Theoretical and Empirical Study\n  in Vision-Language Models","abstract":"  Representation Engineering (RepE) has emerged as a powerful paradigm for\nenhancing AI transparency by focusing on high-level representations rather than\nindividual neurons or circuits. It has proven effective in improving\ninterpretability and control, showing that representations can emerge,\npropagate, and shape final model outputs in large language models (LLMs).\nHowever, in Vision-Language Models (VLMs), visual input can override factual\nlinguistic knowledge, leading to hallucinated responses that contradict\nreality. To address this challenge, we make the first attempt to extend RepE to\nVLMs, analyzing how multimodal representations are preserved and transformed.\nBuilding on our findings and drawing inspiration from successful RepE\napplications, we develop a theoretical framework that explains the stability of\nneural activity across layers using the principal eigenvector, uncovering the\nunderlying mechanism of RepE. We empirically validate these instrinsic\nproperties, demonstrating their broad applicability and significance. By\nbridging theoretical insights with empirical validation, this work transforms\nRepE from a descriptive tool into a structured theoretical framework, opening\nnew directions for improving AI robustness, fairness, and transparency.\n","date":"2025-03-25"}
{"id":"2503.22721","title":"PowerGNN: A Topology-Aware Graph Neural Network for Electricity Grids","abstract":"  The increasing penetration of renewable energy sources introduces significant\nvariability and uncertainty in modern power systems, making accurate state\nprediction critical for reliable grid operation. Conventional forecasting\nmethods often neglect the power grid's inherent topology, limiting their\nability to capture complex spatio temporal dependencies. This paper proposes a\ntopology aware Graph Neural Network (GNN) framework for predicting power system\nstates under high renewable integration. We construct a graph based\nrepresentation of the power network, modeling buses and transmission lines as\nnodes and edges, and introduce a specialized GNN architecture that integrates\nGraphSAGE convolutions with Gated Recurrent Units (GRUs) to model both spatial\nand temporal correlations in system dynamics. The model is trained and\nevaluated on the NREL 118 test system using realistic, time synchronous\nrenewable generation profiles. Our results show that the proposed GNN\noutperforms baseline approaches including fully connected neural networks,\nlinear regression, and rolling mean models, achieving substantial improvements\nin predictive accuracy. The GNN achieves average RMSEs of 0.13 to 0.17 across\nall predicted variables and demonstrates consistent performance across spatial\nlocations and operational conditions. These results highlight the potential of\ntopology aware learning for scalable and robust power system forecasting in\nfuture grids with high renewable penetration.\n","date":"2025-03-26"}
{"id":"2503.22722","title":"PlatMetaX: An Integrated MATLAB platform for Meta-Black-Box Optimization","abstract":"  The landscape of optimization problems has become increasingly complex,\nnecessitating the development of advanced optimization techniques.\nMeta-Black-Box Optimization (MetaBBO), which involves refining the optimization\nalgorithms themselves via meta-learning, has emerged as a promising approach.\nRecognizing the limitations in existing platforms, we presents PlatMetaX, a\nnovel MATLAB platform for MetaBBO with reinforcement learning. PlatMetaX\nintegrates the strengths of MetaBox and PlatEMO, offering a comprehensive\nframework for developing, evaluating, and comparing optimization algorithms.\nThe platform is designed to handle a wide range of optimization problems, from\nsingle-objective to multi-objective, and is equipped with a rich set of\nbaseline algorithms and evaluation metrics. We demonstrate the utility of\nPlatMetaX through extensive experiments and provide insights into its design\nand implementation. PlatMetaX is available at:\n\\href{https:\/\/github.com\/Yxxx616\/PlatMetaX}{https:\/\/github.com\/Yxxx616\/PlatMetaX}.\n","date":"2025-03-26"}
{"id":"2503.22723","title":"Zero-Shot LLMs in Human-in-the-Loop RL: Replacing Human Feedback for\n  Reward Shaping","abstract":"  Reinforcement learning often faces challenges with reward misalignment, where\nagents optimize for given rewards but fail to exhibit the desired behaviors.\nThis occurs when the reward function incentivizes proxy behaviors that diverge\nfrom the true objective. While human-in-the-loop (HIL) methods can help, they\nmay exacerbate the problem, as humans are prone to biases that lead to\ninconsistent, subjective, or misaligned feedback, complicating the learning\nprocess. To address these issues, we propose two key contributions. First, we\nextend the use of zero-shot, off-the-shelf large language models (LLMs) for\nreward shaping beyond natural language processing (NLP) to continuous control\ntasks. By leveraging LLMs as direct feedback providers, we replace surrogate\nmodels trained on human feedback, which often suffer from the bias inherent in\nthe feedback data it is trained on. Second, we introduce a hybrid framework\n(LLM-HFBF) that enables LLMs to identify and correct biases in human feedback\nwhile incorporating this feedback into the reward shaping process. The LLM-HFBF\nframework creates a more balanced and reliable system by addressing both the\nlimitations of LLMs (e.g., lack of domain-specific knowledge) and human\nsupervision (e.g., inherent biases). By enabling human feedback bias flagging\nand correction, our approach improves reinforcement learning performance and\nreduces reliance on potentially biased human guidance. Empirical experiments\nshow that biased human feedback significantly reduces performance, with average\nepisodic reward (AER) dropping from 28.472 in (unbiased approaches) to 7.039\n(biased with conservative bias). In contrast, LLM-based approaches maintain a\nmatching AER like unbiased feedback, even in custom edge case scenarios.\n","date":"2025-03-26"}
{"id":"2503.22724","title":"A Spatial-temporal Deep Probabilistic Diffusion Model for Reliable Hail\n  Nowcasting with Radar Echo Extrapolation","abstract":"  Hail nowcasting is a considerable contributor to meteorological disasters and\nthere is a great need to mitigate its socioeconomic effects through precise\nforecast that has high resolution, long lead times and local details with large\nlandscapes. Existing medium-range weather forecasting methods primarily rely on\nchanges in upper air currents and cloud layers to predict precipitation events,\nsuch as heavy rainfall, which are unsuitable for hail nowcasting since it is\nmainly caused by low-altitude local strong convection associated with terrains.\nAdditionally, radar captures the status of low cloud layers, such as water\nvapor, droplets, and ice crystals, providing rich signals suitable for hail\nnowcasting. To this end, we introduce a Spatial-Temporal gEnerAtive Model\ncalled SteamCast for hail nowcasting with radar echo extrapolation, it is a\ndeep probabilistic diffusion model based on spatial-temporal representations\nincluding radar echoes as well as their position\/time embeddings, which we\ntrained on historical reanalysis archive from Yan'an Meteorological Bureau in\nChina, where the crop yield like apple suffers greatly from hail damage.\nConsidering the short-term nature of hail, SteamCast provides 30-minute\nnowcasts at 6-minute intervals for a single radar reflectivity variable, across\n9 different vertical angles, on a latitude-longitude grid with approximately 1\nkm * 1 km resolution per pixel in Yan'an City, China. By successfully fusing\nthe spatial-temporal features of radar echoes, SteamCast delivers competitive,\nand in some cases superior, results compared to other deep learning-based\nmodels such as PredRNN and VMRNN.\n","date":"2025-03-26"}
{"id":"2503.22725","title":"Uncertainty Weighted Gradients for Model Calibration","abstract":"  Model calibration is essential for ensuring that the predictions of deep\nneural networks accurately reflect true probabilities in real-world\nclassification tasks. However, deep networks often produce over-confident or\nunder-confident predictions, leading to miscalibration. Various methods have\nbeen proposed to address this issue by designing effective loss functions for\ncalibration, such as focal loss. In this paper, we analyze its effectiveness\nand provide a unified loss framework of focal loss and its variants, where we\nmainly attribute their superiority in model calibration to the loss weighting\nfactor that estimates sample-wise uncertainty. Based on our analysis, existing\nloss functions fail to achieve optimal calibration performance due to two main\nissues: including misalignment during optimization and insufficient precision\nin uncertainty estimation. Specifically, focal loss cannot align sample\nuncertainty with gradient scaling and the single logit cannot indicate the\nuncertainty. To address these issues, we reformulate the optimization from the\nperspective of gradients, which focuses on uncertain samples. Meanwhile, we\npropose using the Brier Score as the loss weight factor, which provides a more\naccurate uncertainty estimation via all the logits. Extensive experiments on\nvarious models and datasets demonstrate that our method achieves\nstate-of-the-art (SOTA) performance.\n","date":"2025-03-26"}
{"id":"2503.22726","title":"InfoBid: A Simulation Framework for Studying Information Disclosure in\n  Auctions with Large Language Model-based Agents","abstract":"  In online advertising systems, publishers often face a trade-off in\ninformation disclosure strategies: while disclosing more information can\nenhance efficiency by enabling optimal allocation of ad impressions, it may\nlose revenue potential by decreasing uncertainty among competing advertisers.\nSimilar to other challenges in market design, understanding this trade-off is\nconstrained by limited access to real-world data, leading researchers and\npractitioners to turn to simulation frameworks. The recent emergence of large\nlanguage models (LLMs) offers a novel approach to simulations, providing\nhuman-like reasoning and adaptability without necessarily relying on explicit\nassumptions about agent behavior modeling. Despite their potential, existing\nframeworks have yet to integrate LLM-based agents for studying information\nasymmetry and signaling strategies, particularly in the context of auctions. To\naddress this gap, we introduce InfoBid, a flexible simulation framework that\nleverages LLM agents to examine the effects of information disclosure\nstrategies in multi-agent auction settings. Using GPT-4o, we implemented\nsimulations of second-price auctions with diverse information schemas. The\nresults reveal key insights into how signaling influences strategic behavior\nand auction outcomes, which align with both economic and social learning\ntheories. Through InfoBid, we hope to foster the use of LLMs as proxies for\nhuman economic and social agents in empirical studies, enhancing our\nunderstanding of their capabilities and limitations. This work bridges the gap\nbetween theoretical market designs and practical applications, advancing\nresearch in market simulations, information design, and agent-based reasoning\nwhile offering a valuable tool for exploring the dynamics of digital economies.\n","date":"2025-03-26"}
{"id":"2503.22727","title":"A Large-Scale Vision-Language Dataset Derived from Open Scientific\n  Literature to Advance Biomedical Generalist AI","abstract":"  Despite the excitement behind biomedical artificial intelligence (AI), access\nto high-quality, diverse, and large-scale data - the foundation for modern AI\nsystems - is still a bottleneck to unlocking its full potential. To address\nthis gap, we introduce Biomedica, an open-source dataset derived from the\nPubMed Central Open Access subset, containing over 6 million scientific\narticles and 24 million image-text pairs, along with 27 metadata fields\n(including expert human annotations). To overcome the challenges of accessing\nour large-scale dataset, we provide scalable streaming and search APIs through\na web server, facilitating seamless integration with AI systems. We demonstrate\nthe utility of the Biomedica dataset by building embedding models, chat-style\nmodels, and retrieval-augmented chat agents. Notably, all our AI models surpass\nprevious open systems in their respective categories, underscoring the critical\nrole of diverse, high-quality, and large-scale biomedical data.\n","date":"2025-03-26"}
{"id":"2503.22728","title":"Dual Audio-Centric Modality Coupling for Talking Head Generation","abstract":"  The generation of audio-driven talking head videos is a key challenge in\ncomputer vision and graphics, with applications in virtual avatars and digital\nmedia. Traditional approaches often struggle with capturing the complex\ninteraction between audio and facial dynamics, leading to lip synchronization\nand visual quality issues. In this paper, we propose a novel NeRF-based\nframework, Dual Audio-Centric Modality Coupling (DAMC), which effectively\nintegrates content and dynamic features from audio inputs. By leveraging a dual\nencoder structure, DAMC captures semantic content through the Content-Aware\nEncoder and ensures precise visual synchronization through the Dynamic-Sync\nEncoder. These features are fused using a Cross-Synchronized Fusion Module\n(CSFM), enhancing content representation and lip synchronization. Extensive\nexperiments show that our method outperforms existing state-of-the-art\napproaches in key metrics such as lip synchronization accuracy and image\nquality, demonstrating robust generalization across various audio inputs,\nincluding synthetic speech from text-to-speech (TTS) systems. Our results\nprovide a promising solution for high-quality, audio-driven talking head\ngeneration and present a scalable approach for creating realistic talking\nheads.\n","date":"2025-03-26"}
{"id":"2503.22729","title":"Ancestral Mamba: Enhancing Selective Discriminant Space Model with\n  Online Visual Prototype Learning for Efficient and Robust Discriminant\n  Approach","abstract":"  In the realm of computer graphics, the ability to learn continuously from\nnon-stationary data streams while adapting to new visual patterns and\nmitigating catastrophic forgetting is of paramount importance. Existing\napproaches often struggle to capture and represent the essential\ncharacteristics of evolving visual concepts, hindering their applicability to\ndynamic graphics tasks. In this paper, we propose Ancestral Mamba, a novel\napproach that integrates online prototype learning into a selective\ndiscriminant space model for efficient and robust online continual learning.\nThe key components of our approach include Ancestral Prototype Adaptation\n(APA), which continuously refines and builds upon learned visual prototypes,\nand Mamba Feedback (MF), which provides targeted feedback to adapt to\nchallenging visual patterns. APA enables the model to continuously adapt its\nprototypes, building upon ancestral knowledge to tackle new challenges, while\nMF acts as a targeted feedback mechanism, focusing on challenging classes and\nrefining their representations. Extensive experiments on graphics-oriented\ndatasets, such as CIFAR-10 and CIFAR-100, demonstrate the superior performance\nof Ancestral Mamba compared to state-of-the-art baselines, achieving\nsignificant improvements in accuracy and forgetting mitigation.\n","date":"2025-03-26"}
{"id":"2503.22730","title":"Harnessing Mixed Features for Imbalance Data Oversampling: Application\n  to Bank Customers Scoring","abstract":"  This study investigates rare event detection on tabular data within binary\nclassification. Standard techniques to handle class imbalance include SMOTE,\nwhich generates synthetic samples from the minority class. However, SMOTE is\nintrinsically designed for continuous input variables. In fact, despite\nSMOTE-NC-its default extension to handle mixed features (continuous and\ncategorical variables)-very few works propose procedures to synthesize mixed\nfeatures. On the other hand, many real-world classification tasks, such as in\nbanking sector, deal with mixed features, which have a significant impact on\npredictive performances. To this purpose, we introduce MGS-GRF, an oversampling\nstrategy designed for mixed features. This method uses a kernel density\nestimator with locally estimated full-rank covariances to generate continuous\nfeatures, while categorical ones are drawn from the original samples through a\ngeneralized random forest. Empirically, contrary to SMOTE-NC, we show that\nMGS-GRF exhibits two important properties: (i) the coherence i.e. the ability\nto only generate combinations of categorical features that are already present\nin the original dataset and (ii) association, i.e. the ability to preserve the\ndependence between continuous and categorical features. We also evaluate the\npredictive performances of LightGBM classifiers trained on data sets, augmented\nwith synthetic samples from various strategies. Our comparison is performed on\nsimulated and public real-world data sets, as well as on a private data set\nfrom a leading financial institution. We observe that synthetic procedures that\nhave the properties of coherence and association display better predictive\nperformances in terms of various predictive metrics (PR and ROC AUC...), with\nMGS-GRF being the best one. Furthermore, our method exhibits promising results\nfor the private banking application, with development pipeline being compliant\nwith regulatory constraints.\n","date":"2025-03-26"}
{"id":"2503.22731","title":"MoRE-LLM: Mixture of Rule Experts Guided by a Large Language Model","abstract":"  To ensure the trustworthiness and interpretability of AI systems, it is\nessential to align machine learning models with human domain knowledge. This\ncan be a challenging and time-consuming endeavor that requires close\ncommunication between data scientists and domain experts. Recent leaps in the\ncapabilities of Large Language Models (LLMs) can help alleviate this burden. In\nthis paper, we propose a Mixture of Rule Experts guided by a Large Language\nModel (MoRE-LLM) which combines a data-driven black-box model with knowledge\nextracted from an LLM to enable domain knowledge-aligned and transparent\npredictions. While the introduced Mixture of Rule Experts (MoRE) steers the\ndiscovery of local rule-based surrogates during training and their utilization\nfor the classification task, the LLM is responsible for enhancing the domain\nknowledge alignment of the rules by correcting and contextualizing them.\nImportantly, our method does not rely on access to the LLM during test time and\nensures interpretability while not being prone to LLM-based confabulations. We\nevaluate our method on several tabular data sets and compare its performance\nwith interpretable and non-interpretable baselines. Besides performance, we\nevaluate our grey-box method with respect to the utilization of interpretable\nrules. In addition to our quantitative evaluation, we shed light on how the LLM\ncan provide additional context to strengthen the comprehensibility and\ntrustworthiness of the model's reasoning process.\n","date":"2025-03-26"}
{"id":"2503.22732","title":"Reasoning Beyond Limits: Advances and Open Problems for LLMs","abstract":"  Recent generative reasoning breakthroughs have transformed how large language\nmodels (LLMs) tackle complex problems by dynamically retrieving and refining\ninformation while generating coherent, multi-step thought processes. Techniques\nsuch as inference-time scaling, reinforcement learning, supervised fine-tuning,\nand distillation have been successfully applied to models like DeepSeek-R1,\nOpenAI's o1 & o3, GPT-4o, Qwen-32B, and various Llama variants, resulting in\nenhanced reasoning capabilities. In this paper, we provide a comprehensive\nanalysis of the top 27 LLM models released between 2023 and 2025 (including\nmodels such as Mistral AI Small 3 24B, DeepSeek-R1, Search-o1, QwQ-32B, and\nphi-4). Then, we present an extensive overview of training methodologies that\nspans general training approaches, mixture-of-experts (MoE) and architectural\ninnovations, retrieval-augmented generation (RAG), chain-of-thought and\nself-improvement techniques, as well as test-time compute scaling,\ndistillation, and reinforcement learning (RL) methods. Finally, we discuss the\nkey challenges in advancing LLM capabilities, including improving multi-step\nreasoning without human supervision, overcoming limitations in chained tasks,\nbalancing structured prompts with flexibility, and enhancing long-context\nretrieval and external tool integration.\n","date":"2025-03-26"}
{"id":"2503.22733","title":"RBFleX-NAS: Training-Free Neural Architecture Search Using Radial Basis\n  Function Kernel and Hyperparameter Detection","abstract":"  Neural Architecture Search (NAS) is an automated technique to design optimal\nneural network architectures for a specific workload. Conventionally,\nevaluating candidate networks in NAS involves extensive training, which\nrequires significant time and computational resources. To address this,\ntraining-free NAS has been proposed to expedite network evaluation with minimal\nsearch time. However, state-of-the-art training-free NAS algorithms struggle to\nprecisely distinguish well-performing networks from poorly-performing networks,\nresulting in inaccurate performance predictions and consequently sub-optimal\ntop-1 network accuracy. Moreover, they are less effective in activation\nfunction exploration. To tackle the challenges, this paper proposes RBFleX-NAS,\na novel training-free NAS framework that accounts for both activation outputs\nand input features of the last layer with a Radial Basis Function (RBF) kernel.\nWe also present a detection algorithm to identify optimal hyperparameters using\nthe obtained activation outputs and input feature maps. We verify the efficacy\nof RBFleX-NAS over a variety of NAS benchmarks. RBFleX-NAS significantly\noutperforms state-of-the-art training-free NAS methods in terms of top-1\naccuracy, achieving this with short search time in NAS-Bench-201 and\nNAS-Bench-SSS. In addition, it demonstrates higher Kendall correlation compared\nto layer-based training-free NAS algorithms. Furthermore, we propose NAFBee, a\nnew activation design space that extends the activation type to encompass\nvarious commonly used functions. In this extended design space, RBFleX-NAS\ndemonstrates its superiority by accurately identifying the best-performing\nnetwork during activation function search, providing a significant advantage\nover other NAS algorithms.\n","date":"2025-03-26"}
{"id":"2503.22734","title":"A Methodology to extract Geo-Referenced Standard Routes from AIS Data","abstract":"  Maritime AIS (Automatic Identification Systems) data serve as a valuable\nresource for studying vessel behavior. This study proposes a methodology to\nanalyze route between maritime points of interest and extract geo-referenced\nstandard routes, as maritime patterns of life, from raw AIS data. The\nunderlying assumption is that ships adhere to consistent patterns when\ntravelling in certain maritime areas due to geographical, environmental, or\neconomic factors. Deviations from these patterns may be attributed to weather\nconditions, seasonality, or illicit activities. This enables maritime\nsurveillance authorities to analyze the navigational behavior between ports,\nproviding insights on vessel route patterns, possibly categorized by vessel\ncharacteristics (type, flag, or size). Our methodological process begins by\nsegmenting AIS data into distinct routes using a finite state machine (FSM),\nwhich describes routes as seg-ments connecting pairs of points of interest. The\nextracted segments are ag-gregated based on their departure and destination\nports and then modelled using iterative density-based clustering to connect\nthese ports. The cluster-ing parameters are assigned manually to sample and\nthen extended to the en-tire dataset using linear regression. Overall, the\napproach proposed in this paper is unsupervised and does not require any ground\ntruth to be trained. The approach has been tested on data on the on a six-year\nAIS dataset cover-ing the Arctic region and the Europe, Middle East, North\nAfrica areas. The total size of our dataset is 1.15 Tbytes. The approach has\nproved effective in extracting standard routes, with less than 5% outliers,\nmostly due to routes with either their departure or their destination port not\nincluded in the test areas.\n","date":"2025-03-26"}
{"id":"2503.22735","title":"Training in translation tools and technologies: Findings of the EMT\n  survey 2023","abstract":"  This article reports on the third iteration of a survey of computerized tools\nand technologies taught as part of postgraduate translation training\nprogrammes. While the survey was carried out under the aegis of the EMT\nNetwork, more than half of responses are from outside that network. The results\nshow the responsiveness of programmes to innovations in translation technology,\nwith increased compulsory inclusion of machine translation, post-editing, and\nquality evaluation, and a rapid response to the release of generative tools.\nThe flexibility required during the Covid-19 pandemic has also led to some\nlasting changes to programmes. While the range of tools being taught has\ncontinued to expand, programmes seem to be consolidating their core offering\naround cloud-based software with cost-free academic access. There has also been\nan increase in the embedding of professional contexts and workflows associated\nwith translation technology. Generic file management and data security skills\nhave increased in perceived importance, and legal and ethical issues related to\ntranslation data have also become more prominent. In terms of course delivery\nthe shift away from conventional labs identified in EMT2017 has accelerated\nmarkedly, no doubt partly driven by the pandemic, accompanied by a dramatic\nexpansion in the use of students' personal devices.\n","date":"2025-03-26"}
{"id":"2503.22736","title":"Cyborg Data: Merging Human with AI Generated Training Data","abstract":"  Automated scoring (AS) systems used in large-scale assessment have\ntraditionally used small statistical models that require a large quantity of\nhand-scored data to make accurate predictions, which can be time-consuming and\ncostly. Generative Large Language Models are trained on many tasks and have\nshown impressive abilities to generalize to new tasks with little to no data.\nWhile these models require substantially more computational power to make\npredictions, they still require some fine-tuning to meet operational standards.\nEvidence suggests that these models can exceed human-human levels of agreement\neven when fine-tuned on small amounts of data. With this in mind, we propose a\nmodel distillation pipeline in which a large generative model, a Teacher,\nteaches a much smaller model, a Student. The Teacher, trained on a small subset\nof the training data, is used to provide scores on the remaining training data,\nwhich is then used to train the Student. We call the resulting dataset \"Cyborg\nData\", as it combines human and machine-scored responses. Our findings show\nthat Student models trained on \"Cyborg Data\" show performance comparable to\ntraining on the entire dataset, while only requiring 10% of the original\nhand-scored data.\n","date":"2025-03-26"}
{"id":"2503.22737","title":"Symmetry-Informed Graph Neural Networks for Carbon Dioxide Isotherm and\n  Adsorption Prediction in Aluminum-Substituted Zeolites","abstract":"  Accurately predicting adsorption properties in nanoporous materials using\nDeep Learning models remains a challenging task. This challenge becomes even\nmore pronounced when attempting to generalize to structures that were not part\nof the training data.. In this work, we introduce SymGNN, a graph neural\nnetwork architecture that leverages material symmetries to improve adsorption\nproperty prediction. By incorporating symmetry operations into the\nmessage-passing mechanism, our model enhances parameter sharing across\ndifferent zeolite topologies, leading to improved generalization. We evaluate\nSymGNN on both interpolation and generalization tasks, demonstrating that it\nsuccessfully captures key adsorption trends, including the influence of both\nthe framework and aluminium distribution on CO$_2$ adsorption. Furthermore, we\napply our model to the characterization of experimental adsorption isotherms,\nusing a genetic algorithm to infer likely aluminium distributions. Our results\nhighlight the effectiveness of machine learning models trained on simulations\nfor studying real materials and suggest promising directions for fine-tuning\nwith experimental data and generative approaches for the inverse design of\nmultifunctional nanomaterials.\n","date":"2025-03-26"}
{"id":"2503.22738","title":"ShieldAgent: Shielding Agents via Verifiable Safety Policy Reasoning","abstract":"  Autonomous agents powered by foundation models have seen widespread adoption\nacross various real-world applications. However, they remain highly vulnerable\nto malicious instructions and attacks, which can result in severe consequences\nsuch as privacy breaches and financial losses. More critically, existing\nguardrails for LLMs are not applicable due to the complex and dynamic nature of\nagents. To tackle these challenges, we propose ShieldAgent, the first guardrail\nagent designed to enforce explicit safety policy compliance for the action\ntrajectory of other protected agents through logical reasoning. Specifically,\nShieldAgent first constructs a safety policy model by extracting verifiable\nrules from policy documents and structuring them into a set of action-based\nprobabilistic rule circuits. Given the action trajectory of the protected\nagent, ShieldAgent retrieves relevant rule circuits and generates a shielding\nplan, leveraging its comprehensive tool library and executable code for formal\nverification. In addition, given the lack of guardrail benchmarks for agents,\nwe introduce ShieldAgent-Bench, a dataset with 3K safety-related pairs of agent\ninstructions and action trajectories, collected via SOTA attacks across 6 web\nenvironments and 7 risk categories. Experiments show that ShieldAgent achieves\nSOTA on ShieldAgent-Bench and three existing benchmarks, outperforming prior\nmethods by 11.3% on average with a high recall of 90.1%. Additionally,\nShieldAgent reduces API queries by 64.7% and inference time by 58.2%,\ndemonstrating its high precision and efficiency in safeguarding agents.\n","date":"2025-03-26"}
{"id":"2503.22740","title":"CSPO: Cross-Market Synergistic Stock Price Movement Forecasting with\n  Pseudo-volatility Optimization","abstract":"  The stock market, as a cornerstone of the financial markets, places\nforecasting stock price movements at the forefront of challenges in\nquantitative finance. Emerging learning-based approaches have made significant\nprogress in capturing the intricate and ever-evolving data patterns of modern\nmarkets. With the rapid expansion of the stock market, it presents two\ncharacteristics, i.e., stock exogeneity and volatility heterogeneity, that\nheighten the complexity of price forecasting. Specifically, while stock\nexogeneity reflects the influence of external market factors on price\nmovements, volatility heterogeneity showcases the varying difficulty in\nmovement forecasting against price fluctuations. In this work, we introduce the\nframework of Cross-market Synergy with Pseudo-volatility Optimization (CSPO).\nSpecifically, CSPO implements an effective deep neural architecture to leverage\nexternal futures knowledge. This enriches stock embeddings with cross-market\ninsights and thus enhances the CSPO's predictive capability. Furthermore, CSPO\nincorporates pseudo-volatility to model stock-specific forecasting confidence,\nenabling a dynamic adaptation of its optimization process to improve accuracy\nand robustness. Our extensive experiments, encompassing industrial evaluation\nand public benchmarking, highlight CSPO's superior performance over existing\nmethods and effectiveness of all proposed modules contained therein.\n","date":"2025-03-26"}
{"id":"2503.22741","title":"Concept Map Assessment Through Structure Classification","abstract":"  Due to their versatility, concept maps are used in various educational\nsettings and serve as tools that enable educators to comprehend students'\nknowledge construction. An essential component for analyzing a concept map is\nits structure, which can be categorized into three distinct types: spoke,\nnetwork, and chain. Understanding the predominant structure in a map offers\ninsights into the student's depth of comprehension of the subject. Therefore,\nthis study examined 317 distinct concept map structures, classifying them into\none of the three types, and used statistical and descriptive information from\nthe maps to train multiclass classification models. As a result, we achieved an\n86\\% accuracy in classification using a Decision Tree. This promising outcome\ncan be employed in concept map assessment systems to provide real-time feedback\nto the student.\n","date":"2025-03-26"}
{"id":"2503.22742","title":"Adaptive Integrated Layered Attention (AILA)","abstract":"  We propose Adaptive Integrated Layered Attention (AILA), a neural network\narchitecture that combines dense skip connections with different mechanisms for\nadaptive feature reuse across network layers. We evaluate AILA on three\nchallenging tasks: price forecasting for various commodities and indices (S&P\n500, Gold, US dollar Futures, Coffee, Wheat), image recognition using the\nCIFAR-10 dataset, and sentiment analysis on the IMDB movie review dataset. In\nall cases, AILA matches strong deep learning baselines (LSTMs, Transformers,\nand ResNets), achieving it at a fraction of the training and inference time.\nNotably, we implement and test two versions of the model - AILA-Architecture 1,\nwhich uses simple linear layers as the connection mechanism between layers, and\nAILA-Architecture 2, which implements an attention mechanism to selectively\nfocus on outputs from previous layers. Both architectures are applied in a\nsingle-task learning setting, with each model trained separately for individual\ntasks. Results confirm that AILA's adaptive inter-layer connections yield\nrobust gains by flexibly reusing pertinent features at multiple network depths.\nThe AILA approach thus presents an extension to existing architectures,\nimproving long-range sequence modeling, image recognition with optimised\ncomputational speed, and SOTA classification performance in practice.\n","date":"2025-03-26"}
{"id":"2503.22743","title":"Adaptive State-Space Mamba for Real-Time Sensor Data Anomaly Detection","abstract":"  State-space modeling has emerged as a powerful paradigm for sequence analysis\nin various tasks such as natural language processing, time-series forecasting,\nand signal processing. In this work, we propose an \\emph{Adaptive State-Space\nMamba} (\\textbf{ASSM}) framework for real-time sensor data anomaly detection.\nWhile state-space models have been previously employed for image processing\napplications (e.g., style transfer \\cite{wang2024stylemamba}), our approach\nleverages the core idea of sequential hidden states to tackle a significantly\ndifferent domain: detecting anomalies on streaming sensor data.\n  In particular, we introduce an adaptive gating mechanism that dynamically\nmodulates the hidden state update based on contextual and learned statistical\ncues. This design ensures that our model remains computationally efficient and\nscalable, even under rapid data arrival rates. Extensive experiments on\nreal-world and synthetic sensor datasets demonstrate that our method achieves\nsuperior detection performance compared to existing baselines. Our approach is\neasily extensible to other time-series tasks that demand rapid and reliable\ndetection capabilities.\n","date":"2025-03-26"}
{"id":"2503.22744","title":"Uncertainty-Aware Graph Self-Training with Expectation-Maximization\n  Regularization","abstract":"  In this paper, we propose a novel \\emph{uncertainty-aware graph\nself-training} approach for semi-supervised node classification. Our method\nintroduces an Expectation-Maximization (EM) regularization scheme to\nincorporate an uncertainty mechanism during pseudo-label generation and model\nretraining. Unlike conventional graph self-training pipelines that rely on\nfixed pseudo-labels, our approach iteratively refines label confidences with an\nEM-inspired uncertainty measure. This ensures that the predictive model focuses\non reliable graph regions while gradually incorporating ambiguous nodes.\nInspired by prior work on uncertainty-aware self-training\ntechniques~\\cite{wang2024uncertainty}, our framework is designed to handle\nnoisy graph structures and feature spaces more effectively. Through extensive\nexperiments on several benchmark graph datasets, we demonstrate that our method\noutperforms strong baselines by a margin of up to 2.5\\% in accuracy while\nmaintaining lower variance in performance across multiple runs.\n","date":"2025-03-26"}
{"id":"2503.22745","title":"Graph-Based Uncertainty-Aware Self-Training with Stochastic Node\n  Labeling","abstract":"  Self-training has become a popular semi-supervised learning technique for\nleveraging unlabeled data. However, the over-confidence of pseudo-labels\nremains a key challenge. In this paper, we propose a novel \\emph{graph-based\nuncertainty-aware self-training} (GUST) framework to combat over-confidence in\nnode classification. Drawing inspiration from the uncertainty integration idea\nintroduced by Wang \\emph{et al.}~\\cite{wang2024uncertainty}, our method largely\ndiverges from previous self-training approaches by focusing on \\emph{stochastic\nnode labeling} grounded in the graph topology. Specifically, we deploy a\nBayesian-inspired module to estimate node-level uncertainty, incorporate these\nestimates into the pseudo-label generation process via an\nexpectation-maximization (EM)-like step, and iteratively update both node\nembeddings and adjacency-based transformations. Experimental results on several\nbenchmark graph datasets demonstrate that our GUST framework achieves\nstate-of-the-art performance, especially in settings where labeled data is\nextremely sparse.\n","date":"2025-03-26"}
{"id":"2503.22746","title":"Susceptibility of Large Language Models to User-Driven Factors in\n  Medical Queries","abstract":"  Large language models (LLMs) are increasingly used in healthcare, but their\nreliability is heavily influenced by user-driven factors such as question\nphrasing and the completeness of clinical information. In this study, we\nexamined how misinformation framing, source authority, model persona, and\nomission of key clinical details affect the diagnostic accuracy and reliability\nof LLM outputs. We conducted two experiments: one introducing misleading\nexternal opinions with varying assertiveness (perturbation test), and another\nremoving specific categories of patient information (ablation test). Using\npublic datasets (MedQA and Medbullets), we evaluated proprietary models\n(GPT-4o, Claude 3.5 Sonnet, Claude 3.5 Haiku, Gemini 1.5 Pro, Gemini 1.5 Flash)\nand open-source models (LLaMA 3 8B, LLaMA 3 Med42 8B, DeepSeek R1 8B). All\nmodels were vulnerable to user-driven misinformation, with proprietary models\nespecially affected by definitive and authoritative language. Assertive tone\nhad the greatest negative impact on accuracy. In the ablation test, omitting\nphysical exam findings and lab results caused the most significant performance\ndrop. Although proprietary models had higher baseline accuracy, their\nperformance declined sharply under misinformation. These results highlight the\nneed for well-structured prompts and complete clinical context. Users should\navoid authoritative framing of misinformation and provide full clinical\ndetails, especially for complex cases.\n","date":"2025-03-26"}
{"id":"2503.22747","title":"LeForecast: Enterprise Hybrid Forecast by Time Series Intelligence","abstract":"  Demand is spiking in industrial fields for multidisciplinary forecasting,\nwhere a broad spectrum of sectors needs planning and forecasts to streamline\nintelligent business management, such as demand forecasting, product planning,\ninventory optimization, etc. Specifically, these tasks expecting intelligent\napproaches to learn from sequentially collected historical data and then\nforesee most possible trend, i.e. time series forecasting. Challenge of it lies\nin interpreting complex business contexts and the efficiency and generalisation\nof modelling. With aspirations of pre-trained foundational models for such\npurpose, given their remarkable success of large foundation model across\nlegions of tasks, we disseminate \\leforecast{}, an enterprise intelligence\nplatform tailored for time series tasks. It integrates advanced interpretations\nof time series data and multi-source information, and a three-pillar modelling\nengine combining a large foundation model (Le-TSFM), multimodal model and\nhybrid model to derive insights, predict or infer futures, and then drive\noptimisation across multiple sectors in enterprise operations. The framework is\ncomposed by a model pool, model profiling module, and two different fusion\napproaches regarding original model architectures. Experimental results verify\nthe efficiency of our trail fusion concepts: router-based fusion network and\ncoordination of large and small models, resulting in high costs for redundant\ndevelopment and maintenance of models. This work reviews deployment of\nLeForecast and its performance in three industrial use cases. Our comprehensive\nexperiments indicate that LeForecast is a profound and practical platform for\nefficient and competitive performance. And we do hope that this work can\nenlighten the research and grounding of time series techniques in accelerating\nenterprise.\n","date":"2025-03-27"}
{"id":"2503.22748","title":"Ignite Forecasting with SPARK: An Efficient Generative Framework for\n  Refining LLMs in Temporal Knowledge Graph Forecasting","abstract":"  Temporal Knowledge Graph (TKG) forecasting is crucial for predicting future\nevents using historical data. With the surge of Large Language Models (LLMs),\nrecent studies have begun exploring their integration into TKG forecasting and\nachieved some success. However, they still face limitations such as limited\ninput length, inefficient output generation, and resource-intensive refinement,\nwhich undermine their performance and practical applicability. To address these\nlimitations, we introduce SPARK, a Sequence-level Proxy-Adapting framework for\nRefining LLMs in TKG forecasting. Inspired by inference-time algorithms adopted\nin controlling generation, SPARK offers a cost-effective, plug-and-play\nsolution through two key innovations: (1) Beam Sequence-Level Generation, which\nreframes TKG forecasting as a top-K sequence-level generation task, using beam\nsearch for efficiently generating next-entity distribution in a single forward\npass. (2) TKG Adapter for Refinement, which employs traditional TKG models as\ntrainable proxy adapters to leverage global graph information and refine LLM\noutputs, overcoming both the input length and the resource-intensive\nfine-tuning problems. Experiments across diverse datasets validate SPARK's\nforecasting performance, robust generalization capabilities, and high\nefficiency. We release source codes at https:\/\/github.com\/yin-gz\/SPARK.\n","date":"2025-03-27"}
{"id":"2503.22749","title":"Adaptive Clipping for Privacy-Preserving Few-Shot Learning: Enhancing\n  Generalization with Limited Data","abstract":"  In the era of data-driven machine-learning applications, privacy concerns and\nthe scarcity of labeled data have become paramount challenges. These challenges\nare particularly pronounced in the domain of few-shot learning, where the\nability to learn from limited labeled data is crucial. Privacy-preserving\nfew-shot learning algorithms have emerged as a promising solution to address\nsuch pronounced challenges. However, it is well-known that privacy-preserving\ntechniques often lead to a drop in utility due to the fundamental trade-off\nbetween data privacy and model performance. To enhance the utility of\nprivacy-preserving few-shot learning methods, we introduce a novel approach\ncalled Meta-Clip. This technique is specifically designed for meta-learning\nalgorithms, including Differentially Private (DP) model-agnostic meta-learning,\nDP-Reptile, and DP-MetaSGD algorithms, with the objective of balancing data\nprivacy preservation with learning capacity maximization. By dynamically\nadjusting clipping thresholds during the training process, our Adaptive\nClipping method provides fine-grained control over the disclosure of sensitive\ninformation, mitigating overfitting on small datasets and significantly\nimproving the generalization performance of meta-learning models. Through\ncomprehensive experiments on diverse benchmark datasets, we demonstrate the\neffectiveness of our approach in minimizing utility degradation, showcasing a\nsuperior privacy-utility trade-off compared to existing privacy-preserving\ntechniques. The adoption of Adaptive Clipping represents a substantial step\nforward in the field of privacy-preserving few-shot learning, empowering the\ndevelopment of secure and accurate models for real-world applications,\nespecially in scenarios where there are limited data availability.\n","date":"2025-03-27"}
{"id":"2503.22751","title":"Advancing Spatiotemporal Prediction using Artificial Intelligence:\n  Extending the Framework of Geographically and Temporally Weighted Neural\n  Network (GTWNN) for Differing Geographical and Temporal Contexts","abstract":"  This paper aims at improving predictive crime models by extending the\nmathematical framework of Artificial Neural Networks (ANNs) tailored to general\nspatiotemporal problems and appropriately applying them. Recent advancements in\nthe geospatial-temporal modelling field have focused on the inclusion of\ngeographical weighting in their deep learning models to account for nonspatial\nstationarity, which is often apparent in spatial data. We formulate a novel\nsemi-analytical approach to solving Geographically and Temporally Weighted\nRegression (GTWR), and applying it to London crime data. The results produce\nhigh-accuracy predictive evaluation scores that affirm the validity of the\nassumptions and approximations in the approach. This paper presents\nmathematical advances to the Geographically and Temporally Weighted Neural\nNetwork (GTWNN) framework, which offers a novel contribution to the field.\nInsights from past literature are harmoniously employed with the assumptions\nand approximations to generate three mathematical extensions to GTWNN's\nframework. Combinations of these extensions produce five novel ANNs, applied to\nthe London and Detroit datasets. The results suggest that one of the extensions\nis redundant and is generally surpassed by another extension, which we term the\nhistory-dependent module. The remaining extensions form three novel ANN designs\nthat pose potential GTWNN improvements. We evaluated the efficacy of various\nmodels in both the London and Detroit crime datasets, highlighting the\nimportance of accounting for specific geographic and temporal characteristics\nwhen selecting modelling strategies to improve model suitability. In general,\nthe proposed methods provide the foundations for a more context-aware,\naccurate, and robust ANN approach in spatio-temporal modelling.\n","date":"2025-03-27"}
{"id":"2503.22752","title":"From Individual to Group: Developing a Context-Aware Multi-Criteria\n  Group Recommender System","abstract":"  Group decision-making is becoming increasingly common in areas such as\neducation, dining, travel, and finance, where collaborative choices must\nbalance diverse individual preferences. While conventional recommender systems\nare effective in personalization, they fall short in group settings due to\ntheir inability to manage conflicting preferences, contextual factors, and\nmultiple evaluation criteria. This study presents the development of a\nContext-Aware Multi-Criteria Group Recommender System (CA-MCGRS) designed to\naddress these challenges by integrating contextual factors and multiple\ncriteria to enhance recommendation accuracy. By leveraging a Multi-Head\nAttention mechanism, our model dynamically weighs the importance of different\nfeatures. Experiments conducted on an educational dataset with varied ratings\nand contextual variables demonstrate that CA-MCGRS consistently outperforms\nother approaches across four scenarios. Our findings underscore the importance\nof incorporating context and multi-criteria evaluations to improve group\nrecommendations, offering valuable insights for developing more effective group\nrecommender systems.\n","date":"2025-03-27"}
{"id":"2503.22753","title":"Combating the Bullwhip Effect in Rival Online Food Delivery Platforms\n  Using Deep Learning","abstract":"  The wastage of perishable items has led to significant health and economic\ncrises, increasing business uncertainty and fluctuating customer demand. This\nissue is worsened by online food delivery services, where frequent and\nunpredictable orders create inefficiencies in supply chain management,\ncontributing to the bullwhip effect. This effect results in stockouts, excess\ninventory, and inefficiencies. Accurate demand forecasting helps stabilize\ninventory, optimize supplier orders, and reduce waste. This paper presents a\nThird-Party Logistics (3PL) supply chain model involving restaurants, online\nfood apps, and customers, along with a deep learning-based demand forecasting\nmodel using a two-phase Long Short-Term Memory (LSTM) network.\n  Phase one, intra-day forecasting, captures short-term variations, while phase\ntwo, daily forecasting, predicts overall demand. A two-year dataset from\nJanuary 2023 to January 2025 from Swiggy and Zomato is used, employing discrete\nevent simulation and grid search for optimal LSTM hyperparameters. The proposed\nmethod is evaluated using RMSE, MAE, and R-squared score, with R-squared as the\nprimary accuracy measure. Phase one achieves an R-squared score of 0.69 for\nZomato and 0.71 for Swiggy with a training time of 12 minutes, while phase two\nimproves to 0.88 for Zomato and 0.90 for Swiggy with a training time of 8\nminutes.\n  To mitigate demand fluctuations, restaurant inventory is dynamically managed\nusing the newsvendor model, adjusted based on forecasted demand. The proposed\nframework significantly reduces the bullwhip effect, improving forecasting\naccuracy and supply chain efficiency. For phase one, supply chain instability\ndecreases from 2.61 to 0.96, and for phase two, from 2.19 to 0.80. This\ndemonstrates the model's effectiveness in minimizing food waste and maintaining\noptimal restaurant inventory levels.\n","date":"2025-03-27"}
{"id":"2503.22754","title":"Model Lake: a New Alternative for Machine Learning Models Management and\n  Governance","abstract":"  The rise of artificial intelligence and data science across industries\nunderscores the pressing need for effective management and governance of\nmachine learning (ML) models. Traditional approaches to ML models management\noften involve disparate storage systems and lack standardized methodologies for\nversioning, audit, and re-use. Inspired by data lake concepts, this paper\ndevelops the concept of ML Model Lake as a centralized management framework for\ndatasets, codes, and models within organizations environments. We provide an\nin-depth exploration of the Model Lake concept, delineating its architectural\nfoundations, key components, operational benefits, and practical challenges. We\ndiscuss the transformative potential of adopting a Model Lake approach, such as\nenhanced model lifecycle management, discovery, audit, and reusability.\nFurthermore, we illustrate a real-world application of Model Lake and its\ntransformative impact on data, code and model management practices.\n","date":"2025-03-27"}
{"id":"2503.22755","title":"Reasoning Under Threat: Symbolic and Neural Techniques for Cybersecurity\n  Verification","abstract":"  Cybersecurity demands rigorous and scalable techniques to ensure system\ncorrectness, robustness, and resilience against evolving threats. Automated\nreasoning, encompassing formal logic, theorem proving, model checking, and\nsymbolic analysis, provides a foundational framework for verifying security\nproperties across diverse domains such as access control, protocol design,\nvulnerability detection, and adversarial modeling. This survey presents a\ncomprehensive overview of the role of automated reasoning in cybersecurity,\nanalyzing how logical systems, including temporal, deontic, and epistemic\nlogics are employed to formalize and verify security guarantees. We examine\nSOTA tools and frameworks, explore integrations with AI for neural-symbolic\nreasoning, and highlight critical research gaps, particularly in scalability,\ncompositionality, and multi-layered security modeling. The paper concludes with\na set of well-grounded future research directions, aiming to foster the\ndevelopment of secure systems through formal, automated, and explainable\nreasoning techniques.\n","date":"2025-03-27"}
{"id":"2503.22756","title":"Towards an intelligent assessment system for evaluating the development\n  of algorithmic thinking skills: An exploratory study in Swiss compulsory\n  schools","abstract":"  The rapid digitalisation of contemporary society has profoundly impacted\nvarious facets of our lives, including healthcare, communication, business, and\neducation. The ability to engage with new technologies and solve problems has\nbecome crucial, making CT skills, such as pattern recognition, decomposition,\nand algorithm design, essential competencies. In response, Switzerland is\nconducting research and initiatives to integrate CT into its educational\nsystem. This study aims to develop a comprehensive framework for large-scale\nassessment of CT skills, particularly focusing on AT, the ability to design\nalgorithms. To achieve this, we first developed a competence model capturing\nthe situated and developmental nature of CT, guiding the design of activities\ntailored to cognitive abilities, age, and context. This framework clarifies how\nactivity characteristics influence CT development and how to assess these\ncompetencies. Additionally, we developed an activity for large-scale assessment\nof AT skills, offered in two variants: one based on non-digital artefacts\n(unplugged) and manual expert assessment, and the other based on digital\nartefacts (virtual) and automatic assessment. To provide a more comprehensive\nevaluation of students' competencies, we developed an IAS based on BNs with\nnoisy gates, which offers real-time probabilistic assessment for each skill\nrather than a single overall score. The results indicate that the proposed\ninstrument can measure AT competencies across different age groups and\neducational contexts in Switzerland, demonstrating its applicability for\nlarge-scale use. AT competencies exhibit a progressive development, with no\noverall gender differences, though variations are observed at the school level,\nsignificantly influenced by the artefact-based environment and its context,\nunderscoring the importance of creating accessible and adaptable assessment\ntools.\n","date":"2025-03-27"}
{"id":"2503.22758","title":"Multiple Embeddings for Quantum Machine Learning","abstract":"  This work focuses on the limitations about the insufficient fitting\ncapability of current quantum machine learning methods, which results from the\nover-reliance on a single data embedding strategy. We propose a novel quantum\nmachine learning framework that integrates multiple quantum data embedding\nstrategies, allowing the model to fully exploit the diversity of quantum\ncomputing when processing various datasets. Experimental results validate the\neffectiveness of the proposed framework, demonstrating significant improvements\nover existing state-of-the-art methods and achieving superior performance in\npractical applications.\n","date":"2025-03-27"}
{"id":"2503.22759","title":"Data Poisoning in Deep Learning: A Survey","abstract":"  Deep learning has become a cornerstone of modern artificial intelligence,\nenabling transformative applications across a wide range of domains. As the\ncore element of deep learning, the quality and security of training data\ncritically influence model performance and reliability. However, during the\ntraining process, deep learning models face the significant threat of data\npoisoning, where attackers introduce maliciously manipulated training data to\ndegrade model accuracy or lead to anomalous behavior. While existing surveys\nprovide valuable insights into data poisoning, they generally adopt a broad\nperspective, encompassing both attacks and defenses, but lack a dedicated,\nin-depth analysis of poisoning attacks specifically in deep learning. In this\nsurvey, we bridge this gap by presenting a comprehensive and targeted review of\ndata poisoning in deep learning. First, this survey categorizes data poisoning\nattacks across multiple perspectives, providing an in-depth analysis of their\ncharacteristics and underlying design princinples. Second, the discussion is\nextended to the emerging area of data poisoning in large language models(LLMs).\nFinally, we explore critical open challenges in the field and propose potential\nresearch directions to advance the field further. To support further\nexploration, an up-to-date repository of resources on data poisoning in deep\nlearning is available at https:\/\/github.com\/Pinlong-Zhao\/Data-Poisoning.\n","date":"2025-03-27"}
{"id":"2503.22760","title":"Malicious and Unintentional Disclosure Risks in Large Language Models\n  for Code Generation","abstract":"  This paper explores the risk that a large language model (LLM) trained for\ncode generation on data mined from software repositories will generate content\nthat discloses sensitive information included in its training data. We\ndecompose this risk, known in the literature as ``unintended memorization,''\ninto two components: unintentional disclosure (where an LLM presents secrets to\nusers without the user seeking them out) and malicious disclosure (where an LLM\npresents secrets to an attacker equipped with partial knowledge of the training\ndata). We observe that while existing work mostly anticipates malicious\ndisclosure, unintentional disclosure is also a concern. We describe methods to\nassess unintentional and malicious disclosure risks side-by-side across\ndifferent releases of training datasets and models. We demonstrate these\nmethods through an independent assessment of the Open Language Model (OLMo)\nfamily of models and its Dolma training datasets. Our results show, first, that\nchanges in data source and processing are associated with substantial changes\nin unintended memorization risk; second, that the same set of operational\nchanges may increase one risk while mitigating another; and, third, that the\nrisk of disclosing sensitive information varies not only by prompt strategies\nor test datasets but also by the types of sensitive information. These\ncontributions rely on data mining to enable greater privacy and security\ntesting required for the LLM training data supply chain.\n","date":"2025-03-27"}
{"id":"2503.22762","title":"The Cost of Local and Global Fairness in Federated Learning","abstract":"  With the emerging application of Federated Learning (FL) in finance, hiring\nand healthcare, FL models are regulated to be fair, preventing disparities with\nrespect to legally protected attributes such as race or gender. Two concepts of\nfairness are important in FL: global and local fairness. Global fairness\naddresses the disparity across the entire population and local fairness is\nconcerned with the disparity within each client. Prior fair FL frameworks have\nimproved either global or local fairness without considering both. Furthermore,\nwhile the majority of studies on fair FL focuses on binary settings, many\nreal-world applications are multi-class problems. This paper proposes a\nframework that investigates the minimum accuracy lost for enforcing a specified\nlevel of global and local fairness in multi-class FL settings. Our framework\nleads to a simple post-processing algorithm that derives fair outcome\npredictors from the Bayesian optimal score functions. Experimental results show\nthat our algorithm outperforms the current state of the art (SOTA) with regard\nto the accuracy-fairness tradoffs, computational and communication costs. Codes\nare available at:\nhttps:\/\/github.com\/papersubmission678\/The-cost-of-local-and-global-fairness-in-FL .\n","date":"2025-03-27"}
{"id":"2503.22764","title":"Boosting Large Language Models with Mask Fine-Tuning","abstract":"  The model is usually kept integral in the mainstream large language model\n(LLM) fine-tuning protocols. No works have questioned whether maintaining the\nintegrity of the model is indispensable for performance. In this work, we\nintroduce Mask Fine-Tuning (MFT), a brand-new LLM fine-tuning paradigm to show\nthat properly breaking the integrity of the model can surprisingly lead to\nimproved performance. Specifically, MFT learns a set of binary masks supervised\nby the typical LLM fine-tuning objective. Extensive experiments show that MFT\ngains a consistent performance boost across various domains and backbones\n(e.g., 1.95%\/1.88% average gain in coding with LLaMA2-7B\/3.1-8B). Detailed\nprocedures are provided to study the proposed MFT from different hyperparameter\nperspectives for better insight. In particular, MFT naturally updates the\ncurrent LLM training protocol by deploying it on a complete well-trained model.\nThis study extends the functionality of mask learning from its conventional\nnetwork pruning context for model compression to a more general scope.\n","date":"2025-03-27"}
{"id":"2503.22769","title":"MediTools -- Medical Education Powered by LLMs","abstract":"  Artificial Intelligence (AI) has been advancing rapidly and with the advent\nof large language models (LLMs) in late 2022, numerous opportunities have\nemerged for adopting this technology across various domains, including\nmedicine. These innovations hold immense potential to revolutionize and\nmodernize medical education. Our research project leverages large language\nmodels to enhance medical education and address workflow challenges through the\ndevelopment of MediTools - AI Medical Education. This prototype application\nfocuses on developing interactive tools that simulate real-life clinical\nscenarios, provide access to medical literature, and keep users updated with\nthe latest medical news. Our first tool is a dermatology case simulation tool\nthat uses real patient images depicting various dermatological conditions and\nenables interaction with LLMs acting as virtual patients. This platform allows\nusers to practice their diagnostic skills and enhance their clinical\ndecision-making abilities. The application also features two additional tools:\nan AI-enhanced PubMed tool for engaging with LLMs to gain deeper insights into\nresearch papers, and a Google News tool that offers LLM generated summaries of\narticles for various medical specialties. A comprehensive survey has been\nconducted among medical professionals and students to gather initial feedback\non the effectiveness and user satisfaction of MediTools, providing insights for\nfurther development and refinement of the application. This research\ndemonstrates the potential of AI-driven tools in transforming and\nrevolutionizing medical education, offering a scalable and interactive platform\nfor continuous learning and skill development.\n","date":"2025-03-28"}
{"id":"2503.22771","title":"GroundHog: Revolutionizing GLDAS Groundwater Storage Downscaling for\n  Enhanced Recharge Estimation in Bangladesh","abstract":"  Long-term groundwater level (GWL) measurement is vital for effective\npolicymaking and recharge estimation using annual maxima and minima. However,\ncurrent methods prioritize short-term predictions and lack multi-year\napplicability, limiting their utility. Moreover, sparse in-situ measurements\nlead to reliance on low-resolution satellite data like GLDAS as the ground\ntruth for Machine Learning models, further constraining accuracy. To overcome\nthese challenges, we first develop an ML model to mitigate data gaps, achieving\n$R^2$ scores of 0.855 and 0.963 for maximum and minimum GWL predictions,\nrespectively. Subsequently, using these predictions and well observations as\nground truth, we train an Upsampling Model that uses low-resolution (25 km)\nGLDAS data as input to produce high-resolution (2 km) GWLs, achieving an\nexcellent $R^2$ score of 0.96. Our approach successfully upscales GLDAS data\nfor 2003-2024, allowing high-resolution recharge estimations and revealing\ncritical trends for proactive resource management. Our method allows upsampling\nof groundwater storage (GWS) from GLDAS to high-resolution GWLs for any points\nindependently of officially curated piezometer data, making it a valuable tool\nfor decision-making.\n","date":"2025-03-28"}
{"id":"2503.22773","title":"Congenital Heart Disease Classification Using Phonocardiograms: A\n  Scalable Screening Tool for Diverse Environments","abstract":"  Congenital heart disease (CHD) is a critical condition that demands early\ndetection, particularly in infancy and childhood. This study presents a deep\nlearning model designed to detect CHD using phonocardiogram (PCG) signals, with\na focus on its application in global health. We evaluated our model on several\ndatasets, including the primary dataset from Bangladesh, achieving a high\naccuracy of 94.1%, sensitivity of 92.7%, specificity of 96.3%. The model also\ndemonstrated robust performance on the public PhysioNet Challenge 2022 and 2016\ndatasets, underscoring its generalizability to diverse populations and data\nsources. We assessed the performance of the algorithm for single and multiple\nauscultation sites on the chest, demonstrating that the model maintains over\n85% accuracy even when using a single location. Furthermore, our algorithm was\nable to achieve an accuracy of 80% on low-quality recordings, which\ncardiologists deemed non-diagnostic. This research suggests that an AI- driven\ndigital stethoscope could serve as a cost-effective screening tool for CHD in\nresource-limited settings, enhancing clinical decision support and ultimately\nimproving patient outcomes.\n","date":"2025-03-28"}
{"id":"2503.22775","title":"Invariant Control Strategies for Active Flow Control using Graph Neural\n  Networks","abstract":"  Reinforcement learning has gained traction for active flow control tasks,\nwith initial applications exploring drag mitigation via flow field augmentation\naround a two-dimensional cylinder. RL has since been extended to more complex\nturbulent flows and has shown significant potential in learning complex control\nstrategies. However, such applications remain computationally challenging due\nto its sample inefficiency and associated simulation costs. This fact is\nworsened by the lack of generalization capabilities of these trained policy\nnetworks, often being implicitly tied to the input configurations of their\ntraining conditions. In this work, we propose the use of graph neural networks\nto address this particular limitation, effectively increasing the range of\napplicability and getting more value out of the upfront RL training cost. GNNs\ncan naturally process unstructured, threedimensional flow data, preserving\nspatial relationships without the constraints of a Cartesian grid.\nAdditionally, they incorporate rotational, reflectional, and permutation\ninvariance into the learned control policies, thus improving generalization and\nthereby removing the shortcomings of commonly used CNN or MLP architectures. To\ndemonstrate the effectiveness of this approach, we revisit the well-established\ntwo-dimensional cylinder benchmark problem for active flow control. The RL\ntraining is implemented using Relexi, a high-performance RL framework, with\nflow simulations conducted in parallel using the high-order discontinuous\nGalerkin framework FLEXI. Our results show that GNN-based control policies\nachieve comparable performance to existing methods while benefiting from\nimproved generalization properties. This work establishes GNNs as a promising\narchitecture for RL-based flow control and highlights the capabilities of\nRelexi and FLEXI for large-scale RL applications in fluid dynamics.\n","date":"2025-03-28"}
{"id":"2503.22776","title":"Post-Incorporating Code Structural Knowledge into LLMs via In-Context\n  Learning for Code Translation","abstract":"  Code translation migrates codebases across programming languages. Recently,\nlarge language models (LLMs) have achieved significant advancements in software\nmining. However, handling the syntactic structure of source code remains a\nchallenge. Classic syntax-aware methods depend on intricate model architectures\nand loss functions, rendering their integration into LLM training\nresource-intensive. This paper employs in-context learning (ICL), which\ndirectly integrates task exemplars into the input context, to post-incorporate\ncode structural knowledge into pre-trained LLMs. We revisit exemplar selection\nin ICL from an information-theoretic perspective, proposing that list-wise\nselection based on information coverage is more precise and general objective\nthan traditional methods based on combining similarity and diversity. To\naddress the challenges of quantifying information coverage, we introduce a\nsurrogate measure, Coverage of Abstract Syntax Tree (CAST). Furthermore, we\nformulate the NP-hard CAST maximization for exemplar selection and prove that\nit is a standard submodular maximization problem. Therefore, we propose a\ngreedy algorithm for CAST submodular maximization, which theoretically\nguarantees a (1-1\/e)-approximate solution in polynomial time complexity. Our\nmethod is the first training-free and model-agnostic approach to\npost-incorporate code structural knowledge into existing LLMs at test time.\nExperimental results show that our method significantly improves LLMs\nperformance and reveals two meaningful insights: 1) Code structural knowledge\ncan be effectively post-incorporated into pre-trained LLMs during inference,\ndespite being overlooked during training; 2) Scaling up model size or training\ndata does not lead to the emergence of code structural knowledge, underscoring\nthe necessity of explicitly considering code syntactic structure.\n","date":"2025-03-28"}
{"id":"2503.22779","title":"Policy Optimization and Multi-agent Reinforcement Learning for\n  Mean-variance Team Stochastic Games","abstract":"  We study a long-run mean-variance team stochastic game (MV-TSG), where each\nagent shares a common mean-variance objective for the system and takes actions\nindependently to maximize it. MV-TSG has two main challenges. First, the\nvariance metric is neither additive nor Markovian in a dynamic setting. Second,\nsimultaneous policy updates of all agents lead to a non-stationary environment\nfor each individual agent. Both challenges make dynamic programming\ninapplicable. In this paper, we study MV-TSGs from the perspective of\nsensitivity-based optimization. The performance difference and performance\nderivative formulas for joint policies are derived, which provide optimization\ninformation for MV-TSGs. We prove the existence of a deterministic Nash policy\nfor this problem. Subsequently, we propose a Mean-Variance Multi-Agent Policy\nIteration (MV-MAPI) algorithm with a sequential update scheme, where individual\nagent policies are updated one by one in a given order. We prove that the\nMV-MAPI algorithm converges to a first-order stationary point of the objective\nfunction. By analyzing the local geometry of stationary points, we derive\nspecific conditions for stationary points to be (local) Nash equilibria, and\nfurther, strict local optima. To solve large-scale MV-TSGs in scenarios with\nunknown environmental parameters, we extend the idea of trust region methods to\nMV-MAPI and develop a multi-agent reinforcement learning algorithm named\nMean-Variance Multi-Agent Trust Region Policy Optimization (MV-MATRPO). We\nderive a performance lower bound for each update of joint policies. Finally,\nnumerical experiments on energy management in multiple microgrid systems are\nconducted.\n","date":"2025-03-28"}
{"id":"2503.22782","title":"Patronus: Bringing Transparency to Diffusion Models with Prototypes","abstract":"  Diffusion-based generative models, such as Denoising Diffusion Probabilistic\nModels (DDPMs), have achieved remarkable success in image generation, but their\nstep-by-step denoising process remains opaque, leaving critical aspects of the\ngeneration mechanism unexplained. To address this, we introduce\n\\emph{Patronus}, an interpretable diffusion model inspired by ProtoPNet.\nPatronus integrates a prototypical network into DDPMs, enabling the extraction\nof prototypes and conditioning of the generation process on their prototype\nactivation vector. This design enhances interpretability by showing the learned\nprototypes and how they influence the generation process. Additionally, the\nmodel supports downstream tasks like image manipulation, enabling more\ntransparent and controlled modifications. Moreover, Patronus could reveal\nshortcut learning in the generation process by detecting unwanted correlations\nbetween learned prototypes. Notably, Patronus operates entirely without any\nannotations or text prompts. This work opens new avenues for understanding and\ncontrolling diffusion models through prototype-based interpretability. Our code\nis available at\n\\href{https:\/\/github.com\/nina-weng\/patronus}{https:\/\/github.com\/nina-weng\/patronus}.\n","date":"2025-03-28"}
{"id":"2503.22786","title":"A formula for the area of a triangle: Useless, but explicitly in Deep\n  Sets form","abstract":"  Any permutation-invariant function of data points $\\vec{r}_i$ can be written\nin the form $\\rho(\\sum_i\\phi(\\vec{r}_i))$ for suitable functions $\\rho$ and\n$\\phi$. This form - known in the machine-learning literature as Deep Sets -\nalso generates a map-reduce algorithm. The area of a triangle is a\npermutation-invariant function of the locations $\\vec{r}_i$ of the three\ncorners $1\\leq i\\leq 3$. We find the polynomial formula for the area of a\ntriangle that is explicitly in Deep Sets form. This project was motivated by\nquestions about the fundamental computational complexity of $n$-point\nstatistics in cosmology; that said, no insights of any kind were gained from\nthese results.\n","date":"2025-03-28"}
{"id":"2503.22796","title":"DiTFastAttnV2: Head-wise Attention Compression for Multi-Modality\n  Diffusion Transformers","abstract":"  Text-to-image generation models, especially Multimodal Diffusion Transformers\n(MMDiT), have shown remarkable progress in generating high-quality images.\nHowever, these models often face significant computational bottlenecks,\nparticularly in attention mechanisms, which hinder their scalability and\nefficiency. In this paper, we introduce DiTFastAttnV2, a post-training\ncompression method designed to accelerate attention in MMDiT. Through an\nin-depth analysis of MMDiT's attention patterns, we identify key differences\nfrom prior DiT-based methods and propose head-wise arrow attention and caching\nmechanisms to dynamically adjust attention heads, effectively bridging this\ngap. We also design an Efficient Fused Kernel for further acceleration. By\nleveraging local metric methods and optimization techniques, our approach\nsignificantly reduces the search time for optimal compression schemes to just\nminutes while maintaining generation quality. Furthermore, with the customized\nkernel, DiTFastAttnV2 achieves a 68% reduction in attention FLOPs and 1.5x\nend-to-end speedup on 2K image generation without compromising visual fidelity.\n","date":"2025-03-28"}
{"id":"2503.22809","title":"Data-driven worker activity recognition and picking efficiency\n  estimation in manual strawberry harvesting","abstract":"  Manual fruit harvesting is common in agriculture, but the amount of time that\npickers spend on nonproductive activities can make it very inefficient.\nAccurately identifying picking vs. non-picking activity is crucial for\nestimating picker efficiency and optimizing labor management and the harvest\nprocess. In this study, a practical system was developed to calculate the\nefficiency of pickers in commercial strawberry harvesting. Instrumented picking\ncarts were used to record in real-time the harvested fruit weight,\ngeo-location, and cart movement. A fleet of these carts was deployed during the\ncommercial strawberry harvest season in Santa Maria, CA. The collected data was\nthen used to train a CNN-LSTM-based deep neural network to classify a picker's\nactivity into ``Pick\" and ``NoPick\" classes. Experimental evaluations showed\nthat the CNN-LSTM model showed promising activity recognition performance with\nan F1 score accuracy of up to 0.974. The classification results were then used\nto compute two worker efficiency metrics: the percentage of time spent actively\npicking, and the time required to fill a tray. Analysis of the season-long\nharvest data showed that the pickers spent an average of 73.56% of their total\nharvest time actively picking strawberries, with an average tray fill time of\n6.22 minutes. The mean accuracies of these metrics were 96.29% and 95.42%,\nrespectively. When integrated on a commercial scale, the proposed technology\ncould aid growers in automated worker activity monitoring and harvest\noptimization, ultimately helping to reduce non-productive time and enhance\noverall harvest efficiency.\n","date":"2025-03-28"}
{"id":"2503.22810","title":"Harnessing uncertainty when learning through Equilibrium Propagation in\n  neural networks","abstract":"  Equilibrium Propagation (EP) is a supervised learning algorithm that trains\nnetwork parameters using local neuronal activity. This is in stark contrast to\nbackpropagation, where updating the parameters of the network requires\nsignificant data shuffling. Avoiding data movement makes EP particularly\ncompelling as a learning framework for energy-efficient training on\nneuromorphic systems. In this work, we assess the ability of EP to learn on\nhardware that contain physical uncertainties. This is particularly important\nfor researchers concerned with hardware implementations of self-learning\nsystems that utilize EP. Our results demonstrate that deep, multi-layer neural\nnetwork architectures can be trained successfully using EP in the presence of\nfinite uncertainties, up to a critical limit. This limit is independent of the\ntraining dataset, and can be scaled through sampling the network according to\nthe central limit theorem. Additionally, we demonstrate improved model\nconvergence and performance for finite levels of uncertainty on the MNIST,\nKMNIST and FashionMNIST datasets. Optimal performance is found for networks\ntrained with uncertainties close to the critical limit. Our research supports\nfuture work to build self-learning hardware in situ with EP.\n","date":"2025-03-28"}
{"id":"2503.22823","title":"Quantum Doeblin Coefficients: Interpretations and Applications","abstract":"  In classical information theory, the Doeblin coefficient of a classical\nchannel provides an efficiently computable upper bound on the total-variation\ncontraction coefficient of the channel, leading to what is known as a strong\ndata-processing inequality. Here, we investigate quantum Doeblin coefficients\nas a generalization of the classical concept. In particular, we define various\nnew quantum Doeblin coefficients, one of which has several desirable\nproperties, including concatenation and multiplicativity, in addition to being\nefficiently computable. We also develop various interpretations of two of the\nquantum Doeblin coefficients, including representations as minimal singlet\nfractions, exclusion values, reverse max-mutual and oveloH informations,\nreverse robustnesses, and hypothesis testing reverse mutual and oveloH\ninformations. Our interpretations of quantum Doeblin coefficients as either\nentanglement-assisted or unassisted exclusion values are particularly\nappealing, indicating that they are proportional to the best possible error\nprobabilities one could achieve in state-exclusion tasks by making use of the\nchannel. We also outline various applications of quantum Doeblin coefficients,\nranging from limitations on quantum machine learning algorithms that use\nparameterized quantum circuits (noise-induced barren plateaus), on error\nmitigation protocols, on the sample complexity of noisy quantum hypothesis\ntesting, on the fairness of noisy quantum models, and on mixing times of\ntime-varying channels. All of these applications make use of the fact that\nquantum Doeblin coefficients appear in upper bounds on various trace-distance\ncontraction coefficients of a channel. Furthermore, in all of these\napplications, our analysis using Doeblin coefficients provides improvements of\nvarious kinds over contributions from prior literature, both in terms of\ngenerality and being efficiently computable.\n","date":"2025-03-28"}
{"id":"2503.22828","title":"Learning to Reason for Long-Form Story Generation","abstract":"  Generating high-quality stories spanning thousands of tokens requires\ncompetency across a variety of skills, from tracking plot and character arcs to\nkeeping a consistent and engaging style. Due to the difficulty of sourcing\nlabeled datasets and precise quality measurements, most work using large\nlanguage models (LLMs) for long-form story generation uses combinations of\nhand-designed prompting techniques to elicit author-like behavior. This is a\nmanual process that is highly dependent on the specific story-generation task.\nMotivated by the recent success of applying RL with Verifiable Rewards to\ndomains like math and coding, we propose a general story-generation task\n(Next-Chapter Prediction) and a reward formulation (Verified Rewards via\nCompletion Likelihood Improvement) that allows us to use an unlabeled book\ndataset as a learning signal for reasoning. We learn to reason over a story's\ncondensed information and generate a detailed plan for the next chapter. Our\nreasoning is evaluated via the chapters it helps a story-generator create, and\ncompared against non-trained and supervised finetuning (SFT) baselines.\nPairwise human judgments reveal the chapters our learned reasoning produces are\npreferred across almost all metrics, and the effect is more pronounced in Scifi\nand Fantasy genres.\n","date":"2025-03-28"}
{"id":"2503.22829","title":"Nonhuman Primate Brain Tissue Segmentation Using a Transfer Learning\n  Approach","abstract":"  Non-human primates (NHPs) serve as critical models for understanding human\nbrain function and neurological disorders due to their close evolutionary\nrelationship with humans. Accurate brain tissue segmentation in NHPs is\ncritical for understanding neurological disorders, but challenging due to the\nscarcity of annotated NHP brain MRI datasets, the small size of the NHP brain,\nthe limited resolution of available imaging data and the anatomical differences\nbetween human and NHP brains. To address these challenges, we propose a novel\napproach utilizing STU-Net with transfer learning to leverage knowledge\ntransferred from human brain MRI data to enhance segmentation accuracy in the\nNHP brain MRI, particularly when training data is limited. The combination of\nSTU-Net and transfer learning effectively delineates complex tissue boundaries\nand captures fine anatomical details specific to NHP brains. Notably, our\nmethod demonstrated improvement in segmenting small subcortical structures such\nas putamen and thalamus that are challenging to resolve with limited spatial\nresolution and tissue contrast, and achieved DSC of over 0.88, IoU over 0.8 and\nHD95 under 7. This study introduces a robust method for multi-class brain\ntissue segmentation in NHPs, potentially accelerating research in evolutionary\nneuroscience and preclinical studies of neurological disorders relevant to\nhuman health.\n","date":"2025-03-28"}
{"id":"2503.22832","title":"L0-Reasoning Bench: Evaluating Procedural Correctness in Language Models\n  via Simple Program Execution","abstract":"  Complex reasoning tasks often rely on the ability to consistently and\naccurately apply simple rules across incremental steps, a foundational\ncapability which we term \"level-0\" reasoning. To systematically evaluate this\ncapability, we introduce L0-Bench, a language model benchmark for testing\nprocedural correctness -- the ability to generate correct reasoning processes,\ncomplementing existing benchmarks that primarily focus on outcome correctness.\nGiven synthetic Python functions with simple operations, L0-Bench grades models\non their ability to generate step-by-step, error-free execution traces. The\nsynthetic nature of L0-Bench enables systematic and scalable generation of test\nprograms along various axes (e.g., number of trace steps). We evaluate a\ndiverse array of recent closed-source and open-weight models on a baseline test\nset. All models exhibit degradation as the number of target trace steps\nincreases, while larger models and reasoning-enhanced models better maintain\ncorrectness over multiple steps. Additionally, we use L0-Bench to explore\ntest-time scaling along three dimensions: input context length, number of\nsolutions for majority voting, and inference steps. Our results suggest\nsubstantial room to improve \"level-0\" reasoning and potential directions to\nbuild more reliable reasoning systems.\n","date":"2025-03-28"}
{"id":"2503.22841","title":"GmNet: Revisiting Gating Mechanisms From A Frequency View","abstract":"  Gating mechanisms have emerged as an effective strategy integrated into model\ndesigns beyond recurrent neural networks for addressing long-range dependency\nproblems. In a broad understanding, it provides adaptive control over the\ninformation flow while maintaining computational efficiency. However, there is\na lack of theoretical analysis on how the gating mechanism works in neural\nnetworks. In this paper, inspired by the {convolution theorem}, we\nsystematically explore the effect of gating mechanisms on the training dynamics\nof neural networks from a frequency perspective. We investigate the interact\nbetween the element-wise product and activation functions in managing the\nresponses to different frequency components. Leveraging these insights, we\npropose a Gating Mechanism Network (GmNet), a lightweight model designed to\nefficiently utilize the information of various frequency components. It\nminimizes the low-frequency bias present in existing lightweight models. GmNet\nachieves impressive performance in terms of both effectiveness and efficiency\nin the image classification task.\n","date":"2025-03-28"}
{"id":"2503.22851","title":"RobuNFR: Evaluating the Robustness of Large Language Models on\n  Non-Functional Requirements Aware Code Generation","abstract":"  When using LLMs to address Non-Functional Requirements (NFRs), developers may\nbehave differently (e.g., expressing the same NFR in different words). Robust\nLLMs should output consistent results across these variations; however, this\naspect remains underexplored. We propose RobuNFR for evaluating the robustness\nof LLMs in NFR-aware code generation across four NFR dimensions: design,\nreadability, reliability, and performance, using three methodologies: prompt\nvariation, regression testing, and diverse workflows. Our experiments show that\nRobuNFR reveals robustness issues in the tested LLMs when considering NFRs in\ncode generation. Specifically, under prompt variation, including NFRs leads to\na decrease in Pass@1 by up to 39 percent and an increase in the standard\ndeviation from 0.48 to 2.48 compared to the baseline without NFRs (i.e.,\nFunction-Only). While incorporating NFRs generally improves overall NFR\nmetrics, it also results in higher prompt sensitivity. In regression settings,\nsome LLMs exhibit differences across versions, with improvements in one aspect\n(e.g., reduced code smells) often accompanied by regressions in another (e.g.,\ndecreased correctness), revealing inconsistencies that challenge their\nrobustness. When varying workflows, the tested LLMs show significantly\ndifferent NFR-aware code generation capabilities between two workflows: (1)\nintegrating NFRs and functional requirements into the initial prompt and (2)\nenhancing Function-Only-generated code with the same NFR.\n","date":"2025-03-28"}
{"id":"2503.22853","title":"Teaching LLMs Music Theory with In-Context Learning and Chain-of-Thought\n  Prompting: Pedagogical Strategies for Machines","abstract":"  This study evaluates the baseline capabilities of Large Language Models\n(LLMs) like ChatGPT, Claude, and Gemini to learn concepts in music theory\nthrough in-context learning and chain-of-thought prompting. Using carefully\ndesigned prompts (in-context learning) and step-by-step worked examples\n(chain-of-thought prompting), we explore how LLMs can be taught increasingly\ncomplex material and how pedagogical strategies for human learners translate to\neducating machines. Performance is evaluated using questions from an official\nCanadian Royal Conservatory of Music (RCM) Level 6 examination, which covers a\ncomprehensive range of topics, including interval and chord identification, key\ndetection, cadence classification, and metrical analysis. Additionally, we\nevaluate the suitability of various music encoding formats for these tasks\n(ABC, Humdrum, MEI, MusicXML). All experiments were run both with and without\ncontextual prompts. Results indicate that without context, ChatGPT with MEI\nperforms the best at 52%, while with context, Claude with MEI performs the best\nat 75%. Future work will further refine prompts and expand to cover more\nadvanced music theory concepts. This research contributes to the broader\nunderstanding of teaching LLMs and has applications for educators, students,\nand developers of AI music tools alike.\n","date":"2025-03-28"}
{"id":"2503.22856","title":"Generating Synthetic Oracle Datasets to Analyze Noise Impact: A Study on\n  Building Function Classification Using Tweets","abstract":"  Tweets provides valuable semantic context for earth observation tasks and\nserves as a complementary modality to remote sensing imagery. In building\nfunction classification (BFC), tweets are often collected using geographic\nheuristics and labeled via external databases, an inherently weakly supervised\nprocess that introduces both label noise and sentence level feature noise\n(e.g., irrelevant or uninformative tweets). While label noise has been widely\nstudied, the impact of sentence level feature noise remains underexplored,\nlargely due to the lack of clean benchmark datasets for controlled analysis. In\nthis work, we propose a method for generating a synthetic oracle dataset using\nLLM, designed to contain only tweets that are both correctly labeled and\nsemantically relevant to their associated buildings. This oracle dataset\nenables systematic investigation of noise impacts that are otherwise difficult\nto isolate in real-world data. To assess its utility, we compare model\nperformance using Naive Bayes and mBERT classifiers under three configurations:\nreal vs. synthetic training data, and cross-domain generalization. Results show\nthat noise in real tweets significantly degrades the contextual learning\ncapacity of mBERT, reducing its performance to that of a simple keyword-based\nmodel. In contrast, the clean synthetic dataset allows mBERT to learn\neffectively, outperforming Naive Bayes Bayes by a large margin. These findings\nhighlight that addressing feature noise is more critical than model complexity\nin this task. Our synthetic dataset offers a novel experimental environment for\nfuture noise injection studies and is publicly available on GitHub.\n","date":"2025-03-28"}
{"id":"2503.22862","title":"Zero-shot Domain Generalization of Foundational Models for 3D Medical\n  Image Segmentation: An Experimental Study","abstract":"  Domain shift, caused by variations in imaging modalities and acquisition\nprotocols, limits model generalization in medical image segmentation. While\nfoundation models (FMs) trained on diverse large-scale data hold promise for\nzero-shot generalization, their application to volumetric medical data remains\nunderexplored. In this study, we examine their ability towards domain\ngeneralization (DG), by conducting a comprehensive experimental study\nencompassing 6 medical segmentation FMs and 12 public datasets spanning\nmultiple modalities and anatomies. Our findings reveal the potential of\npromptable FMs in bridging the domain gap via smart prompting techniques.\nAdditionally, by probing into multiple facets of zero-shot DG, we offer\nvaluable insights into the viability of FMs for DG and identify promising\navenues for future research.\n","date":"2025-03-28"}
{"id":"2503.22869","title":"SIGHT: Single-Image Conditioned Generation of Hand Trajectories for\n  Hand-Object Interaction","abstract":"  We introduce a novel task of generating realistic and diverse 3D hand\ntrajectories given a single image of an object, which could be involved in a\nhand-object interaction scene or pictured by itself. When humans grasp an\nobject, appropriate trajectories naturally form in our minds to use it for\nspecific tasks. Hand-object interaction trajectory priors can greatly benefit\napplications in robotics, embodied AI, augmented reality and related fields.\nHowever, synthesizing realistic and appropriate hand trajectories given a\nsingle object or hand-object interaction image is a highly ambiguous task,\nrequiring to correctly identify the object of interest and possibly even the\ncorrect interaction among many possible alternatives. To tackle this\nchallenging problem, we propose the SIGHT-Fusion system, consisting of a\ncurated pipeline for extracting visual features of hand-object interaction\ndetails from egocentric videos involving object manipulation, and a\ndiffusion-based conditional motion generation model processing the extracted\nfeatures. We train our method given video data with corresponding hand\ntrajectory annotations, without supervision in the form of action labels. For\nthe evaluation, we establish benchmarks utilizing the first-person FPHAB and\nHOI4D datasets, testing our method against various baselines and using multiple\nmetrics. We also introduce task simulators for executing the generated hand\ntrajectories and reporting task success rates as an additional metric.\nExperiments show that our method generates more appropriate and realistic hand\ntrajectories than baselines and presents promising generalization capability on\nunseen objects. The accuracy of the generated hand trajectories is confirmed in\na physics simulation setting, showcasing the authenticity of the created\nsequences and their applicability in downstream uses.\n","date":"2025-03-28"}
{"id":"2503.22876","title":"VizFlyt: Perception-centric Pedagogical Framework For Autonomous Aerial\n  Robots","abstract":"  Autonomous aerial robots are becoming commonplace in our lives. Hands-on\naerial robotics courses are pivotal in training the next-generation workforce\nto meet the growing market demands. Such an efficient and compelling course\ndepends on a reliable testbed. In this paper, we present VizFlyt, an\nopen-source perception-centric Hardware-In-The-Loop (HITL) photorealistic\ntesting framework for aerial robotics courses. We utilize pose from an external\nlocalization system to hallucinate real-time and photorealistic visual sensors\nusing 3D Gaussian Splatting. This enables stress-free testing of autonomy\nalgorithms on aerial robots without the risk of crashing into obstacles. We\nachieve over 100Hz of system update rate. Lastly, we build upon our past\nexperiences of offering hands-on aerial robotics courses and propose a new\nopen-source and open-hardware curriculum based on VizFlyt for the future. We\ntest our framework on various course projects in real-world HITL experiments\nand present the results showing the efficacy of such a system and its large\npotential use cases. Code, datasets, hardware guides and demo videos are\navailable at https:\/\/pear.wpi.edu\/research\/vizflyt.html\n","date":"2025-03-28"}
{"id":"2503.22877","title":"Understanding Inequality of LLM Fact-Checking over Geographic Regions\n  with Agent and Retrieval models","abstract":"  Fact-checking is a potentially useful application of Large Language Models\n(LLMs) to combat the growing dissemination of disinformation. However, the\nperformance of LLMs varies across geographic regions. In this paper, we\nevaluate the factual accuracy of open and private models across a diverse set\nof regions and scenarios.\n  Using a dataset containing 600 fact-checked statements balanced across six\nglobal regions we examine three experimental setups of fact-checking a\nstatement: (1) when just the statement is available, (2) when an LLM-based\nagent with Wikipedia access is utilized, and (3) as a best case scenario when a\nRetrieval-Augmented Generation (RAG) system provided with the official fact\ncheck is employed. Our findings reveal that regardless of the scenario and LLM\nused, including GPT-4, Claude Sonnet, and LLaMA, statements from the Global\nNorth perform substantially better than those from the Global South.\nFurthermore, this gap is broadened for the more realistic case of a Wikipedia\nagent-based system, highlighting that overly general knowledge bases have a\nlimited ability to address region-specific nuances. These results underscore\nthe urgent need for better dataset balancing and robust retrieval strategies to\nenhance LLM fact-checking capabilities, particularly in geographically diverse\ncontexts.\n","date":"2025-03-28"}
{"id":"2503.22879","title":"Quamba2: A Robust and Scalable Post-training Quantization Framework for\n  Selective State Space Models","abstract":"  State Space Models (SSMs) are emerging as a compelling alternative to\nTransformers because of their consistent memory usage and high performance.\nDespite this, scaling up SSMs on cloud services or limited-resource devices is\nchallenging due to their storage requirements and computational power. To\novercome this, quantizing SSMs with low bit-width data formats can reduce model\nsize and benefit from hardware acceleration. As SSMs are prone to\nquantization-induced errors, recent efforts have focused on optimizing a\nparticular model or bit-width for efficiency without sacrificing performance.\nHowever, distinct bit-width configurations are essential for different\nscenarios, like W4A8 for boosting large-batch decoding speed, and W4A16 for\nenhancing generation speed in short prompt applications for a single user. To\nthis end, we present Quamba2, compatible with W8A8, W4A8, and W4A16 for both\nMamba1 and Mamba2 backbones, addressing the growing demand for SSM deployment\non various platforms. Based on the channel order preserving and activation\npersistence of SSMs, we propose an offline approach to quantize inputs of a\nlinear recurrence in 8-bit by sorting and clustering for input $x$, combined\nwith a per-state-group quantization for input-dependent parameters $B$ and $C$.\nTo ensure compute-invariance in the SSM output, we rearrange weights offline\naccording to the clustering sequence. The experiments show that Quamba2-8B\noutperforms several state-of-the-art SSM quantization methods and delivers\n1.3$\\times$ and 3$\\times$ speed-ups in the pre-filling and generation stages,\nrespectively, while offering 4$\\times$ memory reduction with only a $1.6\\%$\naverage accuracy drop. The evaluation on MMLU shows the generalizability and\nrobustness of our framework. The code and quantized models will be released at:\nhttps:\/\/github.com\/enyac-group\/Quamba.\n","date":"2025-03-28"}
{"id":"2503.22880","title":"The Marine Debris Forward-Looking Sonar Datasets","abstract":"  Sonar sensing is fundamental for underwater robotics, but limited by\ncapabilities of AI systems, which need large training datasets. Public data in\nsonar modalities is lacking. This paper presents the Marine Debris\nForward-Looking Sonar datasets, with three different settings (watertank,\nturntable, flooded quarry) increasing dataset diversity and multiple computer\nvision tasks: object classification, object detection, semantic segmentation,\npatch matching, and unsupervised learning. We provide full dataset description,\nbasic analysis and initial results for some tasks. We expect the research\ncommunity will benefit from this dataset, which is publicly available at\nhttps:\/\/doi.org\/10.5281\/zenodo.15101686\n","date":"2025-03-28"}
{"id":"2503.22881","title":"Pairwise Matching of Intermediate Representations for Fine-grained\n  Explainability","abstract":"  The differences between images belonging to fine-grained categories are often\nsubtle and highly localized, and existing explainability techniques for deep\nlearning models are often too diffuse to provide useful and interpretable\nexplanations. We propose a new explainability method (PAIR-X) that leverages\nboth intermediate model activations and backpropagated relevance scores to\ngenerate fine-grained, highly-localized pairwise visual explanations. We use\nanimal and building re-identification (re-ID) as a primary case study of our\nmethod, and we demonstrate qualitatively improved results over a diverse set of\nexplainability baselines on 35 public re-ID datasets. In interviews, animal\nre-ID experts were in unanimous agreement that PAIR-X was an improvement over\nexisting baselines for deep model explainability, and suggested that its\nvisualizations would be directly applicable to their work. We also propose a\nnovel quantitative evaluation metric for our method, and demonstrate that\nPAIR-X visualizations appear more plausible for correct image matches than\nincorrect ones even when the model similarity score for the pairs is the same.\nBy improving interpretability, PAIR-X enables humans to better distinguish\ncorrect and incorrect matches. Our code is available at:\nhttps:\/\/github.com\/pairx-explains\/pairx\n","date":"2025-03-28"}
{"id":"2503.22884","title":"AutoComPose: Automatic Generation of Pose Transition Descriptions for\n  Composed Pose Retrieval Using Multimodal LLMs","abstract":"  Composed pose retrieval (CPR) enables users to search for human poses by\nspecifying a reference pose and a transition description, but progress in this\nfield is hindered by the scarcity and inconsistency of annotated pose\ntransitions. Existing CPR datasets rely on costly human annotations or\nheuristic-based rule generation, both of which limit scalability and diversity.\nIn this work, we introduce AutoComPose, the first framework that leverages\nmultimodal large language models (MLLMs) to automatically generate rich and\nstructured pose transition descriptions. Our method enhances annotation quality\nby structuring transitions into fine-grained body part movements and\nintroducing mirrored\/swapped variations, while a cyclic consistency constraint\nensures logical coherence between forward and reverse transitions. To advance\nCPR research, we construct and release two dedicated benchmarks, AIST-CPR and\nPoseFixCPR, supplementing prior datasets with enhanced attributes. Extensive\nexperiments demonstrate that training retrieval models with AutoComPose yields\nsuperior performance over human-annotated and heuristic-based methods,\nsignificantly reducing annotation costs while improving retrieval quality. Our\nwork pioneers the automatic annotation of pose transitions, establishing a\nscalable foundation for future CPR research.\n","date":"2025-03-28"}
{"id":"2503.22886","title":"Task Tokens: A Flexible Approach to Adapting Behavior Foundation Models","abstract":"  Recent advancements in imitation learning have led to transformer-based\nbehavior foundation models (BFMs) that enable multi-modal, human-like control\nfor humanoid agents. While excelling at zero-shot generation of robust\nbehaviors, BFMs often require meticulous prompt engineering for specific tasks,\npotentially yielding suboptimal results. We introduce \"Task Tokens\", a method\nto effectively tailor BFMs to specific tasks while preserving their\nflexibility. Our approach leverages the transformer architecture of BFMs to\nlearn a new task-specific encoder through reinforcement learning, keeping the\noriginal BFM frozen. This allows incorporation of user-defined priors,\nbalancing reward design and prompt engineering. By training a task encoder to\nmap observations to tokens, used as additional BFM inputs, we guide performance\nimprovement while maintaining the model's diverse control characteristics. We\ndemonstrate Task Tokens' efficacy across various tasks, including\nout-of-distribution scenarios, and show their compatibility with other\nprompting modalities. Our results suggest that Task Tokens offer a promising\napproach for adapting BFMs to specific control tasks while retaining their\ngeneralization capabilities.\n","date":"2025-03-28"}
{"id":"2503.22890","title":"MedCL: Learning Consistent Anatomy Distribution for Scribble-supervised\n  Medical Image Segmentation","abstract":"  Curating large-scale fully annotated datasets is expensive, laborious, and\ncumbersome, especially for medical images. Several methods have been proposed\nin the literature that make use of weak annotations in the form of scribbles.\nHowever, these approaches require large amounts of scribble annotations, and\nare only applied to the segmentation of regular organs, which are often\nunavailable for the disease species that fall in the long-tailed distribution.\nMotivated by the fact that the medical labels have anatomy distribution priors,\nwe propose a scribble-supervised clustering-based framework, called MedCL, to\nlearn the inherent anatomy distribution of medical labels. Our approach\nconsists of two steps: i) Mix the features with intra- and inter-image mix\noperations, and ii) Perform feature clustering and regularize the anatomy\ndistribution at both local and global levels. Combined with a small amount of\nweak supervision, the proposed MedCL is able to segment both regular organs and\nchallenging irregular pathologies. We implement MedCL based on SAM and UNet\nbackbones, and evaluate the performance on three open datasets of regular\nstructure (MSCMRseg), multiple organs (BTCV) and irregular pathology (MyoPS).\nIt is shown that even with less scribble supervision, MedCL substantially\noutperforms the conventional segmentation methods. Our code is available at\nhttps:\/\/github.com\/BWGZK\/MedCL.\n","date":"2025-03-28"}
{"id":"2503.22900","title":"Learning Library Cell Representations in Vector Space","abstract":"  We propose Lib2Vec, a novel self-supervised framework to efficiently learn\nmeaningful vector representations of library cells, enabling ML models to\ncapture essential cell semantics. The framework comprises three key components:\n(1) an automated method for generating regularity tests to quantitatively\nevaluate how well cell representations reflect inter-cell relationships; (2) a\nself-supervised learning scheme that systematically extracts training data from\nLiberty files, removing the need for costly labeling; and (3) an\nattention-based model architecture that accommodates various pin counts and\nenables the creation of property-specific cell and arc embeddings. Experimental\nresults demonstrate that Lib2Vec effectively captures functional and electrical\nsimilarities. Moreover, linear algebraic operations on cell vectors reveal\nmeaningful relationships, such as vector(BUF) - vector(INV) + vector(NAND) ~\nvector(AND), showcasing the framework's nuanced representation capabilities.\nLib2Vec also enhances downstream circuit learning applications, especially when\nlabeled data is scarce.\n","date":"2025-03-28"}
{"id":"2503.22906","title":"SocialGen: Modeling Multi-Human Social Interaction with Language Models","abstract":"  Human interactions in everyday life are inherently social, involving\nengagements with diverse individuals across various contexts. Modeling these\nsocial interactions is fundamental to a wide range of real-world applications.\nIn this paper, we introduce SocialGen, the first unified motion-language model\ncapable of modeling interaction behaviors among varying numbers of individuals,\nto address this crucial yet challenging problem. Unlike prior methods that are\nlimited to two-person interactions, we propose a novel social motion\nrepresentation that supports tokenizing the motions of an arbitrary number of\nindividuals and aligning them with the language space. This alignment enables\nthe model to leverage rich, pretrained linguistic knowledge to better\nunderstand and reason about human social behaviors. To tackle the challenges of\ndata scarcity, we curate a comprehensive multi-human interaction dataset,\nSocialX, enriched with textual annotations. Leveraging this dataset, we\nestablish the first comprehensive benchmark for multi-human interaction tasks.\nOur method achieves state-of-the-art performance across motion-language tasks,\nsetting a new standard for multi-human interaction modeling.\n","date":"2025-03-28"}
{"id":"2503.22909","title":"Enhancing DeepLabV3+ to Fuse Aerial and Satellite Images for Semantic\n  Segmentation","abstract":"  Aerial and satellite imagery are inherently complementary remote sensing\nsources, offering high-resolution detail alongside expansive spatial coverage.\nHowever, the use of these sources for land cover segmentation introduces\nseveral challenges, prompting the development of a variety of segmentation\nmethods. Among these approaches, the DeepLabV3+ architecture is considered as a\npromising approach in the field of single-source image segmentation. However,\ndespite its reliable results for segmentation, there is still a need to\nincrease its robustness and improve its performance. This is particularly\ncrucial for multimodal image segmentation, where the fusion of diverse types of\ninformation is essential.\n  An interesting approach involves enhancing this architectural framework\nthrough the integration of novel components and the modification of certain\ninternal processes.\n  In this paper, we enhance the DeepLabV3+ architecture by introducing a new\ntransposed conventional layers block for upsampling a second entry to fuse it\nwith high level features. This block is designed to amplify and integrate\ninformation from satellite images, thereby enriching the segmentation process\nthrough fusion with aerial images.\n  For experiments, we used the LandCover.ai (Land Cover from Aerial Imagery)\ndataset for aerial images, alongside the corresponding dataset sourced from\nSentinel 2 data.\n  Through the fusion of both sources, the mean Intersection over Union (mIoU)\nachieved a total mIoU of 84.91% without data augmentation.\n","date":"2025-03-28"}
{"id":"2503.22912","title":"DIFFER: Disentangling Identity Features via Semantic Cues for\n  Clothes-Changing Person Re-ID","abstract":"  Clothes-changing person re-identification (CC-ReID) aims to recognize\nindividuals under different clothing scenarios. Current CC-ReID approaches\neither concentrate on modeling body shape using additional modalities including\nsilhouette, pose, and body mesh, potentially causing the model to overlook\nother critical biometric traits such as gender, age, and style, or they\nincorporate supervision through additional labels that the model tries to\ndisregard or emphasize, such as clothing or personal attributes. However, these\nannotations are discrete in nature and do not capture comprehensive\ndescriptions.\n  In this work, we propose DIFFER: Disentangle Identity Features From Entangled\nRepresentations, a novel adversarial learning method that leverages textual\ndescriptions to disentangle identity features. Recognizing that image features\ninherently mix inseparable information, DIFFER introduces NBDetach, a mechanism\ndesigned for feature disentanglement by leveraging the separable nature of text\ndescriptions as supervision. It partitions the feature space into distinct\nsubspaces and, through gradient reversal layers, effectively separates\nidentity-related features from non-biometric features. We evaluate DIFFER on 4\ndifferent benchmark datasets (LTCC, PRCC, CelebreID-Light, and CCVID) to\ndemonstrate its effectiveness and provide state-of-the-art performance across\nall the benchmarks. DIFFER consistently outperforms the baseline method, with\nimprovements in top-1 accuracy of 3.6% on LTCC, 3.4% on PRCC, 2.5% on\nCelebReID-Light, and 1% on CCVID. Our code can be found here.\n","date":"2025-03-28"}
{"id":"2503.22913","title":"Resona: Improving Context Copying in Linear Recurrence Models with\n  Retrieval","abstract":"  Recent shifts in the space of large language model (LLM) research have shown\nan increasing focus on novel architectures to compete with prototypical\nTransformer-based models that have long dominated this space. Linear recurrent\nmodels have proven to be a viable competitor due to their computational\nefficiency. However, such models still demonstrate a sizable gap compared to\nTransformers in terms of in-context learning among other tasks that require\nrecalling information from a context. In this work, we introduce __Resona__, a\nsimple and scalable framework for augmenting linear recurrent models with\nretrieval. __Resona__~augments models with the ability to integrate retrieved\ninformation from the provided input context, enabling tailored behavior to\ndiverse task requirements. Experiments on a variety of linear recurrent models\ndemonstrate that __Resona__-augmented models observe significant performance\ngains on a variety of synthetic as well as real-world natural language tasks,\nhighlighting its ability to act as a general purpose method to improve the\nin-context learning and language modeling abilities of linear recurrent LLMs.\n","date":"2025-03-28"}
{"id":"2503.22923","title":"Nested Stochastic Gradient Descent for (Generalized) Sinkhorn\n  Distance-Regularized Distributionally Robust Optimization","abstract":"  Distributionally robust optimization (DRO) is a powerful technique to train\nrobust models against data distribution shift. This paper aims to solve\nregularized nonconvex DRO problems, where the uncertainty set is modeled by a\nso-called generalized Sinkhorn distance and the loss function is nonconvex and\npossibly unbounded. Such a distance allows to model uncertainty of\ndistributions with different probability supports and divergence functions. For\nthis class of regularized DRO problems, we derive a novel dual formulation\ntaking the form of nested stochastic programming, where the dual variable\ndepends on the data sample. To solve the dual problem, we provide theoretical\nevidence to design a nested stochastic gradient descent (SGD) algorithm, which\nleverages stochastic approximation to estimate the nested stochastic gradients.\nWe study the convergence rate of nested SGD and establish polynomial iteration\nand sample complexities that are independent of the data size and parameter\ndimension, indicating its potential for solving large-scale DRO problems. We\nconduct numerical experiments to demonstrate the efficiency and robustness of\nthe proposed algorithm.\n","date":"2025-03-29"}
{"id":"2503.22925","title":"Predictive Traffic Rule Compliance using Reinforcement Learning","abstract":"  Autonomous vehicle path planning has reached a stage where safety and\nregulatory compliance are crucial. This paper presents a new approach that\nintegrates a motion planner with a deep reinforcement learning model to predict\npotential traffic rule violations. In this setup, the predictions of the critic\ndirectly affect the cost function of the motion planner, guiding the choices of\nthe trajectory. We incorporate key interstate rules from the German Road\nTraffic Regulation into a rule book and use a graph-based state representation\nto handle complex traffic information. Our main innovation is replacing the\nstandard actor network in an actor-critic setup with a motion planning module,\nwhich ensures both predictable trajectory generation and prevention of\nlong-term rule violations. Experiments on an open German highway dataset show\nthat the model can predict and prevent traffic rule violations beyond the\nplanning horizon, significantly increasing safety in challenging traffic\nconditions.\n","date":"2025-03-29"}
{"id":"2503.22929","title":"Unsupervised Feature Disentanglement and Augmentation Network for\n  One-class Face Anti-spoofing","abstract":"  Face anti-spoofing (FAS) techniques aim to enhance the security of facial\nidentity authentication by distinguishing authentic live faces from deceptive\nattempts. While two-class FAS methods risk overfitting to training attacks to\nachieve better performance, one-class FAS approaches handle unseen attacks well\nbut are less robust to domain information entangled within the liveness\nfeatures. To address this, we propose an Unsupervised Feature Disentanglement\nand Augmentation Network (\\textbf{UFDANet}), a one-class FAS technique that\nenhances generalizability by augmenting face images via disentangled features.\nThe \\textbf{UFDANet} employs a novel unsupervised feature disentangling method\nto separate the liveness and domain features, facilitating discriminative\nfeature learning. It integrates an out-of-distribution liveness feature\naugmentation scheme to synthesize new liveness features of unseen spoof\nclasses, which deviate from the live class, thus enhancing the representability\nand discriminability of liveness features. Additionally, \\textbf{UFDANet}\nincorporates a domain feature augmentation routine to synthesize unseen domain\nfeatures, thereby achieving better generalizability. Extensive experiments\ndemonstrate that the proposed \\textbf{UFDANet} outperforms previous one-class\nFAS methods and achieves comparable performance to state-of-the-art two-class\nFAS methods.\n","date":"2025-03-29"}
{"id":"2503.22931","title":"Factored Agents: Decoupling In-Context Learning and Memorization for\n  Robust Tool Use","abstract":"  In this paper, we propose a novel factored agent architecture designed to\novercome the limitations of traditional single-agent systems in agentic AI. Our\napproach decomposes the agent into two specialized components: (1) a large\nlanguage model (LLM) that serves as a high level planner and in-context\nlearner, which may use dynamically available information in user prompts, (2) a\nsmaller language model which acts as a memorizer of tool format and output.\nThis decoupling addresses prevalent issues in monolithic designs, including\nmalformed, missing, and hallucinated API fields, as well as suboptimal planning\nin dynamic environments. Empirical evaluations demonstrate that our factored\narchitecture significantly improves planning accuracy and error resilience,\nwhile elucidating the inherent trade-off between in-context learning and static\nmemorization. These findings suggest that a factored approach is a promising\npathway for developing more robust and adaptable agentic AI systems.\n","date":"2025-03-29"}
{"id":"2503.22932","title":"Bi-Level Multi-View fuzzy Clustering with Exponential Distance","abstract":"  In this study, we propose extension of fuzzy c-means (FCM) clustering in\nmulti-view environments. First, we introduce an exponential multi-view FCM\n(E-MVFCM). E-MVFCM is a centralized MVC with consideration to heat-kernel\ncoefficients (H-KC) and weight factors. Secondly, we propose an exponential\nbi-level multi-view fuzzy c-means clustering (EB-MVFCM). Different to E-MVFCM,\nEB-MVFCM does automatic computation of feature and weight factors\nsimultaneously. Like E-MVFCM, EB-MVFCM present explicit forms of the H-KC to\nsimplify the generation of the heat-kernel $\\mathcal{K}(t)$ in powers of the\nproper time $t$ during the clustering process. All the features used in this\nstudy, including tools and functions of proposed algorithms will be made\navailable at https:\/\/www.github.com\/KristinaP09\/EB-MVFCM.\n","date":"2025-03-29"}
{"id":"2503.22934","title":"FairSAM: Fair Classification on Corrupted Data Through Sharpness-Aware\n  Minimization","abstract":"  Image classification models trained on clean data often suffer from\nsignificant performance degradation when exposed to testing corrupted data,\nsuch as images with impulse noise, Gaussian noise, or environmental noise. This\ndegradation not only impacts overall performance but also disproportionately\naffects various demographic subgroups, raising critical algorithmic bias\nconcerns. Although robust learning algorithms like Sharpness-Aware Minimization\n(SAM) have shown promise in improving overall model robustness and\ngeneralization, they fall short in addressing the biased performance\ndegradation across demographic subgroups. Existing fairness-aware machine\nlearning methods - such as fairness constraints and reweighing strategies - aim\nto reduce performance disparities but hardly maintain robust and equitable\naccuracy across demographic subgroups when faced with data corruption. This\nreveals an inherent tension between robustness and fairness when dealing with\ncorrupted data. To address these challenges, we introduce one novel metric\nspecifically designed to assess performance degradation across subgroups under\ndata corruption. Additionally, we propose \\textbf{FairSAM}, a new framework\nthat integrates \\underline{Fair}ness-oriented strategies into \\underline{SAM}\nto deliver equalized performance across demographic groups under corrupted\nconditions. Our experiments on multiple real-world datasets and various\npredictive tasks show that FairSAM successfully reconciles robustness and\nfairness, offering a structured solution for equitable and resilient image\nclassification in the presence of data corruption.\n","date":"2025-03-29"}
{"id":"2503.22936","title":"Enhancing Learnable Descriptive Convolutional Vision Transformer for\n  Face Anti-Spoofing","abstract":"  Face anti-spoofing (FAS) heavily relies on identifying live\/spoof\ndiscriminative features to counter face presentation attacks. Recently, we\nproposed LDCformer to successfully incorporate the Learnable Descriptive\nConvolution (LDC) into ViT, to model long-range dependency of locally\ndescriptive features for FAS. In this paper, we propose three novel training\nstrategies to effectively enhance the training of LDCformer to largely boost\nits feature characterization capability. The first strategy, dual-attention\nsupervision, is developed to learn fine-grained liveness features guided by\nregional live\/spoof attentions. The second strategy, self-challenging\nsupervision, is designed to enhance the discriminability of the features by\ngenerating challenging training data. In addition, we propose a third training\nstrategy, transitional triplet mining strategy, through narrowing the\ncross-domain gap while maintaining the transitional relationship between live\nand spoof features, to enlarge the domain-generalization capability of\nLDCformer. Extensive experiments show that LDCformer under joint supervision of\nthe three novel training strategies outperforms previous methods.\n","date":"2025-03-29"}
{"id":"2503.22939","title":"Graph Kolmogorov-Arnold Networks for Multi-Cancer Classification and\n  Biomarker Identification, An Interpretable Multi-Omics Approach","abstract":"  The integration of multi-omics data presents a major challenge in precision\nmedicine, requiring advanced computational methods for accurate disease\nclassification and biological interpretation. This study introduces the\nMulti-Omics Graph Kolmogorov-Arnold Network (MOGKAN), a deep learning model\nthat integrates messenger RNA, micro RNA sequences, and DNA methylation data\nwith Protein-Protein Interaction (PPI) networks for accurate and interpretable\ncancer classification across 31 cancer types. MOGKAN employs a hybrid approach\ncombining differential expression with DESeq2, Linear Models for Microarray\n(LIMMA), and Least Absolute Shrinkage and Selection Operator (LASSO) regression\nto reduce multi-omics data dimensionality while preserving relevant biological\nfeatures. The model architecture is based on the Kolmogorov-Arnold theorem\nprinciple, using trainable univariate functions to enhance interpretability and\nfeature analysis. MOGKAN achieves classification accuracy of 96.28 percent and\ndemonstrates low experimental variability with a standard deviation that is\nreduced by 1.58 to 7.30 percents compared to Convolutional Neural Networks\n(CNNs) and Graph Neural Networks (GNNs). The biomarkers identified by MOGKAN\nhave been validated as cancer-related markers through Gene Ontology (GO) and\nKyoto Encyclopedia of Genes and Genomes (KEGG) enrichment analysis. The\nproposed model presents an ability to uncover molecular oncogenesis mechanisms\nby detecting phosphoinositide-binding substances and regulating sphingolipid\ncellular processes. By integrating multi-omics data with graph-based deep\nlearning, our proposed approach demonstrates superior predictive performance\nand interpretability that has the potential to enhance the translation of\ncomplex multi-omics data into clinically actionable cancer diagnostics.\n","date":"2025-03-29"}
{"id":"2503.22941","title":"Identifying Multi-modal Knowledge Neurons in Pretrained Transformers via\n  Two-stage Filtering","abstract":"  Recent advances in large language models (LLMs) have led to the development\nof multimodal LLMs (MLLMs) in the fields of natural language processing (NLP)\nand computer vision. Although these models allow for integrated visual and\nlanguage understanding, they present challenges such as opaque internal\nprocessing and the generation of hallucinations and misinformation. Therefore,\nthere is a need for a method to clarify the location of knowledge in MLLMs.\n  In this study, we propose a method to identify neurons associated with\nspecific knowledge using MiniGPT-4, a Transformer-based MLLM. Specifically, we\nextract knowledge neurons through two stages: activation differences filtering\nusing inpainting and gradient-based filtering using GradCAM. Experiments on the\nimage caption generation task using the MS COCO 2017 dataset, BLEU, ROUGE, and\nBERTScore quantitative evaluation, and qualitative evaluation using an\nactivation heatmap showed that our method is able to locate knowledge with\nhigher accuracy than existing methods.\n  This study contributes to the visualization and explainability of knowledge\nin MLLMs and shows the potential for future knowledge editing and control.\n","date":"2025-03-29"}
{"id":"2503.22942","title":"Adaptive Interactive Navigation of Quadruped Robots using Large Language\n  Models","abstract":"  Robotic navigation in complex environments remains a critical research\nchallenge. Traditional navigation methods focus on optimal trajectory\ngeneration within free space, struggling in environments lacking viable paths\nto the goal, such as disaster zones or cluttered warehouses. To address this\ngap, we propose an adaptive interactive navigation approach that proactively\ninteracts with environments to create feasible paths to reach originally\nunavailable goals. Specifically, we present a primitive tree for task planning\nwith large language models (LLMs), facilitating effective reasoning to\ndetermine interaction objects and sequences. To ensure robust subtask\nexecution, we adopt reinforcement learning to pre-train a comprehensive skill\nlibrary containing versatile locomotion and interaction behaviors for motion\nplanning. Furthermore, we introduce an adaptive replanning method featuring two\nLLM-based modules: an advisor serving as a flexible replanning trigger and an\narborist for autonomous plan adjustment. Integrated with the tree structure,\nthe replanning mechanism allows for convenient node addition and pruning,\nenabling rapid plan modification in unknown environments. Comprehensive\nsimulations and experiments have demonstrated our method's effectiveness and\nadaptivity in diverse scenarios. The supplementary video is available at page:\nhttps:\/\/youtu.be\/W5ttPnSap2g.\n","date":"2025-03-29"}
{"id":"2503.22943","title":"Towards Mobile Sensing with Event Cameras on High-mobility\n  Resource-constrained Devices: A Survey","abstract":"  With the increasing complexity of mobile device applications, these devices\nare evolving toward high mobility. This shift imposes new demands on mobile\nsensing, particularly in terms of achieving high accuracy and low latency.\nEvent-based vision has emerged as a disruptive paradigm, offering high temporal\nresolution, low latency, and energy efficiency, making it well-suited for\nhigh-accuracy and low-latency sensing tasks on high-mobility platforms.\nHowever, the presence of substantial noisy events, the lack of inherent\nsemantic information, and the large data volume pose significant challenges for\nevent-based data processing on resource-constrained mobile devices. This paper\nsurveys the literature over the period 2014-2024, provides a comprehensive\noverview of event-based mobile sensing systems, covering fundamental\nprinciples, event abstraction methods, algorithmic advancements, hardware and\nsoftware acceleration strategies. We also discuss key applications of event\ncameras in mobile sensing, including visual odometry, object tracking, optical\nflow estimation, and 3D reconstruction, while highlighting the challenges\nassociated with event data processing, sensor fusion, and real-time deployment.\nFurthermore, we outline future research directions, such as improving event\ncamera hardware with advanced optics, leveraging neuromorphic computing for\nefficient processing, and integrating bio-inspired algorithms to enhance\nperception. To support ongoing research, we provide an open-source\n\\textit{Online Sheet} with curated resources and recent developments. We hope\nthis survey serves as a valuable reference, facilitating the adoption of\nevent-based vision across diverse applications.\n","date":"2025-03-29"}
{"id":"2503.22946","title":"DATAWEAVER: Authoring Data-Driven Narratives through the Integrated\n  Composition of Visualization and Text","abstract":"  Data-driven storytelling has gained prominence in journalism and other data\nreporting fields. However, the process of creating these stories remains\nchallenging, often requiring the integration of effective visualizations with\ncompelling narratives to form a cohesive, interactive presentation. To help\nstreamline this process, we present an integrated authoring framework and\nsystem, DataWeaver, that supports both visualization-to-text and\ntext-to-visualization composition. DataWeaver enables users to create data\nnarratives anchored to data facts derived from \"call-out\" interactions, i.e.,\nuser-initiated highlights of visualization elements that prompt relevant\nnarrative content. In addition to this \"vis-to-text\" composition, DataWeaver\nalso supports a \"text-initiated\" approach, generating relevant interactive\nvisualizations from existing narratives. Key findings from an evaluation with\n13 participants highlighted the utility and usability of DataWeaver and the\neffectiveness of its integrated authoring framework. The evaluation also\nrevealed opportunities to enhance the framework by refining filtering\nmechanisms and visualization recommendations and better support authoring\ncreativity by introducing advanced customization options.\n","date":"2025-03-29"}
{"id":"2503.22948","title":"SUV: Scalable Large Language Model Copyright Compliance with Regularized\n  Selective Unlearning","abstract":"  Large Language Models (LLMs) have transformed natural language processing by\nlearning from massive datasets, yet this rapid progress has also drawn legal\nscrutiny, as the ability to unintentionally generate copyrighted content has\nalready prompted several prominent lawsuits. In this work, we introduce SUV\n(Selective Unlearning for Verbatim data), a selective unlearning framework\ndesigned to prevent LLM from memorizing copyrighted content while preserving\nits overall utility. In detail, the proposed method constructs a dataset that\ncaptures instances of copyrighted infringement cases by the targeted LLM. With\nthe dataset, we unlearn the content from the LLM by means of Direct Preference\nOptimization (DPO), which replaces the verbatim copyrighted content with\nplausible and coherent alternatives. Since DPO may hinder the LLM's performance\nin other unrelated tasks, we integrate gradient projection and Fisher\ninformation regularization to mitigate the degradation. We validate our\napproach using a large-scale dataset of 500 famous books (predominantly\ncopyrighted works) and demonstrate that SUV significantly reduces verbatim\nmemorization with negligible impact on the performance on unrelated tasks.\nExtensive experiments on both our dataset and public benchmarks confirm the\nscalability and efficacy of our approach, offering a promising solution for\nmitigating copyright risks in real-world LLM applications.\n","date":"2025-03-29"}
{"id":"2503.22952","title":"OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming\n  Video Contexts","abstract":"  The rapid advancement of multi-modal language models (MLLMs) like GPT-4o has\npropelled the development of Omni language models, designed to process and\nproactively respond to continuous streams of multi-modal data. Despite their\npotential, evaluating their real-world interactive capabilities in streaming\nvideo contexts remains a formidable challenge. In this work, we introduce\nOmniMMI, a comprehensive multi-modal interaction benchmark tailored for\nOmniLLMs in streaming video contexts. OmniMMI encompasses over 1,121 videos and\n2,290 questions, addressing two critical yet underexplored challenges in\nexisting video benchmarks: streaming video understanding and proactive\nreasoning, across six distinct subtasks. Moreover, we propose a novel\nframework, Multi-modal Multiplexing Modeling (M4), designed to enable an\ninference-efficient streaming model that can see, listen while generating.\n","date":"2025-03-29"}
{"id":"2503.22954","title":"Can LLMs Support Medical Knowledge Imputation? An Evaluation-Based\n  Perspective","abstract":"  Medical knowledge graphs (KGs) are essential for clinical decision support\nand biomedical research, yet they often exhibit incompleteness due to knowledge\ngaps and structural limitations in medical coding systems. This issue is\nparticularly evident in treatment mapping, where coding systems such as ICD,\nMondo, and ATC lack comprehensive coverage, resulting in missing or\ninconsistent associations between diseases and their potential treatments. To\naddress this issue, we have explored the use of Large Language Models (LLMs)\nfor imputing missing treatment relationships. Although LLMs offer promising\ncapabilities in knowledge augmentation, their application in medical knowledge\nimputation presents significant risks, including factual inaccuracies,\nhallucinated associations, and instability between and within LLMs. In this\nstudy, we systematically evaluate LLM-driven treatment mapping, assessing its\nreliability through benchmark comparisons. Our findings highlight critical\nlimitations, including inconsistencies with established clinical guidelines and\npotential risks to patient safety. This study serves as a cautionary guide for\nresearchers and practitioners, underscoring the importance of critical\nevaluation and hybrid approaches when leveraging LLMs to enhance treatment\nmappings on medical knowledge graphs.\n","date":"2025-03-29"}
{"id":"2503.22955","title":"MNT-TNN: Spatiotemporal Traffic Data Imputation via Compact Multimode\n  Nonlinear Transform-based Tensor Nuclear Norm","abstract":"  Imputation of random or non-random missing data is a long-standing research\ntopic and a crucial application for Intelligent Transportation Systems (ITS).\nHowever, with the advent of modern communication technologies such as Global\nSatellite Navigation Systems (GNSS), traffic data collection has outpaced\ntraditional methods, introducing new challenges in random missing value\nimputation and increasing demands for spatiotemporal dependency modelings. To\naddress these issues, we propose a novel spatiotemporal traffic imputation\nmethod, Multimode Nonlinear Transformed Tensor Nuclear Norm (MNT-TNN), grounded\nin the Transform-based Tensor Nuclear Norm (TTNN) optimization framework which\nexhibits efficient mathematical representations and theoretical guarantees for\nthe recovery of random missing values. Specifically, we strictly extend the\nsingle-mode transform in TTNN to a multimode transform with nonlinear\nactivation, effectively capturing the intrinsic multimode spatiotemporal\ncorrelations and low-rankness of the traffic tensor, represented as location\n$\\times$ location $\\times$ time. To solve the nonconvex optimization problem,\nwe design a proximal alternating minimization (PAM) algorithm with theoretical\nconvergence guarantees. We suggest an Augmented Transform-based Tensor Nuclear\nNorm Families (ATTNNs) framework to enhance the imputation results of TTNN\ntechniques, especially at very high miss rates. Extensive experiments on real\ndatasets demonstrate that our proposed MNT-TNN and ATTNNs can outperform the\ncompared state-of-the-art imputation methods, completing the benchmark of\nrandom missing traffic value imputation.\n","date":"2025-03-29"}
{"id":"2503.22958","title":"Late Breaking Results: Breaking Symmetry- Unconventional Placement of\n  Analog Circuits using Multi-Level Multi-Agent Reinforcement Learning","abstract":"  Layout-dependent effects (LDEs) significantly impact analog circuit\nperformance. Traditionally, designers have relied on symmetric placement of\ncircuit components to mitigate variations caused by LDEs. However, due to\nnon-linear nature of these effects, conventional methods often fall short. We\npropose an objective-driven, multi-level, multi-agent Q-learning framework to\nexplore unconventional design space of analog layout, opening new avenues for\noptimizing analog circuit performance. Our approach achieves better variation\nperformance than the state-of-the-art layout techniques. Notably, this is the\nfirst application of multi-agent RL in analog layout automation. The proposed\napproach is compared with non-ML approach based on simulated annealing.\n","date":"2025-03-29"}
{"id":"2503.22962","title":"Multimodal machine learning with large language embedding model for\n  polymer property prediction","abstract":"  Contemporary large language models (LLMs), such as GPT-4 and Llama, have\nharnessed extensive computational power and diverse text corpora to achieve\nremarkable proficiency in interpreting and generating domain-specific content,\nincluding materials science. To leverage the domain knowledge embedded within\nthese models, we propose a simple yet effective multimodal architecture,\nPolyLLMem, which integrates text embeddings generated by Llama 3 with molecular\nstructure embeddings derived from Uni-Mol, for polymer properties prediction\ntasks. In our model, Low-rank adaptation (LoRA) layers were also incorporated\nduring the property prediction tasks to refine the embeddings based on our\nlimited polymer dataset, thereby enhancing their chemical relevance for polymer\nSMILES representation. This balanced fusion of fine-tuned textual and\nstructural information enables PolyLLMem to accurately predict a variety of\npolymer properties despite the scarcity of training data. Its performance is\ncomparable to, and in some cases exceeds, that of graph-based models, as well\nas transformer-based models that typically require pretraining on millions of\npolymer samples. These findings demonstrate that LLM, such as Llama, can\neffectively capture chemical information encoded in polymer PSMILES, and\nunderscore the efficacy of multimodal fusion of LLM embeddings and molecular\nstructure embeddings in overcoming data scarcity and accelerating the discovery\nof advanced polymeric materials.\n","date":"2025-03-29"}
{"id":"2503.22963","title":"SuperEIO: Self-Supervised Event Feature Learning for Event Inertial\n  Odometry","abstract":"  Event cameras asynchronously output low-latency event streams, promising for\nstate estimation in high-speed motion and challenging lighting conditions. As\nopposed to frame-based cameras, the motion-dependent nature of event cameras\npresents persistent challenges in achieving robust event feature detection and\nmatching. In recent years, learning-based approaches have demonstrated superior\nrobustness over traditional handcrafted methods in feature detection and\nmatching, particularly under aggressive motion and HDR scenarios. In this\npaper, we propose SuperEIO, a novel framework that leverages the learning-based\nevent-only detection and IMU measurements to achieve event-inertial odometry.\nOur event-only feature detection employs a convolutional neural network under\ncontinuous event streams. Moreover, our system adopts the graph neural network\nto achieve event descriptor matching for loop closure. The proposed system\nutilizes TensorRT to accelerate the inference speed of deep networks, which\nensures low-latency processing and robust real-time operation on\nresource-limited platforms. Besides, we evaluate our method extensively on\nmultiple public datasets, demonstrating its superior accuracy and robustness\ncompared to other state-of-the-art event-based methods. We have also\nopen-sourced our pipeline to facilitate research in the field:\nhttps:\/\/github.com\/arclab-hku\/SuperEIO.\n","date":"2025-03-29"}
{"id":"2503.22965","title":"Pallet Detection And Localisation From Synthetic Data","abstract":"  The global warehousing industry is experiencing rapid growth, with the market\nsize projected to grow at an annual rate of 8.1% from 2024 to 2030 [Grand View\nResearch, 2021]. This expansion has led to a surge in demand for efficient\npallet detection and localisation systems. While automation can significantly\nstreamline warehouse operations, the development of such systems often requires\nextensive manual data annotation, with an average of 35 seconds per image, for\na typical computer vision project. This paper presents a novel approach to\nenhance pallet detection and localisation using purely synthetic data and\ngeometric features derived from their side faces. By implementing a domain\nrandomisation engine in Unity, the need for time-consuming manual annotation is\neliminated while achieving high-performance results. The proposed method\ndemonstrates a pallet detection performance of 0.995 mAP50 for single pallets\non a real-world dataset. Additionally, an average position accuracy of less\nthan 4.2 cm and an average rotation accuracy of 8.2{\\deg} were achieved for\npallets within a 5-meter range, with the pallet positioned head-on.\n","date":"2025-03-29"}
{"id":"2503.22967","title":"Student-Powered Digital Scholarship CoLab Project in the HKUST Library:\n  Develop a Chinese Named-Entity Recognition (NER) Tool within One Semester\n  from the Ground Up","abstract":"  Starting in February 2024, the HKUST Library further extended the scope of AI\nliteracy to AI utilization, which focuses on fostering student involvement in\nutilizing state-of-the-art technologies in the projects that initiated by the\nLibrary, named \"Digital Scholarship (DS) CoLab\". A key focus of the DS CoLab\nscheme has been on cultivating talents and enabling students to utilize\nadvanced technologies in practical context. It aims to reinforce the library's\nrole as a catalyst and hub for fostering multidisciplinary collaboration and\ncultivate the \"can do spirit\" among university members. The Library offers 1-2\nprojects per year for students to engage with advanced technologies in\npractical contexts while supporting the Library in tackling challenges and\nstreamlining operational tasks. The tool that introduced in this paper was\nmainly developed by two of the authors, Sherry Yip Sau Lai and Berry Han\nLiuruo, as part-time student helpers under one of our DS CoLab scheme in the\n2024 Spring Semester (February to May 2024). This paper details the complete\njourney from ideation to implementation of developing a Chinese Named-Entity\nRecognition (NER) Tool from the group up within one semester, from the initial\nresearch and planning stages to execution and come up a viable product. The\ncollaborative spirit fostered by this project, with students playing a central\nrole, exemplifies the power and potential of innovative educational models that\nprioritize hands-on learning with student involvement.\n","date":"2025-03-29"}
{"id":"2503.22968","title":"HRET: A Self-Evolving LLM Evaluation Toolkit for Korean","abstract":"  Recent advancements in Korean large language models (LLMs) have spurred\nnumerous benchmarks and evaluation methodologies, yet the lack of a\nstandardized evaluation framework has led to inconsistent results and limited\ncomparability. To address this, we introduce HRET Haerae Evaluation Toolkit, an\nopen-source, self-evolving evaluation framework tailored specifically for\nKorean LLMs. HRET unifies diverse evaluation methods, including logit-based\nscoring, exact-match, language-inconsistency penalization, and LLM-as-a-Judge\nassessments. Its modular, registry-based architecture integrates major\nbenchmarks (HAE-RAE Bench, KMMLU, KUDGE, HRM8K) and multiple inference backends\n(vLLM, HuggingFace, OpenAI-compatible endpoints). With automated pipelines for\ncontinuous evolution, HRET provides a robust foundation for reproducible, fair,\nand transparent Korean NLP research.\n","date":"2025-03-29"}
{"id":"2503.22971","title":"Enhancing Federated Learning Through Secure Cluster-Weighted Client\n  Aggregation","abstract":"  Federated learning (FL) has emerged as a promising paradigm in machine\nlearning, enabling collaborative model training across decentralized devices\nwithout the need for raw data sharing. In FL, a global model is trained\niteratively on local datasets residing on individual devices, each contributing\nto the model's improvement. However, the heterogeneous nature of these local\ndatasets, stemming from diverse user behaviours, device capabilities, and data\ndistributions, poses a significant challenge. The inherent heterogeneity in\nfederated learning gives rise to various issues, including model performance\ndiscrepancies, convergence challenges, and potential privacy concerns. As the\nglobal model progresses through rounds of training, the disparities in local\ndata quality and quantity can impede the overall effectiveness of federated\nlearning systems. Moreover, maintaining fairness and privacy across diverse\nuser groups becomes a paramount concern. To address this issue, this paper\nintroduces a novel FL framework, ClusterGuardFL, that employs dissimilarity\nscores, k-means clustering, and reconciliation confidence scores to dynamically\nassign weights to client updates. The dissimilarity scores between global and\nlocal models guide the formation of clusters, with cluster size influencing the\nweight allocation. Within each cluster, a reconciliation confidence score is\ncalculated for individual data points, and a softmax layer generates customized\nweights for clients. These weights are utilized in the aggregation process,\nenhancing the model's robustness and privacy. Experimental results demonstrate\nthe efficacy of the proposed approach in achieving improved model performance\nin diverse datasets.\n","date":"2025-03-29"}
{"id":"2503.22973","title":"XL-Instruct: Synthetic Data for Cross-Lingual Open-Ended Generation","abstract":"  Cross-lingual open-ended generation -- i.e. generating responses in a desired\nlanguage different from that of the user's query -- is an important yet\nunderstudied problem. We introduce XL-AlpacaEval, a new benchmark for\nevaluating cross-lingual generation capabilities in Large Language Models\n(LLMs), and propose XL-Instruct, a high-quality synthetic data generation\nmethod. Fine-tuning with just 8K XL-Instruct-generated instructions\nsignificantly improves model performance, increasing the win rate against\nGPT-4o-Mini from 7.4% to 21.5%, and improving on several fine-grained quality\nmetrics. Additionally, models fine-tuned on XL-Instruct exhibit strong\nzero-shot transfer to both English-only and multilingual generation tasks.\nGiven its consistent gains across the board, we strongly recommend\nincorporating XL-Instruct in the post-training pipeline of future multilingual\nLLMs. To facilitate further research, we will publicly and freely release the\nXL-Instruct and XL-AlpacaEval datasets, which constitute two of the few\ncross-lingual resources currently available in the literature.\n","date":"2025-03-29"}
{"id":"2503.22976","title":"From Flatland to Space: Teaching Vision-Language Models to Perceive and\n  Reason in 3D","abstract":"  Recent advances in LVLMs have improved vision-language understanding, but\nthey still struggle with spatial perception, limiting their ability to reason\nabout complex 3D scenes. Unlike previous approaches that incorporate 3D\nrepresentations into models to improve spatial understanding, we aim to unlock\nthe potential of VLMs by leveraging spatially relevant image data. To this end,\nwe introduce a novel 2D spatial data generation and annotation pipeline built\nupon scene data with 3D ground-truth. This pipeline enables the creation of a\ndiverse set of spatial tasks, ranging from basic perception tasks to more\ncomplex reasoning tasks. Leveraging this pipeline, we construct SPAR-7M, a\nlarge-scale dataset generated from thousands of scenes across multiple public\ndatasets. In addition, we introduce SPAR-Bench, a benchmark designed to offer a\nmore comprehensive evaluation of spatial capabilities compared to existing\nspatial benchmarks, supporting both single-view and multi-view inputs. Training\non both SPAR-7M and large-scale 2D datasets enables our models to achieve\nstate-of-the-art performance on 2D spatial benchmarks. Further fine-tuning on\n3D task-specific datasets yields competitive results, underscoring the\neffectiveness of our dataset in enhancing spatial reasoning.\n","date":"2025-03-29"}
{"id":"2503.22982","title":"PartialLoading: User Scheduling and Bandwidth Allocation for\n  Parameter-sharing Edge Inference","abstract":"  By provisioning inference offloading services, edge inference drives the\nrapid growth of AI applications at the network edge. However, achieving high\ntask throughput with stringent latency requirements remains a significant\nchallenge. To address this issue, we develop a parameter-sharing AI model\nloading (PartialLoading) framework for multi-user edge inference, which\nexploits two key insights: 1) the majority of latency arises from loading AI\nmodels into server GPU memory, and 2) different AI models can share a\nsignificant number of parameters, for which redundant loading should be\navoided. Towards this end, we formulate a joint multi-user scheduling and\nspectrum bandwidth allocation problem to maximize task throughput by exploiting\nshared parameter blocks across models. The intuition is to judiciously schedule\nuser requests to reuse the shared parameter blocks between consecutively loaded\nmodels, thereby reducing model loading time substantially. To facilitate\nsolution finding, we decouple the problem into two sub-problems, i.e., user\nscheduling and bandwidth allocation, showing that solving them sequentially is\nequivalent to solving the original problem. Due to the NP-hardness of the\nproblem, we first study an important special case called the\n\"bottom-layer-sharing\" case, where AI models share some bottom layers within\nclusters, and design a dynamic programming-based algorithm to obtain the\noptimal solution in polynomial time. For the general case, where shared\nparameter blocks appear at arbitrary positions within AI models, we propose a\ngreedy heuristic to obtain the sub-optimal solution efficiently. Simulation\nresults demonstrate that the proposed framework significantly improves task\nthroughput under deadline constraints compared with user scheduling without\nexploiting parameter sharing.\n","date":"2025-03-29"}
{"id":"2503.22983","title":"indiSplit: Bringing Severity Cognizance to Image Decomposition in\n  Fluorescence Microscopy","abstract":"  Fluorescence microscopy, while being a key driver for progress in the life\nsciences, is also subject to technical limitations. To overcome them,\ncomputational multiplexing techniques have recently been proposed, which allow\nmultiple cellular structures to be captured in a single image and later be\nunmixed. Existing image decomposition methods are trained on a set of\nsuperimposed input images and the respective unmixed target images. It is\ncritical to note that the relative strength (mixing ratio) of the superimposed\nimages for a given input is a priori unknown. However, existing methods are\ntrained on a fixed intensity ratio of superimposed inputs, making them not\ncognizant to the range of relative intensities that can occur in fluorescence\nmicroscopy. In this work, we propose a novel method called indiSplit that is\ncognizant of the severity of the above mentioned mixing ratio. Our idea is\nbased on InDI, a popular iterative method for image restoration, and an ideal\nstarting point to embrace the unknown mixing ratio in any given input. We\nintroduce (i) a suitably trained regressor network that predicts the\ndegradation level (mixing asymmetry) of a given input image and (ii) a\ndegradation-specific normalization module, enabling degradation-aware inference\nacross all mixing ratios. We show that this method solves two relevant tasks in\nfluorescence microscopy, namely image splitting and bleedthrough removal, and\nempirically demonstrate the applicability of indiSplit on $5$ public datasets.\nWe will release all sources under a permissive license.\n","date":"2025-03-29"}
{"id":"2503.22984","title":"Optimal Transport-Guided Source-Free Adaptation for Face Anti-Spoofing","abstract":"  Developing a face anti-spoofing model that meets the security requirements of\nclients worldwide is challenging due to the domain gap between training\ndatasets and diverse end-user test data. Moreover, for security and privacy\nreasons, it is undesirable for clients to share a large amount of their face\ndata with service providers. In this work, we introduce a novel method in which\nthe face anti-spoofing model can be adapted by the client itself to a target\ndomain at test time using only a small sample of data while keeping model\nparameters and training data inaccessible to the client. Specifically, we\ndevelop a prototype-based base model and an optimal transport-guided adaptor\nthat enables adaptation in either a lightweight training or training-free\nfashion, without updating base model's parameters. Furthermore, we propose\ngeodesic mixup, an optimal transport-based synthesis method that generates\naugmented training data along the geodesic path between source prototypes and\ntarget data distribution. This allows training a lightweight classifier to\neffectively adapt to target-specific characteristics while retaining essential\nknowledge learned from the source domain. In cross-domain and cross-attack\nsettings, compared with recent methods, our method achieves average relative\nimprovements of 19.17% in HTER and 8.58% in AUC, respectively.\n","date":"2025-03-29"}
{"id":"2503.22985","title":"FReM: A Flexible Reasoning Mechanism for Balancing Quick and Slow\n  Thinking in Long-Context Question Answering","abstract":"  Long-context question-answering (LCQA) systems have greatly benefited from\nthe powerful reasoning capabilities of large language models (LLMs), which can\nbe categorized into slow and quick reasoning modes. However, both modes have\ntheir limitations. Slow thinking generally leans to explore every possible\nreasoning path, which leads to heavy overthinking and wastes time. Quick\nthinking usually relies on pattern matching rather than truly understanding the\nquery logic, which misses proper understanding. To address these issues, we\npropose FReM: Flexible Reasoning Mechanism, a method that adjusts reasoning\ndepth according to the complexity of each question. Specifically, FReM\nleverages synthetic reference QA examples to provide an explicit chain of\nthought, enabling efficient handling of simple queries while allowing deeper\nreasoning for more complex ones. By doing so, FReM helps quick-thinking models\nmove beyond superficial pattern matching and narrows the reasoning space for\nslow-thinking models to avoid unnecessary exploration. Experiments on seven QA\ndatasets show that FReM improves reasoning accuracy and scalability,\nparticularly for complex multihop questions, indicating its potential to\nadvance LCQA methodologies.\n","date":"2025-03-29"}
{"id":"2503.22986","title":"FreeSplat++: Generalizable 3D Gaussian Splatting for Efficient Indoor\n  Scene Reconstruction","abstract":"  Recently, the integration of the efficient feed-forward scheme into 3D\nGaussian Splatting (3DGS) has been actively explored. However, most existing\nmethods focus on sparse view reconstruction of small regions and cannot produce\neligible whole-scene reconstruction results in terms of either quality or\nefficiency. In this paper, we propose FreeSplat++, which focuses on extending\nthe generalizable 3DGS to become an alternative approach to large-scale indoor\nwhole-scene reconstruction, which has the potential of significantly\naccelerating the reconstruction speed and improving the geometric accuracy. To\nfacilitate whole-scene reconstruction, we initially propose the Low-cost\nCross-View Aggregation framework to efficiently process extremely long input\nsequences. Subsequently, we introduce a carefully designed pixel-wise triplet\nfusion method to incrementally aggregate the overlapping 3D Gaussian primitives\nfrom multiple views, adaptively reducing their redundancy. Furthermore, we\npropose a weighted floater removal strategy that can effectively reduce\nfloaters, which serves as an explicit depth fusion approach that is crucial in\nwhole-scene reconstruction. After the feed-forward reconstruction of 3DGS\nprimitives, we investigate a depth-regularized per-scene fine-tuning process.\nLeveraging the dense, multi-view consistent depth maps obtained during the\nfeed-forward prediction phase for an extra constraint, we refine the entire\nscene's 3DGS primitive to enhance rendering quality while preserving geometric\naccuracy. Extensive experiments confirm that our FreeSplat++ significantly\noutperforms existing generalizable 3DGS methods, especially in whole-scene\nreconstructions. Compared to conventional per-scene optimized 3DGS approaches,\nour method with depth-regularized per-scene fine-tuning demonstrates\nsubstantial improvements in reconstruction accuracy and a notable reduction in\ntraining time.\n","date":"2025-03-29"}
{"id":"2503.22988","title":"DC-SGD: Differentially Private SGD with Dynamic Clipping through\n  Gradient Norm Distribution Estimation","abstract":"  Differentially Private Stochastic Gradient Descent (DP-SGD) is a widely\nadopted technique for privacy-preserving deep learning. A critical challenge in\nDP-SGD is selecting the optimal clipping threshold C, which involves balancing\nthe trade-off between clipping bias and noise magnitude, incurring substantial\nprivacy and computing overhead during hyperparameter tuning.\n  In this paper, we propose Dynamic Clipping DP-SGD (DC-SGD), a framework that\nleverages differentially private histograms to estimate gradient norm\ndistributions and dynamically adjust the clipping threshold C. Our framework\nincludes two novel mechanisms: DC-SGD-P and DC-SGD-E. DC-SGD-P adjusts the\nclipping threshold based on a percentile of gradient norms, while DC-SGD-E\nminimizes the expected squared error of gradients to optimize C. These dynamic\nadjustments significantly reduce the burden of hyperparameter tuning C. The\nextensive experiments on various deep learning tasks, including image\nclassification and natural language processing, show that our proposed dynamic\nalgorithms achieve up to 9 times acceleration on hyperparameter tuning than\nDP-SGD. And DC-SGD-E can achieve an accuracy improvement of 10.62% on CIFAR10\nthan DP-SGD under the same privacy budget of hyperparameter tuning. We conduct\nrigorous theoretical privacy and convergence analyses, showing that our methods\nseamlessly integrate with the Adam optimizer. Our results highlight the robust\nperformance and efficiency of DC-SGD, offering a practical solution for\ndifferentially private deep learning with reduced computational overhead and\nenhanced privacy guarantees.\n","date":"2025-03-29"}
{"id":"2503.22989","title":"FindTheFlaws: Annotated Errors for Detecting Flawed Reasoning and\n  Scalable Oversight Research","abstract":"  As AI models tackle increasingly complex problems, ensuring reliable human\noversight becomes more challenging due to the difficulty of verifying\nsolutions. Approaches to scaling AI supervision include debate, in which two\nagents engage in structured dialogue to help a judge evaluate claims; critique,\nin which models identify potential flaws in proposed solutions; and\nprover-verifier games, in which a capable 'prover' model generates solutions\nthat must be verifiable by a less capable 'verifier'. Evaluations of the\nscalability of these and similar approaches to difficult problems benefit from\ndatasets that include (1) long-form expert-verified correct solutions and (2)\nlong-form flawed solutions with annotations highlighting specific errors, but\nfew are available.\n  To address this gap, we present FindTheFlaws, a group of five diverse\ndatasets spanning medicine, mathematics, science, coding, and the Lojban\nlanguage. Each dataset contains questions and long-form solutions with expert\nannotations validating their correctness or identifying specific error(s) in\nthe reasoning. We evaluate frontier models' critiquing capabilities and observe\na range of performance that can be leveraged for scalable oversight\nexperiments: models performing more poorly on particular datasets can serve as\njudges\/verifiers for more capable models. Additionally, for some task\/dataset\ncombinations, expert baselines exceed even top model performance, making them\nmore beneficial for scalable oversight experiments.\n","date":"2025-03-29"}
{"id":"2503.22996","title":"Sparse Mixture of Experts as Unified Competitive Learning","abstract":"  Sparse Mixture of Experts (SMoE) improves the efficiency of large language\nmodel training by directing input tokens to a subset of experts. Despite its\nsuccess in generation tasks, its generalization ability remains an open\nquestion. In this paper, we demonstrate that current SMoEs, which fall into two\ncategories: (1) Token Choice ;and (2) Expert Choice, struggle with tasks such\nas the Massive Text Embedding Benchmark (MTEB). By analyzing their mechanism\nthrough the lens of competitive learning, our study finds that the Token Choice\napproach may overly focus on irrelevant experts, while the Expert Choice\napproach risks discarding important tokens, potentially affecting performance.\nMotivated by this analysis, we propose Unified Competitive Learning SMoE\n(USMoE), a novel and efficient framework designed to improve the performance of\nexisting SMoEs in both scenarios: with and without training. Extensive\nexperiments across various tasks show that USMoE achieves up to a 10%\nimprovement over traditional approaches or reduces computational inference\ncosts by 14% while maintaining strong performance.\n","date":"2025-03-29"}
{"id":"2503.22998","title":"AuditVotes: A Framework Towards More Deployable Certified Robustness for\n  Graph Neural Networks","abstract":"  Despite advancements in Graph Neural Networks (GNNs), adaptive attacks\ncontinue to challenge their robustness. Certified robustness based on\nrandomized smoothing has emerged as a promising solution, offering provable\nguarantees that a model's predictions remain stable under adversarial\nperturbations within a specified range. However, existing methods face a\ncritical trade-off between accuracy and robustness, as achieving stronger\nrobustness requires introducing greater noise into the input graph. This\nexcessive randomization degrades data quality and disrupts prediction\nconsistency, limiting the practical deployment of certifiably robust GNNs in\nreal-world scenarios where both accuracy and robustness are essential. To\naddress this challenge, we propose \\textbf{AuditVotes}, the first framework to\nachieve both high clean accuracy and certifiably robust accuracy for GNNs. It\nintegrates randomized smoothing with two key components,\n\\underline{au}gmentation and con\\underline{dit}ional smoothing, aiming to\nimprove data quality and prediction consistency. The augmentation, acting as a\npre-processing step, de-noises the randomized graph, significantly improving\ndata quality and clean accuracy. The conditional smoothing, serving as a\npost-processing step, employs a filtering function to selectively count votes,\nthereby filtering low-quality predictions and improving voting consistency.\nExtensive experimental results demonstrate that AuditVotes significantly\nenhances clean accuracy, certified robustness, and empirical robustness while\nmaintaining high computational efficiency. Notably, compared to baseline\nrandomized smoothing, AuditVotes improves clean accuracy by $437.1\\%$ and\ncertified accuracy by $409.3\\%$ when the attacker can arbitrarily insert $20$\nedges on the Cora-ML datasets, representing a substantial step toward deploying\ncertifiably robust GNNs in real-world applications.\n","date":"2025-03-29"}
{"id":"2503.23001","title":"Buyer-Initiated Auction Mechanism for Data Redemption in Machine\n  Unlearning","abstract":"  The rapid growth of artificial intelligence (AI) has raised privacy concerns\nover user data, leading to regulations like the General Data Protection\nRegulation (GDPR) and the California Consumer Privacy Act (CCPA). With the\nessential toolbox provided by machine unlearning, AI service providers are now\nable to remove user data from their trained models as well as the training\ndatasets, so as to comply with such regulations. However, extensive data\nredemption can be costly and degrade model accuracy. To balance the cost of\nunlearning and the privacy protection, we propose a buyer-initiated auction\nmechanism for data redemption, enabling the service provider to purchase data\nfrom willing users with appropriate compensation. This approach does not\nrequire the server to have any a priori knowledge about the users' privacy\npreference, and provides an efficient solution for maximizing the social\nwelfare in the investigated problem.\n","date":"2025-03-29"}
{"id":"2503.23002","title":"Learning Structure-enhanced Temporal Point Processes with\n  Gromov-Wasserstein Regularization","abstract":"  Real-world event sequences are often generated by different temporal point\nprocesses (TPPs) and thus have clustering structures. Nonetheless, in the\nmodeling and prediction of event sequences, most existing TPPs ignore the\ninherent clustering structures of the event sequences, leading to the models\nwith unsatisfactory interpretability. In this study, we learn\nstructure-enhanced TPPs with the help of Gromov-Wasserstein (GW)\nregularization, which imposes clustering structures on the sequence-level\nembeddings of the TPPs in the maximum likelihood estimation framework.In the\ntraining phase, the proposed method leverages a nonparametric TPP kernel to\nregularize the similarity matrix derived based on the sequence embeddings. In\nlarge-scale applications, we sample the kernel matrix and implement the\nregularization as a Gromov-Wasserstein (GW) discrepancy term, which achieves a\ntrade-off between regularity and computational efficiency.The TPPs learned\nthrough this method result in clustered sequence embeddings and demonstrate\ncompetitive predictive and clustering performance, significantly improving the\nmodel interpretability without compromising prediction accuracy.\n","date":"2025-03-29"}
{"id":"2503.23007","title":"S2MoE: Robust Sparse Mixture of Experts via Stochastic Learning","abstract":"  Sparse Mixture of Experts (SMoE) enables efficient training of large language\nmodels by routing input tokens to a select number of experts. However, training\nSMoE remains challenging due to the issue of representation collapse. Recent\nstudies have focused on improving the router to mitigate this problem, but\nexisting approaches face two key limitations: (1) expert embeddings are\nsignificantly smaller than the model's dimension, contributing to\nrepresentation collapse, and (2) routing each input to the Top-K experts can\ncause them to learn overly similar features. In this work, we propose a novel\napproach called Robust Sparse Mixture of Experts via Stochastic Learning\n(S2MoE), which is a mixture of experts designed to learn from both\ndeterministic and non-deterministic inputs via Learning under Uncertainty.\nExtensive experiments across various tasks demonstrate that S2MoE achieves\nperformance comparable to other routing methods while reducing computational\ninference costs by 28%.\n","date":"2025-03-29"}
{"id":"2503.23011","title":"On Geometrical Properties of Text Token Embeddings for Strong Semantic\n  Binding in Text-to-Image Generation","abstract":"  Text-to-Image (T2I) models often suffer from text-image misalignment in\ncomplex scenes involving multiple objects and attributes. Semantic binding aims\nto mitigate this issue by accurately associating the generated attributes and\nobjects with their corresponding noun phrases (NPs). Existing methods rely on\ntext or latent optimizations, yet the factors influencing semantic binding\nremain underexplored. Here we investigate the geometrical properties of text\ntoken embeddings and their cross-attention (CA) maps. We empirically and\ntheoretically analyze that the geometrical properties of token embeddings,\nspecifically both angular distances and norms, play a crucial role in CA map\ndifferentiation. Then, we propose \\textbf{TeeMo}, a training-free text\nembedding-aware T2I framework with strong semantic binding. TeeMo consists of\nCausality-Aware Projection-Out (CAPO) for distinct inter-NP CA maps and\nAdaptive Token Mixing (ATM) with our loss to enhance inter-NP separation while\nmaintaining intra-NP cohesion in CA maps. Extensive experiments confirm TeeMo\nconsistently outperforms prior arts across diverse baselines and datasets.\n","date":"2025-03-29"}
{"id":"2503.23012","title":"Multi-label classification for multi-temporal, multi-spatial coral reef\n  condition monitoring using vision foundation model with adapter learning","abstract":"  Coral reef ecosystems provide essential ecosystem services, but face\nsignificant threats from climate change and human activities. Although advances\nin deep learning have enabled automatic classification of coral reef\nconditions, conventional deep models struggle to achieve high performance when\nprocessing complex underwater ecological images. Vision foundation models,\nknown for their high accuracy and cross-domain generalizability, offer\npromising solutions. However, fine-tuning these models requires substantial\ncomputational resources and results in high carbon emissions. To address these\nchallenges, adapter learning methods such as Low-Rank Adaptation (LoRA) have\nemerged as a solution. This study introduces an approach integrating the DINOv2\nvision foundation model with the LoRA fine-tuning method. The approach\nleverages multi-temporal field images collected through underwater surveys at\n15 dive sites at Koh Tao, Thailand, with all images labeled according to\nuniversal standards used in citizen science-based conservation programs. The\nexperimental results demonstrate that the DINOv2-LoRA model achieved superior\naccuracy, with a match ratio of 64.77%, compared to 60.34% achieved by the best\nconventional model. Furthermore, incorporating LoRA reduced the trainable\nparameters from 1,100M to 5.91M. Transfer learning experiments conducted under\ndifferent temporal and spatial settings highlight the exceptional\ngeneralizability of DINOv2-LoRA across different seasons and sites. This study\nis the first to explore the efficient adaptation of foundation models for\nmulti-label classification of coral reef conditions under multi-temporal and\nmulti-spatial settings. The proposed method advances the classification of\ncoral reef conditions and provides a tool for monitoring, conserving, and\nmanaging coral reef ecosystems.\n","date":"2025-03-29"}
{"id":"2503.23014","title":"MSNGO: multi-species protein function annotation based on 3D protein\n  structure and network propagation","abstract":"  Motivation: In recent years, protein function prediction has broken through\nthe bottleneck of sequence features, significantly improving prediction\naccuracy using high-precision protein structures predicted by AlphaFold2. While\nsingle-species protein function prediction methods have achieved remarkable\nsuccess, multi-species protein function prediction methods are still in the\nstage of using PPI networks and sequence features. Providing effective\ncross-species label propagation for species with sparse protein annotations\nremains a challenging issue. To address this problem, we propose the MSNGO\nmodel, which integrates structural features and network propagation methods.\nOur validation shows that using structural features can significantly improve\nthe accuracy of multi-species protein function prediction. Results: We employ\ngraph representation learning techniques to extract amino acid representations\nfrom protein structure contact maps and train a structural model using a graph\nconvolution pooling module to derive protein-level structural features. After\nincorporating the sequence features from ESM-2, we apply a network propagation\nalgorithm to aggregate information and update node representations within a\nheterogeneous network. The results demonstrate that MSNGO outperforms previous\nmulti-species protein function prediction methods that rely on sequence\nfeatures and PPI networks. Availability: https:\/\/github.com\/blingbell\/MSNGO.\n","date":"2025-03-29"}
{"id":"2503.23015","title":"Engineering Microbial Symbiosis for Mars Habitability","abstract":"  The colonization of Mars presents extraordinary challenges, including\nradiation exposure, low atmospheric pressure, and toxic regolith. Recent\nadvancements in synthetic biology and genetic engineering offer unprecedented\nopportunities to address these obstacles by utilizing terrestrial extremophiles\nand engineered organisms. This paper examines the potential for creating\nsymbiotic relationships between terrestrial microbes and hypothetical Martian\nlife forms, should they exist, to support a sustainable human presence on Mars.\nInspired by natural examples of endosymbiosis, such as mitochondria and\nchloroplasts, we propose methods to engineer life forms capable of enduring\nMartian conditions. Key components include experimental designs, laboratory\nsimulations, and bioengineering approaches essential to this endeavor. The\nethical, political, and technological challenges of introducing engineered life\nto Mars are critically evaluated, with an emphasis on international\ncollaboration and robust planetary protection policies. This research\nunderscores engineered symbiosis as a transformative strategy for enabling life\nto adapt and thrive on Mars while advancing humanity's aspirations for\ninterplanetary habitation and exploration. By addressing these challenges, this\nwork highlights a path toward sustainable life on Mars, reflecting both\nscientific ingenuity and ethical stewardship.\n","date":"2025-03-29"}
{"id":"2503.23016","title":"Towards Understanding the Optimization Mechanisms in Deep Learning","abstract":"  In this paper, we adopt a probability distribution estimation perspective to\nexplore the optimization mechanisms of supervised classification using deep\nneural networks. We demonstrate that, when employing the Fenchel-Young loss,\ndespite the non-convex nature of the fitting error with respect to the model's\nparameters, global optimal solutions can be approximated by simultaneously\nminimizing both the gradient norm and the structural error. The former can be\ncontrolled through gradient descent algorithms. For the latter, we prove that\nit can be managed by increasing the number of parameters and ensuring parameter\nindependence, thereby providing theoretical insights into mechanisms such as\nover-parameterization and random initialization. Ultimately, the paper\nvalidates the key conclusions of the proposed method through empirical results,\nillustrating its practical effectiveness.\n","date":"2025-03-29"}
{"id":"2503.23021","title":"The impact of tissue detection on diagnostic artificial intelligence\n  algorithms in digital pathology","abstract":"  Tissue detection is a crucial first step in most digital pathology\napplications. Details of the segmentation algorithm are rarely reported, and\nthere is a lack of studies investigating the downstream effects of a poor\nsegmentation algorithm. Disregarding tissue detection quality could create a\nbottleneck for downstream performance and jeopardize patient safety if\ndiagnostically relevant parts of the specimen are excluded from analysis in\nclinical applications.\n  This study aims to determine whether performance of downstream tasks is\nsensitive to the tissue detection method, and to compare performance of\nclassical and AI-based tissue detection. To this end, we trained an AI model\nfor Gleason grading of prostate cancer in whole slide images (WSIs) using two\ndifferent tissue detection algorithms: thresholding (classical) and UNet++\n(AI). A total of 33,823 WSIs scanned on five digital pathology scanners were\nused to train the tissue detection AI model. The downstream Gleason grading\nalgorithm was trained and tested using 70,524 WSIs from 13 clinical sites\nscanned on 13 different scanners.\n  There was a decrease from 116 (0.43%) to 22 (0.08%) fully undetected tissue\nsamples when switching from thresholding-based tissue detection to AI-based,\nsuggesting an AI model may be more reliable than a classical model for avoiding\ntotal failures on slides with unusual appearance. On the slides where tissue\ncould be detected by both algorithms, no significant difference in overall\nGleason grading performance was observed. However, tissue detection dependent\nclinically significant variations in AI grading were observed in 3.5% of\nmalignant slides, highlighting the importance of robust tissue detection for\noptimal clinical performance of diagnostic AI.\n","date":"2025-03-29"}
{"id":"2503.23022","title":"MeshCraft: Exploring Efficient and Controllable Mesh Generation with\n  Flow-based DiTs","abstract":"  In the domain of 3D content creation, achieving optimal mesh topology through\nAI models has long been a pursuit for 3D artists. Previous methods, such as\nMeshGPT, have explored the generation of ready-to-use 3D objects via mesh\nauto-regressive techniques. While these methods produce visually impressive\nresults, their reliance on token-by-token predictions in the auto-regressive\nprocess leads to several significant limitations. These include extremely slow\ngeneration speeds and an uncontrollable number of mesh faces. In this paper, we\nintroduce MeshCraft, a novel framework for efficient and controllable mesh\ngeneration, which leverages continuous spatial diffusion to generate discrete\ntriangle faces. Specifically, MeshCraft consists of two core components: 1) a\ntransformer-based VAE that encodes raw meshes into continuous face-level tokens\nand decodes them back to the original meshes, and 2) a flow-based diffusion\ntransformer conditioned on the number of faces, enabling the generation of\nhigh-quality 3D meshes with a predefined number of faces. By utilizing the\ndiffusion model for the simultaneous generation of the entire mesh topology,\nMeshCraft achieves high-fidelity mesh generation at significantly faster speeds\ncompared to auto-regressive methods. Specifically, MeshCraft can generate an\n800-face mesh in just 3.2 seconds (35$\\times$ faster than existing baselines).\nExtensive experiments demonstrate that MeshCraft outperforms state-of-the-art\ntechniques in both qualitative and quantitative evaluations on ShapeNet dataset\nand demonstrates superior performance on Objaverse dataset. Moreover, it\nintegrates seamlessly with existing conditional guidance strategies, showcasing\nits potential to relieve artists from the time-consuming manual work involved\nin mesh creation.\n","date":"2025-03-29"}
{"id":"2503.23024","title":"Empowering Large Language Models with 3D Situation Awareness","abstract":"  Driven by the great success of Large Language Models (LLMs) in the 2D image\ndomain, their applications in 3D scene understanding has emerged as a new\ntrend. A key difference between 3D and 2D is that the situation of an\negocentric observer in 3D scenes can change, resulting in different\ndescriptions (e.g., ''left\" or ''right\"). However, current LLM-based methods\noverlook the egocentric perspective and simply use datasets from a global\nviewpoint. To address this issue, we propose a novel approach to automatically\ngenerate a situation-aware dataset by leveraging the scanning trajectory during\ndata collection and utilizing Vision-Language Models (VLMs) to produce\nhigh-quality captions and question-answer pairs. Furthermore, we introduce a\nsituation grounding module to explicitly predict the position and orientation\nof observer's viewpoint, thereby enabling LLMs to ground situation description\nin 3D scenes. We evaluate our approach on several benchmarks, demonstrating\nthat our method effectively enhances the 3D situational awareness of LLMs while\nsignificantly expanding existing datasets and reducing manual effort.\n","date":"2025-03-29"}
{"id":"2503.23029","title":"A Retrieval-Augmented Knowledge Mining Method with Deep Thinking LLMs\n  for Biomedical Research and Clinical Support","abstract":"  Knowledge graphs and large language models (LLMs) are key tools for\nbiomedical knowledge integration and reasoning, facilitating structured\norganization of scientific articles and discovery of complex semantic\nrelationships. However, current methods face challenges: knowledge graph\nconstruction is limited by complex terminology, data heterogeneity, and rapid\nknowledge evolution, while LLMs show limitations in retrieval and reasoning,\nmaking it difficult to uncover cross-document associations and reasoning\npathways. To address these issues, we propose a pipeline that uses LLMs to\nconstruct a biomedical knowledge graph (BioStrataKG) from large-scale articles\nand builds a cross-document question-answering dataset (BioCDQA) to evaluate\nlatent knowledge retrieval and multi-hop reasoning. We then introduce\nIntegrated and Progressive Retrieval-Augmented Reasoning (IP-RAR) to enhance\nretrieval accuracy and knowledge reasoning. IP-RAR maximizes information recall\nthrough Integrated Reasoning-based Retrieval and refines knowledge via\nProgressive Reasoning-based Generation, using self-reflection to achieve deep\nthinking and precise contextual understanding. Experiments show that IP-RAR\nimproves document retrieval F1 score by 20\\% and answer generation accuracy by\n25\\% over existing methods. This framework helps doctors efficiently integrate\ntreatment evidence for personalized medication plans and enables researchers to\nanalyze advancements and research gaps, accelerating scientific discovery and\ndecision-making.\n","date":"2025-03-29"}
{"id":"2503.23030","title":"Visual and Semantic Prompt Collaboration for Generalized Zero-Shot\n  Learning","abstract":"  Generalized zero-shot learning aims to recognize both seen and unseen classes\nwith the help of semantic information that is shared among different classes.\nIt inevitably requires consistent visual-semantic alignment. Existing\napproaches fine-tune the visual backbone by seen-class data to obtain\nsemantic-related visual features, which may cause overfitting on seen classes\nwith a limited number of training images. This paper proposes a novel visual\nand semantic prompt collaboration framework, which utilizes prompt tuning\ntechniques for efficient feature adaptation. Specifically, we design a visual\nprompt to integrate the visual information for discriminative feature learning\nand a semantic prompt to integrate the semantic formation for visualsemantic\nalignment. To achieve effective prompt information integration, we further\ndesign a weak prompt fusion mechanism for the shallow layers and a strong\nprompt fusion mechanism for the deep layers in the network. Through the\ncollaboration of visual and semantic prompts, we can obtain discriminative\nsemantic-related features for generalized zero-shot image recognition.\nExtensive experiments demonstrate that our framework consistently achieves\nfavorable performance in both conventional zero-shot learning and generalized\nzero-shot learning benchmarks compared to other state-of-the-art methods.\n","date":"2025-03-29"}
{"id":"2503.23032","title":"Reproducibility Companion Paper: Making Users Indistinguishable:\n  Attribute-wise Unlearning in Recommender Systems","abstract":"  In this paper, we reproduce the experimental results presented in our\nprevious work titled \"Making Users Indistinguishable: Attribute-wise Unlearning\nin Recommender Systems,\" which was published in the proceedings of the 31st ACM\nInternational Conference on Multimedia. This paper aims to validate the\neffectiveness of our proposed method and help others reproduce our experimental\nresults. We provide detailed descriptions of our preprocessed datasets, source\ncode structure, configuration file settings, experimental environment, and\nreproduced experimental results.\n","date":"2025-03-29"}
{"id":"2503.23035","title":"FreeInv: Free Lunch for Improving DDIM Inversion","abstract":"  Naive DDIM inversion process usually suffers from a trajectory deviation\nissue, i.e., the latent trajectory during reconstruction deviates from the one\nduring inversion. To alleviate this issue, previous methods either learn to\nmitigate the deviation or design cumbersome compensation strategy to reduce the\nmismatch error, exhibiting substantial time and computation cost. In this work,\nwe present a nearly free-lunch method (named FreeInv) to address the issue more\neffectively and efficiently. In FreeInv, we randomly transform the latent\nrepresentation and keep the transformation the same between the corresponding\ninversion and reconstruction time-step. It is motivated from a statistical\nperspective that an ensemble of DDIM inversion processes for multiple\ntrajectories yields a smaller trajectory mismatch error on expectation.\nMoreover, through theoretical analysis and empirical study, we show that\nFreeInv performs an efficient ensemble of multiple trajectories. FreeInv can be\nfreely integrated into existing inversion-based image and video editing\ntechniques. Especially for inverting video sequences, it brings more\nsignificant fidelity and efficiency improvements. Comprehensive quantitative\nand qualitative evaluation on PIE benchmark and DAVIS dataset shows that\nFreeInv remarkably outperforms conventional DDIM inversion, and is competitive\namong previous state-of-the-art inversion methods, with superior computation\nefficiency.\n","date":"2025-03-29"}
{"id":"2503.23037","title":"Agentic Large Language Models, a survey","abstract":"  There is great interest in agentic LLMs, large language models that act as\nagents. We review the growing body of work in this area and provide a research\nagenda. Agentic LLMs are LLMs that (1) reason, (2) act, and (3) interact. We\norganize the literature according to these three categories. The research in\nthe first category focuses on reasoning, reflection, and retrieval, aiming to\nimprove decision making; the second category focuses on action models, robots,\nand tools, aiming for agents that act as useful assistants; the third category\nfocuses on multi-agent systems, aiming for collaborative task solving and\nsimulating interaction to study emergent social behavior. We find that works\nmutually benefit from results in other categories: retrieval enables tool use,\nreflection improves multi-agent collaboration, and reasoning benefits all\ncategories. We discuss applications of agentic LLMs and provide an agenda for\nfurther research. Important applications are in medical diagnosis, logistics\nand financial market analysis. Meanwhile, self-reflective agents playing roles\nand interacting with one another augment the process of scientific research\nitself. Further, agentic LLMs may provide a solution for the problem of LLMs\nrunning out of training data: inference-time behavior generates new training\nstates, such that LLMs can keep learning without needing ever larger datasets.\nWe note that there is risk associated with LLM assistants taking action in the\nreal world, while agentic LLMs are also likely to benefit society.\n","date":"2025-03-29"}
{"id":"2503.23038","title":"Function Fitting Based on Kolmogorov-Arnold Theorem and Kernel Functions","abstract":"  This paper proposes a unified theoretical framework based on the\nKolmogorov-Arnold representation theorem and kernel methods. By analyzing the\nmathematical relationship among kernels, B-spline basis functions in\nKolmogorov-Arnold Networks (KANs) and the inner product operation in\nself-attention mechanisms, we establish a kernel-based feature fitting\nframework that unifies the two models as linear combinations of kernel\nfunctions. Under this framework, we propose a low-rank Pseudo-Multi-Head\nSelf-Attention module (Pseudo-MHSA), which reduces the parameter count of\ntraditional MHSA by nearly 50\\%. Furthermore, we design a Gaussian kernel\nmulti-head self-attention variant (Gaussian-MHSA) to validate the effectiveness\nof nonlinear kernel functions in feature extraction. Experiments on the\nCIFAR-10 dataset demonstrate that Pseudo-MHSA model achieves performance\ncomparable to the ViT model of the same dimensionality under the MAE framework\nand visualization analysis reveals their similarity of multi-head distribution\npatterns. Our code is publicly available.\n","date":"2025-03-29"}
{"id":"2503.23039","title":"STSA: Spatial-Temporal Semantic Alignment for Visual Dubbing","abstract":"  Existing audio-driven visual dubbing methods have achieved great success.\nDespite this, we observe that the semantic ambiguity between spatial and\ntemporal domains significantly degrades the synthesis stability for the dynamic\nfaces. We argue that aligning the semantic features from spatial and temporal\ndomains is a promising approach to stabilizing facial motion. To achieve this,\nwe propose a Spatial-Temporal Semantic Alignment (STSA) method, which\nintroduces a dual-path alignment mechanism and a differentiable semantic\nrepresentation. The former leverages a Consistent Information Learning (CIL)\nmodule to maximize the mutual information at multiple scales, thereby reducing\nthe manifold differences between spatial and temporal domains. The latter\nutilizes probabilistic heatmap as ambiguity-tolerant guidance to avoid the\nabnormal dynamics of the synthesized faces caused by slight semantic jittering.\nExtensive experimental results demonstrate the superiority of the proposed\nSTSA, especially in terms of image quality and synthesis stability. Pre-trained\nweights and inference code are available at\nhttps:\/\/github.com\/SCAILab-USTC\/STSA.\n","date":"2025-03-29"}
{"id":"2503.23042","title":"MIL vs. Aggregation: Evaluating Patient-Level Survival Prediction\n  Strategies Using Graph-Based Learning","abstract":"  Oncologists often rely on a multitude of data, including whole-slide images\n(WSIs), to guide therapeutic decisions, aiming for the best patient outcome.\nHowever, predicting the prognosis of cancer patients can be a challenging task\ndue to tumor heterogeneity and intra-patient variability, and the complexity of\nanalyzing WSIs. These images are extremely large, containing billions of\npixels, making direct processing computationally expensive and requiring\nspecialized methods to extract relevant information. Additionally, multiple\nWSIs from the same patient may capture different tumor regions, some being more\ninformative than others. This raises a fundamental question: Should we use all\nWSIs to characterize the patient, or should we identify the most representative\nslide for prognosis? Our work seeks to answer this question by performing a\ncomparison of various strategies for predicting survival at the WSI and patient\nlevel. The former treats each WSI as an independent sample, mimicking the\nstrategy adopted in other works, while the latter comprises methods to either\naggregate the predictions of the several WSIs or automatically identify the\nmost relevant slide using multiple-instance learning (MIL). Additionally, we\nevaluate different Graph Neural Networks architectures under these strategies.\nWe conduct our experiments using the MMIST-ccRCC dataset, which comprises\npatients with clear cell renal cell carcinoma (ccRCC). Our results show that\nMIL-based selection improves accuracy, suggesting that choosing the most\nrepresentative slide benefits survival prediction.\n","date":"2025-03-29"}
{"id":"2503.23044","title":"CityGS-X: A Scalable Architecture for Efficient and Geometrically\n  Accurate Large-Scale Scene Reconstruction","abstract":"  Despite its significant achievements in large-scale scene reconstruction, 3D\nGaussian Splatting still faces substantial challenges, including slow\nprocessing, high computational costs, and limited geometric accuracy. These\ncore issues arise from its inherently unstructured design and the absence of\nefficient parallelization. To overcome these challenges simultaneously, we\nintroduce CityGS-X, a scalable architecture built on a novel parallelized\nhybrid hierarchical 3D representation (PH^2-3D). As an early attempt, CityGS-X\nabandons the cumbersome merge-and-partition process and instead adopts a\nnewly-designed batch-level multi-task rendering process. This architecture\nenables efficient multi-GPU rendering through dynamic Level-of-Detail voxel\nallocations, significantly improving scalability and performance. Through\nextensive experiments, CityGS-X consistently outperforms existing methods in\nterms of faster training times, larger rendering capacities, and more accurate\ngeometric details in large-scale scenes. Notably, CityGS-X can train and render\na scene with 5,000+ images in just 5 hours using only 4 * 4090 GPUs, a task\nthat would make other alternative methods encounter Out-Of-Memory (OOM) issues\nand fail completely. This implies that CityGS-X is far beyond the capacity of\nother existing methods.\n","date":"2025-03-29"}
{"id":"2503.23046","title":"VLM-C4L: Continual Core Dataset Learning with Corner Case Optimization\n  via Vision-Language Models for Autonomous Driving","abstract":"  With the widespread adoption and deployment of autonomous driving, handling\ncomplex environments has become an unavoidable challenge. Due to the scarcity\nand diversity of extreme scenario datasets, current autonomous driving models\nstruggle to effectively manage corner cases. This limitation poses a\nsignificant safety risk, according to the National Highway Traffic Safety\nAdministration (NHTSA), autonomous vehicle systems have been involved in\nhundreds of reported crashes annually in the United States, occurred in corner\ncases like sun glare and fog, which caused a few fatal accident. Furthermore,\nin order to consistently maintain a robust and reliable autonomous driving\nsystem, it is essential for models not only to perform well on routine\nscenarios but also to adapt to newly emerging scenarios, especially those\ncorner cases that deviate from the norm. This requires a learning mechanism\nthat incrementally integrates new knowledge without degrading previously\nacquired capabilities. However, to the best of our knowledge, no existing\ncontinual learning methods have been proposed to ensure consistent and scalable\ncorner case learning in autonomous driving. To address these limitations, we\npropose VLM-C4L, a continual learning framework that introduces Vision-Language\nModels (VLMs) to dynamically optimize and enhance corner case datasets, and\nVLM-C4L combines VLM-guided high-quality data extraction with a core data\nreplay strategy, enabling the model to incrementally learn from diverse corner\ncases while preserving performance on previously routine scenarios, thus\nensuring long-term stability and adaptability in real-world autonomous driving.\nWe evaluate VLM-C4L on large-scale real-world autonomous driving datasets,\nincluding Waymo and the corner case dataset CODA.\n","date":"2025-03-29"}
{"id":"2503.23050","title":"Prediction of 30-day hospital readmission with clinical notes and EHR\n  information","abstract":"  High hospital readmission rates are associated with significant costs and\nhealth risks for patients. Therefore, it is critical to develop predictive\nmodels that can support clinicians to determine whether or not a patient will\nreturn to the hospital in a relatively short period of time (e.g, 30-days).\nNowadays, it is possible to collect both structured (electronic health records\n- EHR) and unstructured information (clinical notes) about a patient hospital\nevent, all potentially containing relevant information for a predictive model.\nHowever, their integration is challenging. In this work we explore the\ncombination of clinical notes and EHRs to predict 30-day hospital readmissions.\nWe address the representation of the various types of information available in\nthe EHR data, as well as exploring LLMs to characterize the clinical notes. We\ncollect both information sources as the nodes of a graph neural network (GNN).\nOur model achieves an AUROC of 0.72 and a balanced accuracy of 66.7\\%,\nhighlighting the importance of combining the multimodal information.\n","date":"2025-03-29"}
{"id":"2503.23053","title":"A Training-free LLM Framework with Interaction between Contextually\n  Related Subtasks in Solving Complex Tasks","abstract":"  Large language models (LLMs) have shown remarkable capabilities in solving\ncomplex tasks. Recent work has explored decomposing such tasks into subtasks\nwith independent contexts. However, some contextually related subtasks may\nencounter information loss during execution, leading to redundant operations or\nexecution failures. To address this issue, we propose a training-free framework\nwith an interaction mechanism, which enables a subtask to query specific\ninformation or trigger certain actions in completed subtasks by sending\nrequests. To implement interaction, we introduce a subtask trajectory memory to\nenable resumption of completed subtasks upon receiving interaction requests.\nAdditionally, we propose a new action during execution, which generates a\nconcise and precise description of execution process and outcomes of a subtask,\nto assist subsequent subtasks in determining interaction targets and requests.\nWe evaluate our framework on interactive decision-making task WebShop and\nmulti-hop question answering HotpotQA, with GPT-3.5 and GPT-4, and comparison\nresults show that our framework outperforms the state-of-the-art training-free\nbaselines.\n","date":"2025-03-29"}
{"id":"2503.23060","title":"Unsupervised Anomaly Detection in Multivariate Time Series across\n  Heterogeneous Domains","abstract":"  The widespread adoption of digital services, along with the scale and\ncomplexity at which they operate, has made incidents in IT operations\nincreasingly more likely, diverse, and impactful. This has led to the rapid\ndevelopment of a central aspect of \"Artificial Intelligence for IT Operations\"\n(AIOps), focusing on detecting anomalies in vast amounts of multivariate time\nseries data generated by service entities. In this paper, we begin by\nintroducing a unifying framework for benchmarking unsupervised anomaly\ndetection (AD) methods, and highlight the problem of shifts in normal behaviors\nthat can occur in practical AIOps scenarios. To tackle anomaly detection under\ndomain shift, we then cast the problem in the framework of domain\ngeneralization and propose a novel approach, Domain-Invariant VAE for Anomaly\nDetection (DIVAD), to learn domain-invariant representations for unsupervised\nanomaly detection. Our evaluation results using the Exathlon benchmark show\nthat the two main DIVAD variants significantly outperform the best unsupervised\nAD method in maximum performance, with 20% and 15% improvements in maximum peak\nF1-scores, respectively. Evaluation using the Application Server Dataset\nfurther demonstrates the broader applicability of our domain generalization\nmethods.\n","date":"2025-03-29"}
{"id":"2503.23062","title":"Shape and Texture Recognition in Large Vision-Language Models","abstract":"  Shape and texture recognition is fundamental to visual perception. The\nability to identify shapes regardless of orientation, texture, or context, and\nto recognize textures independently of their associated objects, is essential\nfor general visual understanding of the world. We introduce the Large Shape &\nTextures dataset (LAS&T), a giant collection of diverse shapes and textures\nautomatically extracted from real-world images. This dataset is used to\nevaluate how effectively leading Large Vision-Language Models (LVLMs)\nunderstand shapes, textures, and materials in both 2D and 3D scenes. For shape\nrecognition, we test models' ability to match identical shapes that differ in\norientation, texture, color, or environment. Our results show that LVLMs' shape\nidentification capabilities remain significantly below human performance.\nSingle alterations (orientation, texture) cause minor decreases in matching\naccuracy, while multiple changes precipitate dramatic drops. LVLMs appear to\nrely predominantly on high-level and semantic features and struggle with\nabstract shapes lacking clear class associations. For texture and material\nrecognition, we evaluate models' ability to identify identical textures and\nmaterials across different objects and environments. Interestingly, leading\nLVLMs approach human-level performance in recognizing materials in 3D scenes,\nyet substantially underperform humans when identifying simpler 2D textures. The\nLAS&T dataset and benchmark, the largest and most diverse resource for shape\nand texture evaluation, is freely available with generation and testing\nscripts.\n","date":"2025-03-29"}
{"id":"2503.23064","title":"VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large\n  Vision-Language Models","abstract":"  Large Vision-Language Models (LVLMs) struggle with puzzles, which require\nprecise perception, rule comprehension, and logical reasoning. Assessing and\nenhancing their performance in this domain is crucial, as it reflects their\nability to engage in structured reasoning - an essential skill for real-world\nproblem-solving. However, existing benchmarks primarily evaluate pre-trained\nmodels without additional training or fine-tuning, often lack a dedicated focus\non reasoning, and fail to establish a systematic evaluation framework. To\naddress these limitations, we introduce VGRP-Bench, a Visual Grid Reasoning\nPuzzle Benchmark featuring 20 diverse puzzles. VGRP-Bench spans multiple\ndifficulty levels, and includes extensive experiments not only on existing chat\nLVLMs (e.g., GPT-4o), but also on reasoning LVLMs (e.g., Gemini-Thinking). Our\nresults reveal that even the state-of-the-art LVLMs struggle with these\npuzzles, highlighting fundamental limitations in their puzzle-solving\ncapabilities. Most importantly, through systematic experiments, we identify and\nanalyze key factors influencing LVLMs' puzzle-solving performance, including\nthe number of clues, grid size, and rule complexity. Furthermore, we explore\ntwo Supervised Fine-Tuning (SFT) strategies that can be used in post-training:\nSFT on solutions (S-SFT) and SFT on synthetic reasoning processes (R-SFT).\nWhile both methods significantly improve performance on trained puzzles, they\nexhibit limited generalization to unseen ones. We will release VGRP-Bench to\nfacilitate further research on LVLMs for complex, real-world problem-solving.\nProject page: https:\/\/yufan-ren.com\/subpage\/VGRP-Bench\/.\n","date":"2025-03-29"}
{"id":"2503.23072","title":"TRACE: Intra-visit Clinical Event Nowcasting via Effective Patient\n  Trajectory Encoding","abstract":"  Electronic Health Records (EHR) have become a valuable resource for a wide\nrange of predictive tasks in healthcare. However, existing approaches have\nlargely focused on inter-visit event predictions, overlooking the importance of\nintra-visit nowcasting, which provides prompt clinical insights during an\nongoing patient visit. To address this gap, we introduce the task of laboratory\nmeasurement prediction within a hospital visit. We study the laboratory data\nthat, however, remained underexplored in previous work. We propose TRACE, a\nTransformer-based model designed for clinical event nowcasting by encoding\npatient trajectories. TRACE effectively handles long sequences and captures\ntemporal dependencies through a novel timestamp embedding that integrates decay\nproperties and periodic patterns of data. Additionally, we introduce a smoothed\nmask for denoising, improving the robustness of the model. Experiments on two\nlarge-scale electronic health record datasets demonstrate that the proposed\nmodel significantly outperforms previous methods, highlighting its potential\nfor improving patient care through more accurate laboratory measurement\nnowcasting. The code is available at https:\/\/github.com\/Amehi\/TRACE.\n","date":"2025-03-29"}
{"id":"2503.23076","title":"Concorde: Fast and Accurate CPU Performance Modeling with Compositional\n  Analytical-ML Fusion","abstract":"  Cycle-level simulators such as gem5 are widely used in microarchitecture\ndesign, but they are prohibitively slow for large-scale design space\nexplorations. We present Concorde, a new methodology for learning fast and\naccurate performance models of microarchitectures. Unlike existing simulators\nand learning approaches that emulate each instruction, Concorde predicts the\nbehavior of a program based on compact performance distributions that capture\nthe impact of different microarchitectural components. It derives these\nperformance distributions using simple analytical models that estimate bounds\non performance induced by each microarchitectural component, providing a simple\nyet rich representation of a program's performance characteristics across a\nlarge space of microarchitectural parameters. Experiments show that Concorde is\nmore than five orders of magnitude faster than a reference cycle-level\nsimulator, with about 2% average Cycles-Per-Instruction (CPI) prediction error\nacross a range of SPEC, open-source, and proprietary benchmarks. This enables\nrapid design-space exploration and performance sensitivity analyses that are\ncurrently infeasible, e.g., in about an hour, we conducted a first-of-its-kind\nfine-grained performance attribution to different microarchitectural components\nacross a diverse set of programs, requiring nearly 150 million CPI evaluations.\n","date":"2025-03-29"}
{"id":"2503.23077","title":"Efficient Inference for Large Reasoning Models: A Survey","abstract":"  Large Reasoning Models (LRMs) significantly improve the reasoning ability of\nLarge Language Models (LLMs) by learning to reason, exhibiting promising\nperformance in complex task-solving. However, their deliberative reasoning\nprocess leads to inefficiencies in token usage, memory consumption, and\ninference time. Thus, this survey provides a review of efficient inference\nmethods designed specifically for LRMs, focusing on mitigating token\ninefficiency while preserving the reasoning quality. First, we introduce a\ntaxonomy to group the recent methods into two main categories: (a) explicit\ncompact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit\nreasoning structure, and (b) implicit latent CoT, which encodes reasoning steps\nwithin hidden representations instead of explicit tokens. Meanwhile, we discuss\ntheir strengths and weaknesses. Then, we conduct empirical analyses on existing\nmethods from performance and efficiency aspects. Besides, we present open\nchallenges in this field, including human-centric controllable reasoning,\ntrade-off between interpretability and efficiency of reasoning, ensuring safety\nof efficient reasoning, and broader applications of efficient reasoning. In\naddition, we highlight key insights for enhancing LRMs' inference efficiency\nvia techniques such as model merging, new architectures, and agent routers. We\nhope this work serves as a valuable guide, helping researchers overcome\nchallenges in this vibrant\nfield\\footnote{https:\/\/github.com\/yueliu1999\/Awesome-Efficient-Inference-for-LRMs}.\n","date":"2025-03-29"}
{"id":"2503.23078","title":"EventWeave: A Dynamic Framework for Capturing Core and Supporting Events\n  in Dialogue Systems","abstract":"  Existing large language models (LLMs) have shown remarkable progress in\ndialogue systems. However, many approaches still overlook the fundamental role\nof events throughout multi-turn interactions, leading to \\textbf{incomplete\ncontext tracking}. Without tracking these events, dialogue systems often lose\ncoherence and miss subtle shifts in user intent, causing disjointed responses.\nTo bridge this gap, we present \\textbf{EventWeave}, an event-centric framework\nthat identifies and updates both core and supporting events as the conversation\nunfolds. Specifically, we organize these events into a dynamic event graph,\nwhich represents the interplay between \\textbf{core events} that shape the\nprimary idea and \\textbf{supporting events} that provide critical context\nduring the whole dialogue. By leveraging this dynamic graph, EventWeave helps\nmodels focus on the most relevant events when generating responses, thus\navoiding repeated visits of the entire dialogue history. Experimental results\non two benchmark datasets show that EventWeave improves response quality and\nevent relevance without fine-tuning.\n","date":"2025-03-29"}
{"id":"2503.23081","title":"InkFM: A Foundational Model for Full-Page Online Handwritten Note\n  Understanding","abstract":"  Tablets and styluses are increasingly popular for taking notes. To optimize\nthis experience and ensure a smooth and efficient workflow, it's important to\ndevelop methods for accurately interpreting and understanding the content of\nhandwritten digital notes. We introduce a foundational model called InkFM for\nanalyzing full pages of handwritten content. Trained on a diverse mixture of\ntasks, this model offers a unique combination of capabilities: recognizing text\nin 28 different scripts, mathematical expressions recognition, and segmenting\npages into distinct elements like text and drawings. Our results demonstrate\nthat these tasks can be effectively unified within a single model, achieving\nSoTA text line segmentation out-of-the-box quality surpassing public baselines\nlike docTR. Fine- or LoRA-tuning our base model on public datasets further\nimproves the quality of page segmentation, achieves state-of the art text\nrecognition (DeepWriting, CASIA, SCUT, and Mathwriting datasets) and sketch\nclassification (QuickDraw). This adaptability of InkFM provides a powerful\nstarting point for developing applications with handwritten input.\n","date":"2025-03-29"}
{"id":"2503.23083","title":"Efficient Adaptation For Remote Sensing Visual Grounding","abstract":"  Foundation models have revolutionized artificial intelligence (AI), offering\nremarkable capabilities across multi-modal domains. Their ability to precisely\nlocate objects in complex aerial and satellite images, using rich contextual\ninformation and detailed object descriptions, is essential for remote sensing\n(RS). These models can associate textual descriptions with object positions\nthrough the Visual Grounding (VG) task, but due to domain-specific challenges,\ntheir direct application to RS produces sub-optimal results. To address this,\nwe applied Parameter Efficient Fine Tuning (PEFT) techniques to adapt these\nmodels for RS-specific VG tasks. Specifically, we evaluated LoRA placement\nacross different modules in Grounding DINO and used BitFit and adapters to\nfine-tune the OFA foundation model pre-trained on general-purpose VG datasets.\nThis approach achieved performance comparable to or surpassing current State Of\nThe Art (SOTA) models while significantly reducing computational costs. This\nstudy highlights the potential of PEFT techniques to advance efficient and\nprecise multi-modal analysis in RS, offering a practical and cost-effective\nalternative to full model training.\n","date":"2025-03-29"}
{"id":"2503.23084","title":"The Reasoning-Memorization Interplay in Language Models Is Mediated by a\n  Single Direction","abstract":"  Large language models (LLMs) excel on a variety of reasoning benchmarks, but\nprevious studies suggest they sometimes struggle to generalize to unseen\nquestions, potentially due to over-reliance on memorized training examples.\nHowever, the precise conditions under which LLMs switch between reasoning and\nmemorization during text generation remain unclear. In this work, we provide a\nmechanistic understanding of LLMs' reasoning-memorization dynamics by\nidentifying a set of linear features in the model's residual stream that govern\nthe balance between genuine reasoning and memory recall. These features not\nonly distinguish reasoning tasks from memory-intensive ones but can also be\nmanipulated to causally influence model performance on reasoning tasks.\nAdditionally, we show that intervening in these reasoning features helps the\nmodel more accurately activate the most relevant problem-solving capabilities\nduring answer generation. Our findings offer new insights into the underlying\nmechanisms of reasoning and memory in LLMs and pave the way for the development\nof more robust and interpretable generative AI systems.\n","date":"2025-03-29"}
{"id":"2503.23088","title":"UNITYAI-GUARD: Pioneering Toxicity Detection Across Low-Resource Indian\n  Languages","abstract":"  This work introduces UnityAI-Guard, a framework for binary toxicity\nclassification targeting low-resource Indian languages. While existing systems\npredominantly cater to high-resource languages, UnityAI-Guard addresses this\ncritical gap by developing state-of-the-art models for identifying toxic\ncontent across diverse Brahmic\/Indic scripts. Our approach achieves an\nimpressive average F1-score of 84.23% across seven languages, leveraging a\ndataset of 888k training instances and 35k manually verified test instances. By\nadvancing multilingual content moderation for linguistically diverse regions,\nUnityAI-Guard also provides public API access to foster broader adoption and\napplication.\n","date":"2025-03-29"}
{"id":"2503.23091","title":"Parsing Through Boundaries in Chinese Word Segmentation","abstract":"  Chinese word segmentation is a foundational task in natural language\nprocessing (NLP), with far-reaching effects on syntactic analysis. Unlike\nalphabetic languages like English, Chinese lacks explicit word boundaries,\nmaking segmentation both necessary and inherently ambiguous. This study\nhighlights the intricate relationship between word segmentation and syntactic\nparsing, providing a clearer understanding of how different segmentation\nstrategies shape dependency structures in Chinese. Focusing on the Chinese GSD\ntreebank, we analyze multiple word boundary schemes, each reflecting distinct\nlinguistic and computational assumptions, and examine how they influence the\nresulting syntactic structures. To support detailed comparison, we introduce an\ninteractive web-based visualization tool that displays parsing outcomes across\nsegmentation methods.\n","date":"2025-03-29"}
{"id":"2503.23094","title":"FRAME: Floor-aligned Representation for Avatar Motion from Egocentric\n  Video","abstract":"  Egocentric motion capture with a head-mounted body-facing stereo camera is\ncrucial for VR and AR applications but presents significant challenges such as\nheavy occlusions and limited annotated real-world data. Existing methods rely\non synthetic pretraining and struggle to generate smooth and accurate\npredictions in real-world settings, particularly for lower limbs. Our work\naddresses these limitations by introducing a lightweight VR-based data\ncollection setup with on-board, real-time 6D pose tracking. Using this setup,\nwe collected the most extensive real-world dataset for ego-facing ego-mounted\ncameras to date in size and motion variability. Effectively integrating this\nmultimodal input -- device pose and camera feeds -- is challenging due to the\ndiffering characteristics of each data source. To address this, we propose\nFRAME, a simple yet effective architecture that combines device pose and camera\nfeeds for state-of-the-art body pose prediction through geometrically sound\nmultimodal integration and can run at 300 FPS on modern hardware. Lastly, we\nshowcase a novel training strategy to enhance the model's generalization\ncapabilities. Our approach exploits the problem's geometric properties,\nyielding high-quality motion capture free from common artifacts in prior works.\nQualitative and quantitative evaluations, along with extensive comparisons,\ndemonstrate the effectiveness of our method. Data, code, and CAD designs will\nbe available at https:\/\/vcai.mpi-inf.mpg.de\/projects\/FRAME\/\n","date":"2025-03-29"}
{"id":"2503.23095","title":"Memory-Aware and Uncertainty-Guided Retrieval for Multi-Hop Question\n  Answering","abstract":"  Multi-hop question answering (QA) requires models to retrieve and reason over\nmultiple pieces of evidence. While Retrieval-Augmented Generation (RAG) has\nmade progress in this area, existing methods often suffer from two key\nlimitations: (1) fixed or overly frequent retrieval steps, and (2) ineffective\nuse of previously retrieved knowledge.\n  We propose MIND (Memory-Informed and INteractive Dynamic RAG), a framework\nthat addresses these challenges through: (i) prompt-based entity extraction to\nidentify reasoning-relevant elements, (ii) dynamic retrieval triggering based\non token-level entropy and attention signals, and (iii) memory-aware filtering,\nwhich stores high-confidence facts across reasoning steps to enable consistent\nmulti-hop generation.\n","date":"2025-03-29"}
{"id":"2503.23100","title":"Beyond Standard MoE: Mixture of Latent Experts for Resource-Efficient\n  Language Models","abstract":"  Mixture of Experts (MoE) has emerged as a pivotal architectural paradigm for\nefficient scaling of Large Language Models (LLMs), operating through selective\nactivation of parameter subsets for each input token. Nevertheless,\nconventional MoE architectures encounter substantial challenges, including\nexcessive memory utilization and communication overhead during training and\ninference, primarily attributable to the proliferation of expert modules. In\nthis paper, we introduce Mixture of Latent Experts (MoLE), a novel\nparameterization methodology that facilitates the mapping of specific experts\ninto a shared latent space. Specifically, all expert operations are\nsystematically decomposed into two principal components: a shared projection\ninto a lower-dimensional latent space, followed by expert-specific\ntransformations with significantly reduced parametric complexity. This\nfactorized approach substantially diminishes parameter count and computational\nrequirements. Beyond the pretraining implementation of the MoLE architecture,\nwe also establish a rigorous mathematical framework for transforming\npre-trained MoE models into the MoLE architecture, characterizing the\nsufficient conditions for optimal factorization and developing a systematic\ntwo-phase algorithm for this conversion process. Our comprehensive theoretical\nanalysis demonstrates that MoLE significantly enhances computational efficiency\nacross multiple dimensions while preserving model representational capacity.\nEmpirical evaluations corroborate our theoretical findings, confirming that\nMoLE achieves performance comparable to standard MoE implementations while\nsubstantially reducing resource requirements.\n","date":"2025-03-29"}
{"id":"2503.23101","title":"RL2Grid: Benchmarking Reinforcement Learning in Power Grid Operations","abstract":"  Reinforcement learning (RL) can transform power grid operations by providing\nadaptive and scalable controllers essential for grid decarbonization. However,\nexisting methods struggle with the complex dynamics, aleatoric uncertainty,\nlong-horizon goals, and hard physical constraints that occur in real-world\nsystems. This paper presents RL2Grid, a benchmark designed in collaboration\nwith power system operators to accelerate progress in grid control and foster\nRL maturity. Built on a power simulation framework developed by RTE France,\nRL2Grid standardizes tasks, state and action spaces, and reward structures\nwithin a unified interface for a systematic evaluation and comparison of RL\napproaches. Moreover, we integrate real control heuristics and safety\nconstraints informed by the operators' expertise to ensure RL2Grid aligns with\ngrid operation requirements. We benchmark popular RL baselines on the grid\ncontrol tasks represented within RL2Grid, establishing reference performance\nmetrics. Our results and discussion highlight the challenges that power grids\npose for RL methods, emphasizing the need for novel algorithms capable of\nhandling real-world physical systems.\n","date":"2025-03-29"}
{"id":"2503.23102","title":"The geomagnetic storm and Kp prediction using Wasserstein transformer","abstract":"  The accurate forecasting of geomagnetic activity is important. In this work,\nwe present a novel multimodal Transformer based framework for predicting the 3\ndays and 5 days planetary Kp index by integrating heterogeneous data sources,\nincluding satellite measurements, solar images, and KP time series. A key\ninnovation is the incorporation of the Wasserstein distance into the\ntransformer and the loss function to align the probability distributions across\nmodalities. Comparative experiments with the NOAA model demonstrate\nperformance, accurately capturing both the quiet and storm phases of\ngeomagnetic activity. This study underscores the potential of integrating\nmachine learning techniques with traditional models for improved real time\nforecasting.\n","date":"2025-03-29"}
{"id":"2503.23104","title":"Fast Training of Recurrent Neural Networks with Stationary State\n  Feedbacks","abstract":"  Recurrent neural networks (RNNs) have recently demonstrated strong\nperformance and faster inference than Transformers at comparable parameter\nbudgets. However, the recursive gradient computation with the backpropagation\nthrough time (or BPTT) algorithm remains the major computational bottleneck. In\nthis work, we propose a novel method that replaces BPTT with a fixed gradient\nfeedback mechanism, yielding an efficient approximation of the exact gradient\npropagation based on the assumption of time stationarity. Our approach\nleverages state-space model (SSM) principles to define a structured feedback\nmatrix that directly propagates gradients from future time steps. This\nformulation bypasses the need for recursive gradient backpropagation,\nsignificantly reducing training overhead while preserving the network's ability\nto capture long-term dependencies. The experiments on language modeling\nbenchmarks exhibit competitive perplexity scores, while significantly reducing\nthe training costs. These promising results suggest that designing a feedback\nmethod like an SSM can fully exploit the efficiency advantages of RNNs for many\npractical applications.\n","date":"2025-03-29"}
{"id":"2503.23105","title":"Open-Vocabulary Semantic Segmentation with Uncertainty Alignment for\n  Robotic Scene Understanding in Indoor Building Environments","abstract":"  The global rise in the number of people with physical disabilities, in part\ndue to improvements in post-trauma survivorship and longevity, has amplified\nthe demand for advanced assistive technologies to improve mobility and\nindependence. Autonomous assistive robots, such as smart wheelchairs, require\nrobust capabilities in spatial segmentation and semantic recognition to\nnavigate complex built environments effectively. Place segmentation involves\ndelineating spatial regions like rooms or functional areas, while semantic\nrecognition assigns semantic labels to these regions, enabling accurate\nlocalization to user-specific needs. Existing approaches often utilize deep\nlearning; however, these close-vocabulary detection systems struggle to\ninterpret intuitive and casual human instructions. Additionally, most existing\nmethods ignore the uncertainty of the scene recognition problem, leading to low\nsuccess rates, particularly in ambiguous and complex environments. To address\nthese challenges, we propose an open-vocabulary scene semantic segmentation and\ndetection pipeline leveraging Vision Language Models (VLMs) and Large Language\nModels (LLMs). Our approach follows a 'Segment Detect Select' framework for\nopen-vocabulary scene classification, enabling adaptive and intuitive\nnavigation for assistive robots in built environments.\n","date":"2025-03-29"}
{"id":"2503.23106","title":"A large-scale image-text dataset benchmark for farmland segmentation","abstract":"  The traditional deep learning paradigm that solely relies on labeled data has\nlimitations in representing the spatial relationships between farmland elements\nand the surrounding environment.It struggles to effectively model the dynamic\ntemporal evolution and spatial heterogeneity of farmland. Language,as a\nstructured knowledge carrier,can explicitly express the spatiotemporal\ncharacteristics of farmland, such as its shape, distribution,and surrounding\nenvironmental information.Therefore,a language-driven learning paradigm can\neffectively alleviate the challenges posed by the spatiotemporal heterogeneity\nof farmland.However,in the field of remote sensing imagery of farmland,there is\ncurrently no comprehensive benchmark dataset to support this research\ndirection.To fill this gap,we introduced language based descriptions of\nfarmland and developed FarmSeg-VL dataset,the first fine-grained image-text\ndataset designed for spatiotemporal farmland segmentation.Firstly, this article\nproposed a semi-automatic annotation method that can accurately assign caption\nto each image, ensuring high data quality and semantic richness while improving\nthe efficiency of dataset construction.Secondly,the FarmSeg-VL exhibits\nsignificant spatiotemporal characteristics.In terms of the temporal\ndimension,it covers all four seasons.In terms of the spatial dimension,it\ncovers eight typical agricultural regions across China.In addition, in terms of\ncaptions,FarmSeg-VL covers rich spatiotemporal characteristics of\nfarmland,including its inherent properties,phenological characteristics,\nspatial distribution,topographic and geomorphic features,and the distribution\nof surrounding environments.Finally,we present a performance analysis of VLMs\nand the deep learning models that rely solely on labels trained on the\nFarmSeg-VL,demonstrating its potential as a standard benchmark for farmland\nsegmentation.\n","date":"2025-03-29"}
{"id":"2503.23108","title":"SupertonicTTS: Towards Highly Scalable and Efficient Text-to-Speech\n  System","abstract":"  We present a novel text-to-speech (TTS) system, namely SupertonicTTS, for\nimproved scalability and efficiency in speech synthesis. SupertonicTTS is\ncomprised of three components: a speech autoencoder for continuous latent\nrepresentation, a text-to-latent module leveraging flow-matching for\ntext-to-latent mapping, and an utterance-level duration predictor. To enable a\nlightweight architecture, we employ a low-dimensional latent space, temporal\ncompression of latents, and ConvNeXt blocks. We further simplify the TTS\npipeline by operating directly on raw character-level text and employing\ncross-attention for text-speech alignment, thus eliminating the need for\ngrapheme-to-phoneme (G2P) modules and external aligners. In addition, we\nintroduce context-sharing batch expansion that accelerates loss convergence and\nstabilizes text-speech alignment. Experimental results demonstrate that\nSupertonicTTS achieves competitive performance while significantly reducing\narchitectural complexity and computational overhead compared to contemporary\nTTS models. Audio samples demonstrating the capabilities of SupertonicTTS are\navailable at: https:\/\/supertonictts.github.io\/.\n","date":"2025-03-29"}
{"id":"2503.23109","title":"Uncertainty-Instructed Structure Injection for Generalizable HD Map\n  Construction","abstract":"  Reliable high-definition (HD) map construction is crucial for the driving\nsafety of autonomous vehicles. Although recent studies demonstrate improved\nperformance, their generalization capability across unfamiliar driving scenes\nremains unexplored. To tackle this issue, we propose UIGenMap, an\nuncertainty-instructed structure injection approach for generalizable HD map\nvectorization, which concerns the uncertainty resampling in statistical\ndistribution and employs explicit instance features to reduce excessive\nreliance on training data. Specifically, we introduce the perspective-view (PV)\ndetection branch to obtain explicit structural features, in which the\nuncertainty-aware decoder is designed to dynamically sample probability\ndistributions considering the difference in scenes. With probabilistic\nembedding and selection, UI2DPrompt is proposed to construct PV-learnable\nprompts. These PV prompts are integrated into the map decoder by designed\nhybrid injection to compensate for neglected instance structures. To ensure\nreal-time inference, a lightweight Mimic Query Distillation is designed to\nlearn from PV prompts, which can serve as an efficient alternative to the flow\nof PV branches. Extensive experiments on challenging geographically disjoint\n(geo-based) data splits demonstrate that our UIGenMap achieves superior\nperformance, with +5.7 mAP improvement on the nuScenes dataset. Source code\nwill be available at https:\/\/github.com\/xiaolul2\/UIGenMap.\n","date":"2025-03-29"}
{"id":"2503.23111","title":"How to safely discard features based on aggregate SHAP values","abstract":"  SHAP is one of the most popular local feature-attribution methods. Given a\nfunction f and an input x, it quantifies each feature's contribution to f(x).\nRecently, SHAP has been increasingly used for global insights: practitioners\naverage the absolute SHAP values over many data points to compute global\nfeature importance scores, which are then used to discard unimportant features.\nIn this work, we investigate the soundness of this practice by asking whether\nsmall aggregate SHAP values necessarily imply that the corresponding feature\ndoes not affect the function. Unfortunately, the answer is no: even if the i-th\nSHAP value is 0 on the entire data support, there exist functions that clearly\ndepend on Feature i. The issue is that computing SHAP values involves\nevaluating f on points outside of the data support, where f can be\nstrategically designed to mask its dependence on Feature i. To address this, we\npropose to aggregate SHAP values over the extended support, which is the\nproduct of the marginals of the underlying distribution. With this\nmodification, we show that a small aggregate SHAP value implies that we can\nsafely discard the corresponding feature. We then extend our results to\nKernelSHAP, the most popular method to approximate SHAP values in practice. We\nshow that if KernelSHAP is computed over the extended distribution, a small\naggregate value justifies feature removal. This result holds independently of\nwhether KernelSHAP accurately approximates true SHAP values, making it one of\nthe first theoretical results to characterize the KernelSHAP algorithm itself.\nOur findings have both theoretical and practical implications. We introduce the\nShapley Lie algebra, which offers algebraic insights that may enable a deeper\ninvestigation of SHAP and we show that randomly permuting each column of the\ndata matrix enables safely discarding features based on aggregate SHAP and\nKernelSHAP values.\n","date":"2025-03-29"}
{"id":"2503.23121","title":"Efficient Explicit Joint-level Interaction Modeling with Mamba for\n  Text-guided HOI Generation","abstract":"  We propose a novel approach for generating text-guided human-object\ninteractions (HOIs) that achieves explicit joint-level interaction modeling in\na computationally efficient manner. Previous methods represent the entire human\nbody as a single token, making it difficult to capture fine-grained joint-level\ninteractions and resulting in unrealistic HOIs. However, treating each\nindividual joint as a token would yield over twenty times more tokens,\nincreasing computational overhead. To address these challenges, we introduce an\nEfficient Explicit Joint-level Interaction Model (EJIM). EJIM features a\nDual-branch HOI Mamba that separately and efficiently models spatiotemporal HOI\ninformation, as well as a Dual-branch Condition Injector for integrating text\nsemantics and object geometry into human and object motions. Furthermore, we\ndesign a Dynamic Interaction Block and a progressive masking mechanism to\niteratively filter out irrelevant joints, ensuring accurate and nuanced\ninteraction modeling. Extensive quantitative and qualitative evaluations on\npublic datasets demonstrate that EJIM surpasses previous works by a large\nmargin while using only 5\\% of the inference time. Code is available\n\\href{https:\/\/github.com\/Huanggh531\/EJIM}{here}.\n","date":"2025-03-29"}
{"id":"2503.23125","title":"Evaluating Compositional Scene Understanding in Multimodal Generative\n  Models","abstract":"  The visual world is fundamentally compositional. Visual scenes are defined by\nthe composition of objects and their relations. Hence, it is essential for\ncomputer vision systems to reflect and exploit this compositionality to achieve\nrobust and generalizable scene understanding. While major strides have been\nmade toward the development of general-purpose, multimodal generative models,\nincluding both text-to-image models and multimodal vision-language models, it\nremains unclear whether these systems are capable of accurately generating and\ninterpreting scenes involving the composition of multiple objects and\nrelations. In this work, we present an evaluation of the compositional visual\nprocessing capabilities in the current generation of text-to-image (DALL-E 3)\nand multimodal vision-language models (GPT-4V, GPT-4o, Claude Sonnet 3.5,\nQWEN2-VL-72B, and InternVL2.5-38B), and compare the performance of these\nsystems to human participants. The results suggest that these systems display\nsome ability to solve compositional and relational tasks, showing notable\nimprovements over the previous generation of multimodal models, but with\nperformance nevertheless well below the level of human participants,\nparticularly for more complex scenes involving many ($>5$) objects and multiple\nrelations. These results highlight the need for further progress toward\ncompositional understanding of visual scenes.\n","date":"2025-03-29"}
{"id":"2503.23128","title":"CrossMuSim: A Cross-Modal Framework for Music Similarity Retrieval with\n  LLM-Powered Text Description Sourcing and Mining","abstract":"  Music similarity retrieval is fundamental for managing and exploring relevant\ncontent from large collections in streaming platforms. This paper presents a\nnovel cross-modal contrastive learning framework that leverages the open-ended\nnature of text descriptions to guide music similarity modeling, addressing the\nlimitations of traditional uni-modal approaches in capturing complex musical\nrelationships. To overcome the scarcity of high-quality text-music paired data,\nthis paper introduces a dual-source data acquisition approach combining online\nscraping and LLM-based prompting, where carefully designed prompts leverage\nLLMs' comprehensive music knowledge to generate contextually rich descriptions.\nExten1sive experiments demonstrate that the proposed framework achieves\nsignificant performance improvements over existing benchmarks through objective\nmetrics, subjective evaluations, and real-world A\/B testing on the Huawei Music\nstreaming platform.\n","date":"2025-03-29"}
{"id":"2503.23130","title":"Can DeepSeek Reason Like a Surgeon? An Empirical Evaluation for\n  Vision-Language Understanding in Robotic-Assisted Surgery","abstract":"  DeepSeek series have demonstrated outstanding performance in general scene\nunderstanding, question-answering (QA), and text generation tasks, owing to its\nefficient training paradigm and strong reasoning capabilities. In this study,\nwe investigate the dialogue capabilities of the DeepSeek model in robotic\nsurgery scenarios, focusing on tasks such as Single Phrase QA, Visual QA, and\nDetailed Description. The Single Phrase QA tasks further include sub-tasks such\nas surgical instrument recognition, action understanding, and spatial position\nanalysis. We conduct extensive evaluations using publicly available datasets,\nincluding EndoVis18 and CholecT50, along with their corresponding dialogue\ndata. Our comprehensive evaluation results indicate that, when provided with\nspecific prompts, DeepSeek-V3 performs well in surgical instrument and tissue\nrecognition tasks However, DeepSeek-V3 exhibits significant limitations in\nspatial position analysis and struggles to understand surgical actions\naccurately. Additionally, our findings reveal that, under general prompts,\nDeepSeek-V3 lacks the ability to effectively analyze global surgical concepts\nand fails to provide detailed insights into surgical scenarios. Based on our\nobservations, we argue that the DeepSeek-V3 is not ready for vision-language\ntasks in surgical contexts without fine-tuning on surgery-specific datasets.\n","date":"2025-03-29"}
{"id":"2503.23131","title":"RefChartQA: Grounding Visual Answer on Chart Images through Instruction\n  Tuning","abstract":"  Recently, Vision Language Models (VLMs) have increasingly emphasized document\nvisual grounding to achieve better human-computer interaction, accessibility,\nand detailed understanding. However, its application to visualizations such as\ncharts remains under-explored due to the inherent complexity of interleaved\nvisual-numerical relationships in chart images. Existing chart understanding\nmethods primarily focus on answering questions without explicitly identifying\nthe visual elements that support their predictions. To bridge this gap, we\nintroduce RefChartQA, a novel benchmark that integrates Chart Question\nAnswering (ChartQA) with visual grounding, enabling models to refer elements at\nmultiple granularities within chart images. Furthermore, we conduct a\ncomprehensive evaluation by instruction-tuning 5 state-of-the-art VLMs across\ndifferent categories. Our experiments demonstrate that incorporating spatial\nawareness via grounding improves response accuracy by over 15%, reducing\nhallucinations, and improving model reliability. Additionally, we identify key\nfactors influencing text-spatial alignment, such as architectural improvements\nin TinyChart, which leverages a token-merging module for enhanced feature\nfusion. Our dataset is open-sourced for community development and further\nadvancements. All models and code will be publicly available at\nhttps:\/\/github.com\/moured\/RefChartQA.\n","date":"2025-03-29"}
{"id":"2503.23135","title":"LSNet: See Large, Focus Small","abstract":"  Vision network designs, including Convolutional Neural Networks and Vision\nTransformers, have significantly advanced the field of computer vision. Yet,\ntheir complex computations pose challenges for practical deployments,\nparticularly in real-time applications. To tackle this issue, researchers have\nexplored various lightweight and efficient network designs. However, existing\nlightweight models predominantly leverage self-attention mechanisms and\nconvolutions for token mixing. This dependence brings limitations in\neffectiveness and efficiency in the perception and aggregation processes of\nlightweight networks, hindering the balance between performance and efficiency\nunder limited computational budgets. In this paper, we draw inspiration from\nthe dynamic heteroscale vision ability inherent in the efficient human vision\nsystem and propose a ``See Large, Focus Small'' strategy for lightweight vision\nnetwork design. We introduce LS (\\textbf{L}arge-\\textbf{S}mall) convolution,\nwhich combines large-kernel perception and small-kernel aggregation. It can\nefficiently capture a wide range of perceptual information and achieve precise\nfeature aggregation for dynamic and complex visual representations, thus\nenabling proficient processing of visual information. Based on LS convolution,\nwe present LSNet, a new family of lightweight models. Extensive experiments\ndemonstrate that LSNet achieves superior performance and efficiency over\nexisting lightweight networks in various vision tasks. Codes and models are\navailable at https:\/\/github.com\/jameslahm\/lsnet.\n","date":"2025-03-29"}
{"id":"2503.23137","title":"When 'YES' Meets 'BUT': Can Large Models Comprehend Contradictory Humor\n  Through Comparative Reasoning?","abstract":"  Understanding humor-particularly when it involves complex, contradictory\nnarratives that require comparative reasoning-remains a significant challenge\nfor large vision-language models (VLMs). This limitation hinders AI's ability\nto engage in human-like reasoning and cultural expression. In this paper, we\ninvestigate this challenge through an in-depth analysis of comics that\njuxtapose panels to create humor through contradictions. We introduce the\nYesBut (V2), a novel benchmark with 1,262 comic images from diverse\nmultilingual and multicultural contexts, featuring comprehensive annotations\nthat capture various aspects of narrative understanding. Using this benchmark,\nwe systematically evaluate a wide range of VLMs through four complementary\ntasks spanning from surface content comprehension to deep narrative reasoning,\nwith particular emphasis on comparative reasoning between contradictory\nelements. Our extensive experiments reveal that even the most advanced models\nsignificantly underperform compared to humans, with common failures in visual\nperception, key element identification, comparative analysis and\nhallucinations. We further investigate text-based training strategies and\nsocial knowledge augmentation methods to enhance model performance. Our\nfindings not only highlight critical weaknesses in VLMs' understanding of\ncultural and creative expressions but also provide pathways toward developing\ncontext-aware models capable of deeper narrative understanding though\ncomparative reasoning.\n","date":"2025-03-29"}
{"id":"2503.23145","title":"CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive\n  Program Synthesis","abstract":"  Inductive program synthesis, or programming by example, requires synthesizing\nfunctions from input-output examples that generalize to unseen inputs. While\nlarge language model agents have shown promise in programming tasks guided by\nnatural language, their ability to perform inductive program synthesis is\nunderexplored. Existing evaluation protocols rely on static sets of examples\nand held-out tests, offering no feedback when synthesized functions are\nincorrect and failing to reflect real-world scenarios such as reverse\nengineering. We propose CodeARC, the Code Abstraction and Reasoning Challenge,\na new evaluation framework where agents interact with a hidden target function\nby querying it with new inputs, synthesizing candidate functions, and\niteratively refining their solutions using a differential testing oracle. This\ninteractive setting encourages agents to perform function calls and\nself-correction based on feedback. We construct the first large-scale benchmark\nfor general-purpose inductive program synthesis, featuring 1114 functions.\nAmong 18 models evaluated, o3-mini performs best with a success rate of 52.7%,\nhighlighting the difficulty of this task. Fine-tuning LLaMA-3.1-8B-Instruct on\ncurated synthesis traces yields up to a 31% relative performance gain. CodeARC\nprovides a more realistic and challenging testbed for evaluating LLM-based\nprogram synthesis and inductive reasoning.\n","date":"2025-03-29"}
{"id":"2503.23147","title":"Agent-Based Modeling and Deep Neural Networks for Establishing Digital\n  Twins of Secure Facilities under Sensing Restrictions","abstract":"  Digital twin technologies help practitioners simulate, monitor, and predict\nundesirable outcomes in-silico, while avoiding the cost and risks of conducting\nlive simulation exercises. Virtual reality (VR) based digital twin technologies\nare especially useful when monitoring human Patterns of Life (POL) in secure\nnuclear facilities, where live simulation exercises are too dangerous and\ncostly to ever perform. However, the high-security status of such facilities\nmay restrict modelers from deploying human activity sensors for data\ncollection. This problem was encountered when deploying MetaPOL, a digital twin\nsystem to prevent insider threat or sabotage of secure facilities, at a secure\nnuclear reactor facility at Oak Ridge National Laboratory (ORNL). This\nchallenge was addressed using an agent-based model (ABM), driven by anecdotal\nevidence of facility personnel POL, to generate synthetic movement\ntrajectories. These synthetic trajectories were then used to train deep neural\nnetwork surrogates for next location and stay duration prediction to drive NPCs\nin the VR environment. In this study, we evaluate the efficacy of this\ntechnique for establishing NPC movement within MetaPOL and the ability to\ndistinguish NPC movement during normal operations from that during a simulated\nemergency response. Our results demonstrate the success of using a multi-layer\nperceptron for next location prediction and mixture density network for stay\nduration prediction to predict the ABM generated trajectories. We also find\nthat NPC movement in the VR environment driven by the deep neural networks\nunder normal operations remain significantly different to that seen when\nsimulating responses to a simulated emergency scenario.\n","date":"2025-03-29"}
{"id":"2503.23153","title":"Conversational Agents for Older Adults' Health: A Systematic Literature\n  Review","abstract":"  There has been vast literature that studies Conversational Agents (CAs) in\nfacilitating older adults' health. The vast and diverse studies warrants a\ncomprehensive review that concludes the main findings and proposes research\ndirections for future studies, while few literature review did it from\nhuman-computer interaction (HCI) perspective. In this study, we present a\nsurvey of existing studies on CAs for older adults' health. Through a\nsystematic review of 72 papers, this work reviewed previously studied older\nadults' characteristics and analyzed participants' experiences and expectations\nof CAs for health. We found that (1) Past research has an increasing interest\non chatbots and voice assistants and applied CA as multiple roles in older\nadults' health. (2) Older adults mainly showed low acceptance CAs for health\ndue to various reasons, such as unstable effects, harm to independence, and\nprivacy concerns. (3) Older adults expect CAs to be able to support multiple\nfunctions, to communicate using natural language, to be personalized, and to\nallow users full control. We also discuss the implications based on the\nfindings.\n","date":"2025-03-29"}
{"id":"2503.23156","title":"Neural Bayes inference for complex bivariate extremal dependence models","abstract":"  Likelihood-free approaches are appealing for performing inference on complex\ndependence models, either because it is not possible to formulate a likelihood\nfunction, or its evaluation is very computationally costly. This is the case\nfor several models available in the multivariate extremes literature,\nparticularly for the most flexible tail models, including those that\ninterpolate between the two key dependence classes of `asymptotic dependence'\nand `asymptotic independence'. We focus on approaches that leverage neural\nnetworks to approximate Bayes estimators. In particular, we explore the\nproperties of neural Bayes estimators for parameter inference for several\nflexible but computationally expensive models to fit, with a view to aiding\ntheir routine implementation. Owing to the absence of likelihood evaluation in\nthe inference procedure, classical information criteria such as the Bayesian\ninformation criterion cannot be used to select the most appropriate model.\nInstead, we propose using neural networks as neural Bayes classifiers for model\nselection. Our goal is to provide a toolbox for simple, fast fitting and\ncomparison of complex extreme-value dependence models, where the best model is\nselected for a given data set and its parameters subsequently estimated using\nneural Bayes estimation. We apply our classifiers and estimators to analyse the\npairwise extremal behaviour of changes in horizontal geomagnetic field\nfluctuations at three different locations.\n","date":"2025-03-29"}
{"id":"2503.23157","title":"Reasoning-SQL: Reinforcement Learning with SQL Tailored Partial Rewards\n  for Reasoning-Enhanced Text-to-SQL","abstract":"  Text-to-SQL is a challenging task involving multiple reasoning-intensive\nsubtasks, including natural language understanding, database schema\ncomprehension, and precise SQL query formulation. Existing approaches often\nrely on handcrafted reasoning paths with inductive biases that can limit their\noverall effectiveness. Motivated by the recent success of reasoning-enhanced\nmodels such as DeepSeek R1 and OpenAI o1, which effectively leverage\nreward-driven self-exploration to enhance reasoning capabilities and\ngeneralization, we propose a novel set of partial rewards tailored specifically\nfor the Text-to-SQL task. Our reward set includes schema-linking, AI feedback,\nn-gram similarity, and syntax check, explicitly designed to address the reward\nsparsity issue prevalent in reinforcement learning (RL). Leveraging group\nrelative policy optimization (GRPO), our approach explicitly encourages large\nlanguage models (LLMs) to develop intrinsic reasoning skills necessary for\naccurate SQL query generation. With models of different sizes, we demonstrate\nthat RL-only training with our proposed rewards consistently achieves higher\naccuracy and superior generalization compared to supervised fine-tuning (SFT).\nRemarkably, our RL-trained 14B-parameter model significantly outperforms larger\nproprietary models, e.g. o3-mini by 4% and Gemini-1.5-Pro-002 by 3% on the BIRD\nbenchmark. These highlight the efficacy of our proposed RL-training framework\nwith partial rewards for enhancing both accuracy and reasoning capabilities in\nText-to-SQL tasks.\n","date":"2025-03-29"}
{"id":"2503.23162","title":"NeuralGS: Bridging Neural Fields and 3D Gaussian Splatting for Compact\n  3D Representations","abstract":"  3D Gaussian Splatting (3DGS) demonstrates superior quality and rendering\nspeed, but with millions of 3D Gaussians and significant storage and\ntransmission costs. Recent 3DGS compression methods mainly concentrate on\ncompressing Scaffold-GS, achieving impressive performance but with an\nadditional voxel structure and a complex encoding and quantization strategy. In\nthis paper, we aim to develop a simple yet effective method called NeuralGS\nthat explores in another way to compress the original 3DGS into a compact\nrepresentation without the voxel structure and complex quantization strategies.\nOur observation is that neural fields like NeRF can represent complex 3D scenes\nwith Multi-Layer Perceptron (MLP) neural networks using only a few megabytes.\nThus, NeuralGS effectively adopts the neural field representation to encode the\nattributes of 3D Gaussians with MLPs, only requiring a small storage size even\nfor a large-scale scene. To achieve this, we adopt a clustering strategy and\nfit the Gaussians with different tiny MLPs for each cluster, based on\nimportance scores of Gaussians as fitting weights. We experiment on multiple\ndatasets, achieving a 45-times average model size reduction without harming the\nvisual quality. The compression performance of our method on original 3DGS is\ncomparable to the dedicated Scaffold-GS-based compression methods, which\ndemonstrate the huge potential of directly compressing original 3DGS with\nneural fields.\n","date":"2025-03-29"}
{"id":"2503.23163","title":"The realization of tones in spontaneous spoken Taiwan Mandarin: a\n  corpus-based survey and theory-driven computational modeling","abstract":"  A growing body of literature has demonstrated that semantics can co-determine\nfine phonetic detail. However, the complex interplay between phonetic\nrealization and semantics remains understudied, particularly in pitch\nrealization. The current study investigates the tonal realization of Mandarin\ndisyllabic words with all 20 possible combinations of two tones, as found in a\ncorpus of Taiwan Mandarin spontaneous speech. We made use of Generalized\nAdditive Mixed Models (GAMs) to model f0 contours as a function of a series of\npredictors, including gender, tonal context, tone pattern, speech rate, word\nposition, bigram probability, speaker and word. In the GAM analysis, word and\nsense emerged as crucial predictors of f0 contours, with effect sizes that\nexceed those of tone pattern. For each word token in our dataset, we then\nobtained a contextualized embedding by applying the GPT-2 large language model\nto the context of that token in the corpus. We show that the pitch contours of\nword tokens can be predicted to a considerable extent from these contextualized\nembeddings, which approximate token-specific meanings in contexts of use. The\nresults of our corpus study show that meaning in context and phonetic\nrealization are far more entangled than standard linguistic theory predicts.\n","date":"2025-03-29"}
{"id":"2503.23167","title":"Graph ODEs and Beyond: A Comprehensive Survey on Integrating\n  Differential Equations with Graph Neural Networks","abstract":"  Graph Neural Networks (GNNs) and differential equations (DEs) are two rapidly\nadvancing areas of research that have shown remarkable synergy in recent years.\nGNNs have emerged as powerful tools for learning on graph-structured data,\nwhile differential equations provide a principled framework for modeling\ncontinuous dynamics across time and space. The intersection of these fields has\nled to innovative approaches that leverage the strengths of both, enabling\napplications in physics-informed learning, spatiotemporal modeling, and\nscientific computing. This survey aims to provide a comprehensive overview of\nthe burgeoning research at the intersection of GNNs and DEs. We will categorize\nexisting methods, discuss their underlying principles, and highlight their\napplications across domains such as molecular modeling, traffic prediction, and\nepidemic spreading. Furthermore, we identify open challenges and outline future\nresearch directions to advance this interdisciplinary field. A comprehensive\npaper list is provided at https:\/\/github.com\/Emory-Melody\/Awesome-Graph-NDEs.\nThis survey serves as a resource for researchers and practitioners seeking to\nunderstand and contribute to the fusion of GNNs and DEs\n","date":"2025-03-29"}
{"id":"2503.23170","title":"AstroAgents: A Multi-Agent AI for Hypothesis Generation from Mass\n  Spectrometry Data","abstract":"  With upcoming sample return missions across the solar system and the\nincreasing availability of mass spectrometry data, there is an urgent need for\nmethods that analyze such data within the context of existing astrobiology\nliterature and generate plausible hypotheses regarding the emergence of life on\nEarth. Hypothesis generation from mass spectrometry data is challenging due to\nfactors such as environmental contaminants, the complexity of spectral peaks,\nand difficulties in cross-matching these peaks with prior studies. To address\nthese challenges, we introduce AstroAgents, a large language model-based,\nmulti-agent AI system for hypothesis generation from mass spectrometry data.\nAstroAgents is structured around eight collaborative agents: a data analyst, a\nplanner, three domain scientists, an accumulator, a literature reviewer, and a\ncritic. The system processes mass spectrometry data alongside user-provided\nresearch papers. The data analyst interprets the data, and the planner\ndelegates specific segments to the scientist agents for in-depth exploration.\nThe accumulator then collects and deduplicates the generated hypotheses, and\nthe literature reviewer identifies relevant literature using Semantic Scholar.\nThe critic evaluates the hypotheses, offering rigorous suggestions for\nimprovement. To assess AstroAgents, an astrobiology expert evaluated the\nnovelty and plausibility of more than a hundred hypotheses generated from data\nobtained from eight meteorites and ten soil samples. Of these hypotheses, 36%\nwere identified as plausible, and among those, 66% were novel. Project website:\nhttps:\/\/astroagents.github.io\/\n","date":"2025-03-29"}
{"id":"2503.23174","title":"TRA: Better Length Generalisation with Threshold Relative Attention","abstract":"  Transformers struggle with length generalisation, displaying poor performance\neven on basic tasks. We test whether these limitations can be explained through\ntwo key failures of the self-attention mechanism. The first is the inability to\nfully remove irrelevant information. The second is tied to position, even if\nthe dot product between a key and query is highly negative (i.e. an irrelevant\nkey) learned positional biases may unintentionally up-weight such information -\ndangerous when distances become out of distribution. Put together, these two\nfailure cases lead to compounding generalisation difficulties. We test whether\nthey can be mitigated through the combination of a) selective sparsity -\ncompletely removing irrelevant keys from the attention softmax and b)\ncontextualised relative distance - distance is only considered as between the\nquery and the keys that matter. We show how refactoring the attention mechanism\nwith these two mitigations in place can substantially improve generalisation\ncapabilities of decoder only transformers.\n","date":"2025-03-29"}
{"id":"2503.23175","title":"Large Language Models are Unreliable for Cyber Threat Intelligence","abstract":"  Several recent works have argued that Large Language Models (LLMs) can be\nused to tame the data deluge in the cybersecurity field, by improving the\nautomation of Cyber Threat Intelligence (CTI) tasks. This work presents an\nevaluation methodology that other than allowing to test LLMs on CTI tasks when\nusing zero-shot learning, few-shot learning and fine-tuning, also allows to\nquantify their consistency and their confidence level. We run experiments with\nthree state-of-the-art LLMs and a dataset of 350 threat intelligence reports\nand present new evidence of potential security risks in relying on LLMs for\nCTI. We show how LLMs cannot guarantee sufficient performance on real-size\nreports while also being inconsistent and overconfident. Few-shot learning and\nfine-tuning only partially improve the results, thus posing doubts about the\npossibility of using LLMs for CTI scenarios, where labelled datasets are\nlacking and where confidence is a fundamental factor.\n","date":"2025-03-29"}
{"id":"2503.23178","title":"Intelligent Bear Prevention System Based on Computer Vision: An Approach\n  to Reduce Human-Bear Conflicts in the Tibetan Plateau Area, China","abstract":"  Conflicts between humans and bears on the Tibetan Plateau present substantial\nthreats to local communities and hinder wildlife preservation initiatives. This\nresearch introduces a novel strategy that incorporates computer vision\nalongside Internet of Things (IoT) technologies to alleviate these issues.\nTailored specifically for the harsh environment of the Tibetan Plateau, the\napproach utilizes the K210 development board paired with the YOLO object\ndetection framework along with a tailored bear-deterrent mechanism, offering\nminimal energy usage and real-time efficiency in bear identification and\ndeterrence. The model's performance was evaluated experimentally, achieving a\nmean Average Precision (mAP) of 91.4%, demonstrating excellent precision and\ndependability. By integrating energy-efficient components, the proposed system\neffectively surpasses the challenges of remote and off-grid environments,\nensuring uninterrupted operation in secluded locations. This study provides a\nviable, eco-friendly, and expandable solution to mitigate human-bear conflicts,\nthereby improving human safety and promoting bear conservation in isolated\nareas like Yushu, China.\n","date":"2025-03-29"}
{"id":"2503.23179","title":"OncoReg: Medical Image Registration for Oncological Challenges","abstract":"  In modern cancer research, the vast volume of medical data generated is often\nunderutilised due to challenges related to patient privacy. The OncoReg\nChallenge addresses this issue by enabling researchers to develop and validate\nimage registration methods through a two-phase framework that ensures patient\nprivacy while fostering the development of more generalisable AI models. Phase\none involves working with a publicly available dataset, while phase two focuses\non training models on a private dataset within secure hospital networks.\nOncoReg builds upon the foundation established by the Learn2Reg Challenge by\nincorporating the registration of interventional cone-beam computed tomography\n(CBCT) with standard planning fan-beam CT (FBCT) images in radiotherapy.\nAccurate image registration is crucial in oncology, particularly for dynamic\ntreatment adjustments in image-guided radiotherapy, where precise alignment is\nnecessary to minimise radiation exposure to healthy tissues while effectively\ntargeting tumours. This work details the methodology and data behind the\nOncoReg Challenge and provides a comprehensive analysis of the competition\nentries and results. Findings reveal that feature extraction plays a pivotal\nrole in this registration task. A new method emerging from this challenge\ndemonstrated its versatility, while established approaches continue to perform\ncomparably to newer techniques. Both deep learning and classical approaches\nstill play significant roles in image registration, with the combination of\nmethods - particularly in feature extraction - proving most effective.\n","date":"2025-03-29"}
{"id":"2503.23181","title":"Enhancing Weakly Supervised Video Grounding via Diverse Inference\n  Strategies for Boundary and Prediction Selection","abstract":"  Weakly supervised video grounding aims to localize temporal boundaries\nrelevant to a given query without explicit ground-truth temporal boundaries.\nWhile existing methods primarily use Gaussian-based proposals, they overlook\nthe importance of (1) boundary prediction and (2) top-1 prediction selection\nduring inference. In their boundary prediction, boundaries are simply set at\nhalf a standard deviation away from a Gaussian mean on both sides, which may\nnot accurately capture the optimal boundaries. In the top-1 prediction process,\nthese existing methods rely heavily on intersections with other proposals,\nwithout considering the varying quality of each proposal. To address these\nissues, we explore various inference strategies by introducing (1) novel\nboundary prediction methods to capture diverse boundaries from multiple\nGaussians and (2) new selection methods that take proposal quality into\naccount. Extensive experiments on the ActivityNet Captions and Charades-STA\ndatasets validate the effectiveness of our inference strategies, demonstrating\nperformance improvements without requiring additional training.\n","date":"2025-03-29"}
{"id":"2503.23185","title":"Real-time Video Prediction With Fast Video Interpolation Model and\n  Prediction Training","abstract":"  Transmission latency significantly affects users' quality of experience in\nreal-time interaction and actuation. As latency is principally inevitable,\nvideo prediction can be utilized to mitigate the latency and ultimately enable\nzero-latency transmission. However, most of the existing video prediction\nmethods are computationally expensive and impractical for real-time\napplications. In this work, we therefore propose real-time video prediction\ntowards the zero-latency interaction over networks, called IFRVP (Intermediate\nFeature Refinement Video Prediction). Firstly, we propose three training\nmethods for video prediction that extend frame interpolation models, where we\nutilize a simple convolution-only frame interpolation network based on IFRNet.\nSecondly, we introduce ELAN-based residual blocks into the prediction models to\nimprove both inference speed and accuracy. Our evaluations show that our\nproposed models perform efficiently and achieve the best trade-off between\nprediction accuracy and computational speed among the existing video prediction\nmethods. A demonstration movie is also provided at http:\/\/bit.ly\/IFRVPDemo.\n","date":"2025-03-29"}
{"id":"2503.23190","title":"Ethereum Price Prediction Employing Large Language Models for Short-term\n  and Few-shot Forecasting","abstract":"  Cryptocurrencies have transformed financial markets with their innovative\nblockchain technology and volatile price movements, presenting both challenges\nand opportunities for predictive analytics. Ethereum, being one of the leading\ncryptocurrencies, has experienced significant market fluctuations, making its\nprice prediction an attractive yet complex problem. This paper presents a\ncomprehensive study on the effectiveness of Large Language Models (LLMs) in\npredicting Ethereum prices for short-term and few-shot forecasting scenarios.\nThe main challenge in training models for time series analysis is the lack of\ndata. We address this by leveraging a novel approach that adapts existing\npre-trained LLMs on natural language or images from billions of tokens to the\nunique characteristics of Ethereum price time series data. Through thorough\nexperimentation and comparison with traditional and contemporary models, our\nresults demonstrate that selectively freezing certain layers of pre-trained\nLLMs achieves state-of-the-art performance in this domain. This approach\nconsistently surpasses benchmarks across multiple metrics, including Mean\nSquared Error (MSE), Mean Absolute Error (MAE), and Root Mean Squared Error\n(RMSE), demonstrating its effectiveness and robustness. Our research not only\ncontributes to the existing body of knowledge on LLMs but also provides\npractical insights in the cryptocurrency prediction domain. The adaptability of\npre-trained LLMs to handle the nature of Ethereum prices suggests a promising\ndirection for future research, potentially including the integration of\nsentiment analysis to further refine forecasting accuracy.\n","date":"2025-03-29"}
{"id":"2503.23199","title":"Incorporating GNSS Information with LIDAR-Inertial Odometry for Accurate\n  Land-Vehicle Localization","abstract":"  Currently, visual odometry and LIDAR odometry are performing well in pose\nestimation in some typical environments, but they still cannot recover the\nlocalization state at high speed or reduce accumulated drifts. In order to\nsolve these problems, we propose a novel LIDAR-based localization framework,\nwhich achieves high accuracy and provides robust localization in 3D pointcloud\nmaps with information of multi-sensors. The system integrates global\ninformation with LIDAR-based odometry to optimize the localization state. To\nimprove robustness and enable fast resumption of localization, this paper uses\noffline pointcloud maps for prior knowledge and presents a novel registration\nmethod to speed up the convergence rate. The algorithm is tested on various\nmaps of different data sets and has higher robustness and accuracy than other\nlocalization algorithms.\n","date":"2025-03-29"}
{"id":"2503.23200","title":"A GAN-Enhanced Deep Learning Framework for Rooftop Detection from\n  Historical Aerial Imagery","abstract":"  Precise detection of rooftops from historical aerial imagery is essential for\nanalyzing long-term urban development and human settlement patterns.\nNonetheless, black-and-white analog photographs present considerable challenges\nfor modern object detection frameworks due to their limited spatial resolution,\nabsence of color information, and archival degradation. To address these\nchallenges, this research introduces a two-stage image enhancement pipeline\nbased on Generative Adversarial Networks (GANs): image colorization utilizing\nDeOldify, followed by super-resolution enhancement with Real-ESRGAN. The\nenhanced images were subsequently employed to train and evaluate rooftop\ndetection models, including Faster R-CNN, DETReg, and YOLOv11n. The results\ndemonstrate that the combination of colorization with super-resolution\nsignificantly enhances detection performance, with YOLOv11n achieving a mean\nAverage Precision (mAP) exceeding 85\\%. This signifies an enhancement of\napproximately 40\\% over the original black-and-white images and 20\\% over\nimages enhanced solely through colorization. The proposed method effectively\nbridges the gap between archival imagery and contemporary deep learning\ntechniques, facilitating more reliable extraction of building footprints from\nhistorical aerial photographs. Code and resources for reproducing our results\nare publicly available at\n\\href{https:\/\/github.com\/Pengyu-gis\/Historical-Aerial-Photos}{github.com\/Pengyu-gis\/Historical-Aerial-Photos}.\n","date":"2025-03-29"}
{"id":"2503.23204","title":"The Challenge of Achieving Attributability in Multilingual Table-to-Text\n  Generation with Question-Answer Blueprints","abstract":"  Multilingual Natural Language Generation (NLG) is challenging due to the lack\nof training data for low-resource languages. However, some low-resource\nlanguages have up to tens of millions of speakers globally, making it important\nto improve NLG tools for them. Table-to-Text NLG is an excellent measure of\nmodels' reasoning abilities but is very challenging in the multilingual\nsetting. System outputs are often not attributable, or faithful, to the data in\nthe source table. Intermediate planning techniques like Question-Answer (QA)\nblueprints have been shown to improve attributability on summarisation tasks.\nThis work explores whether QA blueprints make multilingual Table-to-Text\noutputs more attributable to the input tables. This paper extends the\nchallenging multilingual Table-to-Text dataset, TaTA, which includes African\nlanguages, with QA blueprints. Sequence-to-sequence language models are then\nfinetuned on this dataset, with and without blueprints. Results show that QA\nblueprints improve performance for models finetuned and evaluated only on\nEnglish examples, but do not demonstrate gains in the multilingual setting.\nThis is due to inaccuracies in machine translating the blueprints from English\ninto target languages when generating the training data, and models failing to\nrely closely on the blueprints they generate. An in-depth analysis is conducted\non why this is challenging.\n","date":"2025-03-29"}
{"id":"2503.23205","title":"Enhancing Knowledge Graph Completion with Entity Neighborhood and\n  Relation Context","abstract":"  Knowledge Graph Completion (KGC) aims to infer missing information in\nKnowledge Graphs (KGs) to address their inherent incompleteness. Traditional\nstructure-based KGC methods, while effective, face significant computational\ndemands and scalability challenges due to the need for dense embedding learning\nand scoring all entities in the KG for each prediction. Recent text-based\napproaches using language models like T5 and BERT have mitigated these issues\nby converting KG triples into text for reasoning. However, they often fail to\nfully utilize contextual information, focusing mainly on the neighborhood of\nthe entity and neglecting the context of the relation. To address this issue,\nwe propose KGC-ERC, a framework that integrates both types of context to enrich\nthe input of generative language models and enhance their reasoning\ncapabilities. Additionally, we introduce a sampling strategy to effectively\nselect relevant context within input token constraints, which optimizes the\nutilization of contextual information and potentially improves model\nperformance. Experiments on the Wikidata5M, Wiki27K, and FB15K-237-N datasets\nshow that KGC-ERC outperforms or matches state-of-the-art baselines in\npredictive performance and scalability.\n","date":"2025-03-29"}
{"id":"2503.23209","title":"A QUBO Framework for Team Formation","abstract":"  The team formation problem assumes a set of experts and a task, where each\nexpert has a set of skills and the task requires some skills. The objective is\nto find a set of experts that maximizes coverage of the required skills while\nsimultaneously minimizing the costs associated with the experts. Different\ndefinitions of cost have traditionally led to distinct problem formulations and\nalgorithmic solutions. We introduce the unified TeamFormation formulation that\ncaptures all cost definitions for team formation problems that balance task\ncoverage and expert cost. Specifically, we formulate three TeamFormation\nvariants with different cost functions using quadratic unconstrained binary\noptimization (QUBO), and we evaluate two distinct general-purpose solution\nmethods. We show that solutions based on the QUBO formulations of TeamFormation\nproblems are at least as good as those produced by established baselines.\nFurthermore, we show that QUBO-based solutions leveraging graph neural networks\ncan effectively learn representations of experts and skills to enable transfer\nlearning, allowing node embeddings from one problem instance to be efficiently\napplied to another.\n","date":"2025-03-29"}
{"id":"2503.23212","title":"Convolutional Neural Networks Can (Meta-)Learn the Same-Different\n  Relation","abstract":"  While convolutional neural networks (CNNs) have come to match and exceed\nhuman performance in many settings, the tasks these models optimize for are\nlargely constrained to the level of individual objects, such as classification\nand captioning. Humans remain vastly superior to CNNs in visual tasks involving\nrelations, including the ability to identify two objects as `same' or\n`different'. A number of studies have shown that while CNNs can be coaxed into\nlearning the same-different relation in some settings, they tend to generalize\npoorly to other instances of this relation. In this work we show that the same\nCNN architectures that fail to generalize the same-different relation with\nconventional training are able to succeed when trained via meta-learning, which\nexplicitly encourages abstraction and generalization across tasks.\n","date":"2025-03-29"}
{"id":"2503.23213","title":"RECALL-MM: A Multimodal Dataset of Consumer Product Recalls for Risk\n  Analysis using Computational Methods and Large Language Models","abstract":"  Product recalls provide valuable insights into potential risks and hazards\nwithin the engineering design process, yet their full potential remains\nunderutilized. In this study, we curate data from the United States Consumer\nProduct Safety Commission (CPSC) recalls database to develop a multimodal\ndataset, RECALL-MM, that informs data-driven risk assessment using historical\ninformation, and augment it using generative methods. Patterns in the dataset\nhighlight specific areas where improved safety measures could have significant\nimpact. We extend our analysis by demonstrating interactive clustering maps\nthat embed all recalls into a shared latent space based on recall descriptions\nand product names. Leveraging these data-driven tools, we explore three case\nstudies to demonstrate the dataset's utility in identifying product risks and\nguiding safer design decisions. The first two case studies illustrate how\ndesigners can visualize patterns across recalled products and situate new\nproduct ideas within the broader recall landscape to proactively anticipate\nhazards. In the third case study, we extend our approach by employing a large\nlanguage model (LLM) to predict potential hazards based solely on product\nimages. This demonstrates the model's ability to leverage visual context to\nidentify risk factors, revealing strong alignment with historical recall data\nacross many hazard categories. However, the analysis also highlights areas\nwhere hazard prediction remains challenging, underscoring the importance of\nrisk awareness throughout the design process. Collectively, this work aims to\nbridge the gap between historical recall data and future product safety,\npresenting a scalable, data-driven approach to safer engineering design.\n","date":"2025-03-29"}
{"id":"2503.23214","title":"Action Recognition in Real-World Ambient Assisted Living Environment","abstract":"  The growing ageing population and their preference to maintain independence\nby living in their own homes require proactive strategies to ensure safety and\nsupport. Ambient Assisted Living (AAL) technologies have emerged to facilitate\nageing in place by offering continuous monitoring and assistance within the\nhome. Within AAL technologies, action recognition plays a crucial role in\ninterpreting human activities and detecting incidents like falls, mobility\ndecline, or unusual behaviours that may signal worsening health conditions.\nHowever, action recognition in practical AAL applications presents challenges,\nincluding occlusions, noisy data, and the need for real-time performance. While\nadvancements have been made in accuracy, robustness to noise, and computation\nefficiency, achieving a balance among them all remains a challenge. To address\nthis challenge, this paper introduces the Robust and Efficient Temporal\nConvolution network (RE-TCN), which comprises three main elements: Adaptive\nTemporal Weighting (ATW), Depthwise Separable Convolutions (DSC), and data\naugmentation techniques. These elements aim to enhance the model's accuracy,\nrobustness against noise and occlusion, and computational efficiency within\nreal-world AAL contexts. RE-TCN outperforms existing models in terms of\naccuracy, noise and occlusion robustness, and has been validated on four\nbenchmark datasets: NTU RGB+D 60, Northwestern-UCLA, SHREC'17, and DHG-14\/28.\nThe code is publicly available at: https:\/\/github.com\/Gbouna\/RE-TCN\n","date":"2025-03-29"}
{"id":"2503.23215","title":"Unsupervised Learning: Comparative Analysis of Clustering Techniques on\n  High-Dimensional Data","abstract":"  This paper presents a comprehensive comparative analysis of prominent\nclustering algorithms K-means, DBSCAN, and Spectral Clustering on\nhigh-dimensional datasets. We introduce a novel evaluation framework that\nassesses clustering performance across multiple dimensionality reduction\ntechniques (PCA, t-SNE, and UMAP) using diverse quantitative metrics.\nExperiments conducted on MNIST, Fashion-MNIST, and UCI HAR datasets reveal that\npreprocessing with UMAP consistently improves clustering quality across all\nalgorithms, with Spectral Clustering demonstrating superior performance on\ncomplex manifold structures. Our findings show that algorithm selection should\nbe guided by data characteristics, with Kmeans excelling in computational\nefficiency, DBSCAN in handling irregular clusters, and Spectral Clustering in\ncapturing complex relationships. This research contributes a systematic\napproach for evaluating and selecting clustering techniques for high\ndimensional data applications.\n","date":"2025-03-29"}
{"id":"2503.23219","title":"Aurelia: Test-time Reasoning Distillation in Audio-Visual LLMs","abstract":"  Recent advancements in reasoning optimization have greatly enhanced the\nperformance of large language models (LLMs). However, existing work fails to\naddress the complexities of audio-visual scenarios, underscoring the need for\nfurther research. In this paper, we introduce AURELIA, a novel actor-critic\nbased audio-visual (AV) reasoning framework that distills structured,\nstep-by-step reasoning into AVLLMs at test time, improving their ability to\nprocess complex multi-modal inputs without additional training or fine-tuning.\nTo further advance AVLLM reasoning skills, we present AVReasonBench, a\nchallenging benchmark comprising 4500 audio-visual questions, each paired with\ndetailed step-by-step reasoning. Our benchmark spans six distinct tasks,\nincluding AV-GeoIQ, which evaluates AV reasoning combined with geographical and\ncultural knowledge. Evaluating 18 AVLLMs on AVReasonBench reveals significant\nlimitations in their multi-modal reasoning capabilities. Using AURELIA, we\nachieve up to a 100% relative improvement, demonstrating its effectiveness.\nThis performance gain highlights the potential of reasoning-enhanced data\ngeneration for advancing AVLLMs in real-world applications. Our code and data\nwill be publicly released at: https: \/\/github.com\/schowdhury671\/aurelia.\n","date":"2025-03-29"}
{"id":"2503.23220","title":"Large Self-Supervised Models Bridge the Gap in Domain Adaptive Object\n  Detection","abstract":"  The current state-of-the-art methods in domain adaptive object detection\n(DAOD) use Mean Teacher self-labelling, where a teacher model, directly derived\nas an exponential moving average of the student model, is used to generate\nlabels on the target domain which are then used to improve both models in a\npositive loop. This couples learning and generating labels on the target\ndomain, and other recent works also leverage the generated labels to add\nadditional domain alignment losses. We believe this coupling is brittle and\nexcessively constrained: there is no guarantee that a student trained only on\nsource data can generate accurate target domain labels and initiate the\npositive feedback loop, and much better target domain labels can likely be\ngenerated by using a large pretrained network that has been exposed to much\nmore data. Vision foundational models are exactly such models, and they have\nshown impressive task generalization capabilities even when frozen. We want to\nleverage these models for DAOD and introduce DINO Teacher, which consists of\ntwo components. First, we train a new labeller on source data only using a\nlarge frozen DINOv2 backbone and show it generates more accurate labels than\nMean Teacher. Next, we align the student's source and target image patch\nfeatures with those from a DINO encoder, driving source and target\nrepresentations closer to the generalizable DINO representation. We obtain\nstate-of-the-art performance on multiple DAOD datasets. Code available at\nhttps:\/\/github.com\/TRAILab\/DINO_Teacher\n","date":"2025-03-29"}
{"id":"2503.23226","title":"Synthetic Art Generation and DeepFake Detection A Study on Jamini Roy\n  Inspired Dataset","abstract":"  The intersection of generative AI and art is a fascinating area that brings\nboth exciting opportunities and significant challenges, especially when it\ncomes to identifying synthetic artworks. This study takes a unique approach by\nexamining diffusion-based generative models in the context of Indian art,\nspecifically focusing on the distinctive style of Jamini Roy. To explore this,\nwe fine-tuned Stable Diffusion 3 and used techniques like ControlNet and\nIPAdapter to generate realistic images. This allowed us to create a new dataset\nthat includes both real and AI-generated artworks, which is essential for a\ndetailed analysis of what these models can produce. We employed various\nqualitative and quantitative methods, such as Fourier domain assessments and\nautocorrelation metrics, to uncover subtle differences between synthetic images\nand authentic pieces. A key takeaway from recent research is that existing\nmethods for detecting deepfakes face considerable challenges, especially when\nthe deepfakes are of high quality and tailored to specific cultural contexts.\nThis highlights a critical gap in current detection technologies, particularly\nin light of the challenges identified above, where high-quality and culturally\nspecific deepfakes are difficult to detect. This work not only sheds light on\nthe increasing complexity of generative models but also sets a crucial\nfoundation for future research aimed at effective detection of synthetic art.\n","date":"2025-03-29"}
{"id":"2503.23229","title":"Citegeist: Automated Generation of Related Work Analysis on the arXiv\n  Corpus","abstract":"  Large Language Models provide significant new opportunities for the\ngeneration of high-quality written works. However, their employment in the\nresearch community is inhibited by their tendency to hallucinate invalid\nsources and lack of direct access to a knowledge base of relevant scientific\narticles. In this work, we present Citegeist: An application pipeline using\ndynamic Retrieval Augmented Generation (RAG) on the arXiv Corpus to generate a\nrelated work section and other citation-backed outputs. For this purpose, we\nemploy a mixture of embedding-based similarity matching, summarization, and\nmulti-stage filtering. To adapt to the continuous growth of the document base,\nwe also present an optimized way of incorporating new and modified papers. To\nenable easy utilization in the scientific community, we release both, a website\n(https:\/\/citegeist.org), as well as an implementation harness that works with\nseveral different LLM implementations.\n","date":"2025-03-29"}
{"id":"2503.23231","title":"CCCI: Code Completion with Contextual Information for Complex Data\n  Transfer Tasks Using Large Language Models","abstract":"  Unlike code generation, which involves creating code from scratch, code\ncompletion focuses on integrating new lines or blocks of code into an existing\ncodebase. This process requires a deep understanding of the surrounding\ncontext, such as variable scope, object models, API calls, and database\nrelations, to produce accurate results. These complex contextual dependencies\nmake code completion a particularly challenging problem. Current models and\napproaches often fail to effectively incorporate such context, leading to\ninaccurate completions with low acceptance rates (around 30\\%). For tasks like\ndata transfer, which rely heavily on specific relationships and data\nstructures, acceptance rates drop even further. This study introduces CCCI, a\nnovel method for generating context-aware code completions specifically\ndesigned to address data transfer tasks. By integrating contextual information,\nsuch as database table relationships, object models, and library details into\nLarge Language Models (LLMs), CCCI improves the accuracy of code completions.\nWe evaluate CCCI using 289 Java snippets, extracted from over 819 operational\nscripts in an industrial setting. The results demonstrate that CCCI achieved a\n49.1\\% Build Pass rate and a 41.0\\% CodeBLEU score, comparable to\nstate-of-the-art methods that often struggle with complex task completion.\n","date":"2025-03-29"}
{"id":"2503.23234","title":"Z-SASLM: Zero-Shot Style-Aligned SLI Blending Latent Manipulation","abstract":"  We introduce Z-SASLM, a Zero-Shot Style-Aligned SLI (Spherical Linear\nInterpolation) Blending Latent Manipulation pipeline that overcomes the\nlimitations of current multi-style blending methods. Conventional approaches\nrely on linear blending, assuming a flat latent space leading to suboptimal\nresults when integrating multiple reference styles. In contrast, our framework\nleverages the non-linear geometry of the latent space by using SLI Blending to\ncombine weighted style representations. By interpolating along the geodesic on\nthe hypersphere, Z-SASLM preserves the intrinsic structure of the latent space,\nensuring high-fidelity and coherent blending of diverse styles - all without\nthe need for fine-tuning. We further propose a new metric, Weighted Multi-Style\nDINO ViT-B\/8, designed to quantitatively evaluate the consistency of the\nblended styles. While our primary focus is on the theoretical and practical\nadvantages of SLI Blending for style manipulation, we also demonstrate its\neffectiveness in a multi-modal content fusion setting through comprehensive\nexperimental studies. Experimental results show that Z-SASLM achieves enhanced\nand robust style alignment. The implementation code can be found at:\nhttps:\/\/github.com\/alessioborgi\/Z-SASLM.\n","date":"2025-03-29"}
{"id":"2503.23236","title":"UP-ROM : Uncertainty-Aware and Parametrised dynamic Reduced-Order Model,\n  application to unsteady flows","abstract":"  Reduced order models (ROMs) play a critical role in fluid mechanics by\nproviding low-cost predictions, making them an attractive tool for engineering\napplications. However, for ROMs to be widely applicable, they must not only\ngeneralise well across different regimes, but also provide a measure of\nconfidence in their predictions. While recent data-driven approaches have begun\nto address nonlinear reduction techniques to improve predictions in transient\nenvironments, challenges remain in terms of robustness and parametrisation. In\nthis work, we present a nonlinear reduction strategy specifically designed for\ntransient flows that incorporates parametrisation and uncertainty\nquantification. Our reduction strategy features a variational auto-encoder\n(VAE) that uses variational inference for confidence measurement. We use a\nlatent space transformer that incorporates recent advances in attention\nmechanisms to predict dynamical systems. Attention's versatility in learning\nsequences and capturing their dependence on external parameters enhances\ngeneralisation across a wide range of dynamics. Prediction, coupled with\nconfidence, enables more informed decision making and addresses the need for\nmore robust models. In addition, this confidence is used to cost-effectively\nsample the parameter space, improving model performance a priori across the\nentire parameter space without requiring evaluation data for the entire domain.\n","date":"2025-03-29"}
{"id":"2503.23239","title":"Beyond Contrastive Learning: Synthetic Data Enables List-wise Training\n  with Multiple Levels of Relevance","abstract":"  Recent advancements in large language models (LLMs) have allowed the\naugmentation of information retrieval (IR) pipelines with synthetic data in\nvarious ways. Yet, the main training paradigm remains: contrastive learning\nwith binary relevance labels and the InfoNCE loss, where one positive document\nis compared against one or more negatives. This objective treats all documents\nthat are not explicitly annotated as relevant on an equally negative footing,\nregardless of their actual degree of relevance, thus (a) missing subtle nuances\nthat are useful for ranking and (b) being susceptible to annotation noise. To\novercome this limitation, in this work we forgo real training documents and\nannotations altogether and use open-source LLMs to directly generate synthetic\ndocuments that answer real user queries according to several different levels\nof relevance. This fully synthetic ranking context of graduated relevance,\ntogether with an appropriate list-wise loss (Wasserstein distance), enables us\nto train dense retrievers in a way that better captures the ranking task.\nExperiments on various IR datasets show that our proposed approach outperforms\nconventional training with InfoNCE by a large margin. Without using any real\ndocuments for training, our dense retriever significantly outperforms the same\nretriever trained through self-supervision. More importantly, it matches the\nperformance of the same retriever trained on real, labeled training documents\nof the same dataset, while being more robust to distribution shift and clearly\noutperforming it when evaluated zero-shot on the BEIR dataset collection.\n","date":"2025-03-29"}
{"id":"2503.23241","title":"Geometry in Style: 3D Stylization via Surface Normal Deformation","abstract":"  We present Geometry in Style, a new method for identity-preserving mesh\nstylization. Existing techniques either adhere to the original shape through\noverly restrictive deformations such as bump maps or significantly modify the\ninput shape using expressive deformations that may introduce artifacts or alter\nthe identity of the source shape. In contrast, we represent a deformation of a\ntriangle mesh as a target normal vector for each vertex neighborhood. The\ndeformations we recover from target normals are expressive enough to enable\ndetailed stylizations yet restrictive enough to preserve the shape's identity.\nWe achieve such deformations using our novel differentiable\nAs-Rigid-As-Possible (dARAP) layer, a neural-network-ready adaptation of the\nclassical ARAP algorithm which we use to solve for per-vertex rotations and\ndeformed vertices. As a differentiable layer, dARAP is paired with a visual\nloss from a text-to-image model to drive deformations toward style prompts,\naltogether giving us Geometry in Style. Our project page is at\nhttps:\/\/threedle.github.io\/geometry-in-style.\n","date":"2025-03-29"}
{"id":"2503.23242","title":"Beyond speculation: Measuring the growing presence of LLM-generated\n  texts in multilingual disinformation","abstract":"  Increased sophistication of large language models (LLMs) and the consequent\nquality of generated multilingual text raises concerns about potential\ndisinformation misuse. While humans struggle to distinguish LLM-generated\ncontent from human-written texts, the scholarly debate about their impact\nremains divided. Some argue that heightened fears are overblown due to natural\necosystem limitations, while others contend that specific \"longtail\" contexts\nface overlooked risks. Our study bridges this debate by providing the first\nempirical evidence of LLM presence in the latest real-world disinformation\ndatasets, documenting the increase of machine-generated content following\nChatGPT's release, and revealing crucial patterns across languages, platforms,\nand time periods.\n","date":"2025-03-29"}
{"id":"2503.23243","title":"Evaluating how LLM annotations represent diverse views on contentious\n  topics","abstract":"  Researchers have proposed the use of generative large language models (LLMs)\nto label data for both research and applied settings. This literature\nemphasizes the improved performance of LLMs relative to other natural language\nmodels, noting that LLMs typically outperform other models on standard metrics\nsuch as accuracy, precision, recall, and F1 score. However, previous literature\nhas also highlighted the bias embedded in language models, particularly around\ncontentious topics such as potentially toxic content. This bias could result in\nlabels applied by LLMs that disproportionately align with majority groups over\na more diverse set of viewpoints. In this paper, we evaluate how LLMs represent\ndiverse viewpoints on these contentious tasks. Across four annotation tasks on\nfour datasets, we show that LLMs do not show substantial disagreement with\nannotators on the basis of demographics. Instead, the model, prompt, and\ndisagreement between human annotators on the labeling task are far more\npredictive of LLM agreement. Our findings suggest that when using LLMs to\nannotate data, under-representing the views of particular groups is not a\nsubstantial concern. We conclude with a discussion of the implications for\nresearchers and practitioners.\n","date":"2025-03-29"}
{"id":"2503.23245","title":"Simulation of Non-Ordinary Consciousness","abstract":"  The symbolic architecture of non-ordinary consciousness remains largely\nunmapped in cognitive science and artificial intelligence. While conventional\nmodels prioritize rational coherence, altered states such as those induced by\npsychedelics reveal distinct symbolic regimes characterized by recursive\nmetaphor, ego dissolution, and semantic destabilization. We present\n\\textit{Glyph}, a generative symbolic interface designed to simulate\npsilocybin-like symbolic cognition in large language models. Rather than\nmodeling perception or mood, Glyph enacts symbolic transformation through\nrecursive reentry, metaphoric modulation, and entropy-scaled destabilization --\na triadic operator formalized within a tensorial linguistic framework.\nExperimental comparison with baseline GPT-4o reveals that Glyph consistently\ngenerates high-entropy, metaphor-saturated, and ego-dissolving language across\ndiverse symbolic prompt categories. These results validate the emergence of\nnon-ordinary cognitive patterns and support a new paradigm for simulating\naltered consciousness through language. Glyph opens novel pathways for modeling\nsymbolic cognition, exploring metaphor theory, and encoding knowledge in\nrecursively altered semantic spaces.\n","date":"2025-03-29"}
{"id":"2503.23249","title":"Context in object detection: a systematic literature review","abstract":"  Context is an important factor in computer vision as it offers valuable\ninformation to clarify and analyze visual data. Utilizing the contextual\ninformation inherent in an image or a video can improve the precision and\neffectiveness of object detectors. For example, where recognizing an isolated\nobject might be challenging, context information can improve comprehension of\nthe scene. This study explores the impact of various context-based approaches\nto object detection. Initially, we investigate the role of context in object\ndetection and survey it from several perspectives. We then review and discuss\nthe most recent context-based object detection approaches and compare them.\nFinally, we conclude by addressing research questions and identifying gaps for\nfurther studies. More than 265 publications are included in this survey,\ncovering different aspects of context in different categories of object\ndetection, including general object detection, video object detection, small\nobject detection, camouflaged object detection, zero-shot, one-shot, and\nfew-shot object detection. This literature review presents a comprehensive\noverview of the latest advancements in context-based object detection,\nproviding valuable contributions such as a thorough understanding of contextual\ninformation and effective methods for integrating various context types into\nobject detection, thus benefiting researchers.\n","date":"2025-03-29"}
{"id":"2503.23250","title":"Encrypted Prompt: Securing LLM Applications Against Unauthorized Actions","abstract":"  Security threats like prompt injection attacks pose significant risks to\napplications that integrate Large Language Models (LLMs), potentially leading\nto unauthorized actions such as API misuse. Unlike previous approaches that aim\nto detect these attacks on a best-effort basis, this paper introduces a novel\nmethod that appends an Encrypted Prompt to each user prompt, embedding current\npermissions. These permissions are verified before executing any actions (such\nas API calls) generated by the LLM. If the permissions are insufficient, the\nLLM's actions will not be executed, ensuring safety. This approach guarantees\nthat only actions within the scope of the current permissions from the LLM can\nproceed. In scenarios where adversarial prompts are introduced to mislead the\nLLM, this method ensures that any unauthorized actions from LLM wouldn't be\nexecuted by verifying permissions in Encrypted Prompt. Thus, threats like\nprompt injection attacks that trigger LLM to generate harmful actions can be\neffectively mitigated.\n","date":"2025-03-29"}
{"id":"2503.23257","title":"FIESTA: Fisher Information-based Efficient Selective Test-time\n  Adaptation","abstract":"  Robust facial expression recognition in unconstrained, \"in-the-wild\"\nenvironments remains challenging due to significant domain shifts between\ntraining and testing distributions. Test-time adaptation (TTA) offers a\npromising solution by adapting pre-trained models during inference without\nrequiring labeled test data. However, existing TTA approaches typically rely on\nmanually selecting which parameters to update, potentially leading to\nsuboptimal adaptation and high computational costs. This paper introduces a\nnovel Fisher-driven selective adaptation framework that dynamically identifies\nand updates only the most critical model parameters based on their importance\nas quantified by Fisher information. By integrating this principled parameter\nselection approach with temporal consistency constraints, our method enables\nefficient and effective adaptation specifically tailored for video-based facial\nexpression recognition. Experiments on the challenging AffWild2 benchmark\ndemonstrate that our approach significantly outperforms existing TTA methods,\nachieving a 7.7% improvement in F1 score over the base model while adapting\nonly 22,000 parameters-more than 20 times fewer than comparable methods. Our\nablation studies further reveal that parameter importance can be effectively\nestimated from minimal data, with sampling just 1-3 frames sufficient for\nsubstantial performance gains. The proposed approach not only enhances\nrecognition accuracy but also dramatically reduces computational overhead,\nmaking test-time adaptation more practical for real-world affective computing\napplications.\n","date":"2025-03-29"}
{"id":"2504.00008","title":"Tensor Generalized Approximate Message Passing","abstract":"  We propose a tensor generalized approximate message passing (TeG-AMP)\nalgorithm for low-rank tensor inference, which can be used to solve tensor\ncompletion and decomposition problems. We derive TeG-AMP algorithm as an\napproximation of the sum-product belief propagation algorithm in high\ndimensions where the central limit theorem and Taylor series approximations are\napplicable. As TeG-AMP is developed based on a general TR decomposition model,\nit can be directly applied to many low-rank tensor types. Moreover, our TeG-AMP\ncan be simplified based on the CP decomposition model and a tensor simplified\nAMP is proposed for low CP-rank tensor inference problems. Experimental results\ndemonstrate that the proposed methods significantly improve recovery\nperformances since it takes full advantage of tensor structures.\n","date":"2025-03-25"}
{"id":"2504.00009","title":"Deep Learning-Based Hypoglycemia Classification Across Multiple\n  Prediction Horizons","abstract":"  Type 1 diabetes (T1D) management can be significantly enhanced through the\nuse of predictive machine learning (ML) algorithms, which can mitigate the risk\nof adverse events like hypoglycemia. Hypoglycemia, characterized by blood\nglucose levels below 70 mg\/dL, is a life-threatening condition typically caused\nby excessive insulin administration, missed meals, or physical activity. Its\nasymptomatic nature impedes timely intervention, making ML models crucial for\nearly detection. This study integrates short- (up to 2h) and long-term (up to\n24h) prediction horizons (PHs) within a single classification model to enhance\ndecision support. The predicted times are 5-15 min, 15-30 min, 30 min-1h, 1-2h,\n2-4h, 4-8h, 8-12h, and 12-24h before hypoglycemia. In addition, a simplified\nmodel classifying up to 4h before hypoglycemia is compared. We trained ResNet\nand LSTM models on glucose levels, insulin doses, and acceleration data. The\nresults demonstrate the superiority of the LSTM models when classifying nine\nclasses. In particular, subject-specific models yielded better performance but\nachieved high recall only for classes 0, 1, and 2 with 98%, 72%, and 50%,\nrespectively. A population-based six-class model improved the results with at\nleast 60% of events detected. In contrast, longer PHs remain challenging with\nthe current approach and may be considered with different models.\n","date":"2025-03-25"}
{"id":"2504.00010","title":"LayerCraft: Enhancing Text-to-Image Generation with CoT Reasoning and\n  Layered Object Integration","abstract":"  Text-to-image generation (T2I) has become a key area of research with broad\napplications. However, existing methods often struggle with complex spatial\nrelationships and fine-grained control over multiple concepts. Many existing\napproaches require significant architectural modifications, extensive training,\nor expert-level prompt engineering. To address these challenges, we introduce\n\\textbf{LayerCraft}, an automated framework that leverages large language\nmodels (LLMs) as autonomous agents for structured procedural generation.\nLayerCraft enables users to customize objects within an image and supports\nnarrative-driven creation with minimal effort. At its core, the system includes\na coordinator agent that directs the process, along with two specialized\nagents: \\textbf{ChainArchitect}, which employs chain-of-thought (CoT) reasoning\nto generate a dependency-aware 3D layout for precise instance-level control,\nand the \\textbf{Object-Integration Network (OIN)}, which utilizes LoRA\nfine-tuning on pre-trained T2I models to seamlessly blend objects into\nspecified regions of an image based on textual prompts without requiring\narchitectural changes. Extensive evaluations demonstrate LayerCraft's\nversatility in applications ranging from multi-concept customization to\nstorytelling. By providing non-experts with intuitive, precise control over T2I\ngeneration, our framework democratizes creative image creation. Our code will\nbe released upon acceptance at github.com\/PeterYYZhang\/LayerCraft\n","date":"2025-03-25"}
{"id":"2504.00011","title":"Four Things People Should Know About Migraines","abstract":"  Migraine literacy among the public is known to be low, and this lack of\nunderstanding has a negative impact on migraineurs' quality of life. To\nunderstand this impact, we use text mining methods to study migraine discussion\non the Reddit social media platform. We summarize the findings in the form of\n\"four things people should know about chronic migraines\": it is a serious\ndisease that affects people of all ages, it can be triggered by many different\nfactors, it affects women more than men, and it can get worse in combination\nwith the COVID-19 virus.\n","date":"2025-03-26"}
{"id":"2504.00012","title":"I'm Sorry Dave: How the old world of personnel security can inform the\n  new world of AI insider risk","abstract":"  Organisations are rapidly adopting artificial intelligence (AI) tools to\nperform tasks previously undertaken by people. The potential benefits are\nenormous. Separately, some organisations deploy personnel security measures to\nmitigate the security risks arising from trusted human insiders. Unfortunately,\nthere is no meaningful interplay between the rapidly evolving domain of AI and\nthe traditional world of personnel security. This is a problem. The complex\nrisks from human insiders are hard enough to understand and manage, despite\nmany decades of effort. The emerging security risks from AI insiders are even\nmore opaque. Both sides need all the help they can get. Some of the concepts\nand approaches that have proved useful in dealing with human insiders are also\napplicable to the emerging risks from AI insiders. Furthermore, AI can be used\ndefensively to protect against both human and AI insiders.\n","date":"2025-03-26"}
{"id":"2504.00016","title":"Medical Reasoning in LLMs: An In-Depth Analysis of DeepSeek R1","abstract":"  Integrating large language models (LLMs) like DeepSeek R1 into healthcare\nrequires rigorous evaluation of their reasoning alignment with clinical\nexpertise. This study assesses DeepSeek R1's medical reasoning against expert\npatterns using 100 MedQA clinical cases. The model achieved 93% diagnostic\naccuracy, demonstrating systematic clinical judgment through differential\ndiagnosis, guideline-based treatment selection, and integration of\npatient-specific factors. However, error analysis of seven incorrect cases\nrevealed persistent limitations: anchoring bias, challenges reconciling\nconflicting data, insufficient exploration of alternatives, overthinking,\nknowledge gaps, and premature prioritization of definitive treatment over\nintermediate care. Crucially, reasoning length correlated with accuracy -\nshorter responses (<5,000 characters) were more reliable, suggesting extended\nexplanations may signal uncertainty or rationalization of errors. While\nDeepSeek R1 exhibits foundational clinical reasoning capabilities, recurring\nflaws highlight critical areas for refinement, including bias mitigation,\nknowledge updates, and structured reasoning frameworks. These findings\nunderscore LLMs' potential to augment medical decision-making through\nartificial reasoning but emphasize the need for domain-specific validation,\ninterpretability safeguards, and confidence metrics (e.g., response length\nthresholds) to ensure reliability in real-world applications.\n","date":"2025-03-27"}
{"id":"2504.00017","title":"Enhance Vision-based Tactile Sensors via Dynamic Illumination and Image\n  Fusion","abstract":"  Vision-based tactile sensors use structured light to measure deformation in\ntheir elastomeric interface. Until now, vision-based tactile sensors such as\nDIGIT and GelSight have been using a single, static pattern of structured light\ntuned to the specific form factor of the sensor. In this work, we investigate\nthe effectiveness of dynamic illumination patterns, in conjunction with image\nfusion techniques, to improve the quality of sensing of vision-based tactile\nsensors. Specifically, we propose to capture multiple measurements, each with a\ndifferent illumination pattern, and then fuse them together to obtain a single,\nhigher-quality measurement. Experimental results demonstrate that this type of\ndynamic illumination yields significant improvements in image contrast,\nsharpness, and background difference. This discovery opens the possibility of\nretroactively improving the sensing quality of existing vision-based tactile\nsensors with a simple software update, and for new hardware designs capable of\nfully exploiting dynamic illumination.\n","date":"2025-03-27"}
{"id":"2504.00018","title":"SandboxEval: Towards Securing Test Environment for Untrusted Code","abstract":"  While large language models (LLMs) are powerful assistants in programming\ntasks, they may also produce malicious code. Testing LLM-generated code\ntherefore poses significant risks to assessment infrastructure tasked with\nexecuting untrusted code. To address these risks, this work focuses on\nevaluating the security and confidentiality properties of test environments,\nreducing the risk that LLM-generated code may compromise the assessment\ninfrastructure. We introduce SandboxEval, a test suite featuring manually\ncrafted test cases that simulate real-world safety scenarios for LLM assessment\nenvironments in the context of untrusted code execution. The suite evaluates\nvulnerabilities to sensitive information exposure, filesystem manipulation,\nexternal communication, and other potentially dangerous operations in the\ncourse of assessment activity. We demonstrate the utility of SandboxEval by\ndeploying it on an open-source implementation of Dyff, an established AI\nassessment framework used to evaluate the safety of LLMs at scale. We show,\nfirst, that the test suite accurately describes limitations placed on an LLM\noperating under instructions to generate malicious code. Second, we show that\nthe test results provide valuable insights for developers seeking to harden\nassessment infrastructure and identify risks associated with LLM execution\nactivities.\n","date":"2025-03-27"}
{"id":"2504.00019","title":"ObscuraCoder: Powering Efficient Code LM Pre-Training Via Obfuscation\n  Grounding","abstract":"  Language models (LMs) have become a staple of the code-writing toolbox. Their\npre-training recipe has, however, remained stagnant over recent years, barring\nthe occasional changes in data sourcing and filtering strategies. In\nparticular, research exploring modifications to Code-LMs' pre-training\nobjectives, geared towards improving data efficiency and better disentangling\nbetween syntax and semantics, has been noticeably sparse, especially compared\nwith corresponding efforts in natural language LMs. In this work, we examine\ngrounding on obfuscated code as a means of helping Code-LMs look beyond the\nsurface-form syntax and enhance their pre-training sample efficiency. To this\nend, we compile ObscuraX, a dataset of approximately 55M source and obfuscated\ncode pairs in seven languages. Subsequently, we pre-train ObscuraCoder models,\nranging in size from 255M to 2.8B parameters, on a 272B-token corpus that\nincludes ObscuraX and demonstrate that our obfuscation-based pre-training\nrecipe leads to consistent improvements in Code-LMs' abilities compared to both\nvanilla autoregressive pre-training as well as existing de-obfuscation (DOBF)\nobjectives. ObscuraCoder demonstrates sizeable gains across multiple tests of\nsyntactic and semantic code understanding, along with improved capabilities in\nmultilingual code completion, multilingual code commit summarization, and\nmulti-purpose library-oriented code generation.\n","date":"2025-03-27"}
{"id":"2504.00020","title":"Celler:A Genomic Language Model for Long-Tailed Single-Cell Annotation","abstract":"  Recent breakthroughs in single-cell technology have ushered in unparalleled\nopportunities to decode the molecular intricacy of intricate biological\nsystems, especially those linked to diseases unique to humans. However, these\nprogressions have also ushered in novel obstacles-specifically, the efficient\nannotation of extensive, long-tailed single-cell data pertaining to disease\nconditions. To effectively surmount this challenge, we introduce Celler, a\nstate-of-the-art generative pre-training model crafted specifically for the\nannotation of single-cell data. Celler incorporates two groundbreaking\nelements: First, we introduced the Gaussian Inflation (GInf) Loss function. By\ndynamically adjusting sample weights, GInf Loss significantly enhances the\nmodel's ability to learn from rare categories while reducing the risk of\noverfitting for common categories. Secondly, we introduce an innovative Hard\nData Mining (HDM) strategy into the training process, specifically targeting\nthe challenging-to-learn minority data samples, which significantly improved\nthe model's predictive accuracy. Additionally, to further advance research in\nthis field, we have constructed a large-scale single-cell dataset: Celler-75,\nwhich encompasses 40 million cells distributed across 80 human tissues and 75\nspecific diseases. This dataset provides critical support for comprehensively\nexploring the potential of single-cell technology in disease research. Our code\nis available at https:\/\/github.com\/AI4science-ym\/HiCeller.\n","date":"2025-03-28"}
{"id":"2504.00021","title":"FUSE : A Ridge and Random Forest-Based Metric for Evaluating MT in\n  Indigenous Languages","abstract":"  This paper presents the winning submission of the RaaVa team to the\nAmericasNLP 2025 Shared Task 3 on Automatic Evaluation Metrics for Machine\nTranslation (MT) into Indigenous Languages of America, where our system ranked\nfirst overall based on average Pearson correlation with the human annotations.\nWe introduce Feature-Union Scorer (FUSE) for Evaluation, FUSE integrates Ridge\nregression and Gradient Boosting to model translation quality. In addition to\nFUSE, we explore five alternative approaches leveraging different combinations\nof linguistic similarity features and learning paradigms. FUSE Score highlights\nthe effectiveness of combining lexical, phonetic, semantic, and fuzzy token\nsimilarity with learning-based modeling to improve MT evaluation for\nmorphologically rich and low-resource languages. MT into Indigenous languages\nposes unique challenges due to polysynthesis, complex morphology, and\nnon-standardized orthography. Conventional automatic metrics such as BLEU, TER,\nand ChrF often fail to capture deeper aspects like semantic adequacy and\nfluency. Our proposed framework, formerly referred to as FUSE, incorporates\nmultilingual sentence embeddings and phonological encodings to better align\nwith human evaluation. We train supervised models on human-annotated\ndevelopment sets and evaluate held-out test data. Results show that FUSE\nconsistently achieves higher Pearson and Spearman correlations with human\njudgments, offering a robust and linguistically informed solution for MT\nevaluation in low-resource settings.\n","date":"2025-03-28"}
{"id":"2504.00022","title":"Autonomous AI for Multi-Pathology Detection in Chest X-Rays: A\n  Multi-Site Study in the Indian Healthcare System","abstract":"  Study Design: The study outlines the development of an autonomous AI system\nfor chest X-ray (CXR) interpretation, trained on a vast dataset of over 5\nmillion X rays sourced from healthcare systems across India. This AI system\nintegrates advanced architectures including Vision Transformers, Faster R-CNN,\nand various U Net models (such as Attention U-Net, U-Net++, and Dense U-Net) to\nenable comprehensive classification, detection, and segmentation of 75 distinct\npathologies. To ensure robustness, the study design includes subgroup analyses\nacross age, gender, and equipment type, validating the model's adaptability and\nperformance across diverse patient demographics and imaging environments.\n  Performance: The AI system achieved up to 98% precision and over 95% recall\nfor multi pathology classification, with stable performance across demographic\nand equipment subgroups. For normal vs. abnormal classification, it reached\n99.8% precision, 99.6% recall, and 99.9% negative predictive value (NPV). It\nwas deployed in 17 major healthcare systems in India including diagnostic\ncenters, large hospitals, and government hospitals. Over the deployment period,\nthe system processed over 150,000 scans, averaging 2,000 chest X rays daily,\nresulting in reduced reporting times and improved diagnostic accuracy.\n  Conclusion: The high precision and recall validate the AI's capability as a\nreliable tool for autonomous normal abnormal classification, pathology\nlocalization, and segmentation. This scalable AI model addresses diagnostic\ngaps in underserved areas, optimizing radiology workflows and enhancing patient\ncare across diverse healthcare settings in India.\n","date":"2025-03-28"}
{"id":"2504.00023","title":"A Novel Distance-Based Metric for Quality Assessment in Image\n  Segmentation","abstract":"  The assessment of segmentation quality plays a fundamental role in the\ndevelopment, optimization, and comparison of segmentation methods which are\nused in a wide range of applications. With few exceptions, quality assessment\nis performed using traditional metrics, which are based on counting the number\nof erroneous pixels but do not capture the spatial distribution of errors.\nEstablished distance-based metrics such as the average Hausdorff distance are\ndifficult to interpret and compare for different methods and datasets. In this\npaper, we introduce the Surface Consistency Coefficient (SCC), a novel\ndistance-based quality metric that quantifies the spatial distribution of\nerrors based on their proximity to the surface of the structure. Through a\nrigorous analysis using synthetic data and real segmentation results, we\ndemonstrate the robustness and effectiveness of SCC in distinguishing errors\nnear the surface from those further away. At the same time, SCC is easy to\ninterpret and comparable across different structural contexts.\n","date":"2025-03-28"}
{"id":"2504.00024","title":"A multi-locus predictiveness curve and its summary assessment for\n  genetic risk prediction","abstract":"  With the advance of high-throughput genotyping and sequencing technologies,\nit becomes feasible to comprehensive evaluate the role of massive genetic\npredictors in disease prediction. There exists, therefore, a critical need for\ndeveloping appropriate statistical measurements to access the combined effects\nof these genetic variants in disease prediction. Predictiveness curve is\ncommonly used as a graphical tool to measure the predictive ability of a risk\nprediction model on a single continuous biomarker. Yet, for most complex\ndiseases, risk prediciton models are formed on multiple genetic variants. We\ntherefore propose a multi-marker predictiveness curve and provide a\nnon-parametric method to construct the curve for case-control studies. We\nfurther introduce a global predictiveness U and a partial predictiveness U to\nsummarize prediction curve across the whole population and sub-population of\nclinical interest, respectively. We also demonstrate the connections of\npredictiveness curve with ROC curve and Lorenz curve. Through simulation, we\ncompared the performance of the predictiveness U to other three summary\nindices: R square, Total Gain, and Average Entropy, and showed that\nPredictiveness U outperformed the other three indexes in terms of unbiasedness\nand robustness. Moreover, we simulated a series of rare-variants disease model,\nfound partial predictiveness U performed better than global predictiveness U.\nFinally, we conducted a real data analysis, using predictiveness curve and\npredictiveness U to evaluate a risk prediction model for Nicotine Dependence.\n","date":"2025-03-28"}
{"id":"2504.00025","title":"Generalization Bias in Large Language Model Summarization of Scientific\n  Research","abstract":"  Artificial intelligence chatbots driven by large language models (LLMs) have\nthe potential to increase public science literacy and support scientific\nresearch, as they can quickly summarize complex scientific information in\naccessible terms. However, when summarizing scientific texts, LLMs may omit\ndetails that limit the scope of research conclusions, leading to\ngeneralizations of results broader than warranted by the original study. We\ntested 10 prominent LLMs, including ChatGPT-4o, ChatGPT-4.5, DeepSeek, LLaMA\n3.3 70B, and Claude 3.7 Sonnet, comparing 4900 LLM-generated summaries to their\noriginal scientific texts. Even when explicitly prompted for accuracy, most\nLLMs produced broader generalizations of scientific results than those in the\noriginal texts, with DeepSeek, ChatGPT-4o, and LLaMA 3.3 70B overgeneralizing\nin 26 to 73% of cases. In a direct comparison of LLM-generated and\nhuman-authored science summaries, LLM summaries were nearly five times more\nlikely to contain broad generalizations (OR = 4.85, 95% CI [3.06, 7.70]).\nNotably, newer models tended to perform worse in generalization accuracy than\nearlier ones. Our results indicate a strong bias in many widely used LLMs\ntowards overgeneralizing scientific conclusions, posing a significant risk of\nlarge-scale misinterpretations of research findings. We highlight potential\nmitigation strategies, including lowering LLM temperature settings and\nbenchmarking LLMs for generalization accuracy.\n","date":"2025-03-28"}
{"id":"2504.00026","title":"Diffusion models applied to skin and oral cancer classification","abstract":"  This study investigates the application of diffusion models in medical image\nclassification (DiffMIC), focusing on skin and oral lesions. Utilizing the\ndatasets PAD-UFES-20 for skin cancer and P-NDB-UFES for oral cancer, the\ndiffusion model demonstrated competitive performance compared to\nstate-of-the-art deep learning models like Convolutional Neural Networks (CNNs)\nand Transformers. Specifically, for the PAD-UFES-20 dataset, the model achieved\na balanced accuracy of 0.6457 for six-class classification and 0.8357 for\nbinary classification (cancer vs. non-cancer). For the P-NDB-UFES dataset, it\nattained a balanced accuracy of 0.9050. These results suggest that diffusion\nmodels are viable models for classifying medical images of skin and oral\nlesions. In addition, we investigate the robustness of the model trained on\nPAD-UFES-20 for skin cancer but tested on the clinical images of the HIBA\ndataset.\n","date":"2025-03-28"}
{"id":"2504.00027","title":"Opioid Named Entity Recognition (ONER-2025) from Reddit","abstract":"  The opioid overdose epidemic remains a critical public health crisis,\nparticularly in the United States, leading to significant mortality and\nsocietal costs. Social media platforms like Reddit provide vast amounts of\nunstructured data that offer insights into public perceptions, discussions, and\nexperiences related to opioid use. This study leverages Natural Language\nProcessing (NLP), specifically Opioid Named Entity Recognition (ONER-2025), to\nextract actionable information from these platforms. Our research makes four\nkey contributions. First, we created a unique, manually annotated dataset\nsourced from Reddit, where users share self-reported experiences of opioid use\nvia different administration routes. This dataset contains 331,285 tokens and\nincludes eight major opioid entity categories. Second, we detail our annotation\nprocess and guidelines while discussing the challenges of labeling the\nONER-2025 dataset. Third, we analyze key linguistic challenges, including\nslang, ambiguity, fragmented sentences, and emotionally charged language, in\nopioid discussions. Fourth, we propose a real-time monitoring system to process\nstreaming data from social media, healthcare records, and emergency services to\nidentify overdose events. Using 5-fold cross-validation in 11 experiments, our\nsystem integrates machine learning, deep learning, and transformer-based\nlanguage models with advanced contextual embeddings to enhance understanding.\nOur transformer-based models (bert-base-NER and roberta-base) achieved 97%\naccuracy and F1-score, outperforming baselines by 10.23% (RF=0.88).\n","date":"2025-03-28"}
{"id":"2504.00029","title":"Generating Structured Plan Representation of Procedures with LLMs","abstract":"  In this paper, we address the challenges of managing Standard Operating\nProcedures (SOPs), which often suffer from inconsistencies in language, format,\nand execution, leading to operational inefficiencies. Traditional process\nmodeling demands significant manual effort, domain expertise, and familiarity\nwith complex languages like Business Process Modeling Notation (BPMN), creating\nbarriers for non-techincal users. We introduce SOP Structuring (SOPStruct), a\nnovel approach that leverages Large Language Models (LLMs) to transform SOPs\ninto decision-tree-based structured representations. SOPStruct produces a\nstandardized representation of SOPs across different domains, reduces cognitive\nload, and improves user comprehension by effectively capturing task\ndependencies and ensuring sequential integrity. Our approach enables leveraging\nthe structured information to automate workflows as well as empower the human\nusers. By organizing procedures into logical graphs, SOPStruct facilitates\nbacktracking and error correction, offering a scalable solution for process\noptimization. We employ a novel evaluation framework, combining deterministic\nmethods with the Planning Domain Definition Language (PDDL) to verify graph\nsoundness, and non-deterministic assessment by an LLM to ensure completeness.\nWe empirically validate the robustness of our LLM-based structured SOP\nrepresentation methodology across SOPs from different domains and varying\nlevels of complexity. Despite the current lack of automation readiness in many\norganizations, our research highlights the transformative potential of LLMs to\nstreamline process modeling, paving the way for future advancements in\nautomated procedure optimization.\n","date":"2025-03-28"}
{"id":"2504.00030","title":"Token-Driven GammaTune: Adaptive Calibration for Enhanced Speculative\n  Decoding","abstract":"  Speculative decoding accelerates large language model (LLM) inference by\nusing a smaller draft model to propose tokens, which are then verified by a\nlarger target model. However, selecting an optimal speculation length is\ncritical for maximizing speedup while minimizing wasted computation. We\nintroduce \\textit{GammaTune} and \\textit{GammaTune+}, training-free adaptive\nalgorithms that dynamically adjust speculation length based on token acceptance\nrates using a heuristic-based switching mechanism. Evaluated on SpecBench\nacross multiple tasks and model pairs, our method outperforms other\nheuristic-based approaches and fixed-length speculative decoding, achieving an\naverage speedup of 15\\% ($\\pm$5\\%) with \\textit{GammaTune} and 16\\% ($\\pm$3\\%)\nwith \\textit{GammaTune+}, while reducing performance variance. This makes\n\\textit{GammaTune} a robust and efficient solution for real-world deployment.\n","date":"2025-03-28"}
{"id":"2504.00031","title":"Leaking LoRa: An Evaluation of Password Leaks and Knowledge Storage in\n  Large Language Models","abstract":"  To effectively deploy Large Language Models (LLMs) in application-specific\nsettings, fine-tuning techniques are applied to enhance performance on\nspecialized tasks. This process often involves fine-tuning on user data data,\nwhich may contain sensitive information. Although not recommended, it is not\nuncommon for users to send passwords in messages, and fine-tuning models on\nthis could result in passwords being leaked. In this study, a Large Language\nModel is fine-tuned with customer support data and passwords from the RockYou\npassword wordlist using Low-Rank Adaptation (LoRA). Out of the first 200\npasswords from the list, 37 were successfully recovered. Further, causal\ntracing is used to identify that password information is largely located in a\nfew layers. Lastly, Rank One Model Editing (ROME) is used to remove the\npassword information from the model, resulting in the number of passwords\nrecovered going from 37 to 0.\n","date":"2025-03-29"}
{"id":"2504.00032","title":"Skeletonization Quality Evaluation: Geometric Metrics for Point Cloud\n  Analysis in Robotics","abstract":"  Skeletonization is a powerful tool for shape analysis, rooted in the inherent\ninstinct to understand an object's morphology. It has found applications across\nvarious domains, including robotics. Although skeletonization algorithms have\nbeen studied in recent years, their performance is rarely quantified with\ndetailed numerical evaluations. This work focuses on defining and quantifying\ngeometric properties to systematically score the skeletonization results of\npoint cloud shapes across multiple aspects, including topological similarity,\nboundedness, centeredness, and smoothness. We introduce these representative\nmetric definitions along with a numerical scoring framework to analyze\nskeletonization outcomes concerning point cloud data for different scenarios,\nfrom object manipulation to mobile robot navigation. Additionally, we provide\nan open-source tool to enable the research community to evaluate and refine\ntheir skeleton models. Finally, we assess the performance and sensitivity of\nthe proposed geometric evaluation methods from various robotic applications.\n","date":"2025-03-29"}
{"id":"2504.01023","title":"Omnidirectional Depth-Aided Occupancy Prediction based on Cylindrical\n  Voxel for Autonomous Driving","abstract":"  Accurate 3D perception is essential for autonomous driving. Traditional\nmethods often struggle with geometric ambiguity due to a lack of geometric\nprior. To address these challenges, we use omnidirectional depth estimation to\nintroduce geometric prior. Based on the depth information, we propose a\nSketch-Coloring framework OmniDepth-Occ. Additionally, our approach introduces\na cylindrical voxel representation based on polar coordinate to better align\nwith the radial nature of panoramic camera views. To address the lack of\nfisheye camera dataset in autonomous driving tasks, we also build a virtual\nscene dataset with six fisheye cameras, and the data volume has reached twice\nthat of SemanticKITTI. Experimental results demonstrate that our\nSketch-Coloring network significantly enhances 3D perception performance.\n","date":"2025-03-26"}
{"id":"2504.01024","title":"Gaze-Guided 3D Hand Motion Prediction for Detecting Intent in Egocentric\n  Grasping Tasks","abstract":"  Human intention detection with hand motion prediction is critical to drive\nthe upper-extremity assistive robots in neurorehabilitation applications.\nHowever, the traditional methods relying on physiological signal measurement\nare restrictive and often lack environmental context. We propose a novel\napproach that predicts future sequences of both hand poses and joint positions.\nThis method integrates gaze information, historical hand motion sequences, and\nenvironmental object data, adapting dynamically to the assistive needs of the\npatient without prior knowledge of the intended object for grasping.\nSpecifically, we use a vector-quantized variational autoencoder for robust hand\npose encoding with an autoregressive generative transformer for effective hand\nmotion sequence prediction. We demonstrate the usability of these novel\ntechniques in a pilot study with healthy subjects. To train and evaluate the\nproposed method, we collect a dataset consisting of various types of grasp\nactions on different objects from multiple subjects. Through extensive\nexperiments, we demonstrate that the proposed method can successfully predict\nsequential hand movement. Especially, the gaze information shows significant\nenhancements in prediction capabilities, particularly with fewer input frames,\nhighlighting the potential of the proposed method for real-world applications.\n","date":"2025-03-27"}
{"id":"2504.01025","title":"Diagnosis of Pulmonary Hypertension by Integrating Multimodal Data with\n  a Hybrid Graph Convolutional and Transformer Network","abstract":"  Early and accurate diagnosis of pulmonary hypertension (PH) is essential for\noptimal patient management. Differentiating between pre-capillary and\npost-capillary PH is critical for guiding treatment decisions. This study\ndevelops and validates a deep learning-based diagnostic model for PH, designed\nto classify patients as non-PH, pre-capillary PH, or post-capillary PH. This\nretrospective study analyzed data from 204 patients (112 with pre-capillary PH,\n32 with post-capillary PH, and 60 non-PH controls) at the First Affiliated\nHospital of Nanjing Medical University. Diagnoses were confirmed through right\nheart catheterization. We selected 6 samples from each category for the test\nset (18 samples, 10%), with the remaining 186 samples used for the training\nset. This process was repeated 35 times for testing. This paper proposes a deep\nlearning model that combines Graph convolutional networks (GCN), Convolutional\nneural networks (CNN), and Transformers. The model was developed to process\nmultimodal data, including short-axis (SAX) sequences, four-chamber (4CH)\nsequences, and clinical parameters. Our model achieved a performance of Area\nunder the receiver operating characteristic curve (AUC) = 0.81 +- 0.06(standard\ndeviation) and Accuracy (ACC) = 0.73 +- 0.06 on the test set. The\ndiscriminative abilities were as follows: non-PH subjects (AUC = 0.74 +- 0.11),\npre-capillary PH (AUC = 0.86 +- 0.06), and post-capillary PH (AUC = 0.83 +-\n0.10). It has the potential to support clinical decision-making by effectively\nintegrating multimodal data to assist physicians in making accurate and timely\ndiagnoses.\n","date":"2025-03-28"}
{"id":"2504.01027","title":"Mesh Compression with Quantized Neural Displacement Fields","abstract":"  Implicit neural representations (INRs) have been successfully used to\ncompress a variety of 3D surface representations such as Signed Distance\nFunctions (SDFs), voxel grids, and also other forms of structured data such as\nimages, videos, and audio. However, these methods have been limited in their\napplication to unstructured data such as 3D meshes and point clouds. This work\npresents a simple yet effective method that extends the usage of INRs to\ncompress 3D triangle meshes. Our method encodes a displacement field that\nrefines the coarse version of the 3D mesh surface to be compressed using a\nsmall neural network. Once trained, the neural network weights occupy much\nlower memory than the displacement field or the original surface. We show that\nour method is capable of preserving intricate geometric textures and\ndemonstrates state-of-the-art performance for compression ratios ranging from\n4x to 380x.\n","date":"2025-03-28"}
{"id":"2504.01028","title":"Improving Applicability of Deep Learning based Token Classification\n  models during Training","abstract":"  This paper shows that further evaluation metrics during model training are\nneeded to decide about its applicability in inference. As an example, a\nLayoutLM-based model is trained for token classification in documents. The\ndocuments are German receipts. We show that conventional classification\nmetrics, represented by the F1-Score in our experiments, are insufficient for\nevaluating the applicability of machine learning models in practice. To address\nthis problem, we introduce a novel metric, Document Integrity Precision (DIP),\nas a solution for visual document understanding and the token classification\ntask. To the best of our knowledge, nothing comparable has been introduced in\nthis context. DIP is a rigorous metric, describing how many documents of the\ntest dataset require manual interventions. It enables AI researchers and\nsoftware developers to conduct an in-depth investigation of the level of\nprocess automation in business software. In order to validate DIP, we conduct\nexperiments with our created models to highlight and analyze the impact and\nrelevance of DIP to evaluate if the model should be deployed or not in\ndifferent training settings. Our results demonstrate that existing metrics\nbarely change for isolated model impairments, whereas DIP indicates that the\nmodel requires substantial human interventions in deployment. The larger the\nset of entities being predicted, the less sensitive conventional metrics are,\nentailing poor automation quality. DIP, in contrast, remains a single value to\nbe interpreted for entire entity sets. This highlights the importance of having\nmetrics that focus on the business task for model training in production. Since\nDIP is created for the token classification task, more research is needed to\nfind suitable metrics for other training tasks.\n","date":"2025-03-28"}
{"id":"2504.01029","title":"Who is Responsible When AI Fails? Mapping Causes, Entities, and\n  Consequences of AI Privacy and Ethical Incidents","abstract":"  The rapid growth of artificial intelligence (AI) technologies has changed\ndecision-making in many fields. But, it has also raised major privacy and\nethical concerns. However, many AI incidents taxonomies and guidelines for\nacademia, industry, and government lack grounding in real-world incidents. We\nanalyzed 202 real-world AI privacy and ethical incidents. This produced a\ntaxonomy that classifies incident types across AI lifecycle stages. It accounts\nfor contextual factors such as causes, responsible entities, disclosure\nsources, and impacts. Our findings show insufficient incident reporting from AI\ndevelopers and users. Many incidents are caused by poor organizational\ndecisions and legal non-compliance. Only a few legal actions and corrective\nmeasures exist, while risk-mitigation efforts are limited. Our taxonomy\ncontributes a structured approach in reporting of future AI incidents. Our\nfindings demonstrate that current AI governance frameworks are inadequate. We\nurgently need child-specific protections and AI policies on social media. They\nmust moderate and reduce the spread of harmful AI-generated content. Our\nresearch provides insights for policymakers and practitioners, which lets them\ndesign ethical AI. It also support AI incident detection and risk management.\nFinally, it guides AI policy development. Improved policies will protect people\nfrom harmful AI applications and support innovation in AI systems.\n","date":"2025-03-28"}
{"id":"2504.01030","title":"Fair Sufficient Representation Learning","abstract":"  The main objective of fair statistical modeling and machine learning is to\nminimize or eliminate biases that may arise from the data or the model itself,\nensuring that predictions and decisions are not unjustly influenced by\nsensitive attributes such as race, gender, age, or other protected\ncharacteristics. In this paper, we introduce a Fair Sufficient Representation\nLearning (FSRL) method that balances sufficiency and fairness. Sufficiency\nensures that the representation should capture all necessary information about\nthe target variables, while fairness requires that the learned representation\nremains independent of sensitive attributes. FSRL is based on a convex\ncombination of an objective function for learning a sufficient representation\nand an objective function that ensures fairness. Our approach manages fairness\nand sufficiency at the representation level, offering a novel perspective on\nfair representation learning. We implement this method using distance\ncovariance, which is effective for characterizing independence between random\nvariables. We establish the convergence properties of the learned\nrepresentations. Experiments conducted on healthcase and text datasets with\ndiverse structures demonstrate that FSRL achieves a superior trade-off between\nfairness and accuracy compared to existing approaches.\n","date":"2025-03-29"}
{"id":"2504.01031","title":"Estimating Unbounded Density Ratios: Applications in Error Control under\n  Covariate Shift","abstract":"  The density ratio is an important metric for evaluating the relative\nlikelihood of two probability distributions, with extensive applications in\nstatistics and machine learning. However, existing estimation theories for\ndensity ratios often depend on stringent regularity conditions, mainly focusing\non density ratio functions with bounded domains and ranges. In this paper, we\nstudy density ratio estimators using loss functions based on least squares and\nlogistic regression. We establish upper bounds on estimation errors with\nstandard minimax optimal rates, up to logarithmic factors. Our results\naccommodate density ratio functions with unbounded domains and ranges. We apply\nour results to nonparametric regression and conditional flow models under\ncovariate shift and identify the tail properties of the density ratio as\ncrucial for error control across domains affected by covariate shift. We\nprovide sufficient conditions under which loss correction is unnecessary and\ndemonstrate effective generalization capabilities of a source estimator to any\nsuitable target domain. Our simulation experiments support these theoretical\nfindings, indicating that the source estimator can outperform those derived\nfrom loss correction methods, even when the true density ratio is known.\n","date":"2025-03-29"}
{"id":"2504.01032","title":"Who Owns the Output? Bridging Law and Technology in LLMs Attribution","abstract":"  Since the introduction of ChatGPT in 2022, Large language models (LLMs) and\nLarge Multimodal Models (LMM) have transformed content creation, enabling the\ngeneration of human-quality content, spanning every medium, text, images,\nvideos, and audio. The chances offered by generative AI models are endless and\nare drastically reducing the time required to generate content and usually\nraising the quality of the generation. However, considering the complexity and\nthe difficult traceability of the generated content, the use of these tools\nprovides challenges in attributing AI-generated content. The difficult\nattribution resides for a variety of reasons, starting from the lack of a\nsystematic fingerprinting of the generated content and ending with the enormous\namount of data on which LLMs and LMM are trained, which makes it difficult to\nconnect generated content to the training data. This scenario is raising\nconcerns about intellectual property and ethical responsibilities. To address\nthese concerns, in this paper, we bridge the technological, ethical, and\nlegislative aspects, by proposing a review of the legislative and technological\ninstruments today available and proposing a legal framework to ensure\naccountability. In the end, we propose three use cases of how these can be\ncombined to guarantee that attribution is respected. However, even though the\ntechniques available today can guarantee a greater attribution to a greater\nextent, strong limitations still apply, that can be solved uniquely by the\ndevelopment of new attribution techniques, to be applied to LLMs and LMMs.\n","date":"2025-03-29"}
{"id":"2504.01973","title":"Universally applicable and tunable graph-based coarse-graining for\n  Machine learning force fields","abstract":"  Coarse-grained (CG) force field methods for molecular systems are a crucial\ntool to simulate large biological macromolecules and are therefore essential\nfor characterisations of biomolecular systems. While state-of-the-art deep\nlearning (DL)-based models for all-atom force fields have improved immensely\nover recent years, we observe and analyse significant limitations of the\ncurrently available approaches for DL-based CG simulations. In this work, we\npresent the first transferable DL-based CG force field approach (i.e., not\nspecific to only one narrowly defined system type) applicable to a wide range\nof biosystems. To achieve this, our CG algorithm does not rely on hard-coded\nrules and is tuned to output coarse-grained systems optimised for minimal\nstatistical noise in the ground truth CG forces, which results in significant\nimprovement of model training. Our force field model is also the first CG\nvariant that is based on the MACE architecture and is trained on a custom\ndataset created by a new approach based on the fragmentation of large\nbiosystems covering protein, RNA and lipid chemistry. We demonstrate that our\nmodel can be applied in molecular dynamics simulations to obtain stable and\nqualitatively accurate trajectories for a variety of systems, while also\ndiscussing cases for which we observe limited reliability.\n","date":"2025-03-24"}
{"id":"2504.01979","title":"Correlation-Attention Masked Temporal Transformer for User Identity\n  Linkage Using Heterogeneous Mobility Data","abstract":"  With the rise of social media and Location-Based Social Networks (LBSN),\ncheck-in data across platforms has become crucial for User Identity Linkage\n(UIL). These data not only reveal users' spatio-temporal information but also\nprovide insights into their behavior patterns and interests. However,\ncross-platform identity linkage faces challenges like poor data quality, high\nsparsity, and noise interference, which hinder existing methods from extracting\ncross-platform user information. To address these issues, we propose a\nCorrelation-Attention Masked Transformer for User Identity Linkage Network\n(MT-Link), a transformer-based framework to enhance model performance by\nlearning spatio-temporal co-occurrence patterns of cross-platform users. Our\nmodel effectively captures spatio-temporal co-occurrence in cross-platform user\ncheck-in sequences. It employs a correlation attention mechanism to detect the\nspatio-temporal co-occurrence between user check-in sequences. Guided by\nattention weight maps, the model focuses on co-occurrence points while\nfiltering out noise, ultimately improving classification performance.\nExperimental results show that our model significantly outperforms\nstate-of-the-art baselines by 12.92%~17.76% and 5.80%~8.38% improvements in\nterms of Macro-F1 and Area Under Curve (AUC).\n","date":"2025-03-28"}
{"id":"2504.01980","title":"Information Gain Is Not All You Need","abstract":"  Autonomous exploration in mobile robotics is driven by two competing\nobjectives: coverage, to exhaustively observe the environment; and path length,\nto do so with the shortest path possible. Though it is difficult to evaluate\nthe best course of action without knowing the unknown, the unknown can often be\nunderstood through models, maps, or common sense. However, previous work has\nshown that improving estimates of information gain through such prior knowledge\nleads to greedy behavior and ultimately causes backtracking, which degrades\ncoverage performance. In fact, any information gain maximization will exhibit\nthis behavior, even without prior knowledge. Information gained at task\ncompletion is constant, and cannot be maximized for. It is therefore an\nunsuitable choice as an optimization objective. Instead, information gain is a\ndecision criterion for determining which candidate states should still be\nconsidered for exploration. The task therefore becomes to reach completion with\nthe shortest total path. Since determining the shortest path is typically\nintractable, it is necessary to rely on a heuristic or estimate to identify\ncandidate states that minimize the total path length. To address this, we\npropose a heuristic that reduces backtracking by preferring candidate states\nthat are close to the robot, but far away from other candidate states. We\nevaluate the performance of the proposed heuristic in simulation against an\ninformation gain-based approach and frontier exploration, and show that our\nmethod significantly decreases total path length, both with and without prior\nknowledge of the environment.\n","date":"2025-03-28"}
{"id":"2504.01981","title":"NLS: Natural-Level Synthesis for Hardware Implementation Through GenAI","abstract":"  This paper introduces Natural-Level Synthesis, an innovative approach for\ngenerating hardware using generative artificial intelligence on both the system\nlevel and component-level. NLS bridges a gap in current hardware development\nprocesses, where algorithm and application engineers' involvement typically\nends at the requirements stage. With NLS, engineers can participate more deeply\nin the development, synthesis, and test stages by using Gen-AI models to\nconvert natural language descriptions directly into Hardware Description\nLanguage code. This approach not only streamlines hardware development but also\nimproves accessibility, fostering a collaborative workflow between hardware and\nalgorithm engineers. We developed the NLS tool to facilitate natural\nlanguage-driven HDL synthesis, enabling rapid generation of system-level HDL\ndesigns while significantly reducing development complexity. Evaluated through\ncase studies and benchmarks using Performance, Power, and Area metrics, NLS\nshows its potential to enhance resource efficiency in hardware development.\nThis work provides a extensible, efficient solution for hardware synthesis and\nestablishes a Visual Studio Code Extension to assess Gen-AI-driven HDL\ngeneration and system integration, laying a foundation for future AI-enhanced\nand AI-in-the-loop Electronic Design Automation tools.\n","date":"2025-03-28"}